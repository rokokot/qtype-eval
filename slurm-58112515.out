SLURM_JOB_ID: 58112515
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: layerwise_probing
SLURM_CLUSTER_NAME: genius
SLURM_JOB_PARTITION: gpu_p100
SLURM_NNODES: 1
SLURM_NODELIST: r24g37
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Tue Apr 29 14:12:35 CEST 2025
Walltime: 01-12:00:00
========================================================================
Environment variables:
PYTHONPATH=:/data/leuven/371/vsc37132/qtype-eval:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval
HF_HOME=/data/leuven/371/vsc37132/qtype-eval/data/cache
GPU information:
Tue Apr 29 14:12:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla P100-SXM2-16GB           Off |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0             32W /  300W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Python executable: /data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/bin/python
PyTorch CUDA available: True
Starting standard experiments...
Running question_type experiment for language ar, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:13:22,615][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/question_type
experiment_name: layer_1_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:13:22,615][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:13:22,616][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:13:22,616][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:13:22,620][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:13:22,620][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:13:26,490][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:13:29,618][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:13:29,619][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:13:29,737][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:13:29,828][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:13:30,247][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:13:30,261][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:13:30,261][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:13:30,262][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:13:30,276][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:13:30,298][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:13:30,340][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:13:30,342][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:13:30,342][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:13:30,343][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:13:30,357][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:13:30,380][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:13:30,415][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:13:30,417][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:13:30,417][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:13:30,418][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:13:30,433][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:13:30,434][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:13:30,434][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:13:30,434][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:13:30,450][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:13:30,450][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:13:30,450][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:13:30,450][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:13:30,450][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:13:30,450][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:13:30,450][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:13:30,451][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:13:30,451][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:13:30,451][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:13:30,451][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:13:30,451][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:13:30,451][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:13:30,451][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:13:30,451][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:13:30,451][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:13:30,452][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:13:30,452][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:13:30,452][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:13:30,452][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:13:30,452][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:13:30,452][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:13:30,453][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:13:30,453][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:13:41,631][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:13:41,632][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:13:41,634][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:13:41,635][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 1
[2025-04-29 14:13:41,635][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:03<03:24,  3.30s/it]Epoch 1/10:   5%|▍         | 3/63 [00:03<00:53,  1.11it/s]Epoch 1/10:   8%|▊         | 5/63 [00:03<00:27,  2.15it/s]Epoch 1/10:  11%|█         | 7/63 [00:03<00:16,  3.42it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:03<00:10,  4.93it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:03<00:07,  6.61it/s]Epoch 1/10:  21%|██        | 13/63 [00:03<00:05,  8.37it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:04<00:04, 10.12it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:04<00:03, 11.75it/s]Epoch 1/10:  30%|███       | 19/63 [00:04<00:03, 13.14it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:04<00:02, 14.32it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:04<00:02, 15.23it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:04<00:02, 15.99it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:04<00:02, 16.55it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:04<00:02, 16.97it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:04<00:01, 17.28it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:05<00:01, 17.50it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:05<00:01, 17.65it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:05<00:01, 17.73it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:05<00:01, 17.77it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:05<00:01, 17.85it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:05<00:01, 17.90it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:05<00:01, 17.94it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:05<00:00, 17.89it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:05<00:00, 17.88it/s]Epoch 1/10:  81%|████████  | 51/63 [00:06<00:00, 17.88it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:06<00:00, 17.93it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:06<00:00, 17.91it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:06<00:00, 17.91it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:06<00:00, 17.88it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:06<00:00, 17.92it/s]Epoch 1/10: 100%|██████████| 63/63 [00:06<00:00,  9.31it/s]
[2025-04-29 14:13:53,633][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6952
[2025-04-29 14:13:53,926][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6926, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.88it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.69it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.27it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.64it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.46it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.98it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.31it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.46it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.59it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.79it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.87it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.92it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.95it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.92it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.88it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.86it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.87it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.87it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.85it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.87it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.87it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.85it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.90it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.87it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.86it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.89it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.87it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.90it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.87it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.88it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.30it/s]
[2025-04-29 14:13:58,134][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6937
[2025-04-29 14:13:58,427][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6931, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:09,  6.24it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:04, 12.06it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.47it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.71it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.45it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.91it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.20it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.42it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.56it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.66it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.72it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.77it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.77it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.80it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.82it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.82it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.86it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.87it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.84it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.86it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.88it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.89it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.88it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.88it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.87it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.88it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.87it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.83it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.84it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.84it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.87it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.33it/s]
[2025-04-29 14:14:02,065][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6940
[2025-04-29 14:14:02,354][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6932, Metrics: {'accuracy': 0.36363636363636365, 'f1': 0.36363636363636365}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  6.17it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.96it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.39it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.69it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.43it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.93it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.24it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.40it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.53it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.61it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.70it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.75it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.75it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.76it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.79it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.79it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.83it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.86it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.82it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.81it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.83it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.80it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.80it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.82it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.81it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.82it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.82it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.81it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.83it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:14:06,013][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6954
[2025-04-29 14:14:06,315][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6934, Metrics: {'accuracy': 0.45454545454545453, 'f1': 0.625}
[2025-04-29 14:14:06,316][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▇▁▂█
wandb:           train_time ▁
wandb:         val_accuracy ██▁▅
wandb:               val_f1 ▁▁▅█
wandb:             val_loss ▁▅▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69261
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69541
wandb:           train_time 19.45182
wandb:         val_accuracy 0.45455
wandb:               val_f1 0.625
wandb:             val_loss 0.69341
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141322-wjk0k0zg
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141322-wjk0k0zg/logs
Standard experiment completed successfully: layer_1_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/question_type/results.json
Running complexity experiment for language ar, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:14:23,379][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/complexity
experiment_name: layer_1_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:14:23,379][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:14:23,379][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:14:23,379][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:14:23,384][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:14:23,384][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:14:24,498][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:14:27,179][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:14:27,179][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:14:27,215][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:14:27,233][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:14:27,548][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:14:27,558][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:14:27,559][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:14:27,569][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:14:27,633][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:14:27,654][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:14:27,664][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:14:27,665][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:14:27,665][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:14:27,666][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:14:27,682][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:14:27,701][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:14:27,710][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:14:27,711][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:14:27,711][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:14:27,712][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:14:27,713][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:14:27,713][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:14:27,713][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:14:27,713][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:14:27,713][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:14:27,713][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:14:27,714][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:14:27,714][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:14:27,714][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:14:27,714][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:14:27,714][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:14:27,714][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:14:27,714][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:14:27,715][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:14:27,715][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:14:27,715][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:14:27,715][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:14:27,715][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:14:27,715][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:14:27,715][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:14:27,716][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:14:27,716][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:14:27,716][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:14:27,716][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:14:27,716][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:14:27,716][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:14:27,716][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:14:27,717][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:14:31,675][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:14:31,676][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:14:31,679][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:14:31,679][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 1
[2025-04-29 14:14:31,679][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:13,  1.18s/it]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:20,  2.87it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:11,  5.04it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:07,  7.25it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:05,  9.34it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 11.23it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 12.82it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.08it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:02<00:03, 15.09it/s]Epoch 1/10:  30%|███       | 19/63 [00:02<00:02, 15.83it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:02<00:02, 16.39it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 16.82it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.13it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.33it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.48it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.54it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.61it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:03<00:01, 17.71it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:03<00:01, 17.71it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.83it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.83it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.85it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:04<00:00, 17.82it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:04<00:00, 17.82it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.84it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.85it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.88it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 13.51it/s]
[2025-04-29 14:14:37,873][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1640
[2025-04-29 14:14:38,162][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1002, Metrics: {'mse': 0.1011941209435463, 'rmse': 0.3181102339497211, 'r2': -0.5597379207611084}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.05it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.82it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.31it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.64it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.43it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.87it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.20it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.39it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.51it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.61it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.70it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.78it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.78it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.78it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.85it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.83it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.82it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.82it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.82it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.82it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.80it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.28it/s]
[2025-04-29 14:14:42,374][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0670
[2025-04-29 14:14:42,665][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0667, Metrics: {'mse': 0.06676701456308365, 'rmse': 0.25839313954337806, 'r2': -0.02910172939300537}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:11,  5.63it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.38it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.98it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.36it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.74it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.06it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.30it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.46it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.57it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.69it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.73it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.75it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.78it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.79it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.79it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.79it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.80it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.81it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.81it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.81it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.82it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.83it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.84it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.15it/s]
[2025-04-29 14:14:46,941][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0438
[2025-04-29 14:14:47,242][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0658, Metrics: {'mse': 0.06546419858932495, 'rmse': 0.25585972443767885, 'r2': -0.00902104377746582}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.64it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.41it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.99it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.39it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.23it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.75it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.46it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.63it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.68it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.78it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.80it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.81it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.82it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.82it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.82it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.11it/s]
[2025-04-29 14:14:51,474][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0416
[2025-04-29 14:14:51,782][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0667, Metrics: {'mse': 0.06633471697568893, 'rmse': 0.2575552697494053, 'r2': -0.022438645362854004}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:09,  6.28it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:04, 12.06it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 14.42it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.69it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.43it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.90it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.20it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.40it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.57it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.63it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.73it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.76it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.75it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.77it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.75it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.79it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 5/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:14:55,440][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0434
[2025-04-29 14:14:55,746][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0672, Metrics: {'mse': 0.06681045144796371, 'rmse': 0.2584771778087259, 'r2': -0.02977120876312256}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:09,  6.45it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:04, 12.18it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:03, 14.53it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.74it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.46it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.89it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.19it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.36it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.58it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.60it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.76it/s]Epoch 6/10:  81%|████████  | 51/63 [00:02<00:00, 17.78it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:14:59,406][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0407
[2025-04-29 14:14:59,710][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0675, Metrics: {'mse': 0.0670170858502388, 'rmse': 0.2588765842061402, 'r2': -0.03295612335205078}
[2025-04-29 14:14:59,711][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁
wandb:     best_val_mse █▁▁
wandb:      best_val_r2 ▁██
wandb:    best_val_rmse █▁▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▁▁▁▁
wandb:          val_mse █▁▁▁▁▁
wandb:           val_r2 ▁█████
wandb:         val_rmse █▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06577
wandb:     best_val_mse 0.06546
wandb:      best_val_r2 -0.00902
wandb:    best_val_rmse 0.25586
wandb:            epoch 6
wandb:   final_test_mse 0.05811
wandb:    final_test_r2 -0.00174
wandb:  final_test_rmse 0.24105
wandb:  final_train_mse 0.03091
wandb:   final_train_r2 -0.00695
wandb: final_train_rmse 0.17581
wandb:    final_val_mse 0.06546
wandb:     final_val_r2 -0.00902
wandb:   final_val_rmse 0.25586
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04069
wandb:       train_time 26.50273
wandb:         val_loss 0.06747
wandb:          val_mse 0.06702
wandb:           val_r2 -0.03296
wandb:         val_rmse 0.25888
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141423-mwyrufun
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141423-mwyrufun/logs
Standard experiment completed successfully: layer_1_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/complexity/results.json
Running question_type experiment for language ar, layer 2
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:15:16,018][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_2/question_type
experiment_name: layer_2_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:15:16,018][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:15:16,018][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:15:16,019][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:15:16,023][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:15:16,023][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:15:17,173][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:15:19,832][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:15:19,833][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:15:19,878][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:15:19,897][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:15:19,992][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:15:20,001][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:15:20,002][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:15:20,003][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:15:20,019][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:15:20,038][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:15:20,048][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:15:20,050][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:15:20,050][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:15:20,051][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:15:20,066][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:15:20,087][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:15:20,096][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:15:20,097][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:15:20,098][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:15:20,098][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:15:20,099][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:15:20,099][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:15:20,099][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:15:20,099][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:15:20,100][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:15:20,100][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:15:20,100][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:15:20,100][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:15:20,100][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:15:20,100][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:15:20,100][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:15:20,100][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:15:20,101][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:15:20,101][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:15:20,101][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:15:20,101][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:15:20,101][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:15:20,101][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:15:20,101][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:15:20,101][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:15:20,101][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:15:20,102][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:15:20,102][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:15:20,102][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:15:20,102][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:15:20,102][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:15:20,102][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:15:20,103][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:15:23,919][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:15:23,920][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:15:23,922][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:15:23,922][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 2
[2025-04-29 14:15:23,922][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:53,  1.15it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:16,  3.73it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:09,  6.28it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.66it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:05, 10.74it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.47it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.85it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.91it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.71it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.31it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.75it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.06it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.28it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.44it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.56it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.79it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.80it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.80it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.82it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.81it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.82it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.45it/s]
[2025-04-29 14:15:29,975][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6953
[2025-04-29 14:15:30,244][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6922, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.02it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.77it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.28it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.59it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.38it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.86it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.17it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.35it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.49it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.59it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.66it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.70it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.74it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.78it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.79it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.80it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.32it/s]
[2025-04-29 14:15:34,439][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6934
[2025-04-29 14:15:34,717][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:09,  6.22it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.97it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.39it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.65it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.40it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.84it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.15it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.35it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.49it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.57it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.79it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.14it/s]
[2025-04-29 14:15:38,396][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6942
[2025-04-29 14:15:38,687][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  6.13it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.88it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.33it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.62it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.36it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.84it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.13it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.34it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.50it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.57it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.64it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.71it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.77it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.29it/s]
[2025-04-29 14:15:42,333][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6944
[2025-04-29 14:15:42,631][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 14:15:42,632][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▁▄▅
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▄▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69216
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69442
wandb:           train_time 17.01866
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.69292
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141516-33zrddzz
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141516-33zrddzz/logs
Standard experiment completed successfully: layer_2_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_2/question_type/results.json
Running complexity experiment for language ar, layer 2
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:15:58,484][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_2/complexity
experiment_name: layer_2_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:15:58,484][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:15:58,485][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:15:58,485][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:15:58,489][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:15:58,490][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:15:59,571][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:16:02,379][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:16:02,380][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:16:02,404][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:16:02,420][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:16:02,476][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:16:02,487][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:16:02,488][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:16:02,489][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:16:02,499][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:16:02,517][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:16:02,525][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:16:02,527][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:16:02,527][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:16:02,528][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:16:02,539][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:16:02,556][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:16:02,567][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:16:02,569][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:16:02,569][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:16:02,570][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:16:02,570][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:16:02,570][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:16:02,570][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:16:02,570][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:16:02,571][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:16:02,571][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:16:02,571][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:16:02,571][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:16:02,571][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:16:02,571][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:16:02,572][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:16:02,572][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:16:02,572][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:16:02,572][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:16:02,572][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:16:02,572][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:16:02,572][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:16:02,573][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:16:02,573][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:16:02,573][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:16:02,573][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:16:02,573][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:16:02,573][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:16:02,573][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:16:02,573][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:16:02,574][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:16:02,574][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:16:02,574][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:16:06,168][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:16:06,169][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:16:06,171][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:16:06,171][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 2
[2025-04-29 14:16:06,171][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:45,  1.37it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:13,  4.32it/s]Epoch 1/10:   8%|▊         | 5/63 [00:00<00:08,  7.07it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:05,  9.50it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.54it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:03, 13.17it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.41it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.37it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 16.05it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.56it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.92it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:01<00:02, 17.16it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.36it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.50it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.58it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.68it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.91it/s]
[2025-04-29 14:16:11,928][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1861
[2025-04-29 14:16:12,223][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1362, Metrics: {'mse': 0.13763229548931122, 'rmse': 0.37098826866804185, 'r2': -1.1213715076446533}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.78it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.53it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.10it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.47it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.27it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.76it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.60it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.74it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.77it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.81it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.82it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.82it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.82it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.82it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.83it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.20it/s]
[2025-04-29 14:16:16,456][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1043
[2025-04-29 14:16:16,747][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0880, Metrics: {'mse': 0.08877066522836685, 'rmse': 0.29794406392537315, 'r2': -0.3682512044906616}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  5.69it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.46it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.04it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.43it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.24it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.75it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.32it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.73it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.76it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.78it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.77it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.76it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.79it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.78it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.80it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.10it/s]
[2025-04-29 14:16:21,027][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0632
[2025-04-29 14:16:21,328][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0699, Metrics: {'mse': 0.07019999623298645, 'rmse': 0.2649528188809971, 'r2': -0.08201539516448975}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.76it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.54it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.06it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.40it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.21it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.73it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.05it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.29it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.43it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.52it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.67it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.79it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.77it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.10it/s]
[2025-04-29 14:16:25,553][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0478
[2025-04-29 14:16:25,861][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0656, Metrics: {'mse': 0.06553687155246735, 'rmse': 0.2560017022452533, 'r2': -0.010141134262084961}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:10,  6.06it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.82it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 14.26it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.54it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.30it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.11it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.46it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.77it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.74it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 5/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.17it/s]
[2025-04-29 14:16:30,096][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0460
[2025-04-29 14:16:30,412][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0655, Metrics: {'mse': 0.06525885313749313, 'rmse': 0.2554581240389374, 'r2': -0.005856037139892578}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:11,  5.41it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.12it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.77it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.22it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.06it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.60it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.98it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.59it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.68it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.73it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.79it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.75it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.77it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.79it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:16:34,667][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0417
[2025-04-29 14:16:34,982][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0663, Metrics: {'mse': 0.06594730168581009, 'rmse': 0.25680206713694903, 'r2': -0.01646721363067627}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:10,  5.83it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.59it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 14.11it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.46it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.26it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.76it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.30it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.63it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.71it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.75it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.77it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.78it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.79it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.75it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.76it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.71it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.15it/s]
[2025-04-29 14:16:38,657][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0431
[2025-04-29 14:16:38,973][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0663, Metrics: {'mse': 0.06590130925178528, 'rmse': 0.25671250310763066, 'r2': -0.015758395195007324}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.59it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.33it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.92it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.33it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.13it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.65it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 17.00it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.20it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:16:42,668][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0412
[2025-04-29 14:16:42,977][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0667, Metrics: {'mse': 0.06632109731435776, 'rmse': 0.25752882812290695, 'r2': -0.022228717803955078}
[2025-04-29 14:16:42,978][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▁▁▁
wandb:     best_val_mse █▃▁▁▁
wandb:      best_val_r2 ▁▆███
wandb:    best_val_rmse █▄▂▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▁▁▁▁▁▁
wandb:          val_mse █▃▁▁▁▁▁▁
wandb:           val_r2 ▁▆██████
wandb:         val_rmse █▄▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06551
wandb:     best_val_mse 0.06526
wandb:      best_val_r2 -0.00586
wandb:    best_val_rmse 0.25546
wandb:            epoch 8
wandb:   final_test_mse 0.05877
wandb:    final_test_r2 -0.01309
wandb:  final_test_rmse 0.24242
wandb:  final_train_mse 0.03156
wandb:   final_train_r2 -0.02798
wandb: final_train_rmse 0.17764
wandb:    final_val_mse 0.06526
wandb:     final_val_r2 -0.00586
wandb:   final_val_rmse 0.25546
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04122
wandb:       train_time 35.27718
wandb:         val_loss 0.06672
wandb:          val_mse 0.06632
wandb:           val_r2 -0.02223
wandb:         val_rmse 0.25753
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141558-xz1z04rj
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141558-xz1z04rj/logs
Standard experiment completed successfully: layer_2_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_2/complexity/results.json
Running question_type experiment for language ar, layer 3
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:16:58,771][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_3/question_type
experiment_name: layer_3_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:16:58,772][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:16:58,772][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:16:58,772][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:16:58,776][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:16:58,776][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:16:59,942][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:17:02,626][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:17:02,627][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:17:02,655][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:02,671][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:02,731][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:17:02,741][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:17:02,741][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:17:02,742][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:17:02,755][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:02,774][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:02,783][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:17:02,784][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:17:02,784][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:17:02,785][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:17:02,798][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:02,815][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:02,824][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:17:02,826][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:17:02,826][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:17:02,827][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:17:02,827][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:17:02,827][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:17:02,828][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:17:02,828][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:17:02,828][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:17:02,828][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:17:02,828][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:17:02,828][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:17:02,828][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:17:02,829][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:17:02,829][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:17:02,829][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:17:02,829][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:17:02,829][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:17:02,829][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:17:02,829][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:17:02,829][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:17:02,829][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:17:02,830][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:17:02,830][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:17:02,830][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:17:02,830][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:17:02,830][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:17:02,830][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:17:02,830][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:17:02,830][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:17:02,831][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:17:02,831][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:17:06,481][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:17:06,482][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:17:06,484][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:17:06,484][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 3
[2025-04-29 14:17:06,484][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:46,  1.33it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.23it/s]Epoch 1/10:   8%|▊         | 5/63 [00:00<00:08,  6.96it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:05,  9.38it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.42it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:03, 13.06it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.33it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.31it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 16.00it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.51it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.88it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:01<00:02, 17.15it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.31it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.45it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.56it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.60it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.66it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.76it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.83it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.84it/s]
[2025-04-29 14:17:12,326][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6953
[2025-04-29 14:17:12,603][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6920, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.75it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.53it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.09it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.43it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.27it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.78it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.07it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.30it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.80it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.76it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.16it/s]
[2025-04-29 14:17:16,843][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6932
[2025-04-29 14:17:17,136][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6924, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:08,  6.90it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:04, 12.58it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:03, 14.80it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.95it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.58it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 17.00it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.26it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.44it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.54it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.59it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.65it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.75it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.76it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.32it/s]
[2025-04-29 14:17:20,775][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6938
[2025-04-29 14:17:21,065][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:09,  6.22it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.96it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.39it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.64it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.40it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.87it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.12it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.30it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.55it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.59it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.67it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.70it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.79it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.79it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.21it/s]
[2025-04-29 14:17:24,728][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6940
[2025-04-29 14:17:25,026][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 14:17:25,027][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▁▃▄
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▄▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69203
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69395
wandb:           train_time 16.94896
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.69276
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141658-xdeqi2mh
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141658-xdeqi2mh/logs
Standard experiment completed successfully: layer_3_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_3/question_type/results.json
Running complexity experiment for language ar, layer 3
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:17:40,145][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_3/complexity
experiment_name: layer_3_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 3
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:17:40,145][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:17:40,145][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:17:40,145][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:17:40,150][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:17:40,150][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:17:41,036][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:17:43,758][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:17:43,758][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:17:43,775][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:43,796][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:43,847][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:17:43,857][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:17:43,857][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:17:43,858][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:17:43,873][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:43,892][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:43,902][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:17:43,903][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:17:43,903][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:17:43,904][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:17:43,916][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:43,936][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:17:43,944][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:17:43,946][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:17:43,946][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:17:43,947][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:17:43,947][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:17:43,947][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:17:43,947][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:17:43,948][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:17:43,948][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:17:43,948][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:17:43,948][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:17:43,948][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:17:43,948][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:17:43,948][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:17:43,949][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:17:43,949][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:17:43,949][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:17:43,949][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:17:43,949][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:17:43,949][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:17:43,949][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:17:43,949][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:17:43,950][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:17:43,950][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:17:43,950][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:17:43,950][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:17:43,950][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:17:43,950][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:17:43,950][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:17:43,950][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:17:43,951][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:17:43,951][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:17:47,480][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:17:47,481][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:17:47,483][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:17:47,483][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 3
[2025-04-29 14:17:47,483][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:51,  1.20it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  3.86it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.45it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.85it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 10.92it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.63it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.98it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.01it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.79it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.36it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.77it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.07it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.28it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.44it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.56it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.79it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.80it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.81it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.83it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.83it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.83it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.83it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.83it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.58it/s]
[2025-04-29 14:17:53,177][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1897
[2025-04-29 14:17:53,461][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1425, Metrics: {'mse': 0.14402025938034058, 'rmse': 0.3795000123588148, 'r2': -1.2198312282562256}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:09,  6.63it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:04, 12.40it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:03, 14.72it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.93it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.61it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 17.00it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.25it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.43it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.54it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.62it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.74it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.75it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.78it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.81it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.34it/s]
[2025-04-29 14:17:57,652][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1100
[2025-04-29 14:17:57,939][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0937, Metrics: {'mse': 0.09453416615724564, 'rmse': 0.3074640892157093, 'r2': -0.4570859670639038}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  5.79it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.54it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.10it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.44it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.26it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.77it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.06it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.27it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.59it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.79it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.80it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.81it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.82it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.78it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.15it/s]
[2025-04-29 14:18:02,204][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0673
[2025-04-29 14:18:02,504][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0728, Metrics: {'mse': 0.07317809760570526, 'rmse': 0.2705145053517561, 'r2': -0.12791788578033447}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:11,  5.58it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.34it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.93it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.33it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.17it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.70it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.04it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.40it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.51it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.60it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.76it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.09it/s]
[2025-04-29 14:18:06,733][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0494
[2025-04-29 14:18:07,021][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0666, Metrics: {'mse': 0.06659357249736786, 'rmse': 0.2580573046774066, 'r2': -0.0264284610748291}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:10,  5.74it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.49it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 14.06it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.41it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.69it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.23it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.66it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.10it/s]
[2025-04-29 14:18:11,250][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0458
[2025-04-29 14:18:11,554][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0656, Metrics: {'mse': 0.06545855849981308, 'rmse': 0.25584870236101076, 'r2': -0.00893402099609375}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:10,  5.69it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.44it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 14.02it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.37it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.20it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.71it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.04it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.26it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.61it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.64it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.65it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.68it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 6/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.09it/s]
[2025-04-29 14:18:15,796][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0414
[2025-04-29 14:18:16,100][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0661, Metrics: {'mse': 0.06581141799688339, 'rmse': 0.25653736179528197, 'r2': -0.014372825622558594}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:09,  6.31it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:04, 12.04it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 14.43it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.69it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.40it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.82it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 17.13it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.32it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.68it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.68it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.69it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.15it/s]
[2025-04-29 14:18:19,777][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0419
[2025-04-29 14:18:20,088][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0661, Metrics: {'mse': 0.06579596549272537, 'rmse': 0.2565072425736267, 'r2': -0.014134645462036133}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:10,  6.10it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.82it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 14.24it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.53it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.28it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.76it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 17.04it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.25it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.42it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 8/10:  81%|████████  | 51/63 [00:02<00:00, 17.69it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.69it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.69it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.13it/s]
[2025-04-29 14:18:23,769][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0401
[2025-04-29 14:18:24,078][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0665, Metrics: {'mse': 0.06610532850027084, 'rmse': 0.2571095651668192, 'r2': -0.018902897834777832}
[2025-04-29 14:18:24,079][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁
wandb:     best_val_mse █▄▂▁▁
wandb:      best_val_r2 ▁▅▇██
wandb:    best_val_rmse █▄▂▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▁▁▁▁▁
wandb:          val_mse █▄▂▁▁▁▁▁
wandb:           val_r2 ▁▅▇█████
wandb:         val_rmse █▄▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06563
wandb:     best_val_mse 0.06546
wandb:      best_val_r2 -0.00893
wandb:    best_val_rmse 0.25585
wandb:            epoch 8
wandb:   final_test_mse 0.05964
wandb:    final_test_r2 -0.02817
wandb:  final_test_rmse 0.24421
wandb:  final_train_mse 0.03229
wandb:   final_train_r2 -0.05173
wandb: final_train_rmse 0.17968
wandb:    final_val_mse 0.06546
wandb:     final_val_r2 -0.00893
wandb:   final_val_rmse 0.25585
wandb:    learning_rate 1e-05
wandb:       train_loss 0.0401
wandb:       train_time 35.22494
wandb:         val_loss 0.06647
wandb:          val_mse 0.06611
wandb:           val_r2 -0.0189
wandb:         val_rmse 0.25711
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141740-5z8p1cf7
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141740-5z8p1cf7/logs
Standard experiment completed successfully: layer_3_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_3/complexity/results.json
Running question_type experiment for language ar, layer 4
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:18:41,062][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_4/question_type
experiment_name: layer_4_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:18:41,062][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:18:41,062][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:18:41,062][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:18:41,067][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:18:41,067][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:18:42,253][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:18:45,088][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:18:45,088][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:18:45,141][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:18:45,161][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:18:45,245][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:18:45,255][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:18:45,255][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:18:45,256][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:18:45,268][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:18:45,286][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:18:45,296][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:18:45,297][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:18:45,297][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:18:45,298][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:18:45,310][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:18:45,328][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:18:45,337][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:18:45,339][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:18:45,339][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:18:45,340][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:18:45,340][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:18:45,340][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:18:45,340][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:18:45,341][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:18:45,341][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:18:45,341][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:18:45,341][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:18:45,341][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:18:45,341][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:18:45,341][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:18:45,341][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:18:45,342][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:18:45,342][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:18:45,342][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:18:45,342][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:18:45,342][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:18:45,342][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:18:45,342][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:18:45,342][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:18:45,343][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:18:45,343][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:18:45,343][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:18:45,343][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:18:45,343][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:18:45,343][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:18:45,343][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:18:45,344][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:18:45,344][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:18:49,674][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:18:49,675][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:18:49,677][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:18:49,678][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 4
[2025-04-29 14:18:49,678][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:49,  1.26it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.04it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.70it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.10it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.15it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.82it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.14it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.16it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.91it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.45it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.84it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.11it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.29it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.43it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.51it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.59it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.73it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.75it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.75it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.71it/s]
[2025-04-29 14:18:55,759][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6958
[2025-04-29 14:18:56,031][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6922, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.82it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.61it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.13it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.47it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.27it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.75it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.10it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.29it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.59it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.74it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.27it/s]
[2025-04-29 14:19:00,237][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6933
[2025-04-29 14:19:00,534][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6926, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  5.95it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.70it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.18it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.51it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.26it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.76it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.07it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.30it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.46it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.52it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.58it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.70it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.17it/s]
[2025-04-29 14:19:04,205][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6941
[2025-04-29 14:19:04,497][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:09,  6.32it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:04, 12.06it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.45it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.71it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.42it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.83it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.11it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.43it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.60it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.75it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.76it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.79it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.25it/s]
[2025-04-29 14:19:08,152][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6942
[2025-04-29 14:19:08,446][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 14:19:08,446][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▁▃▄
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▅▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69217
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69422
wandb:           train_time 16.97329
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.69295
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141841-w2gsqofd
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141841-w2gsqofd/logs
Standard experiment completed successfully: layer_4_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_4/question_type/results.json
Running complexity experiment for language ar, layer 4
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:19:23,395][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_4/complexity
experiment_name: layer_4_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:19:23,395][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:19:23,395][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:19:23,395][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:19:23,400][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:19:23,400][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:19:24,290][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:19:26,967][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:19:26,968][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:19:26,982][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:19:26,999][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:19:27,049][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:19:27,058][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:19:27,059][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:19:27,060][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:19:27,071][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:19:27,089][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:19:27,098][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:19:27,100][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:19:27,100][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:19:27,101][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:19:27,117][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:19:27,146][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:19:27,163][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:19:27,164][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:19:27,164][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:19:27,166][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:19:27,166][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:19:27,166][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:19:27,167][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:19:27,167][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:19:27,167][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:19:27,167][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:19:27,167][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:19:27,167][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:19:27,168][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:19:27,168][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:19:27,168][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:19:27,168][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:19:27,168][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:19:27,168][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:19:27,168][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:19:27,168][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:19:27,169][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:19:27,169][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:19:27,169][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:19:27,169][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:19:27,169][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:19:27,169][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:19:27,169][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:19:27,169][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:19:27,169][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:19:27,170][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:19:27,170][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:19:27,170][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:19:30,976][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:19:30,976][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:19:30,978][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:19:30,979][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 4
[2025-04-29 14:19:30,979][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:50,  1.24it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  3.97it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.62it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.01it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.08it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.77it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.11it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.12it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.88it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.44it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.83it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.09it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.29it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.44it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.54it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.63it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.79it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.77it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.78it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.66it/s]
[2025-04-29 14:19:36,796][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1871
[2025-04-29 14:19:37,071][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1369, Metrics: {'mse': 0.138422429561615, 'rmse': 0.3720516490510625, 'r2': -1.1335501670837402}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.12it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.89it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.34it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.61it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.36it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.80it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.12it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.34it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.57it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.65it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.70it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.73it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.22it/s]
[2025-04-29 14:19:41,290][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1049
[2025-04-29 14:19:41,584][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0887, Metrics: {'mse': 0.08951589465141296, 'rmse': 0.29919206983376573, 'r2': -0.37973761558532715}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:09,  6.25it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 12.00it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.40it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.61it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.34it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.82it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.14it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.35it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.55it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.78it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.20it/s]
[2025-04-29 14:19:45,854][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0642
[2025-04-29 14:19:46,143][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0703, Metrics: {'mse': 0.07052505761384964, 'rmse': 0.2655655429716921, 'r2': -0.08702576160430908}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.74it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.48it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.05it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.40it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.23it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.70it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.02it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.26it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.07it/s]
[2025-04-29 14:19:50,371][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0488
[2025-04-29 14:19:50,684][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0660, Metrics: {'mse': 0.06590817868709564, 'rmse': 0.25672588238643884, 'r2': -0.015864253044128418}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.55it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.28it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.87it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.30it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.14it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.62it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.98it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.35it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.74it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.70it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.16it/s]
[2025-04-29 14:19:54,919][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0478
[2025-04-29 14:19:55,226][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0658, Metrics: {'mse': 0.06557522714138031, 'rmse': 0.25607660404921867, 'r2': -0.010732293128967285}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:10,  5.67it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.40it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.97it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.36it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.15it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.66it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.02it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.57it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.61it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.66it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.68it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.68it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.69it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.69it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.71it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.09it/s]
[2025-04-29 14:19:59,459][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0432
[2025-04-29 14:19:59,775][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0664, Metrics: {'mse': 0.06604313850402832, 'rmse': 0.2569885960583238, 'r2': -0.01794445514678955}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:10,  5.72it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.46it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 14.00it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.37it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.15it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.66it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 17.00it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.67it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.69it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.69it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.71it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.07it/s]
[2025-04-29 14:20:03,470][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0449
[2025-04-29 14:20:03,778][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0661, Metrics: {'mse': 0.06575163453817368, 'rmse': 0.25642081533715955, 'r2': -0.013451337814331055}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:10,  5.79it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.53it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 14.06it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.41it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.69it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.09it/s]
[2025-04-29 14:20:07,468][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0424
[2025-04-29 14:20:07,777][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0665, Metrics: {'mse': 0.06608042120933533, 'rmse': 0.2570611234888219, 'r2': -0.018519163131713867}
[2025-04-29 14:20:07,777][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▁▁▁
wandb:     best_val_mse █▃▁▁▁
wandb:      best_val_r2 ▁▆███
wandb:    best_val_rmse █▄▂▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▁▁▁▁▁▁
wandb:          val_mse █▃▁▁▁▁▁▁
wandb:           val_r2 ▁▆██████
wandb:         val_rmse █▄▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06581
wandb:     best_val_mse 0.06558
wandb:      best_val_r2 -0.01073
wandb:    best_val_rmse 0.25608
wandb:            epoch 8
wandb:   final_test_mse 0.05923
wandb:    final_test_r2 -0.02108
wandb:  final_test_rmse 0.24337
wandb:  final_train_mse 0.03161
wandb:   final_train_r2 -0.0298
wandb: final_train_rmse 0.1778
wandb:    final_val_mse 0.06558
wandb:     final_val_r2 -0.01073
wandb:   final_val_rmse 0.25608
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04237
wandb:       train_time 35.28297
wandb:         val_loss 0.06646
wandb:          val_mse 0.06608
wandb:           val_r2 -0.01852
wandb:         val_rmse 0.25706
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141923-1v4c48lx
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_141923-1v4c48lx/logs
Standard experiment completed successfully: layer_4_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_4/complexity/results.json
Running question_type experiment for language ar, layer 5
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:20:25,209][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_5/question_type
experiment_name: layer_5_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 5
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:20:25,209][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:20:25,209][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:20:25,209][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:20:25,214][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:20:25,214][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:20:26,439][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:20:29,142][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:20:29,142][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:20:29,184][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:20:29,204][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:20:29,268][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:20:29,277][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:20:29,278][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:20:29,279][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:20:29,296][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:20:29,317][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:20:29,327][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:20:29,328][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:20:29,328][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:20:29,329][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:20:29,345][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:20:29,369][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:20:29,378][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:20:29,379][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:20:29,379][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:20:29,380][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:20:29,381][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:20:29,381][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:20:29,381][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:20:29,381][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:20:29,381][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:20:29,381][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:20:29,382][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:20:29,382][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:20:29,382][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:20:29,382][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:20:29,382][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:20:29,382][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:20:29,382][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:20:29,382][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:20:29,383][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:20:29,383][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:20:29,383][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:20:29,383][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:20:29,383][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:20:29,383][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:20:29,383][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:20:29,383][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:20:29,383][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:20:29,384][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:20:29,384][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:20:29,384][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:20:29,384][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:20:29,384][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:20:33,268][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:20:33,269][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:20:33,271][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:20:33,271][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 5
[2025-04-29 14:20:33,271][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:50,  1.23it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  3.95it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.58it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.98it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.04it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.72it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.05it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.08it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.83it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.39it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.80it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.10it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.31it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.47it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.57it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.65it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.73it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.82it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.65it/s]
[2025-04-29 14:20:39,646][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6951
[2025-04-29 14:20:39,924][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6924, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.95it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.73it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.23it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.56it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.33it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.82it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.15it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.35it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.48it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.79it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.80it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.80it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:20:44,143][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6933
[2025-04-29 14:20:44,430][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  6.12it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.88it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.31it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.60it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.36it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.80it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.13it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.32it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.76it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.76it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.20it/s]
[2025-04-29 14:20:48,096][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6937
[2025-04-29 14:20:48,371][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6928, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:09,  6.25it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 12.00it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.42it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.66it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.40it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.86it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.15it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.34it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.48it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.59it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.17it/s]
[2025-04-29 14:20:52,043][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6938
[2025-04-29 14:20:52,336][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5227272727272727, 'f1': 0.08695652173913043}
[2025-04-29 14:20:52,337][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▁▂▃
wandb:           train_time ▁
wandb:         val_accuracy ███▁
wandb:               val_f1 ▁▁▁█
wandb:             val_loss ▁▅▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69241
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69384
wandb:           train_time 16.99443
wandb:         val_accuracy 0.52273
wandb:               val_f1 0.08696
wandb:             val_loss 0.69304
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142025-rtuh6f4z
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142025-rtuh6f4z/logs
Standard experiment completed successfully: layer_5_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_5/question_type/results.json
Running complexity experiment for language ar, layer 5
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:21:07,429][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_5/complexity
experiment_name: layer_5_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 5
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:21:07,429][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:21:07,429][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:21:07,429][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:21:07,433][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:21:07,434][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:21:08,390][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:21:11,117][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:21:11,118][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:21:11,132][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:21:11,148][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:21:11,201][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:21:11,210][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:21:11,211][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:21:11,212][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:21:11,227][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:21:11,249][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:21:11,259][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:21:11,261][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:21:11,261][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:21:11,262][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:21:11,276][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:21:11,293][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:21:11,301][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:21:11,303][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:21:11,303][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:21:11,304][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:21:11,304][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:21:11,304][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:21:11,304][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:21:11,305][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:21:11,305][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:21:11,305][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:21:11,305][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:21:11,305][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:21:11,305][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:21:11,306][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:21:11,306][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:21:11,306][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:21:11,306][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:21:11,306][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:21:11,306][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:21:11,306][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:21:11,306][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:21:11,307][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:21:11,307][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:21:11,307][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:21:11,307][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:21:11,307][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:21:11,307][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:21:11,307][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:21:11,307][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:21:11,307][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:21:11,308][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:21:11,308][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:21:14,703][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:21:14,703][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:21:14,706][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:21:14,706][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 5
[2025-04-29 14:21:14,706][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:48,  1.29it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.09it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.77it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.19it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.25it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.92it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.21it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.20it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.93it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.46it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.84it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.13it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.31it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.46it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.55it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.80it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.81it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.82it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.83it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.83it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.83it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.78it/s]
[2025-04-29 14:21:20,431][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1767
[2025-04-29 14:21:20,706][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1306, Metrics: {'mse': 0.13202466070652008, 'rmse': 0.36335197908711064, 'r2': -1.0349392890930176}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:09,  6.48it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:04, 12.20it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:03, 14.56it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.79it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.49it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.90it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.20it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.39it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.53it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.61it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.66it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.74it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.77it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.80it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.81it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.83it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.83it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.83it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.29it/s]
[2025-04-29 14:21:24,913][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0973
[2025-04-29 14:21:25,191][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0847, Metrics: {'mse': 0.0853947103023529, 'rmse': 0.2922237332975419, 'r2': -0.3162165880203247}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:11,  5.63it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.38it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.97it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.39it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.20it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.73it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.07it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.44it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.78it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.80it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.14it/s]
[2025-04-29 14:21:29,468][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0589
[2025-04-29 14:21:29,757][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0691, Metrics: {'mse': 0.06930001080036163, 'rmse': 0.2632489521353535, 'r2': -0.06814372539520264}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.90it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.63it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.16it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.50it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.30it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.75it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.07it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.29it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.44it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.78it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.79it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.79it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.79it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.80it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.24it/s]
[2025-04-29 14:21:33,963][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0453
[2025-04-29 14:21:34,265][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0661, Metrics: {'mse': 0.06601426750421524, 'rmse': 0.2569324181651962, 'r2': -0.017499446868896484}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.37it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.10it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.75it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.18it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.07it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.61it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.97it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.23it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.58it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.07it/s]
[2025-04-29 14:21:38,524][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0445
[2025-04-29 14:21:38,834][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0664, Metrics: {'mse': 0.06607770174741745, 'rmse': 0.2570558339104901, 'r2': -0.01847708225250244}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:10,  5.80it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.54it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 14.05it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.39it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.70it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.01it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.23it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.58it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.67it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.69it/s]Epoch 6/10:  81%|████████  | 51/63 [00:02<00:00, 17.70it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.04it/s]
[2025-04-29 14:21:42,534][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0408
[2025-04-29 14:21:42,842][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0671, Metrics: {'mse': 0.06674779206514359, 'rmse': 0.2583559406422534, 'r2': -0.02880549430847168}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:10,  5.67it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.40it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.98it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.37it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.20it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.72it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 17.06it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.43it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.78it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.79it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.78it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.79it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.15it/s]
[2025-04-29 14:21:46,518][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0423
[2025-04-29 14:21:46,825][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0668, Metrics: {'mse': 0.0664687380194664, 'rmse': 0.2578153176587194, 'r2': -0.02450430393218994}
[2025-04-29 14:21:46,826][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▁▁
wandb:     best_val_mse █▃▁▁
wandb:      best_val_r2 ▁▆██
wandb:    best_val_rmse █▃▁▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▁▁▁▁▁
wandb:          val_mse █▃▁▁▁▁▁
wandb:           val_r2 ▁▆█████
wandb:         val_rmse █▃▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06613
wandb:     best_val_mse 0.06601
wandb:      best_val_r2 -0.0175
wandb:    best_val_rmse 0.25693
wandb:            epoch 7
wandb:   final_test_mse 0.06133
wandb:    final_test_r2 -0.05725
wandb:  final_test_rmse 0.24764
wandb:  final_train_mse 0.03341
wandb:   final_train_r2 -0.08852
wandb: final_train_rmse 0.1828
wandb:    final_val_mse 0.06601
wandb:     final_val_r2 -0.0175
wandb:   final_val_rmse 0.25693
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04235
wandb:       train_time 30.66079
wandb:         val_loss 0.06684
wandb:          val_mse 0.06647
wandb:           val_r2 -0.0245
wandb:         val_rmse 0.25782
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142107-cks1kr0h
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142107-cks1kr0h/logs
Standard experiment completed successfully: layer_5_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_5/complexity/results.json
Running question_type experiment for language ar, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:22:03,186][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_6/question_type
experiment_name: layer_6_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:22:03,186][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:22:03,186][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:22:03,186][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:22:03,190][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:22:03,191][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:22:04,305][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:22:07,078][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:22:07,078][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:22:07,109][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:07,126][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:07,183][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:22:07,193][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:22:07,193][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:22:07,194][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:22:07,206][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:07,223][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:07,233][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:22:07,234][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:22:07,234][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:22:07,235][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:22:07,245][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:07,262][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:07,270][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:22:07,272][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:22:07,272][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:22:07,273][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:22:07,273][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:22:07,273][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:22:07,273][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:22:07,274][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:22:07,274][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:22:07,274][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:22:07,274][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:22:07,274][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:22:07,274][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:22:07,274][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:22:07,274][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:22:07,275][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:22:07,275][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:22:07,275][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:22:07,275][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:22:07,275][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:22:07,275][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:22:07,275][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:22:07,275][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:22:07,276][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:22:07,276][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:22:07,276][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:22:07,276][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:22:07,276][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:22:07,276][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:22:07,276][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:22:07,277][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:22:07,277][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:22:10,916][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:22:10,917][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:22:10,919][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:22:10,919][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 6
[2025-04-29 14:22:10,919][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:58,  1.06it/s]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:17,  3.47it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:09,  5.91it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.25it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:05, 10.33it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.11it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.54it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.66it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.52it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.15it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:02<00:02, 16.63it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 16.98it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.20it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.38it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.49it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.57it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:03<00:01, 17.72it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.73it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.74it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.76it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.78it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.17it/s]
[2025-04-29 14:22:17,045][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6950
[2025-04-29 14:22:17,324][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6920, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.17it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.96it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.38it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.64it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.40it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.86it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.16it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.35it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.48it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.58it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.65it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.70it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.78it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.74it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.22it/s]
[2025-04-29 14:22:21,546][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6932
[2025-04-29 14:22:21,835][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6924, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  6.14it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.89it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.33it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.61it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.35it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.83it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.14it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.57it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.74it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.21it/s]
[2025-04-29 14:22:25,497][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6938
[2025-04-29 14:22:25,793][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6925, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  6.10it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.84it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.30it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.58it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.34it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.41it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.57it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.59it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.62it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.62it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.64it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.68it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.16it/s]
[2025-04-29 14:22:29,468][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6941
[2025-04-29 14:22:29,762][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6927, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 14:22:29,763][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▁▃▄
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▄▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69202
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69408
wandb:           train_time 17.16773
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.69275
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142203-0ypf8v9g
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142203-0ypf8v9g/logs
Standard experiment completed successfully: layer_6_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_6/question_type/results.json
Running complexity experiment for language ar, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:22:44,997][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_6/complexity
experiment_name: layer_6_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:22:44,997][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:22:44,997][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:22:44,997][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:22:45,001][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:22:45,002][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:22:45,936][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:22:48,632][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:22:48,633][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:22:48,650][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:48,669][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:48,719][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:22:48,728][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:22:48,729][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:22:48,730][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:22:48,746][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:48,767][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:48,777][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:22:48,778][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:22:48,779][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:22:48,779][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:22:48,795][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:48,814][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:22:48,823][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:22:48,824][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:22:48,825][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:22:48,825][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:22:48,826][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:22:48,826][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:22:48,826][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:22:48,826][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:22:48,826][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:22:48,827][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:22:48,827][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:22:48,827][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:22:48,827][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:22:48,827][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:22:48,827][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:22:48,827][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:22:48,827][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:22:48,828][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:22:48,828][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:22:48,828][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:22:48,828][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:22:48,828][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:22:48,828][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:22:48,828][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:22:48,829][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:22:48,829][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:22:48,829][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:22:48,829][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:22:48,829][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:22:48,829][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:22:48,830][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:22:48,830][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:22:52,295][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:22:52,296][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:22:52,298][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:22:52,299][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 6
[2025-04-29 14:22:52,299][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:47,  1.31it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.16it/s]Epoch 1/10:   8%|▊         | 5/63 [00:00<00:08,  6.86it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.28it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.33it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.99it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.26it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.25it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.95it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.46it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.86it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.12it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.33it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.44it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.55it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.62it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.68it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.81it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.78it/s]
[2025-04-29 14:22:58,064][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1949
[2025-04-29 14:22:58,343][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1520, Metrics: {'mse': 0.15365538001060486, 'rmse': 0.3919890049613699, 'r2': -1.3683404922485352}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.05it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.83it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.29it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.59it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.35it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.84it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.14it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.34it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.46it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.22it/s]
[2025-04-29 14:23:02,565][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1213
[2025-04-29 14:23:02,852][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1028, Metrics: {'mse': 0.10386957973241806, 'rmse': 0.3222880384569338, 'r2': -0.6009756326675415}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  6.18it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.92it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.36it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.65it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.36it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.84it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.14it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.43it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.16it/s]
[2025-04-29 14:23:07,118][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0760
[2025-04-29 14:23:07,423][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0782, Metrics: {'mse': 0.0787581279873848, 'rmse': 0.280638785607736, 'r2': -0.21392464637756348}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.74it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.50it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.07it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.43it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.23it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.75it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.05it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.26it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.40it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.17it/s]
[2025-04-29 14:23:11,640][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0538
[2025-04-29 14:23:11,946][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0683, Metrics: {'mse': 0.06853058934211731, 'rmse': 0.26178347797773127, 'r2': -0.056284308433532715}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.63it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.37it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.97it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.34it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.11it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.55it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.88it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.11it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.25it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.37it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.47it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.54it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.60it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.65it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.63it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.62it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.62it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.62it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.63it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.66it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.66it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:23:16,198][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0471
[2025-04-29 14:23:16,500][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0654, Metrics: {'mse': 0.06534486263990402, 'rmse': 0.2556264122501899, 'r2': -0.007181644439697266}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:11,  5.57it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.29it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.89it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.30it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.12it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.66it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.99it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.40it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.66it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:23:20,740][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0409
[2025-04-29 14:23:21,029][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0653, Metrics: {'mse': 0.06507818400859833, 'rmse': 0.2551042610553542, 'r2': -0.0030711889266967773}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:10,  6.11it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.85it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 14.27it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.57it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.33it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 17.12it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.41it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.51it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.67it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.69it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.13it/s]
[2025-04-29 14:23:25,243][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0403
[2025-04-29 14:23:25,556][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0656, Metrics: {'mse': 0.06532509624958038, 'rmse': 0.25558774667338885, 'r2': -0.0068770647048950195}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:10,  5.70it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.43it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.97it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.35it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.15it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.67it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 17.00it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.60it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.68it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.68it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.68it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.69it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.08it/s]
[2025-04-29 14:23:29,247][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0392
[2025-04-29 14:23:29,541][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0661, Metrics: {'mse': 0.06579696387052536, 'rmse': 0.25650918866684946, 'r2': -0.014150023460388184}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:10,  5.80it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.52it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 14.05it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.40it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.69it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 17.01it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 9/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 17.10it/s]
[2025-04-29 14:23:33,228][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0407
[2025-04-29 14:23:33,525][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0661, Metrics: {'mse': 0.06578850001096725, 'rmse': 0.2564926899756936, 'r2': -0.014019608497619629}
[2025-04-29 14:23:33,526][src.training.lm_trainer][INFO] - Early stopping at epoch 9
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁▁
wandb:     best_val_mse █▄▂▁▁▁
wandb:      best_val_r2 ▁▅▇███
wandb:    best_val_rmse █▄▂▁▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▃▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▁▁▁▁▁▁
wandb:          val_mse █▄▂▁▁▁▁▁▁
wandb:           val_r2 ▁▅▇██████
wandb:         val_rmse █▄▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0653
wandb:     best_val_mse 0.06508
wandb:      best_val_r2 -0.00307
wandb:    best_val_rmse 0.2551
wandb:            epoch 9
wandb:   final_test_mse 0.05865
wandb:    final_test_r2 -0.01104
wandb:  final_test_rmse 0.24217
wandb:  final_train_mse 0.03161
wandb:   final_train_r2 -0.02984
wandb: final_train_rmse 0.1778
wandb:    final_val_mse 0.06508
wandb:     final_val_r2 -0.00307
wandb:   final_val_rmse 0.2551
wandb:    learning_rate 1e-05
wandb:       train_loss 0.04074
wandb:       train_time 39.72733
wandb:         val_loss 0.06613
wandb:          val_mse 0.06579
wandb:           val_r2 -0.01402
wandb:         val_rmse 0.25649
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142245-px39e07s
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142245-px39e07s/logs
Standard experiment completed successfully: layer_6_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_6/complexity/results.json
Running question_type experiment for language ar, layer 7
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:23:50,463][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_7/question_type
experiment_name: layer_7_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 7
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:23:50,463][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:23:50,463][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:23:50,463][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:23:50,468][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:23:50,468][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:23:51,669][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:23:54,364][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:23:54,364][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:23:54,413][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:23:54,432][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:23:54,495][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:23:54,504][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:23:54,504][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:23:54,505][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:23:54,516][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:23:54,533][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:23:54,541][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:23:54,543][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:23:54,543][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:23:54,543][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:23:54,555][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:23:54,573][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:23:54,584][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:23:54,586][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:23:54,586][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:23:54,587][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:23:54,587][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:23:54,587][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:23:54,587][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:23:54,588][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:23:54,588][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:23:54,588][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:23:54,588][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:23:54,588][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:23:54,588][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:23:54,588][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:23:54,589][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:23:54,589][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:23:54,589][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:23:54,589][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:23:54,589][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:23:54,589][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:23:54,589][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:23:54,589][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:23:54,590][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:23:54,590][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:23:54,590][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:23:54,590][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:23:54,590][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:23:54,590][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:23:54,590][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:23:54,590][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:23:54,591][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:23:54,591][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:23:58,312][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:23:58,313][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:23:58,315][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:23:58,316][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 7
[2025-04-29 14:23:58,316][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:52,  1.19it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  3.85it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.45it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.83it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 10.91it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.63it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.98it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.02it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.79it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.36it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.79it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.06it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.28it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.44it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.53it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.61it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.65it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.53it/s]
[2025-04-29 14:24:04,245][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6935
[2025-04-29 14:24:04,509][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6916, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.08it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.82it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.28it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.58it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.36it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.81it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.12it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.32it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.76it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.81it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.81it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:24:08,726][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6935
[2025-04-29 14:24:09,020][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6918, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:09,  6.50it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:04, 12.22it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:03, 14.56it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.77it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.48it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.91it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.18it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.37it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.50it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.57it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.63it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.66it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.17it/s]
[2025-04-29 14:24:12,691][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6934
[2025-04-29 14:24:13,003][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6918, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:08,  6.92it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:04, 12.60it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:03, 14.81it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.91it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.58it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.97it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.25it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.40it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.50it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.59it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:24:16,663][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6932
[2025-04-29 14:24:16,964][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6920, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 14:24:16,965][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ██▇▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▄▅█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69158
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69324
wandb:           train_time 17.05912
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.69197
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142350-t4zsiews
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142350-t4zsiews/logs
Standard experiment completed successfully: layer_7_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_7/question_type/results.json
Running complexity experiment for language ar, layer 7
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:24:32,037][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_7/complexity
experiment_name: layer_7_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 7
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:24:32,037][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:24:32,038][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:24:32,038][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:24:32,042][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:24:32,042][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:24:32,962][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:24:35,671][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:24:35,672][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:24:35,700][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:24:35,717][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:24:35,768][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:24:35,778][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:24:35,779][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:24:35,779][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:24:35,794][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:24:35,811][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:24:35,820][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:24:35,821][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:24:35,821][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:24:35,822][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:24:35,833][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:24:35,850][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:24:35,858][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:24:35,860][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:24:35,860][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:24:35,861][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:24:35,861][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:24:35,861][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:24:35,861][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:24:35,862][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:24:35,862][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:24:35,862][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:24:35,862][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:24:35,862][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:24:35,862][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:24:35,862][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:24:35,863][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:24:35,863][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:24:35,863][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:24:35,863][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:24:35,863][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:24:35,863][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:24:35,863][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:24:35,863][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:24:35,864][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:24:35,864][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:24:35,864][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:24:35,864][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:24:35,864][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:24:35,864][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:24:35,864][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:24:35,864][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:24:35,865][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:24:35,865][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:24:39,311][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:24:39,312][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:24:39,314][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:24:39,314][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 7
[2025-04-29 14:24:39,315][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:52,  1.19it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  3.84it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:09,  6.44it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.84it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 10.91it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.64it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.00it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.05it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.81it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.40it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.79it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.09it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.31it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.46it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.56it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.79it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.80it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.82it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.52it/s]
[2025-04-29 14:24:45,085][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.2259
[2025-04-29 14:24:45,358][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2093, Metrics: {'mse': 0.21146632730960846, 'rmse': 0.459854680643362, 'r2': -2.2593994140625}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:09,  6.23it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:04, 12.02it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.45it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.71it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.44it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.90it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.18it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.37it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.50it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.60it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.66it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.74it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.76it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.77it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.78it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.80it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.81it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.82it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.33it/s]
[2025-04-29 14:24:49,545][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1953
[2025-04-29 14:24:49,836][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1810, Metrics: {'mse': 0.18294523656368256, 'rmse': 0.4277209798030517, 'r2': -1.8197944164276123}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:11,  5.60it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.38it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.96it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.37it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.21it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.72it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.06it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.43it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.55it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.63it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.69it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.74it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.76it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.21it/s]
[2025-04-29 14:24:54,105][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1623
[2025-04-29 14:24:54,407][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1559, Metrics: {'mse': 0.15757113695144653, 'rmse': 0.3969523106765428, 'r2': -1.4286952018737793}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.80it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.54it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.09it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.45it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.25it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.76it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.44it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.67it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.74it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.76it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.17it/s]
[2025-04-29 14:24:58,619][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1378
[2025-04-29 14:24:58,915][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1343, Metrics: {'mse': 0.135787233710289, 'rmse': 0.3684931935738963, 'r2': -1.092932939529419}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:10,  5.71it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.46it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 14.04it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.41it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.22it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.71it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.26it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.41it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.51it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.66it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.09it/s]
[2025-04-29 14:25:03,169][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1135
[2025-04-29 14:25:03,458][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1164, Metrics: {'mse': 0.11765534430742264, 'rmse': 0.3430092481368726, 'r2': -0.813460111618042}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:10,  6.04it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.80it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 14.28it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.57it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.32it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.29it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.42it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.52it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.11it/s]
[2025-04-29 14:25:07,698][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0963
[2025-04-29 14:25:08,008][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1015, Metrics: {'mse': 0.10251451283693314, 'rmse': 0.32017887631280917, 'r2': -0.5800896883010864}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:10,  5.70it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.43it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 14.01it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.35it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.17it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.68it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.99it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.12it/s]
[2025-04-29 14:25:12,261][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0802
[2025-04-29 14:25:12,571][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0897, Metrics: {'mse': 0.09052108973264694, 'rmse': 0.30086722940966326, 'r2': -0.39523112773895264}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.58it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.27it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.86it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.26it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.10it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.61it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.97it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.35it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.65it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.02it/s]
[2025-04-29 14:25:16,833][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0673
[2025-04-29 14:25:17,152][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0807, Metrics: {'mse': 0.08133428543806076, 'rmse': 0.2851916643909158, 'r2': -0.2536318302154541}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.62it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.33it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.92it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.34it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.17it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.69it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 16.99it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 9/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 17.13it/s]
[2025-04-29 14:25:21,404][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0568
[2025-04-29 14:25:21,708][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0746, Metrics: {'mse': 0.07503270357847214, 'rmse': 0.27392098053722014, 'r2': -0.15650343894958496}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:11,  5.54it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 11.27it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.88it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.26it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 16.11it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.64it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 16.98it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.60it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.64it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.67it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.68it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 17.05it/s]
[2025-04-29 14:25:25,961][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0524
[2025-04-29 14:25:26,259][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0703, Metrics: {'mse': 0.0706331729888916, 'rmse': 0.2657690218759357, 'r2': -0.0886920690536499}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▅▄▃▃▂▂▁▁
wandb:     best_val_mse █▇▅▄▃▃▂▂▁▁
wandb:      best_val_r2 ▁▂▄▅▆▆▇▇██
wandb:    best_val_rmse █▇▆▅▄▃▂▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▇▅▄▃▃▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▅▄▃▃▂▂▁▁
wandb:          val_mse █▇▅▄▃▃▂▂▁▁
wandb:           val_r2 ▁▂▄▅▆▆▇▇██
wandb:         val_rmse █▇▆▅▄▃▂▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07034
wandb:     best_val_mse 0.07063
wandb:      best_val_r2 -0.08869
wandb:    best_val_rmse 0.26577
wandb:            epoch 10
wandb:   final_test_mse 0.06941
wandb:    final_test_r2 -0.19663
wandb:  final_test_rmse 0.26346
wandb:  final_train_mse 0.04376
wandb:   final_train_r2 -0.42563
wandb: final_train_rmse 0.2092
wandb:    final_val_mse 0.07063
wandb:     final_val_r2 -0.08869
wandb:   final_val_rmse 0.26577
wandb:    learning_rate 1e-05
wandb:       train_loss 0.05239
wandb:       train_time 46.10856
wandb:         val_loss 0.07034
wandb:          val_mse 0.07063
wandb:           val_r2 -0.08869
wandb:         val_rmse 0.26577
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142432-lz3hmfu1
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142432-lz3hmfu1/logs
Standard experiment completed successfully: layer_7_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_7/complexity/results.json
Running question_type experiment for language ar, layer 8
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:25:43,351][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_8/question_type
experiment_name: layer_8_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 8
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:25:43,351][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:25:43,351][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:25:43,351][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:25:43,355][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:25:43,356][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:25:44,472][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:25:47,195][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:25:47,196][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:25:47,230][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:25:47,246][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:25:47,307][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:25:47,317][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:25:47,317][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:25:47,318][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:25:47,331][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:25:47,351][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:25:47,362][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:25:47,363][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:25:47,363][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:25:47,364][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:25:47,375][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:25:47,392][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:25:47,402][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:25:47,403][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:25:47,403][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:25:47,404][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:25:47,405][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:25:47,405][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:25:47,405][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:25:47,405][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:25:47,405][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:25:47,405][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:25:47,405][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:25:47,405][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:25:47,406][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:25:47,406][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:25:47,406][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:25:47,406][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:25:47,406][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:25:47,406][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:25:47,406][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:25:47,406][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:25:47,407][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:25:47,407][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:25:47,407][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:25:47,407][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:25:47,407][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:25:47,407][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:25:47,407][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:25:47,407][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:25:47,408][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:25:47,408][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:25:47,408][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:25:47,408][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:25:51,090][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:25:51,091][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:25:51,093][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:25:51,093][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 8
[2025-04-29 14:25:51,093][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:53,  1.15it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:16,  3.73it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:09,  6.28it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.66it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:05, 10.74it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.48it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.86it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.92it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.72it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.30it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.73it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.04it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.26it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.41it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.52it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.57it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.64it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:03<00:01, 17.72it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.78it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.79it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.43it/s]
[2025-04-29 14:25:56,990][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6939
[2025-04-29 14:25:57,266][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6911, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.18it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.94it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.38it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.65it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.40it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.83it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.15it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.37it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.51it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.61it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.67it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.81it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.25it/s]
[2025-04-29 14:26:01,474][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6939
[2025-04-29 14:26:01,764][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6913, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  6.15it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.92it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.36it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.64it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.36it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.83it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.10it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.46it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.77it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.29it/s]
[2025-04-29 14:26:05,409][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6927
[2025-04-29 14:26:05,695][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6915, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:09,  6.31it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:04, 12.04it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.45it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.69it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.40it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.85it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.16it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.32it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.60it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.67it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.70it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.22it/s]
[2025-04-29 14:26:09,356][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6922
[2025-04-29 14:26:09,658][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6917, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 14:26:09,659][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ██▃▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▄▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.6911
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69223
wandb:           train_time 17.03794
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.69166
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142543-95pw8qx8
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142543-95pw8qx8/logs
Standard experiment completed successfully: layer_8_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_8/question_type/results.json
Running complexity experiment for language ar, layer 8
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:26:24,676][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_8/complexity
experiment_name: layer_8_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 8
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:26:24,676][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:26:24,676][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:26:24,676][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:26:24,680][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:26:24,681][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:26:25,625][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:26:28,325][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:26:28,326][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:26:28,349][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:26:28,369][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:26:28,421][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:26:28,431][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:26:28,432][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:26:28,432][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:26:28,444][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:26:28,461][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:26:28,470][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:26:28,471][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:26:28,471][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:26:28,471][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:26:28,482][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:26:28,498][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:26:28,506][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:26:28,507][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:26:28,507][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:26:28,508][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:26:28,509][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:26:28,509][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:26:28,509][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:26:28,509][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:26:28,509][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:26:28,509][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:26:28,509][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:26:28,510][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:26:28,510][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:26:28,510][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:26:28,510][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:26:28,510][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:26:28,510][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:26:28,510][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:26:28,510][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:26:28,511][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:26:28,511][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:26:28,511][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:26:28,511][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:26:28,511][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:26:28,511][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:26:28,511][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:26:28,512][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:26:28,512][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:26:28,512][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:26:28,512][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:26:28,512][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:26:28,512][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:26:31,941][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:26:31,942][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:26:31,944][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:26:31,945][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 8
[2025-04-29 14:26:31,945][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:49,  1.25it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.02it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.67it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.07it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.12it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.80it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.12it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.13it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.89it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.43it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.82it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.10it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.31it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.45it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.55it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.61it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.83it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.71it/s]
[2025-04-29 14:26:37,635][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.2280
[2025-04-29 14:26:37,915][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2037, Metrics: {'mse': 0.2057933658361435, 'rmse': 0.45364453687457046, 'r2': -2.1719601154327393}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.92it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.69it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.20it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.52it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.31it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.13it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.32it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.64it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.77it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.81it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.82it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.81it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.79it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.80it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.77it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.18it/s]
[2025-04-29 14:26:42,157][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1797
[2025-04-29 14:26:42,463][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1627, Metrics: {'mse': 0.16442202031612396, 'rmse': 0.4054898522973466, 'r2': -1.534290075302124}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:09,  6.23it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.98it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.37it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.66it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.39it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.87it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.15it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.72it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.76it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.22it/s]
[2025-04-29 14:26:46,712][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1371
[2025-04-29 14:26:46,997][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1313, Metrics: {'mse': 0.13271470367908478, 'rmse': 0.36430029327339936, 'r2': -1.0455749034881592}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.90it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.65it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.16it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.51it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.27it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.10it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.29it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.43it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.52it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.79it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.79it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.19it/s]
[2025-04-29 14:26:51,200][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1069
[2025-04-29 14:26:51,503][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1074, Metrics: {'mse': 0.10848569124937057, 'rmse': 0.3293716612724455, 'r2': -0.6721252202987671}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:10,  5.69it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.44it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 14.00it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.39it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.21it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.73it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.02it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.24it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 5/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.11it/s]
[2025-04-29 14:26:55,713][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0838
[2025-04-29 14:26:56,026][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0899, Metrics: {'mse': 0.09073126316070557, 'rmse': 0.3012163062662869, 'r2': -0.39847052097320557}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:11,  5.55it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.24it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.85it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.25it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.05it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.53it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.89it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.14it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.29it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.41it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.48it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.55it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.64it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.65it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.03it/s]
[2025-04-29 14:27:00,269][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0648
[2025-04-29 14:27:00,572][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0781, Metrics: {'mse': 0.0786658525466919, 'rmse': 0.28047433491621276, 'r2': -0.2125023603439331}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:11,  5.42it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.14it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.78it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.22it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.09it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.61it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.96it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.65it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.67it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.68it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 16.97it/s]
[2025-04-29 14:27:04,858][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0517
[2025-04-29 14:27:05,169][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0712, Metrics: {'mse': 0.07152225822210312, 'rmse': 0.2674364564192831, 'r2': -0.10239589214324951}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.60it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.34it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.92it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.31it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.14it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.66it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.99it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.20it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.35it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.64it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.65it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.69it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:27:09,416][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0460
[2025-04-29 14:27:09,736][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0672, Metrics: {'mse': 0.06731073558330536, 'rmse': 0.259443125912608, 'r2': -0.037482261657714844}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:10,  5.90it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.63it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 14.11it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.39it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.70it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 17.02it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.25it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 9/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 17.12it/s]
[2025-04-29 14:27:13,996][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0404
[2025-04-29 14:27:14,306][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0656, Metrics: {'mse': 0.06557353585958481, 'rmse': 0.2560733017313301, 'r2': -0.010706305503845215}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:10,  5.78it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 11.54it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 14.08it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.40it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 16.22it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.71it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.25it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.40it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 10/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 17.10it/s]
[2025-04-29 14:27:18,550][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0370
[2025-04-29 14:27:18,863][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0651, Metrics: {'mse': 0.06494658440351486, 'rmse': 0.25484619754572535, 'r2': -0.0010428428649902344}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▄▃▂▂▁▁▁▁
wandb:     best_val_mse █▆▄▃▂▂▁▁▁▁
wandb:      best_val_r2 ▁▃▅▆▇▇████
wandb:    best_val_rmse █▆▅▄▃▂▁▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▆▅▄▃▂▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▄▃▂▂▁▁▁▁
wandb:          val_mse █▆▄▃▂▂▁▁▁▁
wandb:           val_r2 ▁▃▅▆▇▇████
wandb:         val_rmse █▆▅▄▃▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06509
wandb:     best_val_mse 0.06495
wandb:      best_val_r2 -0.00104
wandb:    best_val_rmse 0.25485
wandb:            epoch 10
wandb:   final_test_mse 0.05944
wandb:    final_test_r2 -0.02475
wandb:  final_test_rmse 0.24381
wandb:  final_train_mse 0.03265
wandb:   final_train_r2 -0.06368
wandb: final_train_rmse 0.1807
wandb:    final_val_mse 0.06495
wandb:     final_val_r2 -0.00104
wandb:   final_val_rmse 0.25485
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03699
wandb:       train_time 46.08004
wandb:         val_loss 0.06509
wandb:          val_mse 0.06495
wandb:           val_r2 -0.00104
wandb:         val_rmse 0.25485
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142624-ctupu10y
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142624-ctupu10y/logs
Standard experiment completed successfully: layer_8_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_8/complexity/results.json
Running question_type experiment for language ar, layer 9
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:27:35,539][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_9/question_type
experiment_name: layer_9_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:27:35,539][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:27:35,539][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:27:35,539][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:27:35,543][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:27:35,543][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:27:36,908][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:27:39,618][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:27:39,619][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:27:39,655][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:27:39,674][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:27:39,744][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:27:39,754][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:27:39,754][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:27:39,755][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:27:39,768][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:27:39,787][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:27:39,797][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:27:39,798][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:27:39,799][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:27:39,800][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:27:39,818][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:27:39,839][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:27:39,849][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:27:39,850][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:27:39,850][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:27:39,851][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:27:39,852][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:27:39,852][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:27:39,852][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:27:39,852][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:27:39,852][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:27:39,852][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:27:39,852][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:27:39,852][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:27:39,853][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:27:39,853][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:27:39,853][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:27:39,853][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:27:39,853][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:27:39,853][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:27:39,853][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:27:39,853][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:27:39,854][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:27:39,854][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:27:39,854][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:27:39,854][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:27:39,854][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:27:39,854][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:27:39,854][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:27:39,854][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:27:39,855][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:27:39,855][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:27:39,855][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:27:39,855][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:27:43,624][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:27:43,625][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:27:43,627][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:27:43,627][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 9
[2025-04-29 14:27:43,628][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:48,  1.27it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.05it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.71it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.10it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.15it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.83it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.14it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.13it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.88it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.42it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.83it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.11it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.32it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.44it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.55it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.60it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.71it/s]
[2025-04-29 14:27:49,565][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6946
[2025-04-29 14:27:49,840][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6881, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.87it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.64it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.18it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.49it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.26it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.77it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.08it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.55it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.60it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.20it/s]
[2025-04-29 14:27:54,064][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6938
[2025-04-29 14:27:54,355][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6877, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:11,  5.56it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.30it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.90it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.32it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.16it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.68it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.26it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.42it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.76it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.11it/s]
[2025-04-29 14:27:58,630][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6918
[2025-04-29 14:27:58,918][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6873, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.67it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.41it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.98it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.37it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.16it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.68it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.24it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.08it/s]
[2025-04-29 14:28:03,148][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6892
[2025-04-29 14:28:03,454][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6870, Metrics: {'accuracy': 0.7045454545454546, 'f1': 0.5185185185185185}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.58it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.32it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.93it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.28it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.13it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.64it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.00it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.68it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:28:07,702][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6897
[2025-04-29 14:28:08,013][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6867, Metrics: {'accuracy': 0.75, 'f1': 0.6666666666666666}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:10,  5.78it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.51it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 14.05it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.43it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.71it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.02it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.26it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 6/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.10it/s]
[2025-04-29 14:28:12,246][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6876
[2025-04-29 14:28:12,551][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6862, Metrics: {'accuracy': 0.7045454545454546, 'f1': 0.7346938775510204}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:11,  5.56it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.31it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.91it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.26it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.10it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.65it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.96it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.35it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.07it/s]
[2025-04-29 14:28:16,810][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6862
[2025-04-29 14:28:17,127][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6859, Metrics: {'accuracy': 0.4772727272727273, 'f1': 0.6349206349206349}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.41it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.14it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.77it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.21it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.05it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.60it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.95it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.02it/s]
[2025-04-29 14:28:21,389][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6858
[2025-04-29 14:28:21,710][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6855, Metrics: {'accuracy': 0.4772727272727273, 'f1': 0.6349206349206349}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.59it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.32it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.90it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.31it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.15it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.69it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.25it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.42it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.52it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 9/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 17.08it/s]
[2025-04-29 14:28:25,992][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6840
[2025-04-29 14:28:26,313][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6849, Metrics: {'accuracy': 0.4772727272727273, 'f1': 0.6349206349206349}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:11,  5.60it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 11.33it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.92it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.31it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 16.15it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.67it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 17.01it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.52it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.67it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 17.05it/s]
[2025-04-29 14:28:30,563][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6849
[2025-04-29 14:28:30,886][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6844, Metrics: {'accuracy': 0.4772727272727273, 'f1': 0.6349206349206349}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▃▃▃▇█▇▁▁▁▁
wandb:          best_val_f1 ▁▁▁▆▇█▇▇▇▇
wandb:        best_val_loss █▇▆▆▅▄▄▃▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▄▅▃▂▂▁▂
wandb:           train_time ▁
wandb:         val_accuracy ▃▃▃▇█▇▁▁▁▁
wandb:               val_f1 ▁▁▁▆▇█▇▇▇▇
wandb:             val_loss █▇▆▆▅▄▄▃▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.47727
wandb:          best_val_f1 0.63492
wandb:        best_val_loss 0.68441
wandb:                epoch 10
wandb:  final_test_accuracy 0.28571
wandb:        final_test_f1 0.44444
wandb: final_train_accuracy 0.4995
wandb:       final_train_f1 0.66577
wandb:   final_val_accuracy 0.47727
wandb:         final_val_f1 0.63492
wandb:        learning_rate 1e-05
wandb:           train_loss 0.68487
wandb:           train_time 46.19688
wandb:         val_accuracy 0.47727
wandb:               val_f1 0.63492
wandb:             val_loss 0.68441
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142735-zdooazfv
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142735-zdooazfv/logs
Standard experiment completed successfully: layer_9_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_9/question_type/results.json
Running complexity experiment for language ar, layer 9
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:28:47,843][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_9/complexity
experiment_name: layer_9_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:28:47,843][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:28:47,843][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:28:47,843][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:28:47,848][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:28:47,848][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:28:48,932][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:28:51,626][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:28:51,626][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:28:51,655][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:28:51,670][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:28:51,729][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:28:51,738][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:28:51,739][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:28:51,740][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:28:51,751][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:28:51,768][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:28:51,777][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:28:51,778][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:28:51,778][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:28:51,779][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:28:51,790][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:28:51,808][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:28:51,817][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:28:51,818][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:28:51,818][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:28:51,819][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:28:51,820][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:28:51,820][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:28:51,820][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:28:51,820][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:28:51,820][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:28:51,821][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:28:51,821][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:28:51,821][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:28:51,821][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:28:51,821][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:28:51,821][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:28:51,821][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:28:51,821][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:28:51,822][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:28:51,822][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:28:51,822][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:28:51,822][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:28:51,822][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:28:51,822][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:28:51,822][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:28:51,822][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:28:51,823][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:28:51,823][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:28:51,823][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:28:51,823][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:28:51,823][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:28:51,823][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:28:51,824][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:28:55,548][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:28:55,549][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:28:55,551][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:28:55,551][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 9
[2025-04-29 14:28:55,552][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:49,  1.25it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  4.00it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.65it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.06it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.12it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.79it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.11it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.11it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.87it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.43it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.83it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.11it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.31it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.46it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.52it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.60it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.76it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.83it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.65it/s]
[2025-04-29 14:29:01,421][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.2492
[2025-04-29 14:29:01,682][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2003, Metrics: {'mse': 0.2022559642791748, 'rmse': 0.4497287674578699, 'r2': -2.1174371242523193}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  6.14it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.91it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.37it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.65it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.36it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.83it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.13it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.35it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.61it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.72it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.75it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.76it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:29:05,887][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1613
[2025-04-29 14:29:06,174][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1306, Metrics: {'mse': 0.13184159994125366, 'rmse': 0.36309998614879296, 'r2': -1.0321176052093506}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  6.17it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.94it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.38it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.63it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.38it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.82it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.11it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.32it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.63it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.68it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.78it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.20it/s]
[2025-04-29 14:29:10,434][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0969
[2025-04-29 14:29:10,742][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0913, Metrics: {'mse': 0.09185783565044403, 'rmse': 0.30308057616819334, 'r2': -0.41583478450775146}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:11,  5.59it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.32it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.94it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.34it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.17it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.70it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.02it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.24it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.75it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.09it/s]
[2025-04-29 14:29:14,975][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0627
[2025-04-29 14:29:15,272][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0731, Metrics: {'mse': 0.07310894131660461, 'rmse': 0.2703866515133552, 'r2': -0.1268519163131714}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:10,  5.68it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.43it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.99it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.36it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.18it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.70it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.24it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.66it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.67it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.05it/s]
[2025-04-29 14:29:19,537][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0471
[2025-04-29 14:29:19,839][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0677, Metrics: {'mse': 0.06739440560340881, 'rmse': 0.2596043250860987, 'r2': -0.038771986961364746}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:10,  6.10it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.81it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 14.26it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.53it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.28it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.74it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.06it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.26it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.41it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.74it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.74it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.16it/s]
[2025-04-29 14:29:24,068][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0386
[2025-04-29 14:29:24,379][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0681, Metrics: {'mse': 0.06752856820821762, 'rmse': 0.2598625948616261, 'r2': -0.0408397912979126}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:09,  6.22it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.97it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 14.38it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.63it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.34it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.41it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.70it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.18it/s]
[2025-04-29 14:29:28,050][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0346
[2025-04-29 14:29:28,340][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0702, Metrics: {'mse': 0.06950265914201736, 'rmse': 0.26363356983134256, 'r2': -0.07126712799072266}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:10,  5.91it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.64it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 14.14it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.47it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.23it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.73it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 17.04it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.25it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.18it/s]
[2025-04-29 14:29:32,009][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0323
[2025-04-29 14:29:32,318][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0720, Metrics: {'mse': 0.07124053686857224, 'rmse': 0.2669092296429111, 'r2': -0.09805357456207275}
[2025-04-29 14:29:32,319][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁
wandb:     best_val_mse █▄▂▁▁
wandb:      best_val_r2 ▁▅▇██
wandb:    best_val_rmse █▅▃▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▃▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▁▁▁▁▁
wandb:          val_mse █▄▂▁▁▁▁▁
wandb:           val_r2 ▁▅▇█████
wandb:         val_rmse █▅▃▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06773
wandb:     best_val_mse 0.06739
wandb:      best_val_r2 -0.03877
wandb:    best_val_rmse 0.2596
wandb:            epoch 8
wandb:   final_test_mse 0.0661
wandb:    final_test_r2 -0.13961
wandb:  final_test_rmse 0.25711
wandb:  final_train_mse 0.0329
wandb:   final_train_r2 -0.07189
wandb: final_train_rmse 0.1814
wandb:    final_val_mse 0.06739
wandb:     final_val_r2 -0.03877
wandb:   final_val_rmse 0.2596
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03233
wandb:       train_time 35.19966
wandb:         val_loss 0.07203
wandb:          val_mse 0.07124
wandb:           val_r2 -0.09805
wandb:         val_rmse 0.26691
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142847-ck44bot8
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142847-ck44bot8/logs
Standard experiment completed successfully: layer_9_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_9/complexity/results.json
Running question_type experiment for language ar, layer 10
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:29:51,554][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_10/question_type
experiment_name: layer_10_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:29:51,554][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:29:51,555][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:29:51,555][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:29:51,559][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:29:51,559][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:29:52,757][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:29:55,616][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:29:55,616][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:29:55,648][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:29:55,664][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:29:55,725][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:29:55,736][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:29:55,737][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:29:55,738][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:29:55,751][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:29:55,770][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:29:55,779][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:29:55,781][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:29:55,781][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:29:55,782][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:29:55,794][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:29:55,812][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:29:55,820][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:29:55,822][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:29:55,822][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:29:55,823][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:29:55,823][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:29:55,823][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:29:55,823][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:29:55,824][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:29:55,824][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:29:55,824][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:29:55,824][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:29:55,824][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:29:55,824][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:29:55,824][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:29:55,825][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:29:55,825][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:29:55,825][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:29:55,825][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:29:55,825][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:29:55,825][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:29:55,825][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:29:55,825][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:29:55,825][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:29:55,826][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:29:55,826][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:29:55,826][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:29:55,826][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:29:55,826][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:29:55,826][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:29:55,826][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:29:55,827][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:29:55,827][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:29:59,634][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:29:59,635][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:29:59,637][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:29:59,638][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 10
[2025-04-29 14:29:59,638][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:52,  1.17it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  3.80it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:09,  6.38it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.77it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 10.84it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.57it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.92it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.97it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.76it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.35it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.75it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.03it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.23it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.41it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.53it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.62it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.70it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.70it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.72it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.71it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.71it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.75it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.81it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.82it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.49it/s]
[2025-04-29 14:30:05,607][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6888
[2025-04-29 14:30:05,897][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6694, Metrics: {'accuracy': 0.7272727272727273, 'f1': 0.76}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.96it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.74it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.23it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.55it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.32it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.80it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.11it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.55it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.74it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.77it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.13it/s]
[2025-04-29 14:30:10,143][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6702
[2025-04-29 14:30:10,441][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6465, Metrics: {'accuracy': 0.75, 'f1': 0.7843137254901961}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:11,  5.40it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.13it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.81it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.23it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.11it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.61it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 16.99it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.23it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.51it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 3/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.80it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:30:14,854][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6517
[2025-04-29 14:30:15,156][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6215, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:11,  5.21it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 10.92it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.64it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.09it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.01it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.57it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 16.92it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.20it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 4/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.01it/s]
[2025-04-29 14:30:19,415][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6341
[2025-04-29 14:30:19,740][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5996, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.36it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.07it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.74it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.19it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.06it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.59it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.95it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.62it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.66it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.70it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.69it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.02it/s]
[2025-04-29 14:30:24,014][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6165
[2025-04-29 14:30:24,341][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5767, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:11,  5.41it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.14it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.80it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.24it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.05it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.60it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.96it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.16it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.33it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.52it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.00it/s]
[2025-04-29 14:30:28,607][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6051
[2025-04-29 14:30:28,935][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5518, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:12,  5.14it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 10.84it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.56it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.06it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 15.99it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.55it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.63it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 16.93it/s]
[2025-04-29 14:30:33,224][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5867
[2025-04-29 14:30:33,551][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.5314, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:10,  5.71it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.44it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 14.00it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.34it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.15it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.66it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.99it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.69it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.08it/s]
[2025-04-29 14:30:37,805][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5740
[2025-04-29 14:30:38,131][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.5040, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.38it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.10it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.70it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.12it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 15.97it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.52it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 16.86it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.09it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.26it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.37it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.42it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.46it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.51it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.52it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.56it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.60it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.62it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.64it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.67it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.68it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 9/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 16.98it/s]
[2025-04-29 14:30:42,414][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5558
[2025-04-29 14:30:42,743][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.4829, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:11,  5.26it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 10.97it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.65it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.12it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 16.01it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.55it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 16.92it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.17it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.33it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.69it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.69it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 16.94it/s]
[2025-04-29 14:30:47,037][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.5394
[2025-04-29 14:30:47,368][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.4612, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▂▇▇▇▇▇███
wandb:          best_val_f1 ▁▂▇▇▇▇▇███
wandb:        best_val_loss █▇▆▆▅▄▃▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▅▅▄▃▃▂▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▂▇▇▇▇▇███
wandb:               val_f1 ▁▂▇▇▇▇▇███
wandb:             val_loss █▇▆▆▅▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.90909
wandb:          best_val_f1 0.90909
wandb:        best_val_loss 0.46115
wandb:                epoch 10
wandb:  final_test_accuracy 0.50649
wandb:        final_test_f1 0.53659
wandb: final_train_accuracy 0.97688
wandb:       final_train_f1 0.97734
wandb:   final_val_accuracy 0.90909
wandb:         final_val_f1 0.90909
wandb:        learning_rate 1e-05
wandb:           train_loss 0.53936
wandb:           train_time 46.68831
wandb:         val_accuracy 0.90909
wandb:               val_f1 0.90909
wandb:             val_loss 0.46115
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142951-g20xkves
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_142951-g20xkves/logs
Standard experiment completed successfully: layer_10_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_10/question_type/results.json
Running complexity experiment for language ar, layer 10
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:31:05,829][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_10/complexity
experiment_name: layer_10_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 10
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:31:05,829][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:31:05,829][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:31:05,829][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:31:05,834][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:31:05,834][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:31:07,039][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:31:09,732][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:31:09,732][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:31:09,773][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:31:09,790][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:31:09,883][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:31:09,892][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:31:09,893][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:31:09,894][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:31:09,906][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:31:09,927][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:31:09,937][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:31:09,938][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:31:09,938][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:31:09,939][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:31:09,954][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:31:09,975][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:31:09,984][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:31:09,985][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:31:09,986][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:31:09,986][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:31:09,987][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:31:09,987][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:31:09,987][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:31:09,987][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:31:09,987][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:31:09,988][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:31:09,988][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:31:09,988][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:31:09,988][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:31:09,988][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:31:09,988][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:31:09,988][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:31:09,988][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:31:09,989][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:31:09,989][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:31:09,989][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:31:09,989][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:31:09,989][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:31:09,989][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:31:09,989][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:31:09,989][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:31:09,990][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:31:09,990][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:31:09,990][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:31:09,990][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:31:09,990][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:31:09,990][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:31:09,991][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:31:13,705][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:31:13,706][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:31:13,708][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:31:13,708][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 10
[2025-04-29 14:31:13,708][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:53,  1.16it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:15,  3.77it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:09,  6.34it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.72it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 10.81it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.52it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.89it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.96it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.76it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.34it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.77it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.05it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.28it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.44it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.51it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.57it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.63it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.73it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.73it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.76it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.76it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.82it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.83it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.47it/s]
[2025-04-29 14:31:19,946][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.2077
[2025-04-29 14:31:20,216][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0756, Metrics: {'mse': 0.07442639023065567, 'rmse': 0.27281200529055843, 'r2': -0.14715826511383057}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.91it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.66it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.19it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.52it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.31it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.81it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.14it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.48it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.64it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.77it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.75it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.15it/s]
[2025-04-29 14:31:24,458][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0855
[2025-04-29 14:31:24,756][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0959, Metrics: {'mse': 0.09494659304618835, 'rmse': 0.3081340504491322, 'r2': -0.46344268321990967}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  5.99it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.75it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.25it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.50it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.29it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.79it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.12it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.43it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.52it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.57it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.71it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.71it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.12it/s]
[2025-04-29 14:31:28,440][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0592
[2025-04-29 14:31:28,743][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0784, Metrics: {'mse': 0.07782407850027084, 'rmse': 0.2789696730834211, 'r2': -0.19952785968780518}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.81it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.56it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.09it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.40it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.69it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.03it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.23it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.72it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.70it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.13it/s]
[2025-04-29 14:31:32,423][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0499
[2025-04-29 14:31:32,701][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0695, Metrics: {'mse': 0.06909547001123428, 'rmse': 0.26286017197596573, 'r2': -0.06499099731445312}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.28it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.00it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.67it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.17it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.05it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.56it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.95it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.16it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.33it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.52it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.74it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.01it/s]
[2025-04-29 14:31:37,167][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0431
[2025-04-29 14:31:37,490][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0645, Metrics: {'mse': 0.0642058327794075, 'rmse': 0.25338869899703004, 'r2': 0.01037454605102539}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:10,  5.67it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.40it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.97it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.35it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.15it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.68it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 17.00it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  81%|████████  | 51/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:31:41,736][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0401
[2025-04-29 14:31:42,065][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0598, Metrics: {'mse': 0.05953184515237808, 'rmse': 0.2439914858194402, 'r2': 0.08241623640060425}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:10,  5.66it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.39it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.97it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.33it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.16it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.68it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 17.01it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.22it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.55it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.09it/s]
[2025-04-29 14:31:46,340][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0346
[2025-04-29 14:31:46,668][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0564, Metrics: {'mse': 0.056073009967803955, 'rmse': 0.23679740278939707, 'r2': 0.13572841882705688}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:12,  5.15it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 10.84it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.55it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.06it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 15.98it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.55it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.92it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.17it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.69it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 16.98it/s]
[2025-04-29 14:31:50,941][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0317
[2025-04-29 14:31:51,268][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0548, Metrics: {'mse': 0.054489389061927795, 'rmse': 0.23342962335986364, 'r2': 0.16013729572296143}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.21it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 10.88it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.57it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.07it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.00it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.54it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 16.92it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.15it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.33it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.64it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.68it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.68it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.69it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 9/10:  81%|████████  | 51/63 [00:03<00:00, 17.69it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.69it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.70it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 17.00it/s]
[2025-04-29 14:31:55,560][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0298
[2025-04-29 14:31:55,890][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0513, Metrics: {'mse': 0.050938233733177185, 'rmse': 0.22569500156888098, 'r2': 0.21487241983413696}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:12,  5.12it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 10.81it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.53it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.04it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 15.97it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.52it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 16.92it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.15it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.32it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.44it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.74it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 16.98it/s]
[2025-04-29 14:32:00,163][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0304
[2025-04-29 14:32:00,478][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0480, Metrics: {'mse': 0.0476047620177269, 'rmse': 0.21818515535601155, 'r2': 0.2662522792816162}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▄▃▃▂▁
wandb:     best_val_mse █▇▅▄▃▃▂▁
wandb:      best_val_r2 ▁▂▄▅▆▆▇█
wandb:    best_val_rmse █▇▆▄▃▃▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▅█▅▄▃▃▂▂▁▁
wandb:          val_mse ▅█▅▄▃▃▂▂▁▁
wandb:           val_r2 ▄▁▄▅▆▆▇▇██
wandb:         val_rmse ▅█▆▄▄▃▂▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04797
wandb:     best_val_mse 0.0476
wandb:      best_val_r2 0.26625
wandb:    best_val_rmse 0.21819
wandb:            epoch 10
wandb:   final_test_mse 0.05468
wandb:    final_test_r2 0.05737
wandb:  final_test_rmse 0.23383
wandb:  final_train_mse 0.01994
wandb:   final_train_r2 0.35031
wandb: final_train_rmse 0.14122
wandb:    final_val_mse 0.0476
wandb:     final_val_r2 0.26625
wandb:   final_val_rmse 0.21819
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03044
wandb:       train_time 45.46754
wandb:         val_loss 0.04797
wandb:          val_mse 0.0476
wandb:           val_r2 0.26625
wandb:         val_rmse 0.21819
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143105-louodjgk
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143105-louodjgk/logs
Standard experiment completed successfully: layer_10_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_10/complexity/results.json
Running question_type experiment for language ar, layer 11
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:32:18,900][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_11/question_type
experiment_name: layer_11_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:32:18,901][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:32:18,901][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:32:18,901][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:32:18,905][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:32:18,905][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:32:20,104][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:32:22,821][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:32:22,822][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:32:22,856][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:32:22,873][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:32:22,983][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:32:22,992][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:32:22,993][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:32:22,993][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:32:23,006][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:32:23,025][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:32:23,036][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:32:23,037][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:32:23,037][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:32:23,038][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:32:23,049][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:32:23,071][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:32:23,081][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:32:23,082][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:32:23,082][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:32:23,083][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:32:23,084][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:32:23,084][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:32:23,084][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:32:23,084][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:32:23,084][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:32:23,084][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:32:23,085][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:32:23,085][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:32:23,085][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:32:23,085][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:32:23,085][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:32:23,085][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:32:23,085][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:32:23,085][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:32:23,085][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:32:23,086][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:32:23,086][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:32:23,086][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:32:23,086][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:32:23,086][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:32:23,086][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:32:23,086][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:32:23,086][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:32:23,086][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:32:23,087][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:32:23,087][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:32:23,087][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:32:23,087][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:32:26,783][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:32:26,783][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:32:26,785][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:32:26,786][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 11
[2025-04-29 14:32:26,786][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:48,  1.27it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.06it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.73it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.14it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.20it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.86it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.18it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.16it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.88it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.43it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.84it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.13it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.31it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.46it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.52it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.59it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.74it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.79it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.80it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.81it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.82it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.81it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.82it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.69it/s]
[2025-04-29 14:32:33,032][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6834
[2025-04-29 14:32:33,316][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6684, Metrics: {'accuracy': 0.5681818181818182, 'f1': 0.6779661016949152}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.92it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.67it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.19it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.51it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.29it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.80it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.45it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.65it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.67it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.75it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.76it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.78it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.73it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.75it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.78it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.80it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.23it/s]
[2025-04-29 14:32:37,520][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6597
[2025-04-29 14:32:37,816][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6392, Metrics: {'accuracy': 0.6818181818181818, 'f1': 0.7407407407407407}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:11,  5.41it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.14it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.80it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.26it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.13it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.65it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.01it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.25it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.40it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.51it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.60it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.72it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.74it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.76it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.76it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.74it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 3/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.05it/s]
[2025-04-29 14:32:42,258][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6371
[2025-04-29 14:32:42,581][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6072, Metrics: {'accuracy': 0.7954545454545454, 'f1': 0.8163265306122449}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:12,  5.14it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 10.83it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.56it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.06it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.00it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.56it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 16.94it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.35it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.78it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  81%|████████  | 51/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.80it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.04it/s]
[2025-04-29 14:32:46,833][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6139
[2025-04-29 14:32:47,157][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5792, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.851063829787234}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.27it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 10.97it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.67it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.14it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.04it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.61it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.94it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.18it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.35it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.57it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.01it/s]
[2025-04-29 14:32:51,482][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5920
[2025-04-29 14:32:51,810][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5499, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.851063829787234}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:11,  5.19it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 10.89it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.55it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.05it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 15.94it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.52it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.90it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.13it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.31it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.50it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.54it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.59it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 16.93it/s]
[2025-04-29 14:32:56,080][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5774
[2025-04-29 14:32:56,409][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5207, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.851063829787234}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:12,  5.13it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 10.81it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.53it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.03it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 15.94it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.53it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.88it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.12it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.31it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.44it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.52it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.64it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.69it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.70it/s]Epoch 7/10:  81%|████████  | 51/63 [00:03<00:00, 17.70it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.69it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.69it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 16.99it/s]
[2025-04-29 14:33:00,702][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5566
[2025-04-29 14:33:01,027][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.4979, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.851063829787234}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:12,  5.11it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 10.79it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.51it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.04it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 15.96it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.54it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.91it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.16it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.69it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.00it/s]
[2025-04-29 14:33:05,302][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5419
[2025-04-29 14:33:05,631][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.4691, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8695652173913043}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.18it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 10.85it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.55it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.03it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 15.92it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.50it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 16.88it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.14it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.32it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.44it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 9/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 17.00it/s]
[2025-04-29 14:33:09,938][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5195
[2025-04-29 14:33:10,262][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.4482, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8888888888888888}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:11,  5.54it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 11.27it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.88it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.29it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 16.13it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.65it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 16.98it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.67it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 17.05it/s]
[2025-04-29 14:33:14,537][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.5073
[2025-04-29 14:33:14,865][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.4282, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▃▆▇▇▇▇▇██
wandb:          best_val_f1 ▁▃▅▆▆▆▆▇▇█
wandb:        best_val_loss █▇▆▅▅▄▃▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▆▅▄▄▃▂▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▃▆▇▇▇▇▇██
wandb:               val_f1 ▁▃▅▆▆▆▆▇▇█
wandb:             val_loss █▇▆▅▅▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.90909
wandb:          best_val_f1 0.90909
wandb:        best_val_loss 0.42815
wandb:                epoch 10
wandb:  final_test_accuracy 0.48052
wandb:        final_test_f1 0.52381
wandb: final_train_accuracy 0.97487
wandb:       final_train_f1 0.97542
wandb:   final_val_accuracy 0.90909
wandb:         final_val_f1 0.90909
wandb:        learning_rate 1e-05
wandb:           train_loss 0.5073
wandb:           train_time 46.71757
wandb:         val_accuracy 0.90909
wandb:               val_f1 0.90909
wandb:             val_loss 0.42815
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143218-smal0x1r
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143218-smal0x1r/logs
Standard experiment completed successfully: layer_11_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_11/question_type/results.json
Running complexity experiment for language ar, layer 11
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:33:33,335][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_11/complexity
experiment_name: layer_11_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:33:33,335][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:33:33,336][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:33:33,336][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:33:33,340][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:33:33,341][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:33:34,794][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:33:37,488][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:33:37,488][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:33:37,519][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:33:37,535][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:33:37,621][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:33:37,630][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:33:37,631][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:33:37,632][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:33:37,643][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:33:37,661][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:33:37,670][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:33:37,671][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:33:37,671][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:33:37,672][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:33:37,684][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:33:37,701][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:33:37,710][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:33:37,711][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:33:37,712][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:33:37,712][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:33:37,713][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:33:37,713][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:33:37,713][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:33:37,713][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:33:37,713][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:33:37,713][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:33:37,714][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:33:37,714][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:33:37,714][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:33:37,714][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:33:37,714][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:33:37,714][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:33:37,714][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:33:37,715][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:33:37,715][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:33:37,715][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:33:37,715][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:33:37,715][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:33:37,715][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:33:37,715][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:33:37,715][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:33:37,716][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:33:37,716][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:33:37,716][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:33:37,716][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:33:37,716][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:33:37,716][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:33:37,717][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:33:41,433][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:33:41,434][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:33:41,436][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:33:41,436][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 11
[2025-04-29 14:33:41,437][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:49,  1.26it/s]Epoch 1/10:   5%|▍         | 3/63 [00:00<00:14,  4.03it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:08,  6.69it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  9.09it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:04, 11.16it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.82it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 14.14it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 15.13it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.88it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.44it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:01<00:02, 16.82it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.11it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.30it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.46it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.57it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.65it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.77it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.74it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.75it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.75it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.79it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.81it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.68it/s]
[2025-04-29 14:33:47,476][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1645
[2025-04-29 14:33:47,750][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0831, Metrics: {'mse': 0.08177215605974197, 'rmse': 0.28595831175145436, 'r2': -0.2603808641433716}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:09,  6.28it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:04, 12.06it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.46it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.71it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.43it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.90it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.18it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.37it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.48it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.59it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.66it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.67it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.78it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.21it/s]
[2025-04-29 14:33:51,986][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0814
[2025-04-29 14:33:52,288][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0931, Metrics: {'mse': 0.09206391125917435, 'rmse': 0.30342035406210693, 'r2': -0.41901111602783203}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:10,  5.98it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 11.73it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 14.22it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.55it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.27it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.78it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 17.10it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.44it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.53it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.66it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.67it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.71it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.73it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  81%|████████  | 51/63 [00:02<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.77it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 17.12it/s]
[2025-04-29 14:33:55,970][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0633
[2025-04-29 14:33:56,273][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0752, Metrics: {'mse': 0.07431359589099884, 'rmse': 0.27260520151126766, 'r2': -0.14541971683502197}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.66it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.42it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.99it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.34it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.17it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.71it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.06it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.44it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.59it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.72it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.74it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.73it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.08it/s]
[2025-04-29 14:34:00,731][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0556
[2025-04-29 14:34:01,055][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0709, Metrics: {'mse': 0.0701315850019455, 'rmse': 0.2648236866330984, 'r2': -0.08096098899841309}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:10,  5.72it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.47it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 14.04it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.43it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.19it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.70it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 17.04it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.23it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.39it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.49it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 5/10:  81%|████████  | 51/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.69it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.06it/s]
[2025-04-29 14:34:05,299][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0490
[2025-04-29 14:34:05,626][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0661, Metrics: {'mse': 0.06547212600708008, 'rmse': 0.2558752156952292, 'r2': -0.009143233299255371}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:11,  5.33it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 11.05it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.71it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.17it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 16.04it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.58it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.94it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.17it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.02it/s]
[2025-04-29 14:34:09,933][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0491
[2025-04-29 14:34:10,257][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0615, Metrics: {'mse': 0.06093612685799599, 'rmse': 0.24685243944104743, 'r2': 0.060771644115448}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:11,  5.26it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 10.98it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.66it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.13it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.04it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.58it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 7/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.04it/s]
[2025-04-29 14:34:14,504][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0399
[2025-04-29 14:34:14,837][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0603, Metrics: {'mse': 0.059783611446619034, 'rmse': 0.24450687402733493, 'r2': 0.07853573560714722}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.22it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 10.91it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.61it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.07it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 15.97it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.53it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.91it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.15it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.32it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.52it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.74it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.74it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.74it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.75it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.01it/s]
[2025-04-29 14:34:19,126][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0352
[2025-04-29 14:34:19,438][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0577, Metrics: {'mse': 0.05719983950257301, 'rmse': 0.23916487932506522, 'r2': 0.11836022138595581}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.50it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.23it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.83it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.24it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.09it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.62it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 16.98it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.21it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.48it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.56it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.61it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.68it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.64it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.64it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.62it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.61it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.60it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.60it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.62it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.66it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.66it/s]Epoch 9/10:  81%|████████  | 51/63 [00:03<00:00, 17.64it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.63it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.63it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.64it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.65it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.67it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 17.10it/s]
[2025-04-29 14:34:23,676][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0332
[2025-04-29 14:34:24,007][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0530, Metrics: {'mse': 0.052506301552057266, 'rmse': 0.2291425354491332, 'r2': 0.19070327281951904}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:12,  5.08it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 10.77it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.49it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.00it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 15.91it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.50it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 16.89it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.14it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.32it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.44it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 16.95it/s]
[2025-04-29 14:34:28,317][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0330
[2025-04-29 14:34:28,629][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0497, Metrics: {'mse': 0.049211110919713974, 'rmse': 0.22183577466160406, 'r2': 0.2414931058883667}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▄▃▃▃▂▁
wandb:     best_val_mse █▆▅▄▄▃▃▂▁
wandb:      best_val_r2 ▁▃▄▅▅▆▆▇█
wandb:    best_val_rmse █▇▆▅▄▃▃▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▅▄▄▃▃▂▂▁
wandb:          val_mse ▆█▅▄▄▃▃▂▂▁
wandb:           val_r2 ▃▁▄▅▅▆▆▇▇█
wandb:         val_rmse ▇█▅▅▄▃▃▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0497
wandb:     best_val_mse 0.04921
wandb:      best_val_r2 0.24149
wandb:    best_val_rmse 0.22184
wandb:            epoch 10
wandb:   final_test_mse 0.0562
wandb:    final_test_r2 0.03112
wandb:  final_test_rmse 0.23707
wandb:  final_train_mse 0.01837
wandb:   final_train_r2 0.40154
wandb: final_train_rmse 0.13554
wandb:    final_val_mse 0.04921
wandb:     final_val_r2 0.24149
wandb:   final_val_rmse 0.22184
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03296
wandb:       train_time 46.01016
wandb:         val_loss 0.0497
wandb:          val_mse 0.04921
wandb:           val_r2 0.24149
wandb:         val_rmse 0.22184
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143333-xzdf9qf9
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143333-xzdf9qf9/logs
Standard experiment completed successfully: layer_11_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_11/complexity/results.json
Running question_type experiment for language ar, layer 12
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:34:47,042][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_12/question_type
experiment_name: layer_12_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 12
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:34:47,042][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:34:47,042][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:34:47,042][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:34:47,047][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 14:34:47,047][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:34:48,199][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:34:50,900][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:34:50,900][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:34:50,929][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:34:50,947][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:34:51,032][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:34:51,041][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:34:51,042][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:34:51,043][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:34:51,057][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:34:51,076][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:34:51,086][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:34:51,087][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:34:51,088][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:34:51,089][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:34:51,101][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:34:51,117][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:34:51,126][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:34:51,128][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:34:51,128][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:34:51,129][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:34:51,129][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:34:51,129][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:34:51,129][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:34:51,129][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:34:51,130][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 14:34:51,130][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 14:34:51,130][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:34:51,130][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:34:51,130][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:34:51,130][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:34:51,130][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:34:51,131][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:34:51,131][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 14:34:51,131][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 14:34:51,131][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:34:51,131][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:34:51,131][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:34:51,131][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:34:51,131][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:34:51,131][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:34:51,132][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 14:34:51,132][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 14:34:51,132][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:34:51,132][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:34:51,132][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:34:51,132][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:34:51,132][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:34:51,133][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:34:54,797][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:34:54,798][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:34:54,800][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:34:54,800][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 12
[2025-04-29 14:34:54,800][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<00:56,  1.10it/s]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:16,  3.60it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:09,  6.11it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.46it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:05, 10.55it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 12.29it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.69it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.79it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.62it/s]Epoch 1/10:  30%|███       | 19/63 [00:01<00:02, 16.24it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:02<00:02, 16.68it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 17.01it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.25it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.42it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.54it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.58it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.63it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.68it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:03<00:01, 17.71it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.74it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.76it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.79it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.76it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.76it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.77it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.79it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.81it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.82it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.32it/s]
[2025-04-29 14:35:00,843][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6933
[2025-04-29 14:35:01,129][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6904, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:10,  5.86it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.66it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 14.18it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.50it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.29it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.80it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.10it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.33it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.47it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.56it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.62it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.68it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.71it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.73it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.74it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.73it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.76it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.76it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.75it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.76it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.77it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.19it/s]
[2025-04-29 14:35:05,352][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6926
[2025-04-29 14:35:05,653][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6901, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:12,  4.95it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 10.62it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.41it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 14.95it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 15.89it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.51it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 16.92it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.20it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.57it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.61it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.67it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.78it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.76it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  81%|████████  | 51/63 [00:03<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 16.96it/s]
[2025-04-29 14:35:10,129][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6910
[2025-04-29 14:35:10,452][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6898, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:11,  5.27it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 10.94it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 13.64it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.13it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.05it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.56it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.18it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.32it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.44it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.75it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.76it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 4/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.78it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.79it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.80it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.81it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.05it/s]
[2025-04-29 14:35:14,686][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6894
[2025-04-29 14:35:14,993][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6894, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.27it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 10.98it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.67it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.15it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.05it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.62it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.98it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.24it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.38it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.60it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.68it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.74it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.11it/s]
[2025-04-29 14:35:19,301][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.6857
[2025-04-29 14:35:19,634][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.6890, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:12,  5.05it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 10.72it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.47it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.01it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 15.90it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.49it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.88it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.13it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.31it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.42it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.50it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.57it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.61it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.64it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.73it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.69it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 17.02it/s]
[2025-04-29 14:35:23,899][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.6881
[2025-04-29 14:35:24,232][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.6886, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:12,  5.12it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 10.80it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.52it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.01it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 15.93it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.52it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.89it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.15it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.33it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.50it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.57it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.73it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 7/10:  81%|████████  | 51/63 [00:03<00:00, 17.70it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 16.97it/s]
[2025-04-29 14:35:28,528][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.6858
[2025-04-29 14:35:28,853][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.6883, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.22it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 10.91it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.60it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.08it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 15.96it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.52it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.90it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.16it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.44it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.57it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.61it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.71it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.71it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.02it/s]
[2025-04-29 14:35:33,112][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.6837
[2025-04-29 14:35:33,444][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.6879, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.34it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.06it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.72it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.18it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.01it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.54it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 16.92it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.17it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.52it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.69it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.71it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 9/10:  81%|████████  | 51/63 [00:03<00:00, 17.71it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.69it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.71it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.70it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.71it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 16.98it/s]
[2025-04-29 14:35:37,746][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.6816
[2025-04-29 14:35:38,067][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.6875, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:11,  5.33it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 11.04it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.69it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.14it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 15.99it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.56it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.36it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.61it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.63it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.65it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.66it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.68it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.68it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.73it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 16.99it/s]
[2025-04-29 14:35:42,330][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.6847
[2025-04-29 14:35:42,660][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.6871, Metrics: {'accuracy': 0.5681818181818182, 'f1': 0.09523809523809523}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁▁▁▁▁█
wandb:          best_val_f1 ▁▁▁▁▁▁▁▁▁█
wandb:        best_val_loss █▇▇▆▅▄▄▃▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▇▆▃▅▄▂▁▃
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁█
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁█
wandb:             val_loss █▇▇▆▅▄▄▃▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.56818
wandb:          best_val_f1 0.09524
wandb:        best_val_loss 0.68711
wandb:                epoch 10
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.50553
wandb:       final_train_f1 0.01992
wandb:   final_val_accuracy 0.56818
wandb:         final_val_f1 0.09524
wandb:        learning_rate 1e-05
wandb:           train_loss 0.68466
wandb:           train_time 46.80303
wandb:         val_accuracy 0.56818
wandb:               val_f1 0.09524
wandb:             val_loss 0.68711
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143447-lo4u35yr
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143447-lo4u35yr/logs
Standard experiment completed successfully: layer_12_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_12/question_type/results.json
Running complexity experiment for language ar, layer 12
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:35:59,845][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_12/complexity
experiment_name: layer_12_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 12
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:35:59,845][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:35:59,845][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:35:59,845][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:35:59,850][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 14:35:59,850][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:36:01,293][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:36:03,983][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:36:03,983][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:36:04,012][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:36:04,029][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:36:04,091][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 14:36:04,100][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:36:04,101][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 14:36:04,102][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:36:04,113][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:36:04,130][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:36:04,138][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 14:36:04,140][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:36:04,140][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 14:36:04,141][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:36:04,152][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:36:04,169][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:36:04,178][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 14:36:04,179][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:36:04,179][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 14:36:04,180][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 14:36:04,180][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:36:04,181][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:36:04,181][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:36:04,181][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:36:04,181][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:36:04,181][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 14:36:04,181][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 14:36:04,181][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 14:36:04,182][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:36:04,182][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:36:04,182][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:36:04,182][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:36:04,182][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:36:04,182][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 14:36:04,182][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 14:36:04,182][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 14:36:04,183][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:36:04,183][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:36:04,183][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:36:04,183][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:36:04,183][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:36:04,183][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 14:36:04,183][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 14:36:04,184][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 14:36:04,184][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 14:36:04,184][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:36:04,184][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:36:04,184][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:36:07,780][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:36:07,781][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:36:07,783][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:36:07,783][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 12
[2025-04-29 14:36:07,783][__main__][INFO] - Successfully created model for ar
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:00<01:01,  1.01it/s]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:17,  3.34it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:10,  5.73it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:06,  8.05it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:05, 10.16it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:01<00:04, 11.96it/s]Epoch 1/10:  21%|██        | 13/63 [00:01<00:03, 13.44it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:01<00:03, 14.59it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:01<00:02, 15.46it/s]Epoch 1/10:  30%|███       | 19/63 [00:02<00:02, 16.13it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:02<00:02, 16.60it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:02<00:02, 16.94it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:02<00:02, 17.21it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:02<00:02, 17.37it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:02<00:01, 17.51it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:02<00:01, 17.60it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:03<00:01, 17.72it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:03<00:01, 17.75it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:03<00:01, 17.77it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:03<00:01, 17.79it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:03<00:01, 17.78it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  81%|████████  | 51/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.78it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:04<00:00, 17.77it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:04<00:00, 17.79it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:04<00:00, 17.81it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:04<00:00, 17.79it/s]Epoch 1/10: 100%|██████████| 63/63 [00:04<00:00, 14.10it/s]
[2025-04-29 14:36:13,833][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.2303
[2025-04-29 14:36:14,121][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2258, Metrics: {'mse': 0.22809863090515137, 'rmse': 0.4775967241356994, 'r2': -2.515758514404297}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:11,  5.53it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:05, 11.26it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:04, 13.89it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:03, 15.34it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:03, 16.20it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:00<00:03, 16.71it/s]Epoch 2/10:  21%|██        | 13/63 [00:00<00:02, 17.07it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:00<00:02, 17.28it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:02, 17.46it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:02, 17.54it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:01<00:02, 17.63it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:01<00:02, 17.70it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:01<00:02, 17.73it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:01<00:02, 17.76it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:01<00:01, 17.77it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:01<00:01, 17.77it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.79it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.80it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.79it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.78it/s]Epoch 2/10:  81%|████████  | 51/63 [00:02<00:00, 17.79it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.80it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:03<00:00, 17.82it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.81it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.82it/s]Epoch 2/10: 100%|██████████| 63/63 [00:03<00:00, 17.14it/s]
[2025-04-29 14:36:18,373][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.2092
[2025-04-29 14:36:18,666][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2054, Metrics: {'mse': 0.20750455558300018, 'rmse': 0.4555266793317381, 'r2': -2.1983354091644287}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:12,  5.14it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:05, 10.84it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:04, 13.58it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:03, 15.08it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:03, 16.00it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:00<00:03, 16.58it/s]Epoch 3/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:02, 17.50it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:01<00:02, 17.58it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:01<00:02, 17.65it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:01<00:02, 17.69it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:01<00:02, 17.70it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:01<00:01, 17.72it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.75it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.73it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.75it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.76it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.77it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.77it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 3/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.76it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 63/63 [00:03<00:00, 16.99it/s]
[2025-04-29 14:36:23,181][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1846
[2025-04-29 14:36:23,508][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1866, Metrics: {'mse': 0.18848638236522675, 'rmse': 0.434150184112856, 'r2': -1.9052019119262695}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:10,  5.76it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:05, 11.53it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:04, 14.09it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:03, 15.46it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:03, 16.26it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:00<00:03, 16.76it/s]Epoch 4/10:  21%|██        | 13/63 [00:00<00:02, 17.09it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:00<00:02, 17.31it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:02, 17.42it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:02, 17.51it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:01<00:02, 17.58it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:01<00:02, 17.64it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:01<00:02, 17.65it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:01<00:02, 17.69it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:01<00:01, 17.71it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:01<00:01, 17.73it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.73it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.74it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  81%|████████  | 51/63 [00:02<00:00, 17.75it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:03<00:00, 17.74it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.72it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.71it/s]Epoch 4/10: 100%|██████████| 63/63 [00:03<00:00, 17.19it/s]
[2025-04-29 14:36:27,730][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1716
[2025-04-29 14:36:28,050][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1710, Metrics: {'mse': 0.1726812869310379, 'rmse': 0.4155493796542571, 'r2': -1.661592960357666}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:11,  5.35it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:05, 11.04it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:04, 13.72it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:03, 15.18it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:03, 16.06it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:00<00:03, 16.58it/s]Epoch 5/10:  21%|██        | 13/63 [00:00<00:02, 16.94it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:02, 17.37it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.71it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.73it/s]Epoch 5/10:  81%|████████  | 51/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.70it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 5/10: 100%|██████████| 63/63 [00:03<00:00, 17.01it/s]
[2025-04-29 14:36:32,335][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1544
[2025-04-29 14:36:32,662][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1569, Metrics: {'mse': 0.15852145850658417, 'rmse': 0.3981475335934962, 'r2': -1.443342924118042}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:12,  5.10it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:05, 10.78it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:04, 13.51it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:03, 15.04it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:03, 15.96it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:00<00:03, 16.53it/s]Epoch 6/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:02, 17.34it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:02, 17.46it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:01<00:02, 17.54it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:01<00:02, 17.63it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:02<00:01, 17.67it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.69it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.70it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.70it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.71it/s]Epoch 6/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.70it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:03<00:00, 17.72it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.73it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.74it/s]Epoch 6/10: 100%|██████████| 63/63 [00:03<00:00, 16.86it/s]
[2025-04-29 14:36:36,973][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1430
[2025-04-29 14:36:37,301][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1440, Metrics: {'mse': 0.14541108906269073, 'rmse': 0.3813280596319798, 'r2': -1.2412683963775635}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:11,  5.35it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:05, 11.06it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:04, 13.73it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:03, 15.15it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:03, 16.02it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:00<00:03, 16.59it/s]Epoch 7/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:00<00:02, 17.18it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:02, 17.35it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:02, 17.47it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:01<00:02, 17.52it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:01<00:02, 17.64it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:01<00:01, 17.68it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:01<00:01, 17.69it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.70it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.71it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.72it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.73it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 7/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.72it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:03<00:00, 17.73it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.74it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.75it/s]Epoch 7/10: 100%|██████████| 63/63 [00:03<00:00, 17.00it/s]
[2025-04-29 14:36:41,596][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.1285
[2025-04-29 14:36:41,903][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1324, Metrics: {'mse': 0.13371588289737701, 'rmse': 0.36567182404087, 'r2': -1.0610063076019287}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:11,  5.35it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:05, 11.06it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:04, 13.72it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:03, 15.17it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:03, 16.02it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:00<00:03, 16.58it/s]Epoch 8/10:  21%|██        | 13/63 [00:00<00:02, 16.95it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:00<00:02, 17.19it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:02, 17.33it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:01<00:02, 17.59it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:01<00:02, 17.66it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:01<00:01, 17.66it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:01<00:01, 17.67it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.68it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.70it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.72it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.69it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.71it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.72it/s]Epoch 8/10:  81%|████████  | 51/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.72it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:03<00:00, 17.70it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.71it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.72it/s]Epoch 8/10: 100%|██████████| 63/63 [00:03<00:00, 17.02it/s]
[2025-04-29 14:36:46,187][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.1167
[2025-04-29 14:36:46,590][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1222, Metrics: {'mse': 0.12340044975280762, 'rmse': 0.35128400156114087, 'r2': -0.9020113945007324}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:11,  5.38it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:05, 11.11it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:04, 13.75it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:03, 15.20it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:03, 16.04it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:00<00:03, 16.60it/s]Epoch 9/10:  21%|██        | 13/63 [00:00<00:02, 16.93it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:00<00:02, 17.16it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:02, 17.33it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:02, 17.45it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:01<00:02, 17.53it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:01<00:02, 17.58it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:01<00:02, 17.62it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:01<00:02, 17.65it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:01<00:01, 17.67it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:01<00:01, 17.68it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.69it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.67it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.68it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.67it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.69it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.66it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.64it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.64it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.65it/s]Epoch 9/10:  81%|████████  | 51/63 [00:03<00:00, 17.64it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.66it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.66it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:03<00:00, 17.66it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.66it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.70it/s]Epoch 9/10: 100%|██████████| 63/63 [00:03<00:00, 16.98it/s]
[2025-04-29 14:36:50,937][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.1043
[2025-04-29 14:36:51,270][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1130, Metrics: {'mse': 0.11408672481775284, 'rmse': 0.33776726427786463, 'r2': -0.7584558725357056}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:11,  5.41it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:05, 11.13it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:04, 13.76it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:03, 15.19it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:03, 16.04it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:00<00:03, 16.55it/s]Epoch 10/10:  21%|██        | 13/63 [00:00<00:02, 16.89it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:00<00:02, 17.12it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:02, 17.28it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:02, 17.38it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:01<00:02, 17.46it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:01<00:02, 17.51it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:01<00:02, 17.55it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:01<00:02, 17.57it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:01<00:01, 17.59it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:01<00:01, 17.60it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:01<00:01, 17.62it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:02<00:01, 17.62it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:02<00:01, 17.61it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:02<00:01, 17.62it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:02<00:01, 17.63it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:02<00:01, 17.63it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:02<00:01, 17.63it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:02<00:00, 17.63it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:02<00:00, 17.62it/s]Epoch 10/10:  81%|████████  | 51/63 [00:03<00:00, 17.63it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:03<00:00, 17.64it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:03<00:00, 17.62it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:03<00:00, 17.64it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:03<00:00, 17.64it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:03<00:00, 17.64it/s]Epoch 10/10: 100%|██████████| 63/63 [00:03<00:00, 16.92it/s]
[2025-04-29 14:36:55,567][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0972
[2025-04-29 14:36:55,881][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1056, Metrics: {'mse': 0.10653552412986755, 'rmse': 0.32639780043662603, 'r2': -0.6420667171478271}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▅▄▃▃▂▁▁
wandb:     best_val_mse █▇▆▅▄▃▃▂▁▁
wandb:      best_val_r2 ▁▂▃▄▅▆▆▇██
wandb:    best_val_rmse █▇▆▅▄▄▃▂▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▇▆▅▄▃▃▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▆▅▄▃▃▂▁▁
wandb:          val_mse █▇▆▅▄▃▃▂▁▁
wandb:           val_r2 ▁▂▃▄▅▆▆▇██
wandb:         val_rmse █▇▆▅▄▄▃▂▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.10558
wandb:     best_val_mse 0.10654
wandb:      best_val_r2 -0.64207
wandb:    best_val_rmse 0.3264
wandb:            epoch 10
wandb:   final_test_mse 0.11551
wandb:    final_test_r2 -0.99135
wandb:  final_test_rmse 0.33987
wandb:  final_train_mse 0.08636
wandb:   final_train_r2 -1.81331
wandb: final_train_rmse 0.29387
wandb:    final_val_mse 0.10654
wandb:     final_val_r2 -0.64207
wandb:   final_val_rmse 0.3264
wandb:    learning_rate 1e-05
wandb:       train_loss 0.09722
wandb:       train_time 47.10005
wandb:         val_loss 0.10558
wandb:          val_mse 0.10654
wandb:           val_r2 -0.64207
wandb:         val_rmse 0.3264
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143559-x651jc5a
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143559-x651jc5a/logs
Standard experiment completed successfully: layer_12_complexity_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_12/complexity/results.json
Running question_type experiment for language en, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:37:13,349][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_1/question_type
experiment_name: layer_1_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:37:13,349][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:37:13,350][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:37:13,350][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:37:13,354][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 14:37:13,355][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:37:14,828][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:37:17,622][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:37:17,623][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:37:17,700][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:37:17,719][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:37:17,803][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:37:17,814][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:37:17,815][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:37:17,816][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:37:17,827][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:37:17,844][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:37:17,853][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:37:17,855][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:37:17,855][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:37:17,855][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:37:17,866][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:37:17,886][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:37:17,894][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:37:17,896][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:37:17,896][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:37:17,897][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:37:17,898][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:37:17,898][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:37:17,898][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:37:17,898][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:37:17,898][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 14:37:17,898][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 14:37:17,898][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:37:17,899][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:37:17,899][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:37:17,899][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:37:17,899][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:37:17,899][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:37:17,899][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 14:37:17,899][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 14:37:17,899][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:37:17,900][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:37:17,900][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:37:17,900][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:37:17,900][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:37:17,900][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:37:17,900][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 14:37:17,900][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 14:37:17,900][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:37:17,900][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:37:17,901][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:37:17,901][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:37:17,901][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:37:17,901][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:37:21,604][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:37:21,604][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:37:21,607][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:37:21,607][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 1
[2025-04-29 14:37:21,607][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:58,  1.26it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:17,  4.02it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.68it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.10it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.15it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 12.83it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.15it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.16it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.90it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.44it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.83it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.10it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.31it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.45it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.55it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.60it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.66it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.81it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.77it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.06it/s]
[2025-04-29 14:37:28,133][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6955
[2025-04-29 14:37:28,531][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.91it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.68it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.18it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.51it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.30it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.79it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.31it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.45it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.56it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.64it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.65it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.69it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.75it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.76it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.79it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.75it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.74it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.72it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.76it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.21it/s]
[2025-04-29 14:37:33,447][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6976
[2025-04-29 14:37:33,853][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:12,  6.04it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.79it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.26it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.56it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.33it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.80it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.09it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.42it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.52it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.60it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.63it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.67it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.69it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.75it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.76it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.74it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.72it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.75it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.20it/s]
[2025-04-29 14:37:38,216][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6934
[2025-04-29 14:37:38,626][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:12,  6.13it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.88it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.31it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.60it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.31it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.78it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.09it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.29it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.43it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.52it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.58it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.63it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.71it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.71it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.73it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.61it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.63it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.66it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.68it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.17it/s]
[2025-04-29 14:37:42,998][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6941
[2025-04-29 14:37:43,401][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 14:37:43,402][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▅█▁▂
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▂█▇
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69288
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69406
wandb:           train_time 20.25218
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69296
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143713-e0h4ho3q
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143713-e0h4ho3q/logs
Standard experiment completed successfully: layer_1_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_1/question_type/results.json
Running complexity experiment for language en, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:37:59,584][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_1/complexity
experiment_name: layer_1_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:37:59,584][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:37:59,584][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:37:59,584][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:37:59,589][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 14:37:59,589][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:38:00,715][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:38:03,415][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:38:03,416][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:38:03,429][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:03,447][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:03,495][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:38:03,506][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:38:03,507][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:38:03,507][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:38:03,518][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:03,536][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:03,545][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:38:03,546][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:38:03,546][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:38:03,547][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:38:03,558][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:03,575][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:03,583][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:38:03,585][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:38:03,585][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:38:03,586][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:38:03,586][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:38:03,586][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:38:03,586][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:38:03,587][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:38:03,587][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:38:03,587][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 14:38:03,587][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:38:03,587][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 14:38:03,587][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:38:03,588][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:38:03,588][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:38:03,588][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:38:03,588][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:38:03,588][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 14:38:03,588][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:38:03,588][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 14:38:03,588][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 14:38:03,589][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 14:38:03,589][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 14:38:03,589][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 14:38:03,589][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 14:38:03,589][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 14:38:03,589][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:38:03,589][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 14:38:03,589][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:38:03,589][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:38:03,590][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:38:03,590][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:38:06,992][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:38:06,992][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:38:06,995][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 14:38:06,995][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 1
[2025-04-29 14:38:06,995][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<00:58,  1.26it/s]Epoch 1/10:   4%|▍         | 3/75 [00:00<00:17,  4.05it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:10,  6.72it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:07,  9.14it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:05, 11.18it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:04, 12.87it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 14.16it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:03, 15.16it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.91it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.42it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:01<00:03, 16.82it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 17.11it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.27it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.42it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.54it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.60it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.65it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.74it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.75it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.79it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.75it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.80it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.82it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.83it/s]Epoch 1/10: 100%|██████████| 75/75 [00:04<00:00, 15.05it/s]
[2025-04-29 14:38:13,390][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1279
[2025-04-29 14:38:13,783][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0473, Metrics: {'mse': 0.050655629485845566, 'rmse': 0.22506805523184664, 'r2': -0.2103743553161621}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.93it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.68it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.19it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.53it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.32it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.80it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.10it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.33it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.44it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.54it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.62it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.67it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.74it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.73it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.73it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.75it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.74it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.77it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.76it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.75it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.79it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.26it/s]
[2025-04-29 14:38:18,686][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0487
[2025-04-29 14:38:19,078][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0427, Metrics: {'mse': 0.04314103722572327, 'rmse': 0.20770420608577783, 'r2': -0.030819416046142578}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  4.94it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 10.59it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:05, 13.38it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 14.93it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 15.89it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.51it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 16.89it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.18it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.35it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.45it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:02<00:02, 17.71it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.72it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.75it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.76it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.77it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.75it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.00it/s]
[2025-04-29 14:38:24,516][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0385
[2025-04-29 14:38:24,966][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0469, Metrics: {'mse': 0.04666321724653244, 'rmse': 0.2160167059431572, 'r2': -0.11497890949249268}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.45it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.18it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:05, 13.83it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.26it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.12it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.63it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 16.98it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.36it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.48it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.55it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.60it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.69it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.73it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.70it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.73it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.72it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.05it/s]
[2025-04-29 14:38:29,367][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0376
[2025-04-29 14:38:29,811][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0481, Metrics: {'mse': 0.04766707122325897, 'rmse': 0.21832789840801145, 'r2': -0.13896512985229492}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.14it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:06, 10.84it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:05, 13.55it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:04, 15.04it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:04, 15.96it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:00<00:03, 16.51it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:00<00:03, 16.90it/s]Epoch 5/10:  20%|██        | 15/75 [00:00<00:03, 17.17it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:03, 17.31it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:03, 17.44it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:01<00:03, 17.53it/s]Epoch 5/10:  31%|███       | 23/75 [00:01<00:02, 17.59it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:01<00:02, 17.63it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:01<00:02, 17.70it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:02<00:02, 17.72it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.70it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  60%|██████    | 45/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.72it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.72it/s]Epoch 5/10:  71%|███████   | 53/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.73it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.69it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.69it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.72it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.71it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.71it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.73it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.73it/s]Epoch 5/10: 100%|██████████| 75/75 [00:04<00:00, 16.99it/s]
[2025-04-29 14:38:34,229][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0364
[2025-04-29 14:38:34,691][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0491, Metrics: {'mse': 0.048533402383327484, 'rmse': 0.2203029786074793, 'r2': -0.15966546535491943}
[2025-04-29 14:38:34,692][src.training.lm_trainer][INFO] - Early stopping at epoch 5
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁
wandb:       train_loss █▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆▁▆▇█
wandb:          val_mse █▁▄▅▆
wandb:           val_r2 ▁█▅▄▃
wandb:         val_rmse █▁▄▅▆
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04267
wandb:     best_val_mse 0.04314
wandb:      best_val_r2 -0.03082
wandb:    best_val_rmse 0.2077
wandb:            epoch 5
wandb:   final_test_mse 0.04052
wandb:    final_test_r2 -0.05136
wandb:  final_test_rmse 0.20129
wandb:  final_train_mse 0.02815
wandb:   final_train_r2 -0.04919
wandb: final_train_rmse 0.16777
wandb:    final_val_mse 0.04314
wandb:     final_val_r2 -0.03082
wandb:   final_val_rmse 0.2077
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03643
wandb:       train_time 26.28866
wandb:         val_loss 0.0491
wandb:          val_mse 0.04853
wandb:           val_r2 -0.15967
wandb:         val_rmse 0.2203
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143759-u8h2xmvt
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143759-u8h2xmvt/logs
Standard experiment completed successfully: layer_1_complexity_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_1/complexity/results.json
Running question_type experiment for language en, layer 2
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:38:52,577][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_2/question_type
experiment_name: layer_2_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 14:38:52,577][__main__][INFO] - Normalized task: question_type
[2025-04-29 14:38:52,577][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 14:38:52,577][__main__][INFO] - Determined Task Type: classification
[2025-04-29 14:38:52,581][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 14:38:52,582][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:38:53,693][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 14:38:56,388][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 14:38:56,389][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:38:56,419][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:56,435][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:56,492][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 14:38:56,503][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:38:56,504][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 14:38:56,505][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:38:56,516][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:56,533][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:56,542][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 14:38:56,543][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:38:56,544][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 14:38:56,544][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 14:38:56,557][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:56,574][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 14:38:56,583][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 14:38:56,584][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 14:38:56,585][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 14:38:56,585][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 14:38:56,586][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:38:56,586][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:38:56,586][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:38:56,586][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:38:56,586][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 14:38:56,587][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 14:38:56,587][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 14:38:56,587][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 14:38:56,587][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:38:56,587][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:38:56,587][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:38:56,587][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:38:56,587][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 14:38:56,587][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 14:38:56,588][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 14:38:56,588][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:38:56,588][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 14:38:56,588][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 14:38:56,588][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 14:38:56,588][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 14:38:56,588][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 14:38:56,588][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 14:38:56,588][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 14:38:56,589][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 14:38:56,589][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 14:38:56,589][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 14:38:56,589][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 14:38:56,589][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 14:39:00,263][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 14:39:00,264][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 14:39:00,266][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 14:39:00,267][src.models.model_factory][INFO] - layer-wise probing: True, layer index: 2
[2025-04-29 14:39:00,267][__main__][INFO] - Successfully created model for en
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:09,  1.07it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:20,  3.52it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:11,  5.99it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:08,  8.34it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:06, 10.43it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:05, 12.19it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:01<00:04, 13.62it/s]Epoch 1/10:  20%|██        | 15/75 [00:01<00:04, 14.74it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:01<00:03, 15.56it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:01<00:03, 16.19it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:03, 16.65it/s]Epoch 1/10:  31%|███       | 23/75 [00:02<00:03, 16.99it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:02<00:02, 17.20it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:02<00:02, 17.38it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:02<00:02, 17.51it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:02<00:02, 17.60it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:02<00:02, 17.67it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:02<00:02, 17.71it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:03<00:02, 17.73it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:03<00:01, 17.76it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:03<00:01, 17.78it/s]Epoch 1/10:  60%|██████    | 45/75 [00:03<00:01, 17.79it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:03<00:01, 17.80it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  71%|███████   | 53/75 [00:03<00:01, 17.81it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.77it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:04<00:01, 17.76it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:04<00:00, 17.75it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:04<00:00, 17.76it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.77it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.78it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.80it/s]Epoch 1/10: 100%|██████████| 75/75 [00:05<00:00, 14.65it/s]
[2025-04-29 14:39:07,003][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6958
[2025-04-29 14:39:07,400][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.78it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:06, 11.53it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:04, 14.10it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:04, 15.46it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:04, 16.25it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:00<00:03, 16.77it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:00<00:03, 17.09it/s]Epoch 2/10:  20%|██        | 15/75 [00:00<00:03, 17.30it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:03, 17.41it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:01<00:03, 17.60it/s]Epoch 2/10:  31%|███       | 23/75 [00:01<00:02, 17.66it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:01<00:02, 17.71it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:01<00:02, 17.70it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:01<00:02, 17.73it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:02<00:02, 17.74it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:02<00:02, 17.72it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.75it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  60%|██████    | 45/75 [00:02<00:01, 17.77it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.76it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.78it/s]Epoch 2/10:  71%|███████   | 53/75 [00:03<00:01, 17.79it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.78it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.77it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.78it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.79it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.80it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.77it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 2/10: 100%|██████████| 75/75 [00:04<00:00, 17.15it/s]
[2025-04-29 14:39:12,339][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6976
[2025-04-29 14:39:12,745][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:12,  6.08it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:06, 11.85it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:04, 14.31it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:04, 15.61it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:04, 16.35it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:00<00:03, 16.83it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:00<00:03, 17.14it/s]Epoch 3/10:  20%|██        | 15/75 [00:00<00:03, 17.31it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:03, 17.43it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:03, 17.54it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:01<00:03, 17.62it/s]Epoch 3/10:  31%|███       | 23/75 [00:01<00:02, 17.64it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:01<00:02, 17.68it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:01<00:02, 17.71it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:01<00:02, 17.72it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:02<00:02, 17.75it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:02<00:02, 17.77it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.74it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  60%|██████    | 45/75 [00:02<00:01, 17.74it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.73it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.75it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.76it/s]Epoch 3/10:  71%|███████   | 53/75 [00:03<00:01, 17.75it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.76it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.73it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.73it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:03<00:00, 17.74it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.76it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.78it/s]Epoch 3/10: 100%|██████████| 75/75 [00:04<00:00, 17.20it/s]
[2025-04-29 14:39:17,107][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6937
[2025-04-29 14:39:17,514][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:11,  6.22it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:06, 11.96it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:04, 14.38it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:04, 15.61it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:04, 16.33it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:00<00:03, 16.80it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:00<00:03, 17.08it/s]Epoch 4/10:  20%|██        | 15/75 [00:00<00:03, 17.28it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:03, 17.42it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:03, 17.51it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:01<00:03, 17.56it/s]Epoch 4/10:  31%|███       | 23/75 [00:01<00:02, 17.62it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:01<00:02, 17.64it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:01<00:02, 17.66it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:01<00:02, 17.68it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:02<00:02, 17.70it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:02<00:02, 17.68it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:02<00:02, 17.69it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:02<00:01, 17.71it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  60%|██████    | 45/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:02<00:01, 17.72it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:02<00:01, 17.69it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:02<00:01, 17.70it/s]Epoch 4/10:  71%|███████   | 53/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:03<00:01, 17.71it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:03<00:01, 17.72it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:03<00:00, 17.71it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:03<00:00, 17.70it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:04<00:00, 17.70it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:04<00:00, 17.72it/s]Epoch 4/10: 100%|██████████| 75/75 [00:04<00:00, 17.18it/s]
[2025-04-29 14:39:21,881][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.6943
[2025-04-29 14:39:22,306][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.6931, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 14:39:22,307][src.training.lm_trainer][INFO] - Early stopping at epoch 4
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss ▅█▁▂
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▂▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69296
wandb:                epoch 4
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69426
wandb:           train_time 20.42568
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69306
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143852-cgmjt4fs
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_143852-cgmjt4fs/logs
Standard experiment completed successfully: layer_2_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_2/question_type/results.json
Running complexity experiment for language en, layer 2
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 14:39:48,582][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_2/complexity
experiment_name: layer_2_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 2
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 14:39:48,582][__main__][INFO] - Normalized task: complexity
[2025-04-29 14:39:48,582][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 14:39:48,582][__main__][INFO] - Determined Task Type: regression
[2025-04-29 14:39:48,587][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 14:39:48,587][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 14:39:49,597][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
