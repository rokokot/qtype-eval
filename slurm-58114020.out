SLURM_JOB_ID: 58114020
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: layerwise_probing
SLURM_CLUSTER_NAME: genius
SLURM_JOB_PARTITION: gpu_p100
SLURM_NNODES: 1
SLURM_NODELIST: r24g37
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 2
Date: Tue Apr 29 20:32:07 CEST 2025
Walltime: 00-02:00:00
========================================================================
Environment variables:
PYTHONPATH=:/data/leuven/371/vsc37132/qtype-eval:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval
HF_HOME=/data/leuven/371/vsc37132/qtype-eval/data/cache
GPU information:
Tue Apr 29 20:32:12 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla P100-SXM2-16GB           Off |   00000000:89:00.0 Off |                    0 |
| N/A   43C    P0             35W /  300W |       0MiB /  16384MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Python executable: /data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/bin/python
PyTorch CUDA available: True
Running test experiment to check for issues...
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:32:30,509][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/test_output
experiment_name: test_layerwise_probing
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: disabled
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 2
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  debug_mode: true
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 20:32:30,509][__main__][INFO] - Normalized task: question_type
[2025-04-29 20:32:30,509][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 20:32:30,509][__main__][INFO] - Determined Task Type: classification
[2025-04-29 20:32:30,513][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 20:32:30,514][__main__][INFO] - Processing language: en
[2025-04-29 20:32:30,514][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:32:33,302][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:32:33,303][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:32:33,456][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:32:33,535][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:32:33,705][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 20:32:33,717][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:32:33,718][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 20:32:33,720][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:32:33,764][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:32:33,803][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:32:33,817][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 20:32:33,818][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:32:33,819][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 20:32:33,820][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:32:33,847][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:32:33,885][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:32:33,902][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 20:32:33,903][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:32:33,903][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 20:32:33,905][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 20:32:33,905][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:32:33,905][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:32:33,905][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:32:33,906][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:32:33,906][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 20:32:33,906][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 20:32:33,906][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 20:32:33,906][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 20:32:33,906][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:32:33,906][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:32:33,906][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:32:33,906][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:32:33,906][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 20:32:33,906][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 20:32:33,906][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 20:32:33,906][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:32:33,906][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:32:33,906][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:32:33,906][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:32:33,907][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:32:33,907][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 20:32:33,907][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 20:32:33,907][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 20:32:33,907][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:32:33,907][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 20:32:33,907][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:32:33,907][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:32:33,907][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:32:40,031][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:32:40,033][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 20:32:40,034][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 20:32:40,034][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=6, freeze_model=True, finetune=False
[2025-04-29 20:32:40,035][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:32:40,035][__main__][INFO] - Successfully created model for en
Epoch 1/2: [Epoch 1/2: [                              ] 1/75 batches, loss: 0.6463Epoch 1/2: [                              ] 2/75 batches, loss: 0.6730Epoch 1/2: [=                             ] 3/75 batches, loss: 0.6900Epoch 1/2: [=                             ] 4/75 batches, loss: 0.6961Epoch 1/2: [==                            ] 5/75 batches, loss: 0.6954Epoch 1/2: [==                            ] 6/75 batches, loss: 0.6964Epoch 1/2: [==                            ] 7/75 batches, loss: 0.7010Epoch 1/2: [===                           ] 8/75 batches, loss: 0.7002Epoch 1/2: [===                           ] 9/75 batches, loss: 0.6976Epoch 1/2: [====                          ] 10/75 batches, loss: 0.6971Epoch 1/2: [====                          ] 11/75 batches, loss: 0.7018Epoch 1/2: [====                          ] 12/75 batches, loss: 0.6999Epoch 1/2: [=====                         ] 13/75 batches, loss: 0.7009Epoch 1/2: [=====                         ] 14/75 batches, loss: 0.7017Epoch 1/2: [======                        ] 15/75 batches, loss: 0.7010Epoch 1/2: [======                        ] 16/75 batches, loss: 0.7020Epoch 1/2: [======                        ] 17/75 batches, loss: 0.7022Epoch 1/2: [=======                       ] 18/75 batches, loss: 0.7021Epoch 1/2: [=======                       ] 19/75 batches, loss: 0.7017Epoch 1/2: [========                      ] 20/75 batches, loss: 0.7016Epoch 1/2: [========                      ] 21/75 batches, loss: 0.7005Epoch 1/2: [========                      ] 22/75 batches, loss: 0.6985Epoch 1/2: [=========                     ] 23/75 batches, loss: 0.6998Epoch 1/2: [=========                     ] 24/75 batches, loss: 0.6983Epoch 1/2: [==========                    ] 25/75 batches, loss: 0.6994Epoch 1/2: [==========                    ] 26/75 batches, loss: 0.6993Epoch 1/2: [==========                    ] 27/75 batches, loss: 0.6995Epoch 1/2: [===========                   ] 28/75 batches, loss: 0.6988Epoch 1/2: [===========                   ] 29/75 batches, loss: 0.6991Epoch 1/2: [============                  ] 30/75 batches, loss: 0.6987Epoch 1/2: [============                  ] 31/75 batches, loss: 0.6983Epoch 1/2: [============                  ] 32/75 batches, loss: 0.6991Epoch 1/2: [=============                 ] 33/75 batches, loss: 0.6989Epoch 1/2: [=============                 ] 34/75 batches, loss: 0.6980Epoch 1/2: [==============                ] 35/75 batches, loss: 0.6978Epoch 1/2: [==============                ] 36/75 batches, loss: 0.6963Epoch 1/2: [==============                ] 37/75 batches, loss: 0.6958Epoch 1/2: [===============               ] 38/75 batches, loss: 0.6953Epoch 1/2: [===============               ] 39/75 batches, loss: 0.6950Epoch 1/2: [================              ] 40/75 batches, loss: 0.6956Epoch 1/2: [================              ] 41/75 batches, loss: 0.6968Epoch 1/2: [================              ] 42/75 batches, loss: 0.6978Epoch 1/2: [=================             ] 43/75 batches, loss: 0.6974Epoch 1/2: [=================             ] 44/75 batches, loss: 0.6974Epoch 1/2: [==================            ] 45/75 batches, loss: 0.6970Epoch 1/2: [==================            ] 46/75 batches, loss: 0.6966Epoch 1/2: [==================            ] 47/75 batches, loss: 0.6966Epoch 1/2: [===================           ] 48/75 batches, loss: 0.6964Epoch 1/2: [===================           ] 49/75 batches, loss: 0.6958Epoch 1/2: [====================          ] 50/75 batches, loss: 0.6953Epoch 1/2: [====================          ] 51/75 batches, loss: 0.6952Epoch 1/2: [====================          ] 52/75 batches, loss: 0.6952Epoch 1/2: [=====================         ] 53/75 batches, loss: 0.6948Epoch 1/2: [=====================         ] 54/75 batches, loss: 0.6950Epoch 1/2: [======================        ] 55/75 batches, loss: 0.6949Epoch 1/2: [======================        ] 56/75 batches, loss: 0.6954Epoch 1/2: [======================        ] 57/75 batches, loss: 0.6956Epoch 1/2: [=======================       ] 58/75 batches, loss: 0.6961Epoch 1/2: [=======================       ] 59/75 batches, loss: 0.6965Epoch 1/2: [========================      ] 60/75 batches, loss: 0.6968Epoch 1/2: [========================      ] 61/75 batches, loss: 0.6967Epoch 1/2: [========================      ] 62/75 batches, loss: 0.6961Epoch 1/2: [=========================     ] 63/75 batches, loss: 0.6958Epoch 1/2: [=========================     ] 64/75 batches, loss: 0.6953Epoch 1/2: [==========================    ] 65/75 batches, loss: 0.6950Epoch 1/2: [==========================    ] 66/75 batches, loss: 0.6948Epoch 1/2: [==========================    ] 67/75 batches, loss: 0.6949Epoch 1/2: [===========================   ] 68/75 batches, loss: 0.6951Epoch 1/2: [===========================   ] 69/75 batches, loss: 0.6957Epoch 1/2: [============================  ] 70/75 batches, loss: 0.6962Epoch 1/2: [============================  ] 71/75 batches, loss: 0.6957Epoch 1/2: [============================  ] 72/75 batches, loss: 0.6956Epoch 1/2: [============================= ] 73/75 batches, loss: 0.6955Epoch 1/2: [============================= ] 74/75 batches, loss: 0.6951Epoch 1/2: [==============================] 75/75 batches, loss: 0.6959
[2025-04-29 20:32:48,272][src.training.lm_trainer][INFO] - Epoch 1/2, Train Loss: 0.6959
[2025-04-29 20:32:48,672][src.training.lm_trainer][INFO] - Epoch 1/2, Val Loss: 0.6942, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/2: [Epoch 2/2: [                              ] 1/75 batches, loss: 0.6729Epoch 2/2: [                              ] 2/75 batches, loss: 0.6912Epoch 2/2: [=                             ] 3/75 batches, loss: 0.6929Epoch 2/2: [=                             ] 4/75 batches, loss: 0.7021Epoch 2/2: [==                            ] 5/75 batches, loss: 0.7075Epoch 2/2: [==                            ] 6/75 batches, loss: 0.7044Epoch 2/2: [==                            ] 7/75 batches, loss: 0.7040Epoch 2/2: [===                           ] 8/75 batches, loss: 0.6997Epoch 2/2: [===                           ] 9/75 batches, loss: 0.6962Epoch 2/2: [====                          ] 10/75 batches, loss: 0.6965Epoch 2/2: [====                          ] 11/75 batches, loss: 0.6973Epoch 2/2: [====                          ] 12/75 batches, loss: 0.6934Epoch 2/2: [=====                         ] 13/75 batches, loss: 0.6953Epoch 2/2: [=====                         ] 14/75 batches, loss: 0.6957Epoch 2/2: [======                        ] 15/75 batches, loss: 0.6957Epoch 2/2: [======                        ] 16/75 batches, loss: 0.6929Epoch 2/2: [======                        ] 17/75 batches, loss: 0.6945Epoch 2/2: [=======                       ] 18/75 batches, loss: 0.6967Epoch 2/2: [=======                       ] 19/75 batches, loss: 0.6955Epoch 2/2: [========                      ] 20/75 batches, loss: 0.6960Epoch 2/2: [========                      ] 21/75 batches, loss: 0.6979Epoch 2/2: [========                      ] 22/75 batches, loss: 0.6959Epoch 2/2: [=========                     ] 23/75 batches, loss: 0.6948Epoch 2/2: [=========                     ] 24/75 batches, loss: 0.6937Epoch 2/2: [==========                    ] 25/75 batches, loss: 0.6935Epoch 2/2: [==========                    ] 26/75 batches, loss: 0.6931Epoch 2/2: [==========                    ] 27/75 batches, loss: 0.6934Epoch 2/2: [===========                   ] 28/75 batches, loss: 0.6935Epoch 2/2: [===========                   ] 29/75 batches, loss: 0.6940Epoch 2/2: [============                  ] 30/75 batches, loss: 0.6937Epoch 2/2: [============                  ] 31/75 batches, loss: 0.6932Epoch 2/2: [============                  ] 32/75 batches, loss: 0.6933Epoch 2/2: [=============                 ] 33/75 batches, loss: 0.6931Epoch 2/2: [=============                 ] 34/75 batches, loss: 0.6925Epoch 2/2: [==============                ] 35/75 batches, loss: 0.6932Epoch 2/2: [==============                ] 36/75 batches, loss: 0.6937Epoch 2/2: [==============                ] 37/75 batches, loss: 0.6935Epoch 2/2: [===============               ] 38/75 batches, loss: 0.6931Epoch 2/2: [===============               ] 39/75 batches, loss: 0.6939Epoch 2/2: [================              ] 40/75 batches, loss: 0.6924Epoch 2/2: [================              ] 41/75 batches, loss: 0.6930Epoch 2/2: [================              ] 42/75 batches, loss: 0.6921Epoch 2/2: [=================             ] 43/75 batches, loss: 0.6920Epoch 2/2: [=================             ] 44/75 batches, loss: 0.6915Epoch 2/2: [==================            ] 45/75 batches, loss: 0.6917Epoch 2/2: [==================            ] 46/75 batches, loss: 0.6916Epoch 2/2: [==================            ] 47/75 batches, loss: 0.6915Epoch 2/2: [===================           ] 48/75 batches, loss: 0.6908Epoch 2/2: [===================           ] 49/75 batches, loss: 0.6906Epoch 2/2: [====================          ] 50/75 batches, loss: 0.6908Epoch 2/2: [====================          ] 51/75 batches, loss: 0.6908Epoch 2/2: [====================          ] 52/75 batches, loss: 0.6912Epoch 2/2: [=====================         ] 53/75 batches, loss: 0.6913Epoch 2/2: [=====================         ] 54/75 batches, loss: 0.6906Epoch 2/2: [======================        ] 55/75 batches, loss: 0.6918Epoch 2/2: [======================        ] 56/75 batches, loss: 0.6917Epoch 2/2: [======================        ] 57/75 batches, loss: 0.6914Epoch 2/2: [=======================       ] 58/75 batches, loss: 0.6916Epoch 2/2: [=======================       ] 59/75 batches, loss: 0.6920Epoch 2/2: [========================      ] 60/75 batches, loss: 0.6926Epoch 2/2: [========================      ] 61/75 batches, loss: 0.6925Epoch 2/2: [========================      ] 62/75 batches, loss: 0.6922Epoch 2/2: [=========================     ] 63/75 batches, loss: 0.6924Epoch 2/2: [=========================     ] 64/75 batches, loss: 0.6929Epoch 2/2: [==========================    ] 65/75 batches, loss: 0.6931Epoch 2/2: [==========================    ] 66/75 batches, loss: 0.6932Epoch 2/2: [==========================    ] 67/75 batches, loss: 0.6930Epoch 2/2: [===========================   ] 68/75 batches, loss: 0.6937Epoch 2/2: [===========================   ] 69/75 batches, loss: 0.6938Epoch 2/2: [============================  ] 70/75 batches, loss: 0.6941Epoch 2/2: [============================  ] 71/75 batches, loss: 0.6938Epoch 2/2: [============================  ] 72/75 batches, loss: 0.6932Epoch 2/2: [============================= ] 73/75 batches, loss: 0.6931Epoch 2/2: [============================= ] 74/75 batches, loss: 0.6935Epoch 2/2: [==============================] 75/75 batches, loss: 0.6938
[2025-04-29 20:32:53,559][src.training.lm_trainer][INFO] - Epoch 2/2, Train Loss: 0.6938
[2025-04-29 20:32:53,968][src.training.lm_trainer][INFO] - Epoch 2/2, Val Loss: 0.6938, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:32:54,561][src.training.lm_trainer][INFO] - Training completed in 11.20 seconds
[2025-04-29 20:32:54,561][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 20:33:00,055][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:33:00,055][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:33:00,055][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:33:02,322][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/test_output/en/model.pt
Test experiment completed successfully. Proceeding with full analysis.
Phase 1: Running key validation experiments
Running question_type experiment for language en, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:33:16,640][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/question_type
experiment_name: layer_6_question_type_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  debug_mode: true
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 20:33:16,640][__main__][INFO] - Normalized task: question_type
[2025-04-29 20:33:16,640][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 20:33:16,640][__main__][INFO] - Determined Task Type: classification
[2025-04-29 20:33:16,644][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 20:33:16,645][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 20:33:19,007][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:33:21,830][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:33:21,831][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:33:21,857][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:21,886][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:21,954][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 20:33:21,966][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:33:21,967][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 20:33:21,968][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:33:21,987][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:22,013][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:22,024][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 20:33:22,025][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:33:22,025][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 20:33:22,026][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:33:22,046][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:22,071][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:33:22,081][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 20:33:22,083][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:33:22,083][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 20:33:22,084][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 20:33:22,085][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:33:22,085][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:33:22,085][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:33:22,085][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:33:22,085][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 20:33:22,085][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 20:33:22,085][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 20:33:22,086][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 20:33:22,086][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:33:22,086][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:33:22,086][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:33:22,086][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:33:22,086][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 20:33:22,086][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 20:33:22,086][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 20:33:22,086][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:33:22,087][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:33:22,087][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:33:22,087][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:33:22,087][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:33:22,087][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 20:33:22,087][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 20:33:22,087][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 20:33:22,087][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:33:22,087][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 20:33:22,088][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:33:22,088][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:33:22,088][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:33:25,782][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:33:25,783][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 20:33:25,784][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 20:33:25,784][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=6, freeze_model=True, finetune=False
[2025-04-29 20:33:25,785][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:33:25,785][__main__][INFO] - Successfully created model for en
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6463Epoch 1/15: [                              ] 2/75 batches, loss: 0.6730Epoch 1/15: [=                             ] 3/75 batches, loss: 0.6901Epoch 1/15: [=                             ] 4/75 batches, loss: 0.6962Epoch 1/15: [==                            ] 5/75 batches, loss: 0.6955Epoch 1/15: [==                            ] 6/75 batches, loss: 0.6965Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7011Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7002Epoch 1/15: [===                           ] 9/75 batches, loss: 0.6977Epoch 1/15: [====                          ] 10/75 batches, loss: 0.6971Epoch 1/15: [====                          ] 11/75 batches, loss: 0.7018Epoch 1/15: [====                          ] 12/75 batches, loss: 0.6999Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.7008Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.7016Epoch 1/15: [======                        ] 15/75 batches, loss: 0.7010Epoch 1/15: [======                        ] 16/75 batches, loss: 0.7019Epoch 1/15: [======                        ] 17/75 batches, loss: 0.7021Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.7020Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.7016Epoch 1/15: [========                      ] 20/75 batches, loss: 0.7014Epoch 1/15: [========                      ] 21/75 batches, loss: 0.7003Epoch 1/15: [========                      ] 22/75 batches, loss: 0.6985Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.6997Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.6982Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.6993Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.6992Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.6993Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.6986Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.6990Epoch 1/15: [============                  ] 30/75 batches, loss: 0.6985Epoch 1/15: [============                  ] 31/75 batches, loss: 0.6982Epoch 1/15: [============                  ] 32/75 batches, loss: 0.6989Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.6988Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.6979Epoch 1/15: [==============                ] 35/75 batches, loss: 0.6977Epoch 1/15: [==============                ] 36/75 batches, loss: 0.6962Epoch 1/15: [==============                ] 37/75 batches, loss: 0.6957Epoch 1/15: [===============               ] 38/75 batches, loss: 0.6952Epoch 1/15: [===============               ] 39/75 batches, loss: 0.6949Epoch 1/15: [================              ] 40/75 batches, loss: 0.6955Epoch 1/15: [================              ] 41/75 batches, loss: 0.6967Epoch 1/15: [================              ] 42/75 batches, loss: 0.6976Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6972Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6972Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6968Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6965Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6964Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6962Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6957Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6952Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6951Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6951Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6947Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6948Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6948Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6952Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6954Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6959Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6962Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6965Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6964Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6959Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6956Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6951Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6948Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6946Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6947Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6948Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6954Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6959Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6954Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6953Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6953Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6949Epoch 1/15: [==============================] 75/75 batches, loss: 0.6955
[2025-04-29 20:33:32,708][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6955
[2025-04-29 20:33:33,099][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6938, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6719Epoch 2/15: [                              ] 2/75 batches, loss: 0.6904Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6923Epoch 2/15: [=                             ] 4/75 batches, loss: 0.7004Epoch 2/15: [==                            ] 5/75 batches, loss: 0.7055Epoch 2/15: [==                            ] 6/75 batches, loss: 0.7028Epoch 2/15: [==                            ] 7/75 batches, loss: 0.7028Epoch 2/15: [===                           ] 8/75 batches, loss: 0.6990Epoch 2/15: [===                           ] 9/75 batches, loss: 0.6959Epoch 2/15: [====                          ] 10/75 batches, loss: 0.6961Epoch 2/15: [====                          ] 11/75 batches, loss: 0.6968Epoch 2/15: [====                          ] 12/75 batches, loss: 0.6933Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.6949Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.6951Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6949Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6923Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6936Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6955Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6944Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6948Epoch 2/15: [========                      ] 21/75 batches, loss: 0.6965Epoch 2/15: [========                      ] 22/75 batches, loss: 0.6948Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6939Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.6929Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.6928Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.6925Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.6927Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.6928Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.6932Epoch 2/15: [============                  ] 30/75 batches, loss: 0.6928Epoch 2/15: [============                  ] 31/75 batches, loss: 0.6925Epoch 2/15: [============                  ] 32/75 batches, loss: 0.6926Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.6924Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.6919Epoch 2/15: [==============                ] 35/75 batches, loss: 0.6926Epoch 2/15: [==============                ] 36/75 batches, loss: 0.6930Epoch 2/15: [==============                ] 37/75 batches, loss: 0.6928Epoch 2/15: [===============               ] 38/75 batches, loss: 0.6924Epoch 2/15: [===============               ] 39/75 batches, loss: 0.6932Epoch 2/15: [================              ] 40/75 batches, loss: 0.6919Epoch 2/15: [================              ] 41/75 batches, loss: 0.6923Epoch 2/15: [================              ] 42/75 batches, loss: 0.6915Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6913Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6909Epoch 2/15: [==================            ] 45/75 batches, loss: 0.6911Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6911Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6909Epoch 2/15: [===================           ] 48/75 batches, loss: 0.6903Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6901Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6903Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6904Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6907Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6908Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6903Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6913Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6912Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6909Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6911Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6914Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6919Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6918Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6915Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6917Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6921Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6924Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6924Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6923Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6929Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6930Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6933Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6930Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6925Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6924Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6928Epoch 2/15: [==============================] 75/75 batches, loss: 0.6930
[2025-04-29 20:33:38,034][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6930
[2025-04-29 20:33:38,444][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6933, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6596Epoch 3/15: [                              ] 2/75 batches, loss: 0.6738Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6788Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6834Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6902Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6944Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6980Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6969Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6957Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6977Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6959Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6955Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6970Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6990Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6984Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6987Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6980Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6977Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6996Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6998Epoch 3/15: [========                      ] 21/75 batches, loss: 0.7001Epoch 3/15: [========                      ] 22/75 batches, loss: 0.7000Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6996Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6992Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6999Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.7001Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6998Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6991Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6988Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6992Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6990Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6986Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6988Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6980Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6984Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6981Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6975Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6976Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6974Epoch 3/15: [================              ] 40/75 batches, loss: 0.6976Epoch 3/15: [================              ] 41/75 batches, loss: 0.6974Epoch 3/15: [================              ] 42/75 batches, loss: 0.6974Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6973Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6969Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6966Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6968Epoch 3/15: [==================            ] 47/75 batches, loss: 0.6974Epoch 3/15: [===================           ] 48/75 batches, loss: 0.6973Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6970Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6971Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6970Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6964Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.6965Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6972Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6971Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6969Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6968Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6968Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6965Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6965Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6964Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6969Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6973Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6969Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6970Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6970Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6972Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6972Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6972Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6969Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6969Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6972Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6971Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6970Epoch 3/15: [==============================] 75/75 batches, loss: 0.6969
[2025-04-29 20:33:43,374][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6969
[2025-04-29 20:33:43,789][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6881Epoch 4/15: [                              ] 2/75 batches, loss: 0.7005Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6980Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6959Epoch 4/15: [==                            ] 5/75 batches, loss: 0.7017Epoch 4/15: [==                            ] 6/75 batches, loss: 0.7015Epoch 4/15: [==                            ] 7/75 batches, loss: 0.7004Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6976Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6975Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6972Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6946Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6943Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6960Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6946Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6937Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6942Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6939Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6940Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6953Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6942Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6946Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6940Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6945Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6943Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6948Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6947Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6952Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6952Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6950Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6944Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6946Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6952Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6953Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6959Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6961Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6964Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6962Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6965Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6962Epoch 4/15: [================              ] 40/75 batches, loss: 0.6961Epoch 4/15: [================              ] 41/75 batches, loss: 0.6956Epoch 4/15: [================              ] 42/75 batches, loss: 0.6960Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6955Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6958Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6965Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6964Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6961Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6962Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6962Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6963Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6962Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6965Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6967Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6963Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6965Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6967Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6966Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6966Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6968Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6971Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6972Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6974Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6976Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6976Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6973Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6974Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6971Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6973Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6976Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6976Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6980Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6978Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6976Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6974Epoch 4/15: [==============================] 75/75 batches, loss: 0.6972
[2025-04-29 20:33:48,706][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6972
[2025-04-29 20:33:49,137][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.7095Epoch 5/15: [                              ] 2/75 batches, loss: 0.7092Epoch 5/15: [=                             ] 3/75 batches, loss: 0.7031Epoch 5/15: [=                             ] 4/75 batches, loss: 0.6985Epoch 5/15: [==                            ] 5/75 batches, loss: 0.6963Epoch 5/15: [==                            ] 6/75 batches, loss: 0.6941Epoch 5/15: [==                            ] 7/75 batches, loss: 0.6931Epoch 5/15: [===                           ] 8/75 batches, loss: 0.6955Epoch 5/15: [===                           ] 9/75 batches, loss: 0.6968Epoch 5/15: [====                          ] 10/75 batches, loss: 0.6981Epoch 5/15: [====                          ] 11/75 batches, loss: 0.6948Epoch 5/15: [====                          ] 12/75 batches, loss: 0.6943Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.6924Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.6932Epoch 5/15: [======                        ] 15/75 batches, loss: 0.6937Epoch 5/15: [======                        ] 16/75 batches, loss: 0.6942Epoch 5/15: [======                        ] 17/75 batches, loss: 0.6934Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.6922Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.6946Epoch 5/15: [========                      ] 20/75 batches, loss: 0.6952Epoch 5/15: [========                      ] 21/75 batches, loss: 0.6940Epoch 5/15: [========                      ] 22/75 batches, loss: 0.6941Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.6946Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.6953Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.6959Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.6955Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.6948Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.6955Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.6956Epoch 5/15: [============                  ] 30/75 batches, loss: 0.6957Epoch 5/15: [============                  ] 31/75 batches, loss: 0.6968Epoch 5/15: [============                  ] 32/75 batches, loss: 0.6964Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.6958Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.6958Epoch 5/15: [==============                ] 35/75 batches, loss: 0.6951Epoch 5/15: [==============                ] 36/75 batches, loss: 0.6956Epoch 5/15: [==============                ] 37/75 batches, loss: 0.6958Epoch 5/15: [===============               ] 38/75 batches, loss: 0.6952Epoch 5/15: [===============               ] 39/75 batches, loss: 0.6958Epoch 5/15: [================              ] 40/75 batches, loss: 0.6958Epoch 5/15: [================              ] 41/75 batches, loss: 0.6960Epoch 5/15: [================              ] 42/75 batches, loss: 0.6958Epoch 5/15: [=================             ] 43/75 batches, loss: 0.6961Epoch 5/15: [=================             ] 44/75 batches, loss: 0.6959Epoch 5/15: [==================            ] 45/75 batches, loss: 0.6961Epoch 5/15: [==================            ] 46/75 batches, loss: 0.6959Epoch 5/15: [==================            ] 47/75 batches, loss: 0.6958Epoch 5/15: [===================           ] 48/75 batches, loss: 0.6958Epoch 5/15: [===================           ] 49/75 batches, loss: 0.6958Epoch 5/15: [====================          ] 50/75 batches, loss: 0.6959Epoch 5/15: [====================          ] 51/75 batches, loss: 0.6957Epoch 5/15: [====================          ] 52/75 batches, loss: 0.6962Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.6961Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.6959Epoch 5/15: [======================        ] 55/75 batches, loss: 0.6959Epoch 5/15: [======================        ] 56/75 batches, loss: 0.6959Epoch 5/15: [======================        ] 57/75 batches, loss: 0.6960Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.6956Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.6956Epoch 5/15: [========================      ] 60/75 batches, loss: 0.6955Epoch 5/15: [========================      ] 61/75 batches, loss: 0.6951Epoch 5/15: [========================      ] 62/75 batches, loss: 0.6950Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.6951Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.6953Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.6956Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.6954Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.6950Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.6947Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.6947Epoch 5/15: [============================  ] 70/75 batches, loss: 0.6946Epoch 5/15: [============================  ] 71/75 batches, loss: 0.6947Epoch 5/15: [============================  ] 72/75 batches, loss: 0.6950Epoch 5/15: [============================= ] 73/75 batches, loss: 0.6949Epoch 5/15: [============================= ] 74/75 batches, loss: 0.6947Epoch 5/15: [==============================] 75/75 batches, loss: 0.6942
[2025-04-29 20:33:54,050][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6942
[2025-04-29 20:33:54,484][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6977Epoch 6/15: [                              ] 2/75 batches, loss: 0.6986Epoch 6/15: [=                             ] 3/75 batches, loss: 0.6980Epoch 6/15: [=                             ] 4/75 batches, loss: 0.6950Epoch 6/15: [==                            ] 5/75 batches, loss: 0.6970Epoch 6/15: [==                            ] 6/75 batches, loss: 0.6965Epoch 6/15: [==                            ] 7/75 batches, loss: 0.6982Epoch 6/15: [===                           ] 8/75 batches, loss: 0.7005Epoch 6/15: [===                           ] 9/75 batches, loss: 0.6983Epoch 6/15: [====                          ] 10/75 batches, loss: 0.6984Epoch 6/15: [====                          ] 11/75 batches, loss: 0.6985Epoch 6/15: [====                          ] 12/75 batches, loss: 0.6979Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.6979Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.6974Epoch 6/15: [======                        ] 15/75 batches, loss: 0.6974Epoch 6/15: [======                        ] 16/75 batches, loss: 0.6966Epoch 6/15: [======                        ] 17/75 batches, loss: 0.6954Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.6947Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.6950Epoch 6/15: [========                      ] 20/75 batches, loss: 0.6950Epoch 6/15: [========                      ] 21/75 batches, loss: 0.6945Epoch 6/15: [========                      ] 22/75 batches, loss: 0.6954Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.6955Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.6954Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.6953Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.6949Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.6947Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.6950Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.6953Epoch 6/15: [============                  ] 30/75 batches, loss: 0.6954Epoch 6/15: [============                  ] 31/75 batches, loss: 0.6962Epoch 6/15: [============                  ] 32/75 batches, loss: 0.6967Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.6966Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.6965Epoch 6/15: [==============                ] 35/75 batches, loss: 0.6967Epoch 6/15: [==============                ] 36/75 batches, loss: 0.6972Epoch 6/15: [==============                ] 37/75 batches, loss: 0.6974Epoch 6/15: [===============               ] 38/75 batches, loss: 0.6976Epoch 6/15: [===============               ] 39/75 batches, loss: 0.6972Epoch 6/15: [================              ] 40/75 batches, loss: 0.6974Epoch 6/15: [================              ] 41/75 batches, loss: 0.6971Epoch 6/15: [================              ] 42/75 batches, loss: 0.6970Epoch 6/15: [=================             ] 43/75 batches, loss: 0.6968Epoch 6/15: [=================             ] 44/75 batches, loss: 0.6970Epoch 6/15: [==================            ] 45/75 batches, loss: 0.6972Epoch 6/15: [==================            ] 46/75 batches, loss: 0.6971Epoch 6/15: [==================            ] 47/75 batches, loss: 0.6966Epoch 6/15: [===================           ] 48/75 batches, loss: 0.6967Epoch 6/15: [===================           ] 49/75 batches, loss: 0.6967Epoch 6/15: [====================          ] 50/75 batches, loss: 0.6970Epoch 6/15: [====================          ] 51/75 batches, loss: 0.6975Epoch 6/15: [====================          ] 52/75 batches, loss: 0.6975Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.6971Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.6972Epoch 6/15: [======================        ] 55/75 batches, loss: 0.6970Epoch 6/15: [======================        ] 56/75 batches, loss: 0.6969Epoch 6/15: [======================        ] 57/75 batches, loss: 0.6967Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.6961Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.6957Epoch 6/15: [========================      ] 60/75 batches, loss: 0.6956Epoch 6/15: [========================      ] 61/75 batches, loss: 0.6954Epoch 6/15: [========================      ] 62/75 batches, loss: 0.6954Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.6953Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.6952Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.6953Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.6953Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.6951Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.6950Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.6947Epoch 6/15: [============================  ] 70/75 batches, loss: 0.6946Epoch 6/15: [============================  ] 71/75 batches, loss: 0.6942Epoch 6/15: [============================  ] 72/75 batches, loss: 0.6943Epoch 6/15: [============================= ] 73/75 batches, loss: 0.6942Epoch 6/15: [============================= ] 74/75 batches, loss: 0.6944Epoch 6/15: [==============================] 75/75 batches, loss: 0.6945
[2025-04-29 20:33:59,421][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6945
[2025-04-29 20:33:59,839][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.7056Epoch 7/15: [                              ] 2/75 batches, loss: 0.6978Epoch 7/15: [=                             ] 3/75 batches, loss: 0.6917Epoch 7/15: [=                             ] 4/75 batches, loss: 0.6898Epoch 7/15: [==                            ] 5/75 batches, loss: 0.6891Epoch 7/15: [==                            ] 6/75 batches, loss: 0.6933Epoch 7/15: [==                            ] 7/75 batches, loss: 0.6919Epoch 7/15: [===                           ] 8/75 batches, loss: 0.6928Epoch 7/15: [===                           ] 9/75 batches, loss: 0.6947Epoch 7/15: [====                          ] 10/75 batches, loss: 0.6938Epoch 7/15: [====                          ] 11/75 batches, loss: 0.6909Epoch 7/15: [====                          ] 12/75 batches, loss: 0.6917Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.6913Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.6915Epoch 7/15: [======                        ] 15/75 batches, loss: 0.6914Epoch 7/15: [======                        ] 16/75 batches, loss: 0.6894Epoch 7/15: [======                        ] 17/75 batches, loss: 0.6899Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.6912Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.6919Epoch 7/15: [========                      ] 20/75 batches, loss: 0.6919Epoch 7/15: [========                      ] 21/75 batches, loss: 0.6923Epoch 7/15: [========                      ] 22/75 batches, loss: 0.6934Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.6931Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.6926Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.6924Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.6928Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.6924Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.6928Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.6932Epoch 7/15: [============                  ] 30/75 batches, loss: 0.6931Epoch 7/15: [============                  ] 31/75 batches, loss: 0.6931Epoch 7/15: [============                  ] 32/75 batches, loss: 0.6934Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.6932Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.6934Epoch 7/15: [==============                ] 35/75 batches, loss: 0.6932Epoch 7/15: [==============                ] 36/75 batches, loss: 0.6934Epoch 7/15: [==============                ] 37/75 batches, loss: 0.6934Epoch 7/15: [===============               ] 38/75 batches, loss: 0.6939Epoch 7/15: [===============               ] 39/75 batches, loss: 0.6939Epoch 7/15: [================              ] 40/75 batches, loss: 0.6937Epoch 7/15: [================              ] 41/75 batches, loss: 0.6932Epoch 7/15: [================              ] 42/75 batches, loss: 0.6932Epoch 7/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 7/15: [=================             ] 44/75 batches, loss: 0.6932Epoch 7/15: [==================            ] 45/75 batches, loss: 0.6930Epoch 7/15: [==================            ] 46/75 batches, loss: 0.6933Epoch 7/15: [==================            ] 47/75 batches, loss: 0.6932Epoch 7/15: [===================           ] 48/75 batches, loss: 0.6936Epoch 7/15: [===================           ] 49/75 batches, loss: 0.6939Epoch 7/15: [====================          ] 50/75 batches, loss: 0.6940Epoch 7/15: [====================          ] 51/75 batches, loss: 0.6934Epoch 7/15: [====================          ] 52/75 batches, loss: 0.6934Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.6934Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.6935Epoch 7/15: [======================        ] 55/75 batches, loss: 0.6933Epoch 7/15: [======================        ] 56/75 batches, loss: 0.6931Epoch 7/15: [======================        ] 57/75 batches, loss: 0.6929Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.6928Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.6930Epoch 7/15: [========================      ] 60/75 batches, loss: 0.6929Epoch 7/15: [========================      ] 61/75 batches, loss: 0.6931Epoch 7/15: [========================      ] 62/75 batches, loss: 0.6933Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.6935Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.6936Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.6930Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.6933Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.6932Epoch 7/15: [============================  ] 70/75 batches, loss: 0.6931Epoch 7/15: [============================  ] 71/75 batches, loss: 0.6931Epoch 7/15: [============================  ] 72/75 batches, loss: 0.6932Epoch 7/15: [============================= ] 73/75 batches, loss: 0.6932Epoch 7/15: [============================= ] 74/75 batches, loss: 0.6931Epoch 7/15: [==============================] 75/75 batches, loss: 0.6932
[2025-04-29 20:34:04,871][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.6932
[2025-04-29 20:34:05,316][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:34:05,316][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.6959Epoch 8/15: [                              ] 2/75 batches, loss: 0.6902Epoch 8/15: [=                             ] 3/75 batches, loss: 0.6923Epoch 8/15: [=                             ] 4/75 batches, loss: 0.6988Epoch 8/15: [==                            ] 5/75 batches, loss: 0.6983Epoch 8/15: [==                            ] 6/75 batches, loss: 0.6968Epoch 8/15: [==                            ] 7/75 batches, loss: 0.6973Epoch 8/15: [===                           ] 8/75 batches, loss: 0.6965Epoch 8/15: [===                           ] 9/75 batches, loss: 0.6936Epoch 8/15: [====                          ] 10/75 batches, loss: 0.6941Epoch 8/15: [====                          ] 11/75 batches, loss: 0.6941Epoch 8/15: [====                          ] 12/75 batches, loss: 0.6929Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.6912Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.6915Epoch 8/15: [======                        ] 15/75 batches, loss: 0.6913Epoch 8/15: [======                        ] 16/75 batches, loss: 0.6909Epoch 8/15: [======                        ] 17/75 batches, loss: 0.6926Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.6918Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.6916Epoch 8/15: [========                      ] 20/75 batches, loss: 0.6919Epoch 8/15: [========                      ] 21/75 batches, loss: 0.6914Epoch 8/15: [========                      ] 22/75 batches, loss: 0.6910Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.6905Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.6901Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.6897Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.6895Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.6890Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.6887Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.6890Epoch 8/15: [============                  ] 30/75 batches, loss: 0.6894Epoch 8/15: [============                  ] 31/75 batches, loss: 0.6896Epoch 8/15: [============                  ] 32/75 batches, loss: 0.6900Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.6901Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.6908Epoch 8/15: [==============                ] 35/75 batches, loss: 0.6909Epoch 8/15: [==============                ] 36/75 batches, loss: 0.6907Epoch 8/15: [==============                ] 37/75 batches, loss: 0.6903Epoch 8/15: [===============               ] 38/75 batches, loss: 0.6899Epoch 8/15: [===============               ] 39/75 batches, loss: 0.6901Epoch 8/15: [================              ] 40/75 batches, loss: 0.6906Epoch 8/15: [================              ] 41/75 batches, loss: 0.6909Epoch 8/15: [================              ] 42/75 batches, loss: 0.6909Epoch 8/15: [=================             ] 43/75 batches, loss: 0.6906Epoch 8/15: [=================             ] 44/75 batches, loss: 0.6912Epoch 8/15: [==================            ] 45/75 batches, loss: 0.6916Epoch 8/15: [==================            ] 46/75 batches, loss: 0.6922Epoch 8/15: [==================            ] 47/75 batches, loss: 0.6924Epoch 8/15: [===================           ] 48/75 batches, loss: 0.6927Epoch 8/15: [===================           ] 49/75 batches, loss: 0.6923Epoch 8/15: [====================          ] 50/75 batches, loss: 0.6923Epoch 8/15: [====================          ] 51/75 batches, loss: 0.6923Epoch 8/15: [====================          ] 52/75 batches, loss: 0.6919Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.6920Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.6920Epoch 8/15: [======================        ] 55/75 batches, loss: 0.6920Epoch 8/15: [======================        ] 56/75 batches, loss: 0.6924Epoch 8/15: [======================        ] 57/75 batches, loss: 0.6926Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.6923Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.6923Epoch 8/15: [========================      ] 60/75 batches, loss: 0.6923Epoch 8/15: [========================      ] 61/75 batches, loss: 0.6922Epoch 8/15: [========================      ] 62/75 batches, loss: 0.6925Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.6928Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.6930Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.6930Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.6927Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.6927Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.6926Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.6927Epoch 8/15: [============================  ] 70/75 batches, loss: 0.6927Epoch 8/15: [============================  ] 71/75 batches, loss: 0.6929Epoch 8/15: [============================  ] 72/75 batches, loss: 0.6927Epoch 8/15: [============================= ] 73/75 batches, loss: 0.6929Epoch 8/15: [============================= ] 74/75 batches, loss: 0.6927Epoch 8/15: [==============================] 75/75 batches, loss: 0.6927
[2025-04-29 20:34:09,713][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.6927
[2025-04-29 20:34:10,140][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:34:10,141][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.6845Epoch 9/15: [                              ] 2/75 batches, loss: 0.6918Epoch 9/15: [=                             ] 3/75 batches, loss: 0.6942Epoch 9/15: [=                             ] 4/75 batches, loss: 0.6943Epoch 9/15: [==                            ] 5/75 batches, loss: 0.6956Epoch 9/15: [==                            ] 6/75 batches, loss: 0.6947Epoch 9/15: [==                            ] 7/75 batches, loss: 0.6929Epoch 9/15: [===                           ] 8/75 batches, loss: 0.6926Epoch 9/15: [===                           ] 9/75 batches, loss: 0.6915Epoch 9/15: [====                          ] 10/75 batches, loss: 0.6932Epoch 9/15: [====                          ] 11/75 batches, loss: 0.6941Epoch 9/15: [====                          ] 12/75 batches, loss: 0.6962Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.6969Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.6976Epoch 9/15: [======                        ] 15/75 batches, loss: 0.6976Epoch 9/15: [======                        ] 16/75 batches, loss: 0.6979Epoch 9/15: [======                        ] 17/75 batches, loss: 0.6973Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.6972Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.6974Epoch 9/15: [========                      ] 20/75 batches, loss: 0.6968Epoch 9/15: [========                      ] 21/75 batches, loss: 0.6965Epoch 9/15: [========                      ] 22/75 batches, loss: 0.6962Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.6964Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.6959Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.6951Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.6956Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.6957Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.6965Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.6967Epoch 9/15: [============                  ] 30/75 batches, loss: 0.6957Epoch 9/15: [============                  ] 31/75 batches, loss: 0.6961Epoch 9/15: [============                  ] 32/75 batches, loss: 0.6961Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.6960Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.6961Epoch 9/15: [==============                ] 35/75 batches, loss: 0.6958Epoch 9/15: [==============                ] 36/75 batches, loss: 0.6956Epoch 9/15: [==============                ] 37/75 batches, loss: 0.6957Epoch 9/15: [===============               ] 38/75 batches, loss: 0.6958Epoch 9/15: [===============               ] 39/75 batches, loss: 0.6956Epoch 9/15: [================              ] 40/75 batches, loss: 0.6955Epoch 9/15: [================              ] 41/75 batches, loss: 0.6954Epoch 9/15: [================              ] 42/75 batches, loss: 0.6955Epoch 9/15: [=================             ] 43/75 batches, loss: 0.6952Epoch 9/15: [=================             ] 44/75 batches, loss: 0.6955Epoch 9/15: [==================            ] 45/75 batches, loss: 0.6965Epoch 9/15: [==================            ] 46/75 batches, loss: 0.6968Epoch 9/15: [==================            ] 47/75 batches, loss: 0.6971Epoch 9/15: [===================           ] 48/75 batches, loss: 0.6972Epoch 9/15: [===================           ] 49/75 batches, loss: 0.6971Epoch 9/15: [====================          ] 50/75 batches, loss: 0.6968Epoch 9/15: [====================          ] 51/75 batches, loss: 0.6966Epoch 9/15: [====================          ] 52/75 batches, loss: 0.6965Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.6962Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.6960Epoch 9/15: [======================        ] 55/75 batches, loss: 0.6959Epoch 9/15: [======================        ] 56/75 batches, loss: 0.6961Epoch 9/15: [======================        ] 57/75 batches, loss: 0.6959Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.6965Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.6962Epoch 9/15: [========================      ] 60/75 batches, loss: 0.6958Epoch 9/15: [========================      ] 61/75 batches, loss: 0.6960Epoch 9/15: [========================      ] 62/75 batches, loss: 0.6962Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.6962Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.6962Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.6957Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.6957Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.6957Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.6954Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.6954Epoch 9/15: [============================  ] 70/75 batches, loss: 0.6952Epoch 9/15: [============================  ] 71/75 batches, loss: 0.6956Epoch 9/15: [============================  ] 72/75 batches, loss: 0.6958Epoch 9/15: [============================= ] 73/75 batches, loss: 0.6958Epoch 9/15: [============================= ] 74/75 batches, loss: 0.6961Epoch 9/15: [==============================] 75/75 batches, loss: 0.6959
[2025-04-29 20:34:14,544][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.6959
[2025-04-29 20:34:14,985][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:34:14,986][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 20:34:14,986][src.training.lm_trainer][INFO] - Early stopping at epoch 9
[2025-04-29 20:34:14,987][src.training.lm_trainer][INFO] - Training completed in 47.25 seconds
[2025-04-29 20:34:14,987][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 20:34:20,618][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:34:20,618][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:34:20,619][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:34:22,912][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/question_type/en/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁▁▁
wandb:        best_val_loss █▄▂▁▁▁
wandb:                epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ███████▁▁
wandb:           train_loss ▅▁▇█▃▄▂▁▆
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁
wandb:             val_loss █▄▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69281
wandb:                epoch 9
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.6959
wandb:           train_time 47.2541
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69284
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203316-mxi9lghq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203316-mxi9lghq/logs
Standard experiment completed successfully: layer_6_question_type_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/question_type/results.json
Running complexity experiment for language en, layer 6
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:34:39,112][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/complexity
experiment_name: layer_6_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  debug_mode: true
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 20:34:39,112][__main__][INFO] - Normalized task: complexity
[2025-04-29 20:34:39,112][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 20:34:39,112][__main__][INFO] - Determined Task Type: regression
[2025-04-29 20:34:39,117][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-29 20:34:39,117][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 20:34:42,102][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:34:44,885][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:34:44,886][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:34:44,965][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:34:45,007][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:34:45,120][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 20:34:45,132][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:34:45,132][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 20:34:45,135][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:34:45,164][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:34:45,200][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:34:45,212][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 20:34:45,213][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:34:45,213][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 20:34:45,215][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:34:45,242][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:34:45,275][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:34:45,287][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 20:34:45,289][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:34:45,289][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 20:34:45,291][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 20:34:45,291][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:34:45,291][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:34:45,291][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:34:45,291][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:34:45,292][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:34:45,292][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-29 20:34:45,292][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 20:34:45,292][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-29 20:34:45,292][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:34:45,292][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:34:45,292][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:34:45,293][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:34:45,293][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:34:45,293][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-29 20:34:45,293][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 20:34:45,293][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-29 20:34:45,293][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:34:45,293][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:34:45,293][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:34:45,294][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:34:45,294][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:34:45,294][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-29 20:34:45,294][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 20:34:45,294][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-29 20:34:45,294][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 20:34:45,294][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:34:45,295][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:34:45,295][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:34:50,467][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:34:50,468][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 20:34:50,469][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 20:34:50,469][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=6, freeze_model=True, finetune=False
[2025-04-29 20:34:50,470][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:34:50,470][__main__][INFO] - Successfully created model for en
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.3392Epoch 1/15: [                              ] 2/75 batches, loss: 0.3497Epoch 1/15: [=                             ] 3/75 batches, loss: 0.3755Epoch 1/15: [=                             ] 4/75 batches, loss: 0.3768Epoch 1/15: [==                            ] 5/75 batches, loss: 0.3721Epoch 1/15: [==                            ] 6/75 batches, loss: 0.3720Epoch 1/15: [==                            ] 7/75 batches, loss: 0.3741Epoch 1/15: [===                           ] 8/75 batches, loss: 0.3564Epoch 1/15: [===                           ] 9/75 batches, loss: 0.3557Epoch 1/15: [====                          ] 10/75 batches, loss: 0.3521Epoch 1/15: [====                          ] 11/75 batches, loss: 0.3529Epoch 1/15: [====                          ] 12/75 batches, loss: 0.3538Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.3550Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.3540Epoch 1/15: [======                        ] 15/75 batches, loss: 0.3573Epoch 1/15: [======                        ] 16/75 batches, loss: 0.3652Epoch 1/15: [======                        ] 17/75 batches, loss: 0.3619Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.3609Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.3567Epoch 1/15: [========                      ] 20/75 batches, loss: 0.3579Epoch 1/15: [========                      ] 21/75 batches, loss: 0.3544Epoch 1/15: [========                      ] 22/75 batches, loss: 0.3509Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.3487Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.3470Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.3433Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.3435Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.3390Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.3403Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.3363Epoch 1/15: [============                  ] 30/75 batches, loss: 0.3341Epoch 1/15: [============                  ] 31/75 batches, loss: 0.3345Epoch 1/15: [============                  ] 32/75 batches, loss: 0.3340Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.3341Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.3342Epoch 1/15: [==============                ] 35/75 batches, loss: 0.3314Epoch 1/15: [==============                ] 36/75 batches, loss: 0.3284Epoch 1/15: [==============                ] 37/75 batches, loss: 0.3302Epoch 1/15: [===============               ] 38/75 batches, loss: 0.3278Epoch 1/15: [===============               ] 39/75 batches, loss: 0.3269Epoch 1/15: [================              ] 40/75 batches, loss: 0.3272Epoch 1/15: [================              ] 41/75 batches, loss: 0.3258Epoch 1/15: [================              ] 42/75 batches, loss: 0.3231Epoch 1/15: [=================             ] 43/75 batches, loss: 0.3234Epoch 1/15: [=================             ] 44/75 batches, loss: 0.3222Epoch 1/15: [==================            ] 45/75 batches, loss: 0.3192Epoch 1/15: [==================            ] 46/75 batches, loss: 0.3172Epoch 1/15: [==================            ] 47/75 batches, loss: 0.3154Epoch 1/15: [===================           ] 48/75 batches, loss: 0.3146Epoch 1/15: [===================           ] 49/75 batches, loss: 0.3129Epoch 1/15: [====================          ] 50/75 batches, loss: 0.3115Epoch 1/15: [====================          ] 51/75 batches, loss: 0.3086Epoch 1/15: [====================          ] 52/75 batches, loss: 0.3112Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.3108Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.3099Epoch 1/15: [======================        ] 55/75 batches, loss: 0.3086Epoch 1/15: [======================        ] 56/75 batches, loss: 0.3080Epoch 1/15: [======================        ] 57/75 batches, loss: 0.3069Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.3055Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.3043Epoch 1/15: [========================      ] 60/75 batches, loss: 0.3028Epoch 1/15: [========================      ] 61/75 batches, loss: 0.3028Epoch 1/15: [========================      ] 62/75 batches, loss: 0.3023Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.3008Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.2989Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.2977Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.2961Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.2947Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.2929Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.2918Epoch 1/15: [============================  ] 70/75 batches, loss: 0.2912Epoch 1/15: [============================  ] 71/75 batches, loss: 0.2907Epoch 1/15: [============================  ] 72/75 batches, loss: 0.2899Epoch 1/15: [============================= ] 73/75 batches, loss: 0.2889Epoch 1/15: [============================= ] 74/75 batches, loss: 0.2880Epoch 1/15: [==============================] 75/75 batches, loss: 0.2882
[2025-04-29 20:34:57,424][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.2882
[2025-04-29 20:34:57,815][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1645, Metrics: {'mse': 0.17400795221328735, 'rmse': 0.41714260416947024, 'r2': -3.157776355743408}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.2240Epoch 2/15: [                              ] 2/75 batches, loss: 0.1953Epoch 2/15: [=                             ] 3/75 batches, loss: 0.1915Epoch 2/15: [=                             ] 4/75 batches, loss: 0.1930Epoch 2/15: [==                            ] 5/75 batches, loss: 0.2159Epoch 2/15: [==                            ] 6/75 batches, loss: 0.2179Epoch 2/15: [==                            ] 7/75 batches, loss: 0.2189Epoch 2/15: [===                           ] 8/75 batches, loss: 0.2181Epoch 2/15: [===                           ] 9/75 batches, loss: 0.2196Epoch 2/15: [====                          ] 10/75 batches, loss: 0.2216Epoch 2/15: [====                          ] 11/75 batches, loss: 0.2224Epoch 2/15: [====                          ] 12/75 batches, loss: 0.2222Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.2176Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.2238Epoch 2/15: [======                        ] 15/75 batches, loss: 0.2232Epoch 2/15: [======                        ] 16/75 batches, loss: 0.2195Epoch 2/15: [======                        ] 17/75 batches, loss: 0.2240Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.2209Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.2207Epoch 2/15: [========                      ] 20/75 batches, loss: 0.2184Epoch 2/15: [========                      ] 21/75 batches, loss: 0.2216Epoch 2/15: [========                      ] 22/75 batches, loss: 0.2270Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.2269Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.2241Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.2247Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.2236Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.2215Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.2201Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.2193Epoch 2/15: [============                  ] 30/75 batches, loss: 0.2188Epoch 2/15: [============                  ] 31/75 batches, loss: 0.2187Epoch 2/15: [============                  ] 32/75 batches, loss: 0.2183Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.2161Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.2145Epoch 2/15: [==============                ] 35/75 batches, loss: 0.2144Epoch 2/15: [==============                ] 36/75 batches, loss: 0.2118Epoch 2/15: [==============                ] 37/75 batches, loss: 0.2091Epoch 2/15: [===============               ] 38/75 batches, loss: 0.2083Epoch 2/15: [===============               ] 39/75 batches, loss: 0.2074Epoch 2/15: [================              ] 40/75 batches, loss: 0.2065Epoch 2/15: [================              ] 41/75 batches, loss: 0.2073Epoch 2/15: [================              ] 42/75 batches, loss: 0.2069Epoch 2/15: [=================             ] 43/75 batches, loss: 0.2054Epoch 2/15: [=================             ] 44/75 batches, loss: 0.2037Epoch 2/15: [==================            ] 45/75 batches, loss: 0.2051Epoch 2/15: [==================            ] 46/75 batches, loss: 0.2045Epoch 2/15: [==================            ] 47/75 batches, loss: 0.2039Epoch 2/15: [===================           ] 48/75 batches, loss: 0.2041Epoch 2/15: [===================           ] 49/75 batches, loss: 0.2055Epoch 2/15: [====================          ] 50/75 batches, loss: 0.2041Epoch 2/15: [====================          ] 51/75 batches, loss: 0.2019Epoch 2/15: [====================          ] 52/75 batches, loss: 0.2007Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.1991Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.1974Epoch 2/15: [======================        ] 55/75 batches, loss: 0.1963Epoch 2/15: [======================        ] 56/75 batches, loss: 0.1949Epoch 2/15: [======================        ] 57/75 batches, loss: 0.1947Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.1944Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.1948Epoch 2/15: [========================      ] 60/75 batches, loss: 0.1940Epoch 2/15: [========================      ] 61/75 batches, loss: 0.1937Epoch 2/15: [========================      ] 62/75 batches, loss: 0.1924Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.1914Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.1905Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.1899Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.1882Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.1864Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.1868Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.1863Epoch 2/15: [============================  ] 70/75 batches, loss: 0.1860Epoch 2/15: [============================  ] 71/75 batches, loss: 0.1859Epoch 2/15: [============================  ] 72/75 batches, loss: 0.1855Epoch 2/15: [============================= ] 73/75 batches, loss: 0.1846Epoch 2/15: [============================= ] 74/75 batches, loss: 0.1847Epoch 2/15: [==============================] 75/75 batches, loss: 0.1852
[2025-04-29 20:35:02,719][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1852
[2025-04-29 20:35:03,128][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1002, Metrics: {'mse': 0.1072300374507904, 'rmse': 0.3274599783955139, 'r2': -1.5621728897094727}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.1676Epoch 3/15: [                              ] 2/75 batches, loss: 0.1536Epoch 3/15: [=                             ] 3/75 batches, loss: 0.1474Epoch 3/15: [=                             ] 4/75 batches, loss: 0.1416Epoch 3/15: [==                            ] 5/75 batches, loss: 0.1308Epoch 3/15: [==                            ] 6/75 batches, loss: 0.1336Epoch 3/15: [==                            ] 7/75 batches, loss: 0.1465Epoch 3/15: [===                           ] 8/75 batches, loss: 0.1568Epoch 3/15: [===                           ] 9/75 batches, loss: 0.1527Epoch 3/15: [====                          ] 10/75 batches, loss: 0.1592Epoch 3/15: [====                          ] 11/75 batches, loss: 0.1574Epoch 3/15: [====                          ] 12/75 batches, loss: 0.1552Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.1531Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.1526Epoch 3/15: [======                        ] 15/75 batches, loss: 0.1509Epoch 3/15: [======                        ] 16/75 batches, loss: 0.1532Epoch 3/15: [======                        ] 17/75 batches, loss: 0.1510Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.1493Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.1462Epoch 3/15: [========                      ] 20/75 batches, loss: 0.1469Epoch 3/15: [========                      ] 21/75 batches, loss: 0.1452Epoch 3/15: [========                      ] 22/75 batches, loss: 0.1418Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.1408Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.1418Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.1421Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.1435Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.1434Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.1410Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.1401Epoch 3/15: [============                  ] 30/75 batches, loss: 0.1400Epoch 3/15: [============                  ] 31/75 batches, loss: 0.1385Epoch 3/15: [============                  ] 32/75 batches, loss: 0.1352Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.1350Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.1340Epoch 3/15: [==============                ] 35/75 batches, loss: 0.1336Epoch 3/15: [==============                ] 36/75 batches, loss: 0.1328Epoch 3/15: [==============                ] 37/75 batches, loss: 0.1316Epoch 3/15: [===============               ] 38/75 batches, loss: 0.1311Epoch 3/15: [===============               ] 39/75 batches, loss: 0.1299Epoch 3/15: [================              ] 40/75 batches, loss: 0.1284Epoch 3/15: [================              ] 41/75 batches, loss: 0.1268Epoch 3/15: [================              ] 42/75 batches, loss: 0.1257Epoch 3/15: [=================             ] 43/75 batches, loss: 0.1252Epoch 3/15: [=================             ] 44/75 batches, loss: 0.1253Epoch 3/15: [==================            ] 45/75 batches, loss: 0.1245Epoch 3/15: [==================            ] 46/75 batches, loss: 0.1244Epoch 3/15: [==================            ] 47/75 batches, loss: 0.1240Epoch 3/15: [===================           ] 48/75 batches, loss: 0.1248Epoch 3/15: [===================           ] 49/75 batches, loss: 0.1250Epoch 3/15: [====================          ] 50/75 batches, loss: 0.1245Epoch 3/15: [====================          ] 51/75 batches, loss: 0.1239Epoch 3/15: [====================          ] 52/75 batches, loss: 0.1238Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.1226Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.1228Epoch 3/15: [======================        ] 55/75 batches, loss: 0.1225Epoch 3/15: [======================        ] 56/75 batches, loss: 0.1210Epoch 3/15: [======================        ] 57/75 batches, loss: 0.1200Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.1195Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.1190Epoch 3/15: [========================      ] 60/75 batches, loss: 0.1186Epoch 3/15: [========================      ] 61/75 batches, loss: 0.1195Epoch 3/15: [========================      ] 62/75 batches, loss: 0.1194Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.1191Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.1192Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.1185Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.1185Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.1181Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.1170Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.1178Epoch 3/15: [============================  ] 70/75 batches, loss: 0.1173Epoch 3/15: [============================  ] 71/75 batches, loss: 0.1171Epoch 3/15: [============================  ] 72/75 batches, loss: 0.1176Epoch 3/15: [============================= ] 73/75 batches, loss: 0.1174Epoch 3/15: [============================= ] 74/75 batches, loss: 0.1171Epoch 3/15: [==============================] 75/75 batches, loss: 0.1164
[2025-04-29 20:35:08,078][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.1164
[2025-04-29 20:35:08,494][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.0655, Metrics: {'mse': 0.07064883410930634, 'rmse': 0.26579848402371736, 'r2': -0.6880954504013062}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.0902Epoch 4/15: [                              ] 2/75 batches, loss: 0.1114Epoch 4/15: [=                             ] 3/75 batches, loss: 0.1071Epoch 4/15: [=                             ] 4/75 batches, loss: 0.1056Epoch 4/15: [==                            ] 5/75 batches, loss: 0.1008Epoch 4/15: [==                            ] 6/75 batches, loss: 0.0985Epoch 4/15: [==                            ] 7/75 batches, loss: 0.0998Epoch 4/15: [===                           ] 8/75 batches, loss: 0.0963Epoch 4/15: [===                           ] 9/75 batches, loss: 0.0973Epoch 4/15: [====                          ] 10/75 batches, loss: 0.0937Epoch 4/15: [====                          ] 11/75 batches, loss: 0.0936Epoch 4/15: [====                          ] 12/75 batches, loss: 0.0947Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.0936Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.0963Epoch 4/15: [======                        ] 15/75 batches, loss: 0.0966Epoch 4/15: [======                        ] 16/75 batches, loss: 0.0936Epoch 4/15: [======                        ] 17/75 batches, loss: 0.0940Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.0933Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.0920Epoch 4/15: [========                      ] 20/75 batches, loss: 0.0921Epoch 4/15: [========                      ] 21/75 batches, loss: 0.0926Epoch 4/15: [========                      ] 22/75 batches, loss: 0.0901Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.0903Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.0908Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.0896Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.0894Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.0889Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.0893Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.0902Epoch 4/15: [============                  ] 30/75 batches, loss: 0.0901Epoch 4/15: [============                  ] 31/75 batches, loss: 0.0886Epoch 4/15: [============                  ] 32/75 batches, loss: 0.0875Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.0880Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.0885Epoch 4/15: [==============                ] 35/75 batches, loss: 0.0874Epoch 4/15: [==============                ] 36/75 batches, loss: 0.0876Epoch 4/15: [==============                ] 37/75 batches, loss: 0.0872Epoch 4/15: [===============               ] 38/75 batches, loss: 0.0864Epoch 4/15: [===============               ] 39/75 batches, loss: 0.0855Epoch 4/15: [================              ] 40/75 batches, loss: 0.0862Epoch 4/15: [================              ] 41/75 batches, loss: 0.0860Epoch 4/15: [================              ] 42/75 batches, loss: 0.0865Epoch 4/15: [=================             ] 43/75 batches, loss: 0.0863Epoch 4/15: [=================             ] 44/75 batches, loss: 0.0873Epoch 4/15: [==================            ] 45/75 batches, loss: 0.0871Epoch 4/15: [==================            ] 46/75 batches, loss: 0.0864Epoch 4/15: [==================            ] 47/75 batches, loss: 0.0856Epoch 4/15: [===================           ] 48/75 batches, loss: 0.0855Epoch 4/15: [===================           ] 49/75 batches, loss: 0.0847Epoch 4/15: [====================          ] 50/75 batches, loss: 0.0853Epoch 4/15: [====================          ] 51/75 batches, loss: 0.0850Epoch 4/15: [====================          ] 52/75 batches, loss: 0.0845Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.0838Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.0835Epoch 4/15: [======================        ] 55/75 batches, loss: 0.0832Epoch 4/15: [======================        ] 56/75 batches, loss: 0.0828Epoch 4/15: [======================        ] 57/75 batches, loss: 0.0824Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.0816Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.0811Epoch 4/15: [========================      ] 60/75 batches, loss: 0.0807Epoch 4/15: [========================      ] 61/75 batches, loss: 0.0810Epoch 4/15: [========================      ] 62/75 batches, loss: 0.0812Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.0806Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.0803Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.0799Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.0799Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.0797Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.0795Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.0791Epoch 4/15: [============================  ] 70/75 batches, loss: 0.0787Epoch 4/15: [============================  ] 71/75 batches, loss: 0.0784Epoch 4/15: [============================  ] 72/75 batches, loss: 0.0782Epoch 4/15: [============================= ] 73/75 batches, loss: 0.0779Epoch 4/15: [============================= ] 74/75 batches, loss: 0.0773Epoch 4/15: [==============================] 75/75 batches, loss: 0.0769
[2025-04-29 20:35:13,450][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.0769
[2025-04-29 20:35:13,875][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.0488, Metrics: {'mse': 0.052340321242809296, 'rmse': 0.2287800717781365, 'r2': -0.2506287097930908}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.0946Epoch 5/15: [                              ] 2/75 batches, loss: 0.0927Epoch 5/15: [=                             ] 3/75 batches, loss: 0.0862Epoch 5/15: [=                             ] 4/75 batches, loss: 0.0824Epoch 5/15: [==                            ] 5/75 batches, loss: 0.0782Epoch 5/15: [==                            ] 6/75 batches, loss: 0.0753Epoch 5/15: [==                            ] 7/75 batches, loss: 0.0722Epoch 5/15: [===                           ] 8/75 batches, loss: 0.0658Epoch 5/15: [===                           ] 9/75 batches, loss: 0.0651Epoch 5/15: [====                          ] 10/75 batches, loss: 0.0652Epoch 5/15: [====                          ] 11/75 batches, loss: 0.0662Epoch 5/15: [====                          ] 12/75 batches, loss: 0.0649Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.0680Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.0661Epoch 5/15: [======                        ] 15/75 batches, loss: 0.0639Epoch 5/15: [======                        ] 16/75 batches, loss: 0.0627Epoch 5/15: [======                        ] 17/75 batches, loss: 0.0604Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.0597Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.0615Epoch 5/15: [========                      ] 20/75 batches, loss: 0.0610Epoch 5/15: [========                      ] 21/75 batches, loss: 0.0612Epoch 5/15: [========                      ] 22/75 batches, loss: 0.0604Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.0596Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.0595Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.0608Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.0614Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.0610Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.0617Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.0619Epoch 5/15: [============                  ] 30/75 batches, loss: 0.0632Epoch 5/15: [============                  ] 31/75 batches, loss: 0.0631Epoch 5/15: [============                  ] 32/75 batches, loss: 0.0630Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.0637Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.0629Epoch 5/15: [==============                ] 35/75 batches, loss: 0.0623Epoch 5/15: [==============                ] 36/75 batches, loss: 0.0619Epoch 5/15: [==============                ] 37/75 batches, loss: 0.0612Epoch 5/15: [===============               ] 38/75 batches, loss: 0.0614Epoch 5/15: [===============               ] 39/75 batches, loss: 0.0625Epoch 5/15: [================              ] 40/75 batches, loss: 0.0623Epoch 5/15: [================              ] 41/75 batches, loss: 0.0618Epoch 5/15: [================              ] 42/75 batches, loss: 0.0617Epoch 5/15: [=================             ] 43/75 batches, loss: 0.0616Epoch 5/15: [=================             ] 44/75 batches, loss: 0.0610Epoch 5/15: [==================            ] 45/75 batches, loss: 0.0600Epoch 5/15: [==================            ] 46/75 batches, loss: 0.0599Epoch 5/15: [==================            ] 47/75 batches, loss: 0.0597Epoch 5/15: [===================           ] 48/75 batches, loss: 0.0597Epoch 5/15: [===================           ] 49/75 batches, loss: 0.0606Epoch 5/15: [====================          ] 50/75 batches, loss: 0.0602Epoch 5/15: [====================          ] 51/75 batches, loss: 0.0608Epoch 5/15: [====================          ] 52/75 batches, loss: 0.0612Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.0606Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.0604Epoch 5/15: [======================        ] 55/75 batches, loss: 0.0601Epoch 5/15: [======================        ] 56/75 batches, loss: 0.0596Epoch 5/15: [======================        ] 57/75 batches, loss: 0.0591Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.0591Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.0593Epoch 5/15: [========================      ] 60/75 batches, loss: 0.0593Epoch 5/15: [========================      ] 61/75 batches, loss: 0.0594Epoch 5/15: [========================      ] 62/75 batches, loss: 0.0593Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.0590Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.0591Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.0593Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.0591Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.0586Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.0588Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.0586Epoch 5/15: [============================  ] 70/75 batches, loss: 0.0581Epoch 5/15: [============================  ] 71/75 batches, loss: 0.0578Epoch 5/15: [============================  ] 72/75 batches, loss: 0.0577Epoch 5/15: [============================= ] 73/75 batches, loss: 0.0573Epoch 5/15: [============================= ] 74/75 batches, loss: 0.0570Epoch 5/15: [==============================] 75/75 batches, loss: 0.0564
[2025-04-29 20:35:18,797][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.0564
[2025-04-29 20:35:19,222][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.0420, Metrics: {'mse': 0.04435524344444275, 'rmse': 0.21060684567326568, 'r2': -0.059831857681274414}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.0334Epoch 6/15: [                              ] 2/75 batches, loss: 0.0446Epoch 6/15: [=                             ] 3/75 batches, loss: 0.0407Epoch 6/15: [=                             ] 4/75 batches, loss: 0.0398Epoch 6/15: [==                            ] 5/75 batches, loss: 0.0430Epoch 6/15: [==                            ] 6/75 batches, loss: 0.0405Epoch 6/15: [==                            ] 7/75 batches, loss: 0.0407Epoch 6/15: [===                           ] 8/75 batches, loss: 0.0444Epoch 6/15: [===                           ] 9/75 batches, loss: 0.0419Epoch 6/15: [====                          ] 10/75 batches, loss: 0.0426Epoch 6/15: [====                          ] 11/75 batches, loss: 0.0419Epoch 6/15: [====                          ] 12/75 batches, loss: 0.0410Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.0430Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.0422Epoch 6/15: [======                        ] 15/75 batches, loss: 0.0452Epoch 6/15: [======                        ] 16/75 batches, loss: 0.0449Epoch 6/15: [======                        ] 17/75 batches, loss: 0.0462Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.0469Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.0464Epoch 6/15: [========                      ] 20/75 batches, loss: 0.0457Epoch 6/15: [========                      ] 21/75 batches, loss: 0.0452Epoch 6/15: [========                      ] 22/75 batches, loss: 0.0460Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.0470Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.0469Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.0463Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.0458Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.0466Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.0458Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.0455Epoch 6/15: [============                  ] 30/75 batches, loss: 0.0454Epoch 6/15: [============                  ] 31/75 batches, loss: 0.0450Epoch 6/15: [============                  ] 32/75 batches, loss: 0.0447Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.0445Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.0448Epoch 6/15: [==============                ] 35/75 batches, loss: 0.0442Epoch 6/15: [==============                ] 36/75 batches, loss: 0.0443Epoch 6/15: [==============                ] 37/75 batches, loss: 0.0443Epoch 6/15: [===============               ] 38/75 batches, loss: 0.0443Epoch 6/15: [===============               ] 39/75 batches, loss: 0.0447Epoch 6/15: [================              ] 40/75 batches, loss: 0.0445Epoch 6/15: [================              ] 41/75 batches, loss: 0.0441Epoch 6/15: [================              ] 42/75 batches, loss: 0.0443Epoch 6/15: [=================             ] 43/75 batches, loss: 0.0449Epoch 6/15: [=================             ] 44/75 batches, loss: 0.0449Epoch 6/15: [==================            ] 45/75 batches, loss: 0.0451Epoch 6/15: [==================            ] 46/75 batches, loss: 0.0450Epoch 6/15: [==================            ] 47/75 batches, loss: 0.0454Epoch 6/15: [===================           ] 48/75 batches, loss: 0.0450Epoch 6/15: [===================           ] 49/75 batches, loss: 0.0449Epoch 6/15: [====================          ] 50/75 batches, loss: 0.0456Epoch 6/15: [====================          ] 51/75 batches, loss: 0.0454Epoch 6/15: [====================          ] 52/75 batches, loss: 0.0452Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.0452Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.0453Epoch 6/15: [======================        ] 55/75 batches, loss: 0.0452Epoch 6/15: [======================        ] 56/75 batches, loss: 0.0451Epoch 6/15: [======================        ] 57/75 batches, loss: 0.0451Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.0449Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.0447Epoch 6/15: [========================      ] 60/75 batches, loss: 0.0448Epoch 6/15: [========================      ] 61/75 batches, loss: 0.0448Epoch 6/15: [========================      ] 62/75 batches, loss: 0.0448Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.0452Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.0451Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.0451Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.0450Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.0451Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.0451Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.0453Epoch 6/15: [============================  ] 70/75 batches, loss: 0.0454Epoch 6/15: [============================  ] 71/75 batches, loss: 0.0453Epoch 6/15: [============================  ] 72/75 batches, loss: 0.0452Epoch 6/15: [============================= ] 73/75 batches, loss: 0.0457Epoch 6/15: [============================= ] 74/75 batches, loss: 0.0457Epoch 6/15: [==============================] 75/75 batches, loss: 0.0454
[2025-04-29 20:35:24,177][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.0454
[2025-04-29 20:35:24,598][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0406, Metrics: {'mse': 0.04206984117627144, 'rmse': 0.20510933956373473, 'r2': -0.005224108695983887}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.0207Epoch 7/15: [                              ] 2/75 batches, loss: 0.0281Epoch 7/15: [=                             ] 3/75 batches, loss: 0.0325Epoch 7/15: [=                             ] 4/75 batches, loss: 0.0299Epoch 7/15: [==                            ] 5/75 batches, loss: 0.0293Epoch 7/15: [==                            ] 6/75 batches, loss: 0.0323Epoch 7/15: [==                            ] 7/75 batches, loss: 0.0340Epoch 7/15: [===                           ] 8/75 batches, loss: 0.0341Epoch 7/15: [===                           ] 9/75 batches, loss: 0.0334Epoch 7/15: [====                          ] 10/75 batches, loss: 0.0355Epoch 7/15: [====                          ] 11/75 batches, loss: 0.0367Epoch 7/15: [====                          ] 12/75 batches, loss: 0.0382Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.0371Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.0364Epoch 7/15: [======                        ] 15/75 batches, loss: 0.0361Epoch 7/15: [======                        ] 16/75 batches, loss: 0.0361Epoch 7/15: [======                        ] 17/75 batches, loss: 0.0363Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.0373Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.0373Epoch 7/15: [========                      ] 20/75 batches, loss: 0.0369Epoch 7/15: [========                      ] 21/75 batches, loss: 0.0372Epoch 7/15: [========                      ] 22/75 batches, loss: 0.0398Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.0395Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.0383Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.0382Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.0384Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.0375Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.0375Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.0379Epoch 7/15: [============                  ] 30/75 batches, loss: 0.0381Epoch 7/15: [============                  ] 31/75 batches, loss: 0.0380Epoch 7/15: [============                  ] 32/75 batches, loss: 0.0377Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.0377Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.0385Epoch 7/15: [==============                ] 35/75 batches, loss: 0.0389Epoch 7/15: [==============                ] 36/75 batches, loss: 0.0387Epoch 7/15: [==============                ] 37/75 batches, loss: 0.0392Epoch 7/15: [===============               ] 38/75 batches, loss: 0.0395Epoch 7/15: [===============               ] 39/75 batches, loss: 0.0396Epoch 7/15: [================              ] 40/75 batches, loss: 0.0397Epoch 7/15: [================              ] 41/75 batches, loss: 0.0397Epoch 7/15: [================              ] 42/75 batches, loss: 0.0397Epoch 7/15: [=================             ] 43/75 batches, loss: 0.0395Epoch 7/15: [=================             ] 44/75 batches, loss: 0.0390Epoch 7/15: [==================            ] 45/75 batches, loss: 0.0391Epoch 7/15: [==================            ] 46/75 batches, loss: 0.0393Epoch 7/15: [==================            ] 47/75 batches, loss: 0.0393Epoch 7/15: [===================           ] 48/75 batches, loss: 0.0389Epoch 7/15: [===================           ] 49/75 batches, loss: 0.0388Epoch 7/15: [====================          ] 50/75 batches, loss: 0.0389Epoch 7/15: [====================          ] 51/75 batches, loss: 0.0395Epoch 7/15: [====================          ] 52/75 batches, loss: 0.0399Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.0401Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.0406Epoch 7/15: [======================        ] 55/75 batches, loss: 0.0408Epoch 7/15: [======================        ] 56/75 batches, loss: 0.0407Epoch 7/15: [======================        ] 57/75 batches, loss: 0.0402Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.0401Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.0401Epoch 7/15: [========================      ] 60/75 batches, loss: 0.0400Epoch 7/15: [========================      ] 61/75 batches, loss: 0.0398Epoch 7/15: [========================      ] 62/75 batches, loss: 0.0402Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.0409Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.0409Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.0412Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.0412Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.0409Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.0409Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.0408Epoch 7/15: [============================  ] 70/75 batches, loss: 0.0407Epoch 7/15: [============================  ] 71/75 batches, loss: 0.0406Epoch 7/15: [============================  ] 72/75 batches, loss: 0.0408Epoch 7/15: [============================= ] 73/75 batches, loss: 0.0408Epoch 7/15: [============================= ] 74/75 batches, loss: 0.0406Epoch 7/15: [==============================] 75/75 batches, loss: 0.0405
[2025-04-29 20:35:29,619][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.0405
[2025-04-29 20:35:30,062][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0415, Metrics: {'mse': 0.04232846200466156, 'rmse': 0.2057388198776827, 'r2': -0.011403560638427734}
[2025-04-29 20:35:30,063][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.0284Epoch 8/15: [                              ] 2/75 batches, loss: 0.0431Epoch 8/15: [=                             ] 3/75 batches, loss: 0.0429Epoch 8/15: [=                             ] 4/75 batches, loss: 0.0457Epoch 8/15: [==                            ] 5/75 batches, loss: 0.0475Epoch 8/15: [==                            ] 6/75 batches, loss: 0.0457Epoch 8/15: [==                            ] 7/75 batches, loss: 0.0442Epoch 8/15: [===                           ] 8/75 batches, loss: 0.0425Epoch 8/15: [===                           ] 9/75 batches, loss: 0.0413Epoch 8/15: [====                          ] 10/75 batches, loss: 0.0395Epoch 8/15: [====                          ] 11/75 batches, loss: 0.0404Epoch 8/15: [====                          ] 12/75 batches, loss: 0.0412Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.0387Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.0373Epoch 8/15: [======                        ] 15/75 batches, loss: 0.0399Epoch 8/15: [======                        ] 16/75 batches, loss: 0.0395Epoch 8/15: [======                        ] 17/75 batches, loss: 0.0392Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.0384Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.0380Epoch 8/15: [========                      ] 20/75 batches, loss: 0.0389Epoch 8/15: [========                      ] 21/75 batches, loss: 0.0385Epoch 8/15: [========                      ] 22/75 batches, loss: 0.0390Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.0385Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.0386Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.0390Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.0386Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.0391Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.0387Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.0391Epoch 8/15: [============                  ] 30/75 batches, loss: 0.0395Epoch 8/15: [============                  ] 31/75 batches, loss: 0.0393Epoch 8/15: [============                  ] 32/75 batches, loss: 0.0392Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.0386Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.0392Epoch 8/15: [==============                ] 35/75 batches, loss: 0.0397Epoch 8/15: [==============                ] 36/75 batches, loss: 0.0395Epoch 8/15: [==============                ] 37/75 batches, loss: 0.0398Epoch 8/15: [===============               ] 38/75 batches, loss: 0.0406Epoch 8/15: [===============               ] 39/75 batches, loss: 0.0407Epoch 8/15: [================              ] 40/75 batches, loss: 0.0407Epoch 8/15: [================              ] 41/75 batches, loss: 0.0402Epoch 8/15: [================              ] 42/75 batches, loss: 0.0398Epoch 8/15: [=================             ] 43/75 batches, loss: 0.0392Epoch 8/15: [=================             ] 44/75 batches, loss: 0.0393Epoch 8/15: [==================            ] 45/75 batches, loss: 0.0392Epoch 8/15: [==================            ] 46/75 batches, loss: 0.0390Epoch 8/15: [==================            ] 47/75 batches, loss: 0.0389Epoch 8/15: [===================           ] 48/75 batches, loss: 0.0386Epoch 8/15: [===================           ] 49/75 batches, loss: 0.0391Epoch 8/15: [====================          ] 50/75 batches, loss: 0.0395Epoch 8/15: [====================          ] 51/75 batches, loss: 0.0393Epoch 8/15: [====================          ] 52/75 batches, loss: 0.0392Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.0387Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.0385Epoch 8/15: [======================        ] 55/75 batches, loss: 0.0384Epoch 8/15: [======================        ] 56/75 batches, loss: 0.0388Epoch 8/15: [======================        ] 57/75 batches, loss: 0.0390Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.0387Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.0387Epoch 8/15: [========================      ] 60/75 batches, loss: 0.0384Epoch 8/15: [========================      ] 61/75 batches, loss: 0.0379Epoch 8/15: [========================      ] 62/75 batches, loss: 0.0380Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.0384Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.0385Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.0384Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.0385Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.0381Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.0384Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.0387Epoch 8/15: [============================  ] 70/75 batches, loss: 0.0385Epoch 8/15: [============================  ] 71/75 batches, loss: 0.0384Epoch 8/15: [============================  ] 72/75 batches, loss: 0.0385Epoch 8/15: [============================= ] 73/75 batches, loss: 0.0387Epoch 8/15: [============================= ] 74/75 batches, loss: 0.0388Epoch 8/15: [==============================] 75/75 batches, loss: 0.0386
[2025-04-29 20:35:34,463][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.0386
[2025-04-29 20:35:34,909][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0431, Metrics: {'mse': 0.04346707835793495, 'rmse': 0.20848759761178828, 'r2': -0.038609862327575684}
[2025-04-29 20:35:34,910][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.0268Epoch 9/15: [                              ] 2/75 batches, loss: 0.0344Epoch 9/15: [=                             ] 3/75 batches, loss: 0.0346Epoch 9/15: [=                             ] 4/75 batches, loss: 0.0420Epoch 9/15: [==                            ] 5/75 batches, loss: 0.0414Epoch 9/15: [==                            ] 6/75 batches, loss: 0.0384Epoch 9/15: [==                            ] 7/75 batches, loss: 0.0365Epoch 9/15: [===                           ] 8/75 batches, loss: 0.0351Epoch 9/15: [===                           ] 9/75 batches, loss: 0.0351Epoch 9/15: [====                          ] 10/75 batches, loss: 0.0354Epoch 9/15: [====                          ] 11/75 batches, loss: 0.0345Epoch 9/15: [====                          ] 12/75 batches, loss: 0.0340Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.0346Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.0367Epoch 9/15: [======                        ] 15/75 batches, loss: 0.0352Epoch 9/15: [======                        ] 16/75 batches, loss: 0.0374Epoch 9/15: [======                        ] 17/75 batches, loss: 0.0371Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.0370Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.0383Epoch 9/15: [========                      ] 20/75 batches, loss: 0.0384Epoch 9/15: [========                      ] 21/75 batches, loss: 0.0379Epoch 9/15: [========                      ] 22/75 batches, loss: 0.0381Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.0379Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.0375Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.0373Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.0368Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.0369Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.0367Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.0363Epoch 9/15: [============                  ] 30/75 batches, loss: 0.0364Epoch 9/15: [============                  ] 31/75 batches, loss: 0.0366Epoch 9/15: [============                  ] 32/75 batches, loss: 0.0361Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.0360Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.0360Epoch 9/15: [==============                ] 35/75 batches, loss: 0.0359Epoch 9/15: [==============                ] 36/75 batches, loss: 0.0353Epoch 9/15: [==============                ] 37/75 batches, loss: 0.0346Epoch 9/15: [===============               ] 38/75 batches, loss: 0.0347Epoch 9/15: [===============               ] 39/75 batches, loss: 0.0354Epoch 9/15: [================              ] 40/75 batches, loss: 0.0352Epoch 9/15: [================              ] 41/75 batches, loss: 0.0361Epoch 9/15: [================              ] 42/75 batches, loss: 0.0358Epoch 9/15: [=================             ] 43/75 batches, loss: 0.0358Epoch 9/15: [=================             ] 44/75 batches, loss: 0.0360Epoch 9/15: [==================            ] 45/75 batches, loss: 0.0360Epoch 9/15: [==================            ] 46/75 batches, loss: 0.0363Epoch 9/15: [==================            ] 47/75 batches, loss: 0.0363Epoch 9/15: [===================           ] 48/75 batches, loss: 0.0364Epoch 9/15: [===================           ] 49/75 batches, loss: 0.0367Epoch 9/15: [====================          ] 50/75 batches, loss: 0.0370Epoch 9/15: [====================          ] 51/75 batches, loss: 0.0369Epoch 9/15: [====================          ] 52/75 batches, loss: 0.0369Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.0370Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.0368Epoch 9/15: [======================        ] 55/75 batches, loss: 0.0367Epoch 9/15: [======================        ] 56/75 batches, loss: 0.0369Epoch 9/15: [======================        ] 57/75 batches, loss: 0.0368Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.0368Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.0372Epoch 9/15: [========================      ] 60/75 batches, loss: 0.0372Epoch 9/15: [========================      ] 61/75 batches, loss: 0.0375Epoch 9/15: [========================      ] 62/75 batches, loss: 0.0378Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.0380Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.0379Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.0377Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.0375Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.0375Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.0374Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.0373Epoch 9/15: [============================  ] 70/75 batches, loss: 0.0380Epoch 9/15: [============================  ] 71/75 batches, loss: 0.0379Epoch 9/15: [============================  ] 72/75 batches, loss: 0.0379Epoch 9/15: [============================= ] 73/75 batches, loss: 0.0377Epoch 9/15: [============================= ] 74/75 batches, loss: 0.0375Epoch 9/15: [==============================] 75/75 batches, loss: 0.0374
[2025-04-29 20:35:39,313][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.0374
[2025-04-29 20:35:39,744][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0444, Metrics: {'mse': 0.044557854533195496, 'rmse': 0.21108731495093563, 'r2': -0.06467306613922119}
[2025-04-29 20:35:39,745][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 20:35:39,745][src.training.lm_trainer][INFO] - Early stopping at epoch 9
[2025-04-29 20:35:39,745][src.training.lm_trainer][INFO] - Training completed in 47.28 seconds
[2025-04-29 20:35:39,745][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 20:35:45,292][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.03356577455997467, 'rmse': 0.18320964647085228, 'r2': -0.25111591815948486}
[2025-04-29 20:35:45,293][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.04206984117627144, 'rmse': 0.20510933956373473, 'r2': -0.005224108695983887}
[2025-04-29 20:35:45,293][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.04674325883388519, 'rmse': 0.2162018936870933, 'r2': -0.21289420127868652}
[2025-04-29 20:35:47,540][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/complexity/en/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▁▁▁
wandb:     best_val_mse █▄▃▂▁▁
wandb:      best_val_r2 ▁▅▆▇██
wandb:    best_val_rmse █▅▃▂▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▃▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▁▁▁▁▁▁
wandb:          val_mse █▄▃▂▁▁▁▁▁
wandb:           val_r2 ▁▅▆▇█████
wandb:         val_rmse █▅▃▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04058
wandb:     best_val_mse 0.04207
wandb:      best_val_r2 -0.00522
wandb:    best_val_rmse 0.20511
wandb:            epoch 9
wandb:   final_test_mse 0.04674
wandb:    final_test_r2 -0.21289
wandb:  final_test_rmse 0.2162
wandb:  final_train_mse 0.03357
wandb:   final_train_r2 -0.25112
wandb: final_train_rmse 0.18321
wandb:    final_val_mse 0.04207
wandb:     final_val_r2 -0.00522
wandb:   final_val_rmse 0.20511
wandb:    learning_rate 2e-05
wandb:       train_loss 0.03745
wandb:       train_time 47.28165
wandb:         val_loss 0.04442
wandb:          val_mse 0.04456
wandb:           val_r2 -0.06467
wandb:         val_rmse 0.21109
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203439-v2rmrxm6
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203439-v2rmrxm6/logs
Standard experiment completed successfully: layer_6_complexity_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/complexity/results.json
Running question_type control experiment for language en, layer 6, control 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:36:01,342][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/question_type/control1
experiment_name: layer_6_question_type_control1_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  debug_mode: true
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 20:36:01,342][__main__][INFO] - Normalized task: question_type
[2025-04-29 20:36:01,342][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 20:36:01,342][__main__][INFO] - Determined Task Type: classification
[2025-04-29 20:36:01,346][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-29 20:36:01,347][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 20:36:02,808][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:36:05,733][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:36:05,734][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:36:05,789][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-04-29 20:36:05,818][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-04-29 20:36:05,908][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-29 20:36:05,920][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:36:05,920][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-29 20:36:05,922][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:36:05,948][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:36:05,979][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:36:05,992][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-29 20:36:05,993][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:36:05,993][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-29 20:36:05,995][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:36:06,017][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:36:06,047][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:36:06,058][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-29 20:36:06,060][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:36:06,060][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-29 20:36:06,061][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-29 20:36:06,062][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:36:06,062][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:36:06,062][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:36:06,062][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:36:06,062][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-29 20:36:06,062][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-29 20:36:06,062][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-29 20:36:06,062][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 20:36:06,063][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:36:06,063][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:36:06,063][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:36:06,063][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:36:06,063][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-29 20:36:06,063][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-29 20:36:06,063][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-29 20:36:06,064][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:36:06,064][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:36:06,064][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:36:06,064][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:36:06,064][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:36:06,064][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-29 20:36:06,064][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-29 20:36:06,064][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-29 20:36:06,065][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:36:06,065][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-29 20:36:06,065][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:36:06,065][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:36:06,065][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:36:10,340][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:36:10,341][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 20:36:10,342][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 20:36:10,342][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=6, freeze_model=True, finetune=False
[2025-04-29 20:36:10,343][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:36:10,343][__main__][INFO] - Successfully created model for en
Epoch 1/15: [Epoch 1/15: [                              ] 1/75 batches, loss: 0.6951Epoch 1/15: [                              ] 2/75 batches, loss: 0.7206Epoch 1/15: [=                             ] 3/75 batches, loss: 0.7145Epoch 1/15: [=                             ] 4/75 batches, loss: 0.7103Epoch 1/15: [==                            ] 5/75 batches, loss: 0.7058Epoch 1/15: [==                            ] 6/75 batches, loss: 0.7094Epoch 1/15: [==                            ] 7/75 batches, loss: 0.7049Epoch 1/15: [===                           ] 8/75 batches, loss: 0.7069Epoch 1/15: [===                           ] 9/75 batches, loss: 0.7036Epoch 1/15: [====                          ] 10/75 batches, loss: 0.7043Epoch 1/15: [====                          ] 11/75 batches, loss: 0.7052Epoch 1/15: [====                          ] 12/75 batches, loss: 0.7063Epoch 1/15: [=====                         ] 13/75 batches, loss: 0.7027Epoch 1/15: [=====                         ] 14/75 batches, loss: 0.7046Epoch 1/15: [======                        ] 15/75 batches, loss: 0.7071Epoch 1/15: [======                        ] 16/75 batches, loss: 0.7073Epoch 1/15: [======                        ] 17/75 batches, loss: 0.7064Epoch 1/15: [=======                       ] 18/75 batches, loss: 0.7049Epoch 1/15: [=======                       ] 19/75 batches, loss: 0.7050Epoch 1/15: [========                      ] 20/75 batches, loss: 0.7028Epoch 1/15: [========                      ] 21/75 batches, loss: 0.7037Epoch 1/15: [========                      ] 22/75 batches, loss: 0.7052Epoch 1/15: [=========                     ] 23/75 batches, loss: 0.7052Epoch 1/15: [=========                     ] 24/75 batches, loss: 0.7045Epoch 1/15: [==========                    ] 25/75 batches, loss: 0.7035Epoch 1/15: [==========                    ] 26/75 batches, loss: 0.7030Epoch 1/15: [==========                    ] 27/75 batches, loss: 0.7036Epoch 1/15: [===========                   ] 28/75 batches, loss: 0.7037Epoch 1/15: [===========                   ] 29/75 batches, loss: 0.7037Epoch 1/15: [============                  ] 30/75 batches, loss: 0.7034Epoch 1/15: [============                  ] 31/75 batches, loss: 0.7030Epoch 1/15: [============                  ] 32/75 batches, loss: 0.7032Epoch 1/15: [=============                 ] 33/75 batches, loss: 0.7029Epoch 1/15: [=============                 ] 34/75 batches, loss: 0.7019Epoch 1/15: [==============                ] 35/75 batches, loss: 0.7013Epoch 1/15: [==============                ] 36/75 batches, loss: 0.7005Epoch 1/15: [==============                ] 37/75 batches, loss: 0.7013Epoch 1/15: [===============               ] 38/75 batches, loss: 0.7011Epoch 1/15: [===============               ] 39/75 batches, loss: 0.7010Epoch 1/15: [================              ] 40/75 batches, loss: 0.7001Epoch 1/15: [================              ] 41/75 batches, loss: 0.6993Epoch 1/15: [================              ] 42/75 batches, loss: 0.6987Epoch 1/15: [=================             ] 43/75 batches, loss: 0.6980Epoch 1/15: [=================             ] 44/75 batches, loss: 0.6981Epoch 1/15: [==================            ] 45/75 batches, loss: 0.6985Epoch 1/15: [==================            ] 46/75 batches, loss: 0.6989Epoch 1/15: [==================            ] 47/75 batches, loss: 0.6980Epoch 1/15: [===================           ] 48/75 batches, loss: 0.6977Epoch 1/15: [===================           ] 49/75 batches, loss: 0.6977Epoch 1/15: [====================          ] 50/75 batches, loss: 0.6976Epoch 1/15: [====================          ] 51/75 batches, loss: 0.6977Epoch 1/15: [====================          ] 52/75 batches, loss: 0.6966Epoch 1/15: [=====================         ] 53/75 batches, loss: 0.6962Epoch 1/15: [=====================         ] 54/75 batches, loss: 0.6965Epoch 1/15: [======================        ] 55/75 batches, loss: 0.6970Epoch 1/15: [======================        ] 56/75 batches, loss: 0.6966Epoch 1/15: [======================        ] 57/75 batches, loss: 0.6962Epoch 1/15: [=======================       ] 58/75 batches, loss: 0.6955Epoch 1/15: [=======================       ] 59/75 batches, loss: 0.6954Epoch 1/15: [========================      ] 60/75 batches, loss: 0.6953Epoch 1/15: [========================      ] 61/75 batches, loss: 0.6957Epoch 1/15: [========================      ] 62/75 batches, loss: 0.6955Epoch 1/15: [=========================     ] 63/75 batches, loss: 0.6959Epoch 1/15: [=========================     ] 64/75 batches, loss: 0.6950Epoch 1/15: [==========================    ] 65/75 batches, loss: 0.6956Epoch 1/15: [==========================    ] 66/75 batches, loss: 0.6953Epoch 1/15: [==========================    ] 67/75 batches, loss: 0.6958Epoch 1/15: [===========================   ] 68/75 batches, loss: 0.6962Epoch 1/15: [===========================   ] 69/75 batches, loss: 0.6961Epoch 1/15: [============================  ] 70/75 batches, loss: 0.6964Epoch 1/15: [============================  ] 71/75 batches, loss: 0.6968Epoch 1/15: [============================  ] 72/75 batches, loss: 0.6973Epoch 1/15: [============================= ] 73/75 batches, loss: 0.6970Epoch 1/15: [============================= ] 74/75 batches, loss: 0.6968Epoch 1/15: [==============================] 75/75 batches, loss: 0.6970
[2025-04-29 20:36:18,279][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6970
[2025-04-29 20:36:18,679][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6939, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/75 batches, loss: 0.6820Epoch 2/15: [                              ] 2/75 batches, loss: 0.6943Epoch 2/15: [=                             ] 3/75 batches, loss: 0.6987Epoch 2/15: [=                             ] 4/75 batches, loss: 0.6980Epoch 2/15: [==                            ] 5/75 batches, loss: 0.6991Epoch 2/15: [==                            ] 6/75 batches, loss: 0.6961Epoch 2/15: [==                            ] 7/75 batches, loss: 0.6999Epoch 2/15: [===                           ] 8/75 batches, loss: 0.7034Epoch 2/15: [===                           ] 9/75 batches, loss: 0.7038Epoch 2/15: [====                          ] 10/75 batches, loss: 0.7053Epoch 2/15: [====                          ] 11/75 batches, loss: 0.7039Epoch 2/15: [====                          ] 12/75 batches, loss: 0.7008Epoch 2/15: [=====                         ] 13/75 batches, loss: 0.7024Epoch 2/15: [=====                         ] 14/75 batches, loss: 0.7004Epoch 2/15: [======                        ] 15/75 batches, loss: 0.6990Epoch 2/15: [======                        ] 16/75 batches, loss: 0.6992Epoch 2/15: [======                        ] 17/75 batches, loss: 0.6983Epoch 2/15: [=======                       ] 18/75 batches, loss: 0.6982Epoch 2/15: [=======                       ] 19/75 batches, loss: 0.6992Epoch 2/15: [========                      ] 20/75 batches, loss: 0.6999Epoch 2/15: [========                      ] 21/75 batches, loss: 0.7000Epoch 2/15: [========                      ] 22/75 batches, loss: 0.7002Epoch 2/15: [=========                     ] 23/75 batches, loss: 0.6999Epoch 2/15: [=========                     ] 24/75 batches, loss: 0.7008Epoch 2/15: [==========                    ] 25/75 batches, loss: 0.7012Epoch 2/15: [==========                    ] 26/75 batches, loss: 0.7007Epoch 2/15: [==========                    ] 27/75 batches, loss: 0.7014Epoch 2/15: [===========                   ] 28/75 batches, loss: 0.7018Epoch 2/15: [===========                   ] 29/75 batches, loss: 0.7017Epoch 2/15: [============                  ] 30/75 batches, loss: 0.7030Epoch 2/15: [============                  ] 31/75 batches, loss: 0.7024Epoch 2/15: [============                  ] 32/75 batches, loss: 0.7023Epoch 2/15: [=============                 ] 33/75 batches, loss: 0.7018Epoch 2/15: [=============                 ] 34/75 batches, loss: 0.7011Epoch 2/15: [==============                ] 35/75 batches, loss: 0.7021Epoch 2/15: [==============                ] 36/75 batches, loss: 0.7018Epoch 2/15: [==============                ] 37/75 batches, loss: 0.7014Epoch 2/15: [===============               ] 38/75 batches, loss: 0.7015Epoch 2/15: [===============               ] 39/75 batches, loss: 0.7017Epoch 2/15: [================              ] 40/75 batches, loss: 0.7011Epoch 2/15: [================              ] 41/75 batches, loss: 0.7005Epoch 2/15: [================              ] 42/75 batches, loss: 0.7002Epoch 2/15: [=================             ] 43/75 batches, loss: 0.6997Epoch 2/15: [=================             ] 44/75 batches, loss: 0.6998Epoch 2/15: [==================            ] 45/75 batches, loss: 0.7001Epoch 2/15: [==================            ] 46/75 batches, loss: 0.6999Epoch 2/15: [==================            ] 47/75 batches, loss: 0.6997Epoch 2/15: [===================           ] 48/75 batches, loss: 0.7003Epoch 2/15: [===================           ] 49/75 batches, loss: 0.6991Epoch 2/15: [====================          ] 50/75 batches, loss: 0.6992Epoch 2/15: [====================          ] 51/75 batches, loss: 0.6986Epoch 2/15: [====================          ] 52/75 batches, loss: 0.6981Epoch 2/15: [=====================         ] 53/75 batches, loss: 0.6979Epoch 2/15: [=====================         ] 54/75 batches, loss: 0.6976Epoch 2/15: [======================        ] 55/75 batches, loss: 0.6981Epoch 2/15: [======================        ] 56/75 batches, loss: 0.6982Epoch 2/15: [======================        ] 57/75 batches, loss: 0.6984Epoch 2/15: [=======================       ] 58/75 batches, loss: 0.6984Epoch 2/15: [=======================       ] 59/75 batches, loss: 0.6982Epoch 2/15: [========================      ] 60/75 batches, loss: 0.6976Epoch 2/15: [========================      ] 61/75 batches, loss: 0.6978Epoch 2/15: [========================      ] 62/75 batches, loss: 0.6974Epoch 2/15: [=========================     ] 63/75 batches, loss: 0.6970Epoch 2/15: [=========================     ] 64/75 batches, loss: 0.6969Epoch 2/15: [==========================    ] 65/75 batches, loss: 0.6971Epoch 2/15: [==========================    ] 66/75 batches, loss: 0.6969Epoch 2/15: [==========================    ] 67/75 batches, loss: 0.6969Epoch 2/15: [===========================   ] 68/75 batches, loss: 0.6966Epoch 2/15: [===========================   ] 69/75 batches, loss: 0.6969Epoch 2/15: [============================  ] 70/75 batches, loss: 0.6965Epoch 2/15: [============================  ] 71/75 batches, loss: 0.6965Epoch 2/15: [============================  ] 72/75 batches, loss: 0.6966Epoch 2/15: [============================= ] 73/75 batches, loss: 0.6963Epoch 2/15: [============================= ] 74/75 batches, loss: 0.6964Epoch 2/15: [==============================] 75/75 batches, loss: 0.6962
[2025-04-29 20:36:23,600][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6962
[2025-04-29 20:36:24,009][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6933, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/15: [Epoch 3/15: [                              ] 1/75 batches, loss: 0.6966Epoch 3/15: [                              ] 2/75 batches, loss: 0.6926Epoch 3/15: [=                             ] 3/75 batches, loss: 0.6853Epoch 3/15: [=                             ] 4/75 batches, loss: 0.6840Epoch 3/15: [==                            ] 5/75 batches, loss: 0.6857Epoch 3/15: [==                            ] 6/75 batches, loss: 0.6862Epoch 3/15: [==                            ] 7/75 batches, loss: 0.6903Epoch 3/15: [===                           ] 8/75 batches, loss: 0.6897Epoch 3/15: [===                           ] 9/75 batches, loss: 0.6915Epoch 3/15: [====                          ] 10/75 batches, loss: 0.6891Epoch 3/15: [====                          ] 11/75 batches, loss: 0.6892Epoch 3/15: [====                          ] 12/75 batches, loss: 0.6900Epoch 3/15: [=====                         ] 13/75 batches, loss: 0.6913Epoch 3/15: [=====                         ] 14/75 batches, loss: 0.6935Epoch 3/15: [======                        ] 15/75 batches, loss: 0.6943Epoch 3/15: [======                        ] 16/75 batches, loss: 0.6967Epoch 3/15: [======                        ] 17/75 batches, loss: 0.6975Epoch 3/15: [=======                       ] 18/75 batches, loss: 0.6976Epoch 3/15: [=======                       ] 19/75 batches, loss: 0.6979Epoch 3/15: [========                      ] 20/75 batches, loss: 0.6978Epoch 3/15: [========                      ] 21/75 batches, loss: 0.6975Epoch 3/15: [========                      ] 22/75 batches, loss: 0.6965Epoch 3/15: [=========                     ] 23/75 batches, loss: 0.6961Epoch 3/15: [=========                     ] 24/75 batches, loss: 0.6969Epoch 3/15: [==========                    ] 25/75 batches, loss: 0.6969Epoch 3/15: [==========                    ] 26/75 batches, loss: 0.6967Epoch 3/15: [==========                    ] 27/75 batches, loss: 0.6972Epoch 3/15: [===========                   ] 28/75 batches, loss: 0.6981Epoch 3/15: [===========                   ] 29/75 batches, loss: 0.6977Epoch 3/15: [============                  ] 30/75 batches, loss: 0.6972Epoch 3/15: [============                  ] 31/75 batches, loss: 0.6974Epoch 3/15: [============                  ] 32/75 batches, loss: 0.6974Epoch 3/15: [=============                 ] 33/75 batches, loss: 0.6976Epoch 3/15: [=============                 ] 34/75 batches, loss: 0.6982Epoch 3/15: [==============                ] 35/75 batches, loss: 0.6980Epoch 3/15: [==============                ] 36/75 batches, loss: 0.6975Epoch 3/15: [==============                ] 37/75 batches, loss: 0.6980Epoch 3/15: [===============               ] 38/75 batches, loss: 0.6979Epoch 3/15: [===============               ] 39/75 batches, loss: 0.6982Epoch 3/15: [================              ] 40/75 batches, loss: 0.6985Epoch 3/15: [================              ] 41/75 batches, loss: 0.6992Epoch 3/15: [================              ] 42/75 batches, loss: 0.6996Epoch 3/15: [=================             ] 43/75 batches, loss: 0.6998Epoch 3/15: [=================             ] 44/75 batches, loss: 0.6999Epoch 3/15: [==================            ] 45/75 batches, loss: 0.6995Epoch 3/15: [==================            ] 46/75 batches, loss: 0.6996Epoch 3/15: [==================            ] 47/75 batches, loss: 0.7001Epoch 3/15: [===================           ] 48/75 batches, loss: 0.7000Epoch 3/15: [===================           ] 49/75 batches, loss: 0.6993Epoch 3/15: [====================          ] 50/75 batches, loss: 0.6995Epoch 3/15: [====================          ] 51/75 batches, loss: 0.6993Epoch 3/15: [====================          ] 52/75 batches, loss: 0.6995Epoch 3/15: [=====================         ] 53/75 batches, loss: 0.7001Epoch 3/15: [=====================         ] 54/75 batches, loss: 0.6992Epoch 3/15: [======================        ] 55/75 batches, loss: 0.6989Epoch 3/15: [======================        ] 56/75 batches, loss: 0.6989Epoch 3/15: [======================        ] 57/75 batches, loss: 0.6991Epoch 3/15: [=======================       ] 58/75 batches, loss: 0.6987Epoch 3/15: [=======================       ] 59/75 batches, loss: 0.6984Epoch 3/15: [========================      ] 60/75 batches, loss: 0.6984Epoch 3/15: [========================      ] 61/75 batches, loss: 0.6987Epoch 3/15: [========================      ] 62/75 batches, loss: 0.6992Epoch 3/15: [=========================     ] 63/75 batches, loss: 0.6994Epoch 3/15: [=========================     ] 64/75 batches, loss: 0.6990Epoch 3/15: [==========================    ] 65/75 batches, loss: 0.6990Epoch 3/15: [==========================    ] 66/75 batches, loss: 0.6988Epoch 3/15: [==========================    ] 67/75 batches, loss: 0.6987Epoch 3/15: [===========================   ] 68/75 batches, loss: 0.6984Epoch 3/15: [===========================   ] 69/75 batches, loss: 0.6984Epoch 3/15: [============================  ] 70/75 batches, loss: 0.6983Epoch 3/15: [============================  ] 71/75 batches, loss: 0.6982Epoch 3/15: [============================  ] 72/75 batches, loss: 0.6979Epoch 3/15: [============================= ] 73/75 batches, loss: 0.6982Epoch 3/15: [============================= ] 74/75 batches, loss: 0.6976Epoch 3/15: [==============================] 75/75 batches, loss: 0.6978
[2025-04-29 20:36:28,986][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6978
[2025-04-29 20:36:29,407][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6930, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 4/15: [Epoch 4/15: [                              ] 1/75 batches, loss: 0.6694Epoch 4/15: [                              ] 2/75 batches, loss: 0.6718Epoch 4/15: [=                             ] 3/75 batches, loss: 0.6776Epoch 4/15: [=                             ] 4/75 batches, loss: 0.6795Epoch 4/15: [==                            ] 5/75 batches, loss: 0.6795Epoch 4/15: [==                            ] 6/75 batches, loss: 0.6829Epoch 4/15: [==                            ] 7/75 batches, loss: 0.6844Epoch 4/15: [===                           ] 8/75 batches, loss: 0.6889Epoch 4/15: [===                           ] 9/75 batches, loss: 0.6907Epoch 4/15: [====                          ] 10/75 batches, loss: 0.6905Epoch 4/15: [====                          ] 11/75 batches, loss: 0.6933Epoch 4/15: [====                          ] 12/75 batches, loss: 0.6941Epoch 4/15: [=====                         ] 13/75 batches, loss: 0.6931Epoch 4/15: [=====                         ] 14/75 batches, loss: 0.6946Epoch 4/15: [======                        ] 15/75 batches, loss: 0.6943Epoch 4/15: [======                        ] 16/75 batches, loss: 0.6945Epoch 4/15: [======                        ] 17/75 batches, loss: 0.6942Epoch 4/15: [=======                       ] 18/75 batches, loss: 0.6936Epoch 4/15: [=======                       ] 19/75 batches, loss: 0.6933Epoch 4/15: [========                      ] 20/75 batches, loss: 0.6929Epoch 4/15: [========                      ] 21/75 batches, loss: 0.6920Epoch 4/15: [========                      ] 22/75 batches, loss: 0.6920Epoch 4/15: [=========                     ] 23/75 batches, loss: 0.6932Epoch 4/15: [=========                     ] 24/75 batches, loss: 0.6938Epoch 4/15: [==========                    ] 25/75 batches, loss: 0.6940Epoch 4/15: [==========                    ] 26/75 batches, loss: 0.6935Epoch 4/15: [==========                    ] 27/75 batches, loss: 0.6932Epoch 4/15: [===========                   ] 28/75 batches, loss: 0.6939Epoch 4/15: [===========                   ] 29/75 batches, loss: 0.6941Epoch 4/15: [============                  ] 30/75 batches, loss: 0.6940Epoch 4/15: [============                  ] 31/75 batches, loss: 0.6937Epoch 4/15: [============                  ] 32/75 batches, loss: 0.6937Epoch 4/15: [=============                 ] 33/75 batches, loss: 0.6938Epoch 4/15: [=============                 ] 34/75 batches, loss: 0.6936Epoch 4/15: [==============                ] 35/75 batches, loss: 0.6935Epoch 4/15: [==============                ] 36/75 batches, loss: 0.6934Epoch 4/15: [==============                ] 37/75 batches, loss: 0.6932Epoch 4/15: [===============               ] 38/75 batches, loss: 0.6933Epoch 4/15: [===============               ] 39/75 batches, loss: 0.6926Epoch 4/15: [================              ] 40/75 batches, loss: 0.6934Epoch 4/15: [================              ] 41/75 batches, loss: 0.6936Epoch 4/15: [================              ] 42/75 batches, loss: 0.6932Epoch 4/15: [=================             ] 43/75 batches, loss: 0.6936Epoch 4/15: [=================             ] 44/75 batches, loss: 0.6933Epoch 4/15: [==================            ] 45/75 batches, loss: 0.6940Epoch 4/15: [==================            ] 46/75 batches, loss: 0.6941Epoch 4/15: [==================            ] 47/75 batches, loss: 0.6940Epoch 4/15: [===================           ] 48/75 batches, loss: 0.6941Epoch 4/15: [===================           ] 49/75 batches, loss: 0.6940Epoch 4/15: [====================          ] 50/75 batches, loss: 0.6942Epoch 4/15: [====================          ] 51/75 batches, loss: 0.6943Epoch 4/15: [====================          ] 52/75 batches, loss: 0.6945Epoch 4/15: [=====================         ] 53/75 batches, loss: 0.6945Epoch 4/15: [=====================         ] 54/75 batches, loss: 0.6948Epoch 4/15: [======================        ] 55/75 batches, loss: 0.6945Epoch 4/15: [======================        ] 56/75 batches, loss: 0.6948Epoch 4/15: [======================        ] 57/75 batches, loss: 0.6949Epoch 4/15: [=======================       ] 58/75 batches, loss: 0.6952Epoch 4/15: [=======================       ] 59/75 batches, loss: 0.6951Epoch 4/15: [========================      ] 60/75 batches, loss: 0.6950Epoch 4/15: [========================      ] 61/75 batches, loss: 0.6948Epoch 4/15: [========================      ] 62/75 batches, loss: 0.6945Epoch 4/15: [=========================     ] 63/75 batches, loss: 0.6946Epoch 4/15: [=========================     ] 64/75 batches, loss: 0.6943Epoch 4/15: [==========================    ] 65/75 batches, loss: 0.6947Epoch 4/15: [==========================    ] 66/75 batches, loss: 0.6949Epoch 4/15: [==========================    ] 67/75 batches, loss: 0.6947Epoch 4/15: [===========================   ] 68/75 batches, loss: 0.6945Epoch 4/15: [===========================   ] 69/75 batches, loss: 0.6945Epoch 4/15: [============================  ] 70/75 batches, loss: 0.6943Epoch 4/15: [============================  ] 71/75 batches, loss: 0.6946Epoch 4/15: [============================  ] 72/75 batches, loss: 0.6949Epoch 4/15: [============================= ] 73/75 batches, loss: 0.6948Epoch 4/15: [============================= ] 74/75 batches, loss: 0.6948Epoch 4/15: [==============================] 75/75 batches, loss: 0.6953
[2025-04-29 20:36:34,330][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6953
[2025-04-29 20:36:34,749][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 5/15: [Epoch 5/15: [                              ] 1/75 batches, loss: 0.6793Epoch 5/15: [                              ] 2/75 batches, loss: 0.6740Epoch 5/15: [=                             ] 3/75 batches, loss: 0.6696Epoch 5/15: [=                             ] 4/75 batches, loss: 0.6753Epoch 5/15: [==                            ] 5/75 batches, loss: 0.6777Epoch 5/15: [==                            ] 6/75 batches, loss: 0.6787Epoch 5/15: [==                            ] 7/75 batches, loss: 0.6809Epoch 5/15: [===                           ] 8/75 batches, loss: 0.6813Epoch 5/15: [===                           ] 9/75 batches, loss: 0.6812Epoch 5/15: [====                          ] 10/75 batches, loss: 0.6847Epoch 5/15: [====                          ] 11/75 batches, loss: 0.6873Epoch 5/15: [====                          ] 12/75 batches, loss: 0.6882Epoch 5/15: [=====                         ] 13/75 batches, loss: 0.6893Epoch 5/15: [=====                         ] 14/75 batches, loss: 0.6905Epoch 5/15: [======                        ] 15/75 batches, loss: 0.6897Epoch 5/15: [======                        ] 16/75 batches, loss: 0.6894Epoch 5/15: [======                        ] 17/75 batches, loss: 0.6907Epoch 5/15: [=======                       ] 18/75 batches, loss: 0.6905Epoch 5/15: [=======                       ] 19/75 batches, loss: 0.6895Epoch 5/15: [========                      ] 20/75 batches, loss: 0.6901Epoch 5/15: [========                      ] 21/75 batches, loss: 0.6903Epoch 5/15: [========                      ] 22/75 batches, loss: 0.6904Epoch 5/15: [=========                     ] 23/75 batches, loss: 0.6908Epoch 5/15: [=========                     ] 24/75 batches, loss: 0.6918Epoch 5/15: [==========                    ] 25/75 batches, loss: 0.6918Epoch 5/15: [==========                    ] 26/75 batches, loss: 0.6914Epoch 5/15: [==========                    ] 27/75 batches, loss: 0.6929Epoch 5/15: [===========                   ] 28/75 batches, loss: 0.6928Epoch 5/15: [===========                   ] 29/75 batches, loss: 0.6926Epoch 5/15: [============                  ] 30/75 batches, loss: 0.6940Epoch 5/15: [============                  ] 31/75 batches, loss: 0.6944Epoch 5/15: [============                  ] 32/75 batches, loss: 0.6944Epoch 5/15: [=============                 ] 33/75 batches, loss: 0.6947Epoch 5/15: [=============                 ] 34/75 batches, loss: 0.6947Epoch 5/15: [==============                ] 35/75 batches, loss: 0.6949Epoch 5/15: [==============                ] 36/75 batches, loss: 0.6950Epoch 5/15: [==============                ] 37/75 batches, loss: 0.6951Epoch 5/15: [===============               ] 38/75 batches, loss: 0.6946Epoch 5/15: [===============               ] 39/75 batches, loss: 0.6944Epoch 5/15: [================              ] 40/75 batches, loss: 0.6948Epoch 5/15: [================              ] 41/75 batches, loss: 0.6946Epoch 5/15: [================              ] 42/75 batches, loss: 0.6945Epoch 5/15: [=================             ] 43/75 batches, loss: 0.6943Epoch 5/15: [=================             ] 44/75 batches, loss: 0.6949Epoch 5/15: [==================            ] 45/75 batches, loss: 0.6946Epoch 5/15: [==================            ] 46/75 batches, loss: 0.6944Epoch 5/15: [==================            ] 47/75 batches, loss: 0.6945Epoch 5/15: [===================           ] 48/75 batches, loss: 0.6941Epoch 5/15: [===================           ] 49/75 batches, loss: 0.6941Epoch 5/15: [====================          ] 50/75 batches, loss: 0.6947Epoch 5/15: [====================          ] 51/75 batches, loss: 0.6948Epoch 5/15: [====================          ] 52/75 batches, loss: 0.6946Epoch 5/15: [=====================         ] 53/75 batches, loss: 0.6947Epoch 5/15: [=====================         ] 54/75 batches, loss: 0.6950Epoch 5/15: [======================        ] 55/75 batches, loss: 0.6949Epoch 5/15: [======================        ] 56/75 batches, loss: 0.6949Epoch 5/15: [======================        ] 57/75 batches, loss: 0.6953Epoch 5/15: [=======================       ] 58/75 batches, loss: 0.6951Epoch 5/15: [=======================       ] 59/75 batches, loss: 0.6956Epoch 5/15: [========================      ] 60/75 batches, loss: 0.6952Epoch 5/15: [========================      ] 61/75 batches, loss: 0.6954Epoch 5/15: [========================      ] 62/75 batches, loss: 0.6951Epoch 5/15: [=========================     ] 63/75 batches, loss: 0.6953Epoch 5/15: [=========================     ] 64/75 batches, loss: 0.6954Epoch 5/15: [==========================    ] 65/75 batches, loss: 0.6954Epoch 5/15: [==========================    ] 66/75 batches, loss: 0.6953Epoch 5/15: [==========================    ] 67/75 batches, loss: 0.6950Epoch 5/15: [===========================   ] 68/75 batches, loss: 0.6949Epoch 5/15: [===========================   ] 69/75 batches, loss: 0.6952Epoch 5/15: [============================  ] 70/75 batches, loss: 0.6953Epoch 5/15: [============================  ] 71/75 batches, loss: 0.6955Epoch 5/15: [============================  ] 72/75 batches, loss: 0.6956Epoch 5/15: [============================= ] 73/75 batches, loss: 0.6956Epoch 5/15: [============================= ] 74/75 batches, loss: 0.6952Epoch 5/15: [==============================] 75/75 batches, loss: 0.6947
[2025-04-29 20:36:39,688][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6947
[2025-04-29 20:36:40,115][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/75 batches, loss: 0.6934Epoch 6/15: [                              ] 2/75 batches, loss: 0.7021Epoch 6/15: [=                             ] 3/75 batches, loss: 0.6939Epoch 6/15: [=                             ] 4/75 batches, loss: 0.6928Epoch 6/15: [==                            ] 5/75 batches, loss: 0.6917Epoch 6/15: [==                            ] 6/75 batches, loss: 0.6931Epoch 6/15: [==                            ] 7/75 batches, loss: 0.6920Epoch 6/15: [===                           ] 8/75 batches, loss: 0.6934Epoch 6/15: [===                           ] 9/75 batches, loss: 0.6926Epoch 6/15: [====                          ] 10/75 batches, loss: 0.6929Epoch 6/15: [====                          ] 11/75 batches, loss: 0.6928Epoch 6/15: [====                          ] 12/75 batches, loss: 0.6940Epoch 6/15: [=====                         ] 13/75 batches, loss: 0.6939Epoch 6/15: [=====                         ] 14/75 batches, loss: 0.6944Epoch 6/15: [======                        ] 15/75 batches, loss: 0.6942Epoch 6/15: [======                        ] 16/75 batches, loss: 0.6933Epoch 6/15: [======                        ] 17/75 batches, loss: 0.6927Epoch 6/15: [=======                       ] 18/75 batches, loss: 0.6926Epoch 6/15: [=======                       ] 19/75 batches, loss: 0.6926Epoch 6/15: [========                      ] 20/75 batches, loss: 0.6921Epoch 6/15: [========                      ] 21/75 batches, loss: 0.6917Epoch 6/15: [========                      ] 22/75 batches, loss: 0.6919Epoch 6/15: [=========                     ] 23/75 batches, loss: 0.6922Epoch 6/15: [=========                     ] 24/75 batches, loss: 0.6921Epoch 6/15: [==========                    ] 25/75 batches, loss: 0.6917Epoch 6/15: [==========                    ] 26/75 batches, loss: 0.6926Epoch 6/15: [==========                    ] 27/75 batches, loss: 0.6927Epoch 6/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 6/15: [===========                   ] 29/75 batches, loss: 0.6934Epoch 6/15: [============                  ] 30/75 batches, loss: 0.6938Epoch 6/15: [============                  ] 31/75 batches, loss: 0.6929Epoch 6/15: [============                  ] 32/75 batches, loss: 0.6929Epoch 6/15: [=============                 ] 33/75 batches, loss: 0.6934Epoch 6/15: [=============                 ] 34/75 batches, loss: 0.6930Epoch 6/15: [==============                ] 35/75 batches, loss: 0.6931Epoch 6/15: [==============                ] 36/75 batches, loss: 0.6933Epoch 6/15: [==============                ] 37/75 batches, loss: 0.6929Epoch 6/15: [===============               ] 38/75 batches, loss: 0.6932Epoch 6/15: [===============               ] 39/75 batches, loss: 0.6929Epoch 6/15: [================              ] 40/75 batches, loss: 0.6933Epoch 6/15: [================              ] 41/75 batches, loss: 0.6929Epoch 6/15: [================              ] 42/75 batches, loss: 0.6931Epoch 6/15: [=================             ] 43/75 batches, loss: 0.6931Epoch 6/15: [=================             ] 44/75 batches, loss: 0.6929Epoch 6/15: [==================            ] 45/75 batches, loss: 0.6926Epoch 6/15: [==================            ] 46/75 batches, loss: 0.6927Epoch 6/15: [==================            ] 47/75 batches, loss: 0.6926Epoch 6/15: [===================           ] 48/75 batches, loss: 0.6924Epoch 6/15: [===================           ] 49/75 batches, loss: 0.6927Epoch 6/15: [====================          ] 50/75 batches, loss: 0.6926Epoch 6/15: [====================          ] 51/75 batches, loss: 0.6923Epoch 6/15: [====================          ] 52/75 batches, loss: 0.6927Epoch 6/15: [=====================         ] 53/75 batches, loss: 0.6930Epoch 6/15: [=====================         ] 54/75 batches, loss: 0.6931Epoch 6/15: [======================        ] 55/75 batches, loss: 0.6930Epoch 6/15: [======================        ] 56/75 batches, loss: 0.6930Epoch 6/15: [======================        ] 57/75 batches, loss: 0.6931Epoch 6/15: [=======================       ] 58/75 batches, loss: 0.6928Epoch 6/15: [=======================       ] 59/75 batches, loss: 0.6928Epoch 6/15: [========================      ] 60/75 batches, loss: 0.6928Epoch 6/15: [========================      ] 61/75 batches, loss: 0.6926Epoch 6/15: [========================      ] 62/75 batches, loss: 0.6923Epoch 6/15: [=========================     ] 63/75 batches, loss: 0.6924Epoch 6/15: [=========================     ] 64/75 batches, loss: 0.6929Epoch 6/15: [==========================    ] 65/75 batches, loss: 0.6931Epoch 6/15: [==========================    ] 66/75 batches, loss: 0.6930Epoch 6/15: [==========================    ] 67/75 batches, loss: 0.6930Epoch 6/15: [===========================   ] 68/75 batches, loss: 0.6930Epoch 6/15: [===========================   ] 69/75 batches, loss: 0.6930Epoch 6/15: [============================  ] 70/75 batches, loss: 0.6932Epoch 6/15: [============================  ] 71/75 batches, loss: 0.6933Epoch 6/15: [============================  ] 72/75 batches, loss: 0.6933Epoch 6/15: [============================= ] 73/75 batches, loss: 0.6933Epoch 6/15: [============================= ] 74/75 batches, loss: 0.6933Epoch 6/15: [==============================] 75/75 batches, loss: 0.6936
[2025-04-29 20:36:45,063][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.6936
[2025-04-29 20:36:45,485][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 7/15: [Epoch 7/15: [                              ] 1/75 batches, loss: 0.7155Epoch 7/15: [                              ] 2/75 batches, loss: 0.7075Epoch 7/15: [=                             ] 3/75 batches, loss: 0.7006Epoch 7/15: [=                             ] 4/75 batches, loss: 0.6986Epoch 7/15: [==                            ] 5/75 batches, loss: 0.6996Epoch 7/15: [==                            ] 6/75 batches, loss: 0.7032Epoch 7/15: [==                            ] 7/75 batches, loss: 0.7049Epoch 7/15: [===                           ] 8/75 batches, loss: 0.7030Epoch 7/15: [===                           ] 9/75 batches, loss: 0.7013Epoch 7/15: [====                          ] 10/75 batches, loss: 0.6983Epoch 7/15: [====                          ] 11/75 batches, loss: 0.6967Epoch 7/15: [====                          ] 12/75 batches, loss: 0.6954Epoch 7/15: [=====                         ] 13/75 batches, loss: 0.6933Epoch 7/15: [=====                         ] 14/75 batches, loss: 0.6940Epoch 7/15: [======                        ] 15/75 batches, loss: 0.6930Epoch 7/15: [======                        ] 16/75 batches, loss: 0.6924Epoch 7/15: [======                        ] 17/75 batches, loss: 0.6930Epoch 7/15: [=======                       ] 18/75 batches, loss: 0.6929Epoch 7/15: [=======                       ] 19/75 batches, loss: 0.6931Epoch 7/15: [========                      ] 20/75 batches, loss: 0.6928Epoch 7/15: [========                      ] 21/75 batches, loss: 0.6934Epoch 7/15: [========                      ] 22/75 batches, loss: 0.6930Epoch 7/15: [=========                     ] 23/75 batches, loss: 0.6925Epoch 7/15: [=========                     ] 24/75 batches, loss: 0.6920Epoch 7/15: [==========                    ] 25/75 batches, loss: 0.6923Epoch 7/15: [==========                    ] 26/75 batches, loss: 0.6922Epoch 7/15: [==========                    ] 27/75 batches, loss: 0.6923Epoch 7/15: [===========                   ] 28/75 batches, loss: 0.6923Epoch 7/15: [===========                   ] 29/75 batches, loss: 0.6933Epoch 7/15: [============                  ] 30/75 batches, loss: 0.6935Epoch 7/15: [============                  ] 31/75 batches, loss: 0.6939Epoch 7/15: [============                  ] 32/75 batches, loss: 0.6934Epoch 7/15: [=============                 ] 33/75 batches, loss: 0.6937Epoch 7/15: [=============                 ] 34/75 batches, loss: 0.6932Epoch 7/15: [==============                ] 35/75 batches, loss: 0.6927Epoch 7/15: [==============                ] 36/75 batches, loss: 0.6935Epoch 7/15: [==============                ] 37/75 batches, loss: 0.6939Epoch 7/15: [===============               ] 38/75 batches, loss: 0.6946Epoch 7/15: [===============               ] 39/75 batches, loss: 0.6944Epoch 7/15: [================              ] 40/75 batches, loss: 0.6943Epoch 7/15: [================              ] 41/75 batches, loss: 0.6943Epoch 7/15: [================              ] 42/75 batches, loss: 0.6942Epoch 7/15: [=================             ] 43/75 batches, loss: 0.6945Epoch 7/15: [=================             ] 44/75 batches, loss: 0.6944Epoch 7/15: [==================            ] 45/75 batches, loss: 0.6944Epoch 7/15: [==================            ] 46/75 batches, loss: 0.6944Epoch 7/15: [==================            ] 47/75 batches, loss: 0.6946Epoch 7/15: [===================           ] 48/75 batches, loss: 0.6943Epoch 7/15: [===================           ] 49/75 batches, loss: 0.6945Epoch 7/15: [====================          ] 50/75 batches, loss: 0.6948Epoch 7/15: [====================          ] 51/75 batches, loss: 0.6948Epoch 7/15: [====================          ] 52/75 batches, loss: 0.6945Epoch 7/15: [=====================         ] 53/75 batches, loss: 0.6943Epoch 7/15: [=====================         ] 54/75 batches, loss: 0.6945Epoch 7/15: [======================        ] 55/75 batches, loss: 0.6942Epoch 7/15: [======================        ] 56/75 batches, loss: 0.6941Epoch 7/15: [======================        ] 57/75 batches, loss: 0.6938Epoch 7/15: [=======================       ] 58/75 batches, loss: 0.6937Epoch 7/15: [=======================       ] 59/75 batches, loss: 0.6935Epoch 7/15: [========================      ] 60/75 batches, loss: 0.6935Epoch 7/15: [========================      ] 61/75 batches, loss: 0.6937Epoch 7/15: [========================      ] 62/75 batches, loss: 0.6939Epoch 7/15: [=========================     ] 63/75 batches, loss: 0.6938Epoch 7/15: [=========================     ] 64/75 batches, loss: 0.6938Epoch 7/15: [==========================    ] 65/75 batches, loss: 0.6936Epoch 7/15: [==========================    ] 66/75 batches, loss: 0.6934Epoch 7/15: [==========================    ] 67/75 batches, loss: 0.6937Epoch 7/15: [===========================   ] 68/75 batches, loss: 0.6939Epoch 7/15: [===========================   ] 69/75 batches, loss: 0.6941Epoch 7/15: [============================  ] 70/75 batches, loss: 0.6941Epoch 7/15: [============================  ] 71/75 batches, loss: 0.6943Epoch 7/15: [============================  ] 72/75 batches, loss: 0.6943Epoch 7/15: [============================= ] 73/75 batches, loss: 0.6942Epoch 7/15: [============================= ] 74/75 batches, loss: 0.6941Epoch 7/15: [==============================] 75/75 batches, loss: 0.6943
[2025-04-29 20:36:50,537][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.6943
[2025-04-29 20:36:50,966][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:36:50,967][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 8/15: [Epoch 8/15: [                              ] 1/75 batches, loss: 0.6986Epoch 8/15: [                              ] 2/75 batches, loss: 0.7018Epoch 8/15: [=                             ] 3/75 batches, loss: 0.6975Epoch 8/15: [=                             ] 4/75 batches, loss: 0.6989Epoch 8/15: [==                            ] 5/75 batches, loss: 0.6976Epoch 8/15: [==                            ] 6/75 batches, loss: 0.6982Epoch 8/15: [==                            ] 7/75 batches, loss: 0.6940Epoch 8/15: [===                           ] 8/75 batches, loss: 0.6940Epoch 8/15: [===                           ] 9/75 batches, loss: 0.6942Epoch 8/15: [====                          ] 10/75 batches, loss: 0.6963Epoch 8/15: [====                          ] 11/75 batches, loss: 0.6955Epoch 8/15: [====                          ] 12/75 batches, loss: 0.6977Epoch 8/15: [=====                         ] 13/75 batches, loss: 0.6963Epoch 8/15: [=====                         ] 14/75 batches, loss: 0.6961Epoch 8/15: [======                        ] 15/75 batches, loss: 0.6948Epoch 8/15: [======                        ] 16/75 batches, loss: 0.6938Epoch 8/15: [======                        ] 17/75 batches, loss: 0.6940Epoch 8/15: [=======                       ] 18/75 batches, loss: 0.6941Epoch 8/15: [=======                       ] 19/75 batches, loss: 0.6936Epoch 8/15: [========                      ] 20/75 batches, loss: 0.6949Epoch 8/15: [========                      ] 21/75 batches, loss: 0.6943Epoch 8/15: [========                      ] 22/75 batches, loss: 0.6939Epoch 8/15: [=========                     ] 23/75 batches, loss: 0.6935Epoch 8/15: [=========                     ] 24/75 batches, loss: 0.6937Epoch 8/15: [==========                    ] 25/75 batches, loss: 0.6930Epoch 8/15: [==========                    ] 26/75 batches, loss: 0.6926Epoch 8/15: [==========                    ] 27/75 batches, loss: 0.6931Epoch 8/15: [===========                   ] 28/75 batches, loss: 0.6930Epoch 8/15: [===========                   ] 29/75 batches, loss: 0.6932Epoch 8/15: [============                  ] 30/75 batches, loss: 0.6936Epoch 8/15: [============                  ] 31/75 batches, loss: 0.6940Epoch 8/15: [============                  ] 32/75 batches, loss: 0.6941Epoch 8/15: [=============                 ] 33/75 batches, loss: 0.6936Epoch 8/15: [=============                 ] 34/75 batches, loss: 0.6939Epoch 8/15: [==============                ] 35/75 batches, loss: 0.6941Epoch 8/15: [==============                ] 36/75 batches, loss: 0.6944Epoch 8/15: [==============                ] 37/75 batches, loss: 0.6943Epoch 8/15: [===============               ] 38/75 batches, loss: 0.6946Epoch 8/15: [===============               ] 39/75 batches, loss: 0.6951Epoch 8/15: [================              ] 40/75 batches, loss: 0.6953Epoch 8/15: [================              ] 41/75 batches, loss: 0.6954Epoch 8/15: [================              ] 42/75 batches, loss: 0.6954Epoch 8/15: [=================             ] 43/75 batches, loss: 0.6952Epoch 8/15: [=================             ] 44/75 batches, loss: 0.6952Epoch 8/15: [==================            ] 45/75 batches, loss: 0.6947Epoch 8/15: [==================            ] 46/75 batches, loss: 0.6951Epoch 8/15: [==================            ] 47/75 batches, loss: 0.6952Epoch 8/15: [===================           ] 48/75 batches, loss: 0.6951Epoch 8/15: [===================           ] 49/75 batches, loss: 0.6956Epoch 8/15: [====================          ] 50/75 batches, loss: 0.6957Epoch 8/15: [====================          ] 51/75 batches, loss: 0.6959Epoch 8/15: [====================          ] 52/75 batches, loss: 0.6964Epoch 8/15: [=====================         ] 53/75 batches, loss: 0.6962Epoch 8/15: [=====================         ] 54/75 batches, loss: 0.6961Epoch 8/15: [======================        ] 55/75 batches, loss: 0.6958Epoch 8/15: [======================        ] 56/75 batches, loss: 0.6954Epoch 8/15: [======================        ] 57/75 batches, loss: 0.6949Epoch 8/15: [=======================       ] 58/75 batches, loss: 0.6948Epoch 8/15: [=======================       ] 59/75 batches, loss: 0.6948Epoch 8/15: [========================      ] 60/75 batches, loss: 0.6947Epoch 8/15: [========================      ] 61/75 batches, loss: 0.6946Epoch 8/15: [========================      ] 62/75 batches, loss: 0.6948Epoch 8/15: [=========================     ] 63/75 batches, loss: 0.6948Epoch 8/15: [=========================     ] 64/75 batches, loss: 0.6949Epoch 8/15: [==========================    ] 65/75 batches, loss: 0.6948Epoch 8/15: [==========================    ] 66/75 batches, loss: 0.6949Epoch 8/15: [==========================    ] 67/75 batches, loss: 0.6949Epoch 8/15: [===========================   ] 68/75 batches, loss: 0.6951Epoch 8/15: [===========================   ] 69/75 batches, loss: 0.6949Epoch 8/15: [============================  ] 70/75 batches, loss: 0.6950Epoch 8/15: [============================  ] 71/75 batches, loss: 0.6954Epoch 8/15: [============================  ] 72/75 batches, loss: 0.6955Epoch 8/15: [============================= ] 73/75 batches, loss: 0.6953Epoch 8/15: [============================= ] 74/75 batches, loss: 0.6953Epoch 8/15: [==============================] 75/75 batches, loss: 0.6950
[2025-04-29 20:36:55,369][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.6950
[2025-04-29 20:36:55,804][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.6928, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:36:55,805][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 9/15: [Epoch 9/15: [                              ] 1/75 batches, loss: 0.6788Epoch 9/15: [                              ] 2/75 batches, loss: 0.6833Epoch 9/15: [=                             ] 3/75 batches, loss: 0.6887Epoch 9/15: [=                             ] 4/75 batches, loss: 0.6901Epoch 9/15: [==                            ] 5/75 batches, loss: 0.6917Epoch 9/15: [==                            ] 6/75 batches, loss: 0.6943Epoch 9/15: [==                            ] 7/75 batches, loss: 0.6932Epoch 9/15: [===                           ] 8/75 batches, loss: 0.6911Epoch 9/15: [===                           ] 9/75 batches, loss: 0.6909Epoch 9/15: [====                          ] 10/75 batches, loss: 0.6896Epoch 9/15: [====                          ] 11/75 batches, loss: 0.6919Epoch 9/15: [====                          ] 12/75 batches, loss: 0.6928Epoch 9/15: [=====                         ] 13/75 batches, loss: 0.6923Epoch 9/15: [=====                         ] 14/75 batches, loss: 0.6924Epoch 9/15: [======                        ] 15/75 batches, loss: 0.6924Epoch 9/15: [======                        ] 16/75 batches, loss: 0.6918Epoch 9/15: [======                        ] 17/75 batches, loss: 0.6935Epoch 9/15: [=======                       ] 18/75 batches, loss: 0.6953Epoch 9/15: [=======                       ] 19/75 batches, loss: 0.6955Epoch 9/15: [========                      ] 20/75 batches, loss: 0.6952Epoch 9/15: [========                      ] 21/75 batches, loss: 0.6944Epoch 9/15: [========                      ] 22/75 batches, loss: 0.6941Epoch 9/15: [=========                     ] 23/75 batches, loss: 0.6939Epoch 9/15: [=========                     ] 24/75 batches, loss: 0.6940Epoch 9/15: [==========                    ] 25/75 batches, loss: 0.6946Epoch 9/15: [==========                    ] 26/75 batches, loss: 0.6955Epoch 9/15: [==========                    ] 27/75 batches, loss: 0.6942Epoch 9/15: [===========                   ] 28/75 batches, loss: 0.6938Epoch 9/15: [===========                   ] 29/75 batches, loss: 0.6940Epoch 9/15: [============                  ] 30/75 batches, loss: 0.6941Epoch 9/15: [============                  ] 31/75 batches, loss: 0.6942Epoch 9/15: [============                  ] 32/75 batches, loss: 0.6944Epoch 9/15: [=============                 ] 33/75 batches, loss: 0.6946Epoch 9/15: [=============                 ] 34/75 batches, loss: 0.6943Epoch 9/15: [==============                ] 35/75 batches, loss: 0.6941Epoch 9/15: [==============                ] 36/75 batches, loss: 0.6944Epoch 9/15: [==============                ] 37/75 batches, loss: 0.6944Epoch 9/15: [===============               ] 38/75 batches, loss: 0.6944Epoch 9/15: [===============               ] 39/75 batches, loss: 0.6945Epoch 9/15: [================              ] 40/75 batches, loss: 0.6943Epoch 9/15: [================              ] 41/75 batches, loss: 0.6942Epoch 9/15: [================              ] 42/75 batches, loss: 0.6939Epoch 9/15: [=================             ] 43/75 batches, loss: 0.6933Epoch 9/15: [=================             ] 44/75 batches, loss: 0.6928Epoch 9/15: [==================            ] 45/75 batches, loss: 0.6932Epoch 9/15: [==================            ] 46/75 batches, loss: 0.6934Epoch 9/15: [==================            ] 47/75 batches, loss: 0.6935Epoch 9/15: [===================           ] 48/75 batches, loss: 0.6932Epoch 9/15: [===================           ] 49/75 batches, loss: 0.6935Epoch 9/15: [====================          ] 50/75 batches, loss: 0.6936Epoch 9/15: [====================          ] 51/75 batches, loss: 0.6940Epoch 9/15: [====================          ] 52/75 batches, loss: 0.6940Epoch 9/15: [=====================         ] 53/75 batches, loss: 0.6941Epoch 9/15: [=====================         ] 54/75 batches, loss: 0.6938Epoch 9/15: [======================        ] 55/75 batches, loss: 0.6939Epoch 9/15: [======================        ] 56/75 batches, loss: 0.6943Epoch 9/15: [======================        ] 57/75 batches, loss: 0.6939Epoch 9/15: [=======================       ] 58/75 batches, loss: 0.6944Epoch 9/15: [=======================       ] 59/75 batches, loss: 0.6945Epoch 9/15: [========================      ] 60/75 batches, loss: 0.6946Epoch 9/15: [========================      ] 61/75 batches, loss: 0.6946Epoch 9/15: [========================      ] 62/75 batches, loss: 0.6951Epoch 9/15: [=========================     ] 63/75 batches, loss: 0.6952Epoch 9/15: [=========================     ] 64/75 batches, loss: 0.6947Epoch 9/15: [==========================    ] 65/75 batches, loss: 0.6947Epoch 9/15: [==========================    ] 66/75 batches, loss: 0.6947Epoch 9/15: [==========================    ] 67/75 batches, loss: 0.6948Epoch 9/15: [===========================   ] 68/75 batches, loss: 0.6948Epoch 9/15: [===========================   ] 69/75 batches, loss: 0.6950Epoch 9/15: [============================  ] 70/75 batches, loss: 0.6949Epoch 9/15: [============================  ] 71/75 batches, loss: 0.6948Epoch 9/15: [============================  ] 72/75 batches, loss: 0.6951Epoch 9/15: [============================= ] 73/75 batches, loss: 0.6954Epoch 9/15: [============================= ] 74/75 batches, loss: 0.6954Epoch 9/15: [==============================] 75/75 batches, loss: 0.6952
[2025-04-29 20:37:00,220][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.6952
[2025-04-29 20:37:00,676][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.6929, Metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:37:00,677][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 20:37:00,677][src.training.lm_trainer][INFO] - Early stopping at epoch 9
[2025-04-29 20:37:00,677][src.training.lm_trainer][INFO] - Training completed in 47.48 seconds
[2025-04-29 20:37:00,677][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 20:37:06,264][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:37:06,264][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:37:06,264][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.5, 'f1': 0.0}
[2025-04-29 20:37:08,511][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/question_type/control1/en/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁▁▁▁▁
wandb:          best_val_f1 ▁▁▁▁▁▁
wandb:        best_val_loss █▄▂▁▁▁
wandb:                epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ███████▁▁
wandb:           train_loss ▇▅█▄▃▁▂▃▄
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁▁▁▁▁▁
wandb:               val_f1 ▁▁▁▁▁▁▁▁▁
wandb:             val_loss █▄▂▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69282
wandb:                epoch 9
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 1e-05
wandb:           train_loss 0.69525
wandb:           train_time 47.48478
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69286
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203601-0thaip8z
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203601-0thaip8z/logs
Control experiment completed successfully: layer_6_question_type_control1_en
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/en/layer_6/question_type/control1/results.json
Initial validation results:
experiment_type,language,layer,task,submetric,control_index,metric,value

Initial experiments completed. Please check the results above.
Press Enter to continue with full experiment suite, or Ctrl+C to abort.
Phase 2: Running main experiments
Running question_type experiment for language ar, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:37:25,782][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/question_type
experiment_name: layer_1_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-04-29 20:37:25,782][__main__][INFO] - Normalized task: question_type
[2025-04-29 20:37:25,782][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-29 20:37:25,782][__main__][INFO] - Determined Task Type: classification
[2025-04-29 20:37:25,786][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-04-29 20:37:25,787][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 20:37:27,311][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:37:30,043][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:37:30,044][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:37:30,089][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:37:30,116][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:37:30,202][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 20:37:30,212][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:37:30,212][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 20:37:30,213][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:37:30,234][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:37:30,261][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:37:30,272][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 20:37:30,273][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:37:30,273][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 20:37:30,274][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:37:30,295][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:37:30,320][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:37:30,330][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 20:37:30,331][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:37:30,332][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 20:37:30,332][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 20:37:30,333][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:37:30,333][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:37:30,333][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:37:30,333][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:37:30,333][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-29 20:37:30,334][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-29 20:37:30,334][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 20:37:30,334][src.data.datasets][INFO] - Sample label: 1
[2025-04-29 20:37:30,334][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:37:30,334][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:37:30,334][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:37:30,334][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:37:30,335][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-29 20:37:30,335][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-29 20:37:30,335][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 20:37:30,335][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:37:30,335][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-29 20:37:30,335][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-29 20:37:30,335][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-29 20:37:30,335][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-29 20:37:30,336][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-29 20:37:30,336][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-29 20:37:30,336][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 20:37:30,336][src.data.datasets][INFO] - Sample label: 0
[2025-04-29 20:37:30,336][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 20:37:30,336][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:37:30,336][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:37:30,337][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:37:34,484][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:37:34,484][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 20:37:34,485][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-29 20:37:34,486][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=1, freeze_model=True, finetune=False
[2025-04-29 20:37:34,487][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:37:34,487][__main__][INFO] - Successfully created model for ar
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7575Epoch 1/15: [                              ] 2/63 batches, loss: 0.7216Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7077Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7189Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7006Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7035Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7030Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7025Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7022Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7029Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7037Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7096Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7056Epoch 1/15: [======                        ] 14/63 batches, loss: 0.7036Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.7067Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.7066Epoch 1/15: [========                      ] 17/63 batches, loss: 0.7072Epoch 1/15: [========                      ] 18/63 batches, loss: 0.7055Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.7076Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.7065Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.7053Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.7018Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.7028Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.7038Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.7031Epoch 1/15: [============                  ] 26/63 batches, loss: 0.7033Epoch 1/15: [============                  ] 27/63 batches, loss: 0.7038Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.7037Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.7054Epoch 1/15: [==============                ] 30/63 batches, loss: 0.7032Epoch 1/15: [==============                ] 31/63 batches, loss: 0.7031Epoch 1/15: [===============               ] 32/63 batches, loss: 0.7027Epoch 1/15: [===============               ] 33/63 batches, loss: 0.7029Epoch 1/15: [================              ] 34/63 batches, loss: 0.7042Epoch 1/15: [================              ] 35/63 batches, loss: 0.7039Epoch 1/15: [=================             ] 36/63 batches, loss: 0.7030Epoch 1/15: [=================             ] 37/63 batches, loss: 0.7026Epoch 1/15: [==================            ] 38/63 batches, loss: 0.7025Epoch 1/15: [==================            ] 39/63 batches, loss: 0.7040Epoch 1/15: [===================           ] 40/63 batches, loss: 0.7037Epoch 1/15: [===================           ] 41/63 batches, loss: 0.7040Epoch 1/15: [====================          ] 42/63 batches, loss: 0.7041Epoch 1/15: [====================          ] 43/63 batches, loss: 0.7031Epoch 1/15: [====================          ] 44/63 batches, loss: 0.7029Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.7022Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.7020Epoch 1/15: [======================        ] 47/63 batches, loss: 0.7018Epoch 1/15: [======================        ] 48/63 batches, loss: 0.7019Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.7010Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.7007Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6988Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6984Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6989Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6990Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6997Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6994Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6997Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6990Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6992Epoch 1/15: [============================  ] 60/63 batches, loss: 0.7001Epoch 1/15: [============================= ] 61/63 batches, loss: 0.7005Epoch 1/15: [============================= ] 62/63 batches, loss: 0.7015Epoch 1/15: [==============================] 63/63 batches, loss: 0.7005
[2025-04-29 20:37:40,865][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.7005
[2025-04-29 20:37:41,159][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6883, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6950Epoch 2/15: [                              ] 2/63 batches, loss: 0.7020Epoch 2/15: [=                             ] 3/63 batches, loss: 0.7051Epoch 2/15: [=                             ] 4/63 batches, loss: 0.7086Epoch 2/15: [==                            ] 5/63 batches, loss: 0.7085Epoch 2/15: [==                            ] 6/63 batches, loss: 0.7047Epoch 2/15: [===                           ] 7/63 batches, loss: 0.7120Epoch 2/15: [===                           ] 8/63 batches, loss: 0.7109Epoch 2/15: [====                          ] 9/63 batches, loss: 0.7111Epoch 2/15: [====                          ] 10/63 batches, loss: 0.7107Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.7029Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.7029Epoch 2/15: [======                        ] 13/63 batches, loss: 0.7020Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6986Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6982Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.7007Epoch 2/15: [========                      ] 17/63 batches, loss: 0.7027Epoch 2/15: [========                      ] 18/63 batches, loss: 0.7029Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.7011Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6976Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6963Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6960Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6955Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6948Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6944Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6952Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6960Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6988Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6977Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6977Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6984Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6987Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6987Epoch 2/15: [================              ] 34/63 batches, loss: 0.6989Epoch 2/15: [================              ] 35/63 batches, loss: 0.6980Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6987Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6980Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6977Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6972Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6980Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6983Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6987Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6990Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6993Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6994Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6993Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6989Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6987Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6986Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6987Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6987Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6985Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6990Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6996Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.7001Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.7000Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6999Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.7001Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6998Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6994Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6993Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6992Epoch 2/15: [==============================] 63/63 batches, loss: 0.6998
[2025-04-29 20:37:45,394][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6998
[2025-04-29 20:37:45,702][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6889, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 20:37:45,702][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6737Epoch 3/15: [                              ] 2/63 batches, loss: 0.6965Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6872Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6891Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6935Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6961Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6949Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6928Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6972Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6957Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6954Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6993Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6969Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6947Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6972Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6967Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6956Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6951Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6954Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6954Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6951Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6960Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6967Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6964Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6970Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6979Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6983Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6977Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6982Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6968Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6965Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6968Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6971Epoch 3/15: [================              ] 34/63 batches, loss: 0.6977Epoch 3/15: [================              ] 35/63 batches, loss: 0.6971Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6974Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6975Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6980Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6982Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6980Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6979Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6983Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6985Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6988Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6980Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6974Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6972Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6968Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6962Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6960Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6962Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6960Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6965Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6965Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6967Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6967Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6967Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6965Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6964Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6962Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6965Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6969Epoch 3/15: [==============================] 63/63 batches, loss: 0.6971
[2025-04-29 20:37:49,364][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6971
[2025-04-29 20:37:49,666][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6898, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 20:37:49,667][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.7056Epoch 4/15: [                              ] 2/63 batches, loss: 0.7046Epoch 4/15: [=                             ] 3/63 batches, loss: 0.7060Epoch 4/15: [=                             ] 4/63 batches, loss: 0.7027Epoch 4/15: [==                            ] 5/63 batches, loss: 0.6984Epoch 4/15: [==                            ] 6/63 batches, loss: 0.6971Epoch 4/15: [===                           ] 7/63 batches, loss: 0.6961Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6951Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6958Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6931Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6916Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6899Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6914Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6925Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6912Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6914Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6920Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6943Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6942Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6933Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6934Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6936Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6943Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6946Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6940Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6944Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6944Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6948Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6959Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6960Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6954Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6957Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6960Epoch 4/15: [================              ] 34/63 batches, loss: 0.6967Epoch 4/15: [================              ] 35/63 batches, loss: 0.6967Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6965Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6970Epoch 4/15: [==================            ] 38/63 batches, loss: 0.6968Epoch 4/15: [==================            ] 39/63 batches, loss: 0.6963Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6958Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6962Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6964Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6957Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6963Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.6963Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.6962Epoch 4/15: [======================        ] 47/63 batches, loss: 0.6959Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6959Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6960Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6963Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6962Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6961Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6959Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6961Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6957Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6958Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6953Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6952Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6953Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6955Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6959Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6961Epoch 4/15: [==============================] 63/63 batches, loss: 0.6958
[2025-04-29 20:37:53,331][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6958
[2025-04-29 20:37:53,639][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6904, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 20:37:53,640][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-04-29 20:37:53,640][src.training.lm_trainer][INFO] - Early stopping at epoch 4
[2025-04-29 20:37:53,640][src.training.lm_trainer][INFO] - Training completed in 17.10 seconds
[2025-04-29 20:37:53,640][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-04-29 20:37:58,227][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.5005025125628141, 'f1': 0.0}
[2025-04-29 20:37:58,227][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0}
[2025-04-29 20:37:58,227][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7142857142857143, 'f1': 0.0}
[2025-04-29 20:38:00,476][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/question_type/ar/model.pt
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁▃▃▆▆██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁
wandb:           train_loss █▇▃▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁▁▁
wandb:               val_f1 ▁▁▁▁
wandb:             val_loss ▁▃▆█
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.54545
wandb:          best_val_f1 0
wandb:        best_val_loss 0.6883
wandb:                epoch 4
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5005
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.54545
wandb:         final_val_f1 0
wandb:        learning_rate 2e-05
wandb:           train_loss 0.6958
wandb:           train_time 17.09695
wandb:         val_accuracy 0.54545
wandb:               val_f1 0
wandb:             val_loss 0.6904
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203725-ivj4bvj3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250429_203725-ivj4bvj3/logs
Standard experiment completed successfully: layer_1_question_type_ar
Warning: Results file not found: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/question_type/results.json
Running complexity experiment for language ar, layer 1
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-29 20:38:14,963][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/layerwise_output/ar/layer_1/complexity
experiment_name: layer_1_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
  probe_hidden_size: 96
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-04-29 20:38:14,963][__main__][INFO] - Normalized task: complexity
[2025-04-29 20:38:14,963][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-29 20:38:14,963][__main__][INFO] - Determined Task Type: regression
[2025-04-29 20:38:14,968][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-04-29 20:38:14,968][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-29 20:38:17,071][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-29 20:38:19,841][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-29 20:38:19,841][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:38:19,879][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:38:19,922][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:38:20,029][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-29 20:38:20,039][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:38:20,040][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-29 20:38:20,042][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:38:20,082][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:38:20,126][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:38:20,141][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-29 20:38:20,142][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:38:20,142][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-29 20:38:20,144][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-29 20:38:20,178][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:38:20,225][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-29 20:38:20,238][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-29 20:38:20,239][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-29 20:38:20,240][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-29 20:38:20,241][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-29 20:38:20,242][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:38:20,242][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:38:20,242][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:38:20,242][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:38:20,242][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:38:20,243][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-29 20:38:20,243][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-29 20:38:20,243][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-29 20:38:20,243][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:38:20,243][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:38:20,243][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:38:20,243][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:38:20,243][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:38:20,244][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-29 20:38:20,244][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-29 20:38:20,244][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-29 20:38:20,244][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-29 20:38:20,244][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-29 20:38:20,244][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-29 20:38:20,244][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-29 20:38:20,244][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-29 20:38:20,245][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-29 20:38:20,245][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-29 20:38:20,245][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-29 20:38:20,245][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-29 20:38:20,245][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-29 20:38:20,245][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-29 20:38:20,246][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-29 20:38:24,261][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-29 20:38:24,262][src.models.model_factory][INFO] - Language model parameters frozen
[2025-04-29 20:38:24,263][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-29 20:38:24,263][src.models.model_factory][INFO] - Model configuration: layer-wise=True, layer_index=1, freeze_model=True, finetune=False
[2025-04-29 20:38:24,264][src.models.model_factory][INFO] - Model has 73,921 trainable parameters out of 394,195,393 total parameters
[2025-04-29 20:38:24,264][__main__][INFO] - Successfully created model for ar
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.5958Epoch 1/15: [                              ] 2/63 batches, loss: 0.5217Epoch 1/15: [=                             ] 3/63 batches, loss: 0.5066Epoch 1/15: [=                             ] 4/63 batches, loss: 0.4734Epoch 1/15: [==                            ] 5/63 batches, loss: 0.4702Epoch 1/15: [==                            ] 6/63 batches, loss: 0.4848Epoch 1/15: [===                           ] 7/63 batches, loss: 0.4764Epoch 1/15: [===                           ] 8/63 batches, loss: 0.4603Epoch 1/15: [====                          ] 9/63 batches, loss: 0.4560Epoch 1/15: [====                          ] 10/63 batches, loss: 0.4507Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.4563Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.4649Epoch 1/15: [======                        ] 13/63 batches, loss: 0.4631Epoch 1/15: [======                        ] 14/63 batches, loss: 0.4642Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.4601Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.4643Epoch 1/15: [========                      ] 17/63 batches, loss: 0.4671Epoch 1/15: [========                      ] 18/63 batches, loss: 0.4608Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.4625Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.4637Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.4596Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.4523Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.4485Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.4489Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.4434Epoch 1/15: [============                  ] 26/63 batches, loss: 0.4382Epoch 1/15: [============                  ] 27/63 batches, loss: 0.4338Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.4264Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.4289Epoch 1/15: [==============                ] 30/63 batches, loss: 0.4282Epoch 1/15: [==============                ] 31/63 batches, loss: 0.4283Epoch 1/15: [===============               ] 32/63 batches, loss: 0.4260Epoch 1/15: [===============               ] 33/63 batches, loss: 0.4222Epoch 1/15: [================              ] 34/63 batches, loss: 0.4231Epoch 1/15: [================              ] 35/63 batches, loss: 0.4223Epoch 1/15: [=================             ] 36/63 batches, loss: 0.4183Epoch 1/15: [=================             ] 37/63 batches, loss: 0.4144Epoch 1/15: [==================            ] 38/63 batches, loss: 0.4127Epoch 1/15: [==================            ] 39/63 batches, loss: 0.4088Epoch 1/15: [===================           ] 40/63 batches, loss: 0.4086Epoch 1/15: [===================           ] 41/63 batches, loss: 0.4045Epoch 1/15: [====================          ] 42/63 batches, loss: 0.4013Epoch 1/15: [====================          ] 43/63 batches, loss: 0.3971Epoch 1/15: [====================          ] 44/63 batches, loss: 0.3949Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.3918Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.3898Epoch 1/15: [======================        ] 47/63 batches, loss: 0.3868Epoch 1/15: [======================        ] 48/63 batches, loss: 0.3836Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.3801Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.3778Epoch 1/15: [========================      ] 51/63 batches, loss: 0.3744Epoch 1/15: [========================      ] 52/63 batches, loss: 0.3732Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.3712Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.3691Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.3659Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.3626Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.3615Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.3595Epoch 1/15: [============================  ] 59/63 batches, loss: 0.3590Epoch 1/15: [============================  ] 60/63 batches, loss: 0.3577Epoch 1/15: [============================= ] 61/63 batches, loss: 0.3557Epoch 1/15: [============================= ] 62/63 batches, loss: 0.3548Epoch 1/15: [==============================] 63/63 batches, loss: 0.3519
[2025-04-29 20:38:31,059][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.3519
[2025-04-29 20:38:31,338][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.2305, Metrics: {'mse': 0.2328200787305832, 'rmse': 0.4825143300779607, 'r2': -2.588531732559204}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.2425Epoch 2/15: [                              ] 2/63 batches, loss: 0.2508Epoch 2/15: [=                             ] 3/63 batches, loss: 0.2443Epoch 2/15: [=                             ] 4/63 batches, loss: 0.2430Epoch 2/15: [==                            ] 5/63 batches, loss: 0.2413Epoch 2/15: [==                            ] 6/63 batches, loss: 0.2249Epoch 2/15: [===                           ] 7/63 batches, loss: 0.2306Epoch 2/15: [===                           ] 8/63 batches, loss: 0.2273Epoch 2/15: [====                          ] 9/63 batches, loss: 0.2277Epoch 2/15: [====                          ] 10/63 batches, loss: 0.2235Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.2189Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.2142Epoch 2/15: [======                        ] 13/63 batches, loss: 0.2126Epoch 2/15: [======                        ] 14/63 batches, loss: 0.2086Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.2065Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.2108Epoch 2/15: [========                      ] 17/63 batches, loss: 0.2067Epoch 2/15: [========                      ] 18/63 batches, loss: 0.2049Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.2046Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.2079Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.2049Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.2013Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.2017Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.2026Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.2016Epoch 2/15: [============                  ] 26/63 batches, loss: 0.2020Epoch 2/15: [============                  ] 27/63 batches, loss: 0.2006Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.2012Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.1991Epoch 2/15: [==============                ] 30/63 batches, loss: 0.1988Epoch 2/15: [==============                ] 31/63 batches, loss: 0.1977Epoch 2/15: [===============               ] 32/63 batches, loss: 0.1955Epoch 2/15: [===============               ] 33/63 batches, loss: 0.1944Epoch 2/15: [================              ] 34/63 batches, loss: 0.1954Epoch 2/15: [================              ] 35/63 batches, loss: 0.1934Epoch 2/15: [=================             ] 36/63 batches, loss: 0.1920Epoch 2/15: [=================             ] 37/63 batches, loss: 0.1905Epoch 2/15: [==================            ] 38/63 batches, loss: 0.1899Epoch 2/15: [==================            ] 39/63 batches, loss: 0.1897Epoch 2/15: [===================           ] 40/63 batches, loss: 0.1895Epoch 2/15: [===================           ] 41/63 batches, loss: 0.1885Epoch 2/15: [====================          ] 42/63 batches, loss: 0.1874Epoch 2/15: [====================          ] 43/63 batches, loss: 0.1850Epoch 2/15: [====================          ] 44/63 batches, loss: 0.1829Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.1815Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.1793Epoch 2/15: [======================        ] 47/63 batches, loss: 0.1774Epoch 2/15: [======================        ] 48/63 batches, loss: 0.1776Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.1762Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.1741Epoch 2/15: [========================      ] 51/63 batches, loss: 0.1726Epoch 2/15: [========================      ] 52/63 batches, loss: 0.1713Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.1704Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.1697Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.1701Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.1695Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.1684Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.1680Epoch 2/15: [============================  ] 59/63 batches, loss: 0.1672Epoch 2/15: [============================  ] 60/63 batches, loss: 0.1665Epoch 2/15: [============================= ] 61/63 batches, loss: 0.1655Epoch 2/15: [============================= ] 62/63 batches, loss: 0.1648Epoch 2/15: [==============================] 63/63 batches, loss: 0.1631
[2025-04-29 20:38:35,550][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.1631
[2025-04-29 20:38:35,854][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1176, Metrics: {'mse': 0.11887263506650925, 'rmse': 0.3447791105425461, 'r2': -0.832222580909729}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.1110Epoch 3/15: [                              ] 2/63 batches, loss: 0.1063Epoch 3/15: [=                             ] 3/63 batches, loss: 0.1043Epoch 3/15: [=                             ] 4/63 batches, loss: 0.1091Epoch 3/15: [==                            ] 5/63 batches, loss: 0.1094Epoch 3/15: [==                            ] 6/63 batches, loss: 0.1141Epoch 3/15: [===                           ] 7/63 batches, loss: 0.1115Epoch 3/15: [===                           ] 8/63 batches, loss: 0.1127Epoch 3/15: [====                          ] 9/63 batches, loss: 0.1161Epoch 3/15: [====                          ] 10/63 batches, loss: 0.1152Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.1124Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.1158Epoch 3/15: [======                        ] 13/63 batches, loss: 0.1170Epoch 3/15: [======                        ] 14/63 batches, loss: 0.1128Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.1166Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.1154Epoch 3/15: [========                      ] 17/63 batches, loss: 0.1136Epoch 3/15: [========                      ] 18/63 batches, loss: 0.1120Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.1125Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.1126Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.1106Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.1096Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.1088Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.1069Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.1062Epoch 3/15: [============                  ] 26/63 batches, loss: 0.1060Epoch 3/15: [============                  ] 27/63 batches, loss: 0.1061Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.1049Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.1062Epoch 3/15: [==============                ] 30/63 batches, loss: 0.1043Epoch 3/15: [==============                ] 31/63 batches, loss: 0.1026Epoch 3/15: [===============               ] 32/63 batches, loss: 0.1021Epoch 3/15: [===============               ] 33/63 batches, loss: 0.1011Epoch 3/15: [================              ] 34/63 batches, loss: 0.1005Epoch 3/15: [================              ] 35/63 batches, loss: 0.1001Epoch 3/15: [=================             ] 36/63 batches, loss: 0.0991Epoch 3/15: [=================             ] 37/63 batches, loss: 0.0988Epoch 3/15: [==================            ] 38/63 batches, loss: 0.0987Epoch 3/15: [==================            ] 39/63 batches, loss: 0.0977slurmstepd: error: *** JOB 58114020 ON r24g37 CANCELLED AT 2025-04-29T20:38:38 ***
