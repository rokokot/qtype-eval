SLURM_JOB_ID: 64315171
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: submetric_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Fri Apr 11 10:03:20 CEST 2025
Walltime: 00-00:30:00
========================================================================
Retrieving notices: - \ done
Channels:
 - pytorch
 - nvidia
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): ...working... done
Solving environment: ...working... done


==> WARNING: A newer version of conda exists. <==
    current version: 25.1.1
    latest version: 25.3.1

Please update conda by running

    $ conda update -n base -c defaults conda



# All requested packages already installed.

Requirement already satisfied: hydra-core in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (1.3.2)
Requirement already satisfied: hydra-submitit-launcher in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (1.2.0)
Requirement already satisfied: omegaconf<2.4,>=2.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (2.3.0)
Requirement already satisfied: antlr4-python3-runtime==4.9.* in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (4.9.3)
Requirement already satisfied: packaging in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (24.2)
Requirement already satisfied: submitit>=1.3.3 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-submitit-launcher) (1.5.2)
Requirement already satisfied: PyYAML>=5.1.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.2)
Requirement already satisfied: cloudpickle>=1.2.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from submitit>=1.3.3->hydra-submitit-launcher) (3.1.1)
Requirement already satisfied: typing_extensions>=3.7.4.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from submitit>=1.3.3->hydra-submitit-launcher) (4.12.2)
Requirement already satisfied: transformers<4.36.0,>=4.30.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (4.35.2)
Requirement already satisfied: torch in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (2.5.1)
Requirement already satisfied: datasets in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (3.5.0)
Requirement already satisfied: wandb in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (0.19.9)
Requirement already satisfied: filelock in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (3.18.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.30.1)
Requirement already satisfied: numpy>=1.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (1.26.4)
Requirement already satisfied: packaging>=20.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (24.2)
Requirement already satisfied: pyyaml>=5.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (2024.11.6)
Requirement already satisfied: requests in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (2.32.3)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.15.2)
Requirement already satisfied: safetensors>=0.3.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.5.3)
Requirement already satisfied: tqdm>=4.27 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (4.67.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (4.12.2)
Requirement already satisfied: networkx in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (3.2.1)
Requirement already satisfied: jinja2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (3.1.6)
Requirement already satisfied: fsspec in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (2024.12.0)
Requirement already satisfied: sympy==1.13.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)
Requirement already satisfied: pyarrow>=15.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (19.0.1)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (0.3.8)
Requirement already satisfied: pandas in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (2.2.3)
Requirement already satisfied: xxhash in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (3.5.0)
Requirement already satisfied: multiprocess<0.70.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (0.70.16)
Requirement already satisfied: aiohttp in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (3.11.16)
Requirement already satisfied: click!=8.0.0,>=7.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (8.1.8)
Requirement already satisfied: docker-pycreds>=0.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (0.4.0)
Requirement already satisfied: eval-type-backport in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (0.2.2)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (3.1.44)
Requirement already satisfied: platformdirs in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (4.3.7)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.15.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (5.29.4)
Requirement already satisfied: psutil>=5.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (7.0.0)
Requirement already satisfied: pydantic<3 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (2.11.1)
Requirement already satisfied: sentry-sdk>=2.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (2.25.0)
Requirement already satisfied: setproctitle in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (1.3.5)
Requirement already satisfied: setuptools in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (78.1.0)
Requirement already satisfied: six>=1.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)
Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (2.6.1)
Requirement already satisfied: aiosignal>=1.1.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.2)
Requirement already satisfied: async-timeout<6.0,>=4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (5.0.1)
Requirement already satisfied: attrs>=17.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (6.3.1)
Requirement already satisfied: propcache>=0.2.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (0.3.1)
Requirement already satisfied: yarl<2.0,>=1.17.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.18.3)
Requirement already satisfied: gitdb<5,>=4.0.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)
Requirement already satisfied: annotated-types>=0.6.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (0.7.0)
Requirement already satisfied: pydantic-core==2.33.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (2.33.0)
Requirement already satisfied: typing-inspection>=0.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (0.4.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (2025.1.31)
Requirement already satisfied: MarkupSafe>=2.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)
Requirement already satisfied: python-dateutil>=2.8.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: smmap<6,>=3.0.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)
Environment variables:
PYTHONPATH=:/data/leuven/371/vsc37132/qtype-eval:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval
HF_HOME=/data/leuven/371/vsc37132/qtype-eval/data/cache
TRANSFORMERS_OFFLINE=1
HF_DATASETS_OFFLINE=1
HYDRA_JOB_CHDIR=False
GPU information:
Fri Apr 11 10:03:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          On  |   00000000:17:00.0 Off |                    0 |
| N/A   34C    P0             44W /  300W |       1MiB /  81920MiB |      0%   E. Process |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Python executable: /data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/bin/python
PyTorch CUDA available: True
Running submetric avg_links_len for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:04:07,733][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_links_len
experiment_name: avg_links_len_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:04:07,733][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:04:07,734][__main__][INFO] - Using submetric: avg_links_len
[2025-04-11 10:04:07,734][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:04:07,734][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:04:07,738][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:04:07,738][__main__][INFO] - Using submetric: avg_links_len
[2025-04-11 10:04:07,739][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:04:09,125][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:04:11,511][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:04:11,512][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:04:11,620][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:04:11,916][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:04:12,021][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:04:12,030][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:04:12,031][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:04:12,032][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:04:12,052][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:04:12,080][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:04:12,105][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:04:12,106][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:04:12,106][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:04:12,107][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:04:12,123][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:04:12,152][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:04:12,190][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:04:12,191][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:04:12,192][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:04:12,192][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:04:12,193][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:04:12,193][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-04-11 10:04:12,193][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-04-11 10:04:12,193][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-04-11 10:04:12,194][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9000
[2025-04-11 10:04:12,194][src.data.datasets][INFO] -   Mean: 0.2497, Std: 0.1826
[2025-04-11 10:04:12,194][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:04:12,194][src.data.datasets][INFO] - Sample label: 0.04500000178813934
[2025-04-11 10:04:12,194][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:04:12,194][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-04-11 10:04:12,194][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-04-11 10:04:12,194][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-04-11 10:04:12,195][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8000
[2025-04-11 10:04:12,195][src.data.datasets][INFO] -   Mean: 0.2557, Std: 0.1728
[2025-04-11 10:04:12,195][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:04:12,195][src.data.datasets][INFO] - Sample label: 0.23399999737739563
[2025-04-11 10:04:12,195][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:04:12,195][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-04-11 10:04:12,195][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-04-11 10:04:12,195][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-04-11 10:04:12,195][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6270
[2025-04-11 10:04:12,195][src.data.datasets][INFO] -   Mean: 0.2617, Std: 0.1298
[2025-04-11 10:04:12,196][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:04:12,196][src.data.datasets][INFO] - Sample label: 0.14000000059604645
[2025-04-11 10:04:12,196][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:04:12,196][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:04:12,196][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:04:12,196][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:04:16,912][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:04:16,914][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:04:16,915][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:04:16,915][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:20,  1.08s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:25,  2.83it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:18,  3.75it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.48it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.79it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  7.31it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:01<00:08,  7.82it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.67it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  9.20it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.49it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.63it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.81it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.84it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05,  9.96it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04,  9.94it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.04it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.09it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.13it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:04, 10.17it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.19it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.20it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.22it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.22it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:03, 10.21it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.22it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.23it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.22it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.23it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.24it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01,  9.91it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01,  9.90it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.01it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.03it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.05it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.11it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.14it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.17it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.20it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.21it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.59it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.94it/s]
[2025-04-11 10:04:27,456][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0908
[2025-04-11 10:04:27,732][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0539, Metrics: {'mse': 0.058340881019830704, 'rmse': 0.24153857045993857, 'r2': -0.9548016786575317}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.78it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.51it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.31it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.69it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 2/10:  16%|█▌        | 12/75 [00:01<00:06,  9.96it/s]Epoch 2/10:  19%|█▊        | 14/75 [00:01<00:06, 10.05it/s]Epoch 2/10:  21%|██▏       | 16/75 [00:01<00:05, 10.05it/s]Epoch 2/10:  24%|██▍       | 18/75 [00:01<00:05, 10.11it/s]Epoch 2/10:  27%|██▋       | 20/75 [00:02<00:05, 10.09it/s]Epoch 2/10:  29%|██▉       | 22/75 [00:02<00:05, 10.12it/s]Epoch 2/10:  32%|███▏      | 24/75 [00:02<00:05, 10.05it/s]Epoch 2/10:  35%|███▍      | 26/75 [00:02<00:04, 10.11it/s]Epoch 2/10:  37%|███▋      | 28/75 [00:02<00:04, 10.15it/s]Epoch 2/10:  40%|████      | 30/75 [00:03<00:04, 10.17it/s]Epoch 2/10:  43%|████▎     | 32/75 [00:03<00:04, 10.19it/s]Epoch 2/10:  45%|████▌     | 34/75 [00:03<00:04, 10.21it/s]Epoch 2/10:  48%|████▊     | 36/75 [00:03<00:03, 10.22it/s]Epoch 2/10:  51%|█████     | 38/75 [00:03<00:03, 10.21it/s]Epoch 2/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.23it/s]Epoch 2/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.22it/s]Epoch 2/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.22it/s]Epoch 2/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.23it/s]Epoch 2/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.21it/s]Epoch 2/10:  67%|██████▋   | 50/75 [00:04<00:02, 10.21it/s]Epoch 2/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.22it/s]Epoch 2/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.22it/s]Epoch 2/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.17it/s]Epoch 2/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.12it/s]Epoch 2/10:  80%|████████  | 60/75 [00:05<00:01, 10.08it/s]Epoch 2/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.12it/s]Epoch 2/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.11it/s]Epoch 2/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.14it/s]Epoch 2/10:  91%|█████████ | 68/75 [00:06<00:00, 10.14it/s]Epoch 2/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.16it/s]Epoch 2/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.19it/s]Epoch 2/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.19it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.00it/s]
[2025-04-11 10:04:35,633][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0470
[2025-04-11 10:04:35,873][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0365, Metrics: {'mse': 0.038712963461875916, 'rmse': 0.19675610146035094, 'r2': -0.2971378564834595}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.41it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.28it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.19it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.60it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.74it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.89it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.01it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.08it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.11it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.16it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.18it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.19it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.20it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.21it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.21it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.20it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.21it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.21it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.21it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.22it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.23it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.21it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.21it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.22it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.21it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.21it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.22it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.22it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.20it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.21it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.22it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.20it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.19it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.20it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.21it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.21it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.22it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.64it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.05it/s]
[2025-04-11 10:04:43,747][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0405
[2025-04-11 10:04:43,994][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0263, Metrics: {'mse': 0.025451939553022385, 'rmse': 0.1595366401583736, 'r2': 0.14719325304031372}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.06it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.06it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 4/10:   8%|▊         | 6/75 [00:00<00:07,  9.25it/s]Epoch 4/10:  11%|█         | 8/75 [00:00<00:06,  9.65it/s]Epoch 4/10:  13%|█▎        | 10/75 [00:01<00:06,  9.85it/s]Epoch 4/10:  16%|█▌        | 12/75 [00:01<00:06,  9.98it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.96it/s]Epoch 4/10:  19%|█▊        | 14/75 [00:01<00:06,  9.90it/s]Epoch 4/10:  21%|██▏       | 16/75 [00:01<00:05, 10.04it/s]Epoch 4/10:  24%|██▍       | 18/75 [00:01<00:05, 10.04it/s]Epoch 4/10:  27%|██▋       | 20/75 [00:02<00:05, 10.08it/s]Epoch 4/10:  29%|██▉       | 22/75 [00:02<00:05, 10.13it/s]Epoch 4/10:  32%|███▏      | 24/75 [00:02<00:05, 10.16it/s]Epoch 4/10:  35%|███▍      | 26/75 [00:02<00:04, 10.18it/s]Epoch 4/10:  37%|███▋      | 28/75 [00:02<00:04, 10.18it/s]Epoch 4/10:  40%|████      | 30/75 [00:03<00:04, 10.16it/s]Epoch 4/10:  43%|████▎     | 32/75 [00:03<00:04, 10.17it/s]Epoch 4/10:  45%|████▌     | 34/75 [00:03<00:04, 10.17it/s]Epoch 4/10:  48%|████▊     | 36/75 [00:03<00:03, 10.20it/s]Epoch 4/10:  51%|█████     | 38/75 [00:03<00:03, 10.19it/s]Epoch 4/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.19it/s]Epoch 4/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.19it/s]Epoch 4/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.20it/s]Epoch 4/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.20it/s]Epoch 4/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.20it/s]Epoch 4/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.21it/s]Epoch 4/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.16it/s]Epoch 4/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.15it/s]Epoch 4/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.17it/s]Epoch 4/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.18it/s]Epoch 4/10:  80%|████████  | 60/75 [00:05<00:01, 10.18it/s]Epoch 4/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.19it/s]Epoch 4/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.21it/s]Epoch 4/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.20it/s]Epoch 4/10:  91%|█████████ | 68/75 [00:06<00:00, 10.20it/s]Epoch 4/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.21it/s]Epoch 4/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.20it/s]Epoch 4/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.21it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.02it/s]
[2025-04-11 10:04:51,845][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0243
[2025-04-11 10:04:52,169][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0227, Metrics: {'mse': 0.021851757541298866, 'rmse': 0.14782339984352566, 'r2': 0.26782292127609253}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.40it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.29it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.17it/s]Epoch 5/10:   8%|▊         | 6/75 [00:00<00:07,  9.32it/s]Epoch 5/10:  11%|█         | 8/75 [00:00<00:06,  9.62it/s]Epoch 5/10:  13%|█▎        | 10/75 [00:01<00:06,  9.76it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.77it/s]Epoch 5/10:  16%|█▌        | 12/75 [00:01<00:06,  9.78it/s]Epoch 5/10:  19%|█▊        | 14/75 [00:01<00:06,  9.87it/s]Epoch 5/10:  21%|██▏       | 16/75 [00:01<00:05, 10.00it/s]Epoch 5/10:  24%|██▍       | 18/75 [00:01<00:05, 10.08it/s]Epoch 5/10:  27%|██▋       | 20/75 [00:02<00:05, 10.11it/s]Epoch 5/10:  29%|██▉       | 22/75 [00:02<00:05, 10.15it/s]Epoch 5/10:  32%|███▏      | 24/75 [00:02<00:05, 10.16it/s]Epoch 5/10:  35%|███▍      | 26/75 [00:02<00:04, 10.16it/s]Epoch 5/10:  37%|███▋      | 28/75 [00:02<00:04, 10.17it/s]Epoch 5/10:  40%|████      | 30/75 [00:03<00:04, 10.18it/s]Epoch 5/10:  43%|████▎     | 32/75 [00:03<00:04, 10.17it/s]Epoch 5/10:  45%|████▌     | 34/75 [00:03<00:04, 10.17it/s]Epoch 5/10:  48%|████▊     | 36/75 [00:03<00:03, 10.19it/s]Epoch 5/10:  51%|█████     | 38/75 [00:03<00:03, 10.18it/s]Epoch 5/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.18it/s]Epoch 5/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.12it/s]Epoch 5/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.11it/s]Epoch 5/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.15it/s]Epoch 5/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.16it/s]Epoch 5/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.16it/s]Epoch 5/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.14it/s]Epoch 5/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.16it/s]Epoch 5/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.12it/s]Epoch 5/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.13it/s]Epoch 5/10:  80%|████████  | 60/75 [00:06<00:01, 10.10it/s]Epoch 5/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.08it/s]Epoch 5/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.12it/s]Epoch 5/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.14it/s]Epoch 5/10:  91%|█████████ | 68/75 [00:06<00:00, 10.16it/s]Epoch 5/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.18it/s]Epoch 5/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.19it/s]Epoch 5/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.18it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.00it/s]
[2025-04-11 10:05:00,042][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0225
[2025-04-11 10:05:00,293][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0162, Metrics: {'mse': 0.015046685934066772, 'rmse': 0.12266493359581936, 'r2': 0.4958374500274658}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:02<02:33,  2.07s/it]Epoch 6/10:   4%|▍         | 3/75 [00:02<00:43,  1.64it/s]Epoch 6/10:   5%|▌         | 4/75 [00:02<00:30,  2.30it/s]Epoch 6/10:   8%|▊         | 6/75 [00:02<00:18,  3.75it/s]Epoch 6/10:  11%|█         | 8/75 [00:02<00:13,  5.08it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:02<00:11,  5.71it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:03<00:09,  6.92it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:03<00:07,  7.77it/s]Epoch 6/10:  20%|██        | 15/75 [00:03<00:07,  8.46it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:03<00:06,  8.96it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:03<00:06,  9.26it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:04<00:05,  9.53it/s]Epoch 6/10:  31%|███       | 23/75 [00:04<00:05,  9.67it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:04<00:05,  9.83it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:04<00:04,  9.94it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:04<00:04, 10.00it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:05<00:04, 10.07it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:05<00:04, 10.12it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:05<00:03, 10.09it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:05<00:03, 10.11it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.08it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:06<00:03, 10.13it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:06<00:03, 10.11it/s]Epoch 6/10:  60%|██████    | 45/75 [00:06<00:02, 10.13it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:06<00:02, 10.09it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.13it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:07<00:02, 10.16it/s]Epoch 6/10:  71%|███████   | 53/75 [00:07<00:02, 10.17it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:07<00:01, 10.17it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:07<00:01, 10.19it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:07<00:01, 10.19it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.19it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:08<00:01, 10.19it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:08<00:00, 10.15it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:08<00:00, 10.12it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:08<00:00, 10.15it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.15it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:09<00:00, 10.17it/s]Epoch 6/10: 100%|██████████| 75/75 [00:09<00:00, 10.58it/s]Epoch 6/10: 100%|██████████| 75/75 [00:09<00:00,  8.00it/s]
[2025-04-11 10:05:10,049][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0326
[2025-04-11 10:05:10,304][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0306, Metrics: {'mse': 0.029093347489833832, 'rmse': 0.17056772112517019, 'r2': 0.025182247161865234}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.15it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:08,  8.11it/s]Epoch 7/10:   5%|▌         | 4/75 [00:00<00:08,  8.63it/s]Epoch 7/10:   8%|▊         | 6/75 [00:00<00:07,  9.33it/s]Epoch 7/10:  11%|█         | 8/75 [00:00<00:06,  9.63it/s]Epoch 7/10:  13%|█▎        | 10/75 [00:01<00:06,  9.84it/s]Epoch 7/10:  16%|█▌        | 12/75 [00:01<00:06,  9.95it/s]Epoch 7/10:  19%|█▊        | 14/75 [00:01<00:06, 10.01it/s]Epoch 7/10:  21%|██▏       | 16/75 [00:01<00:05, 10.07it/s]Epoch 7/10:  24%|██▍       | 18/75 [00:01<00:05, 10.10it/s]Epoch 7/10:  27%|██▋       | 20/75 [00:02<00:05, 10.13it/s]Epoch 7/10:  29%|██▉       | 22/75 [00:02<00:05, 10.15it/s]Epoch 7/10:  32%|███▏      | 24/75 [00:02<00:05, 10.16it/s]Epoch 7/10:  35%|███▍      | 26/75 [00:02<00:04, 10.16it/s]Epoch 7/10:  37%|███▋      | 28/75 [00:02<00:04, 10.17it/s]Epoch 7/10:  40%|████      | 30/75 [00:03<00:04, 10.18it/s]Epoch 7/10:  43%|████▎     | 32/75 [00:03<00:04, 10.13it/s]Epoch 7/10:  45%|████▌     | 34/75 [00:03<00:04, 10.15it/s]Epoch 7/10:  48%|████▊     | 36/75 [00:03<00:03, 10.10it/s]Epoch 7/10:  51%|█████     | 38/75 [00:03<00:03, 10.11it/s]Epoch 7/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.13it/s]Epoch 7/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.15it/s]Epoch 7/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.16it/s]Epoch 7/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.15it/s]Epoch 7/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.14it/s]Epoch 7/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.16it/s]Epoch 7/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.15it/s]Epoch 7/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.15it/s]Epoch 7/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.16it/s]Epoch 7/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.17it/s]Epoch 7/10:  80%|████████  | 60/75 [00:06<00:01, 10.17it/s]Epoch 7/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.16it/s]Epoch 7/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.17it/s]Epoch 7/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.17it/s]Epoch 7/10:  91%|█████████ | 68/75 [00:06<00:00, 10.17it/s]Epoch 7/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.18it/s]Epoch 7/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.19it/s]Epoch 7/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.20it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.96it/s]
[2025-04-11 10:05:17,833][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0202
[2025-04-11 10:05:18,105][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0285, Metrics: {'mse': 0.026118803769350052, 'rmse': 0.16161312994107271, 'r2': 0.12484896183013916}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  5.24it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:08,  8.17it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  9.08it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 8/10:  11%|█         | 8/75 [00:00<00:06,  9.58it/s]Epoch 8/10:  13%|█▎        | 10/75 [00:01<00:06,  9.82it/s]Epoch 8/10:  16%|█▌        | 12/75 [00:01<00:06,  9.89it/s]Epoch 8/10:  19%|█▊        | 14/75 [00:01<00:06,  9.99it/s]Epoch 8/10:  21%|██▏       | 16/75 [00:01<00:05, 10.04it/s]Epoch 8/10:  24%|██▍       | 18/75 [00:01<00:05, 10.08it/s]Epoch 8/10:  27%|██▋       | 20/75 [00:02<00:05, 10.12it/s]Epoch 8/10:  29%|██▉       | 22/75 [00:02<00:05, 10.14it/s]Epoch 8/10:  32%|███▏      | 24/75 [00:02<00:05, 10.14it/s]Epoch 8/10:  35%|███▍      | 26/75 [00:02<00:04, 10.15it/s]Epoch 8/10:  37%|███▋      | 28/75 [00:02<00:04, 10.16it/s]Epoch 8/10:  40%|████      | 30/75 [00:03<00:04, 10.16it/s]Epoch 8/10:  43%|████▎     | 32/75 [00:03<00:04, 10.11it/s]Epoch 8/10:  45%|████▌     | 34/75 [00:03<00:04, 10.12it/s]Epoch 8/10:  48%|████▊     | 36/75 [00:03<00:03, 10.15it/s]Epoch 8/10:  51%|█████     | 38/75 [00:03<00:03, 10.15it/s]Epoch 8/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.15it/s]Epoch 8/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.16it/s]Epoch 8/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.17it/s]Epoch 8/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.17it/s]Epoch 8/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.16it/s]Epoch 8/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.17it/s]Epoch 8/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.18it/s]Epoch 8/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.17it/s]Epoch 8/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.12it/s]Epoch 8/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.13it/s]Epoch 8/10:  80%|████████  | 60/75 [00:06<00:01, 10.10it/s]Epoch 8/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.13it/s]Epoch 8/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.15it/s]Epoch 8/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.14it/s]Epoch 8/10:  91%|█████████ | 68/75 [00:06<00:00, 10.15it/s]Epoch 8/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.17it/s]Epoch 8/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.18it/s]Epoch 8/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.18it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.00it/s]
[2025-04-11 10:05:25,610][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0196
[2025-04-11 10:05:25,871][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0162, Metrics: {'mse': 0.015256358310580254, 'rmse': 0.1235166317164626, 'r2': 0.48881202936172485}
[2025-04-11 10:05:25,872][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▃▂▁
wandb:     best_val_mse █▅▃▂▁
wandb:      best_val_r2 ▁▄▆▇█
wandb:    best_val_rmse █▅▃▂▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▁▁▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▃▂▁▄▃▁
wandb:          val_mse █▅▃▂▁▃▃▁
wandb:           val_r2 ▁▄▆▇█▆▆█
wandb:         val_rmse █▅▃▂▁▄▃▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01622
wandb:     best_val_mse 0.01505
wandb:      best_val_r2 0.49584
wandb:    best_val_rmse 0.12266
wandb:            epoch 8
wandb:   final_test_mse 0.01695
wandb:    final_test_r2 -0.00595
wandb:  final_test_rmse 0.13017
wandb:  final_train_mse 0.0121
wandb:   final_train_r2 0.63694
wandb: final_train_rmse 0.11002
wandb:    final_val_mse 0.01505
wandb:     final_val_r2 0.49584
wandb:   final_val_rmse 0.12266
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01958
wandb:       train_time 66.80322
wandb:         val_loss 0.01623
wandb:          val_mse 0.01526
wandb:           val_r2 0.48881
wandb:         val_rmse 0.12352
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_100407-gse5xrp9
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_100407-gse5xrp9/logs
Standard experiment for avg_links_len (ru) completed successfully
Running submetric avg_max_depth for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:05:42,136][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_max_depth
experiment_name: avg_max_depth_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:05:42,136][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:05:42,136][__main__][INFO] - Using submetric: avg_max_depth
[2025-04-11 10:05:42,136][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:05:42,136][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:05:42,141][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:05:42,141][__main__][INFO] - Using submetric: avg_max_depth
[2025-04-11 10:05:42,141][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:05:43,398][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:05:45,729][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:05:45,729][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:05:45,774][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:05:45,802][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:05:45,889][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:05:45,898][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:05:45,898][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:05:45,899][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:05:45,918][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:05:45,948][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:05:45,962][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:05:45,964][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:05:45,964][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:05:45,965][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:05:45,986][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:05:46,016][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:05:46,030][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:05:46,032][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:05:46,032][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:05:46,033][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:05:46,034][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:05:46,035][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-04-11 10:05:46,035][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-04-11 10:05:46,035][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-04-11 10:05:46,035][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:05:46,035][src.data.datasets][INFO] -   Mean: 0.2416, Std: 0.1460
[2025-04-11 10:05:46,035][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:05:46,035][src.data.datasets][INFO] - Sample label: 0.125
[2025-04-11 10:05:46,036][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:05:46,036][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-04-11 10:05:46,036][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-04-11 10:05:46,036][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-04-11 10:05:46,036][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-04-11 10:05:46,036][src.data.datasets][INFO] -   Mean: 0.2865, Std: 0.1898
[2025-04-11 10:05:46,036][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:05:46,036][src.data.datasets][INFO] - Sample label: 0.25
[2025-04-11 10:05:46,036][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:05:46,037][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-04-11 10:05:46,037][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-04-11 10:05:46,037][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-04-11 10:05:46,037][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-04-11 10:05:46,037][src.data.datasets][INFO] -   Mean: 0.2284, Std: 0.1562
[2025-04-11 10:05:46,037][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:05:46,037][src.data.datasets][INFO] - Sample label: 0.25
[2025-04-11 10:05:46,037][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:05:46,037][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:05:46,038][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:05:46,038][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:05:49,992][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:05:49,994][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:05:49,995][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:05:49,995][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:16,  1.04s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:24,  2.92it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:18,  3.86it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.59it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.88it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:01<00:08,  7.83it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.53it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.99it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.35it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.54it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.73it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.81it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05,  9.93it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.00it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.05it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.11it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.12it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:04, 10.14it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.16it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.18it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.18it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.19it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:03, 10.20it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.19it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.20it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.20it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.19it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.19it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.21it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.20it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.19it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.20it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.15it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.15it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.17it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.19it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.18it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.18it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.02it/s]
[2025-04-11 10:05:59,979][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0740
[2025-04-11 10:06:00,217][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0467, Metrics: {'mse': 0.045377153903245926, 'rmse': 0.2130191397580178, 'r2': -0.25966405868530273}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.35it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.26it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.15it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.58it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 2/10:  13%|█▎        | 10/75 [00:01<00:06,  9.81it/s]Epoch 2/10:  16%|█▌        | 12/75 [00:01<00:06,  9.96it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06,  9.93it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.02it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.09it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.09it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.14it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.16it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.17it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.15it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.16it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.17it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.18it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.17it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.18it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.17it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.18it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.18it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.18it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.11it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.16it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.15it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.11it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.14it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.14it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.13it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.16it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.02it/s]
[2025-04-11 10:06:08,113][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0279
[2025-04-11 10:06:08,363][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0353, Metrics: {'mse': 0.03263099491596222, 'rmse': 0.18064051294203695, 'r2': 0.09416764974594116}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.49it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.31it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.07it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.75it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.89it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06,  9.98it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.05it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.10it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.11it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.13it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.15it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.16it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.17it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.16it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.12it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.11it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.12it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.13it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.10it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.12it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.14it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.13it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.16it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.16it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.17it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.17it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.13it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.09it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.06it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.08it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.12it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.16it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.01it/s]
[2025-04-11 10:06:16,267][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0204
[2025-04-11 10:06:16,527][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0287, Metrics: {'mse': 0.028164472430944443, 'rmse': 0.16782274110186748, 'r2': 0.2181578278541565}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.14it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.10it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.68it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.84it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:06,  9.96it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.04it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.09it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.10it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.14it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.15it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.15it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.16it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.17it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.15it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.15it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.13it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.08it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.12it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.09it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.11it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.13it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.14it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.15it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.15it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.15it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.10it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.11it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.13it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.14it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.16it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.18it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.18it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.59it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.99it/s]
[2025-04-11 10:06:24,401][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0176
[2025-04-11 10:06:24,644][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0215, Metrics: {'mse': 0.019909154623746872, 'rmse': 0.1410998037693422, 'r2': 0.44732439517974854}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.32it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.22it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.11it/s]Epoch 5/10:   8%|▊         | 6/75 [00:00<00:07,  9.28it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.72it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.90it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.02it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.06it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.11it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.13it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.12it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.15it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.16it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.15it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.11it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.13it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.11it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.11it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.08it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.06it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.05it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.09it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.11it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.12it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.14it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.14it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.12it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.09it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.12it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.13it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:06:32,535][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0172
[2025-04-11 10:06:32,797][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0262, Metrics: {'mse': 0.025695256888866425, 'rmse': 0.16029740137901932, 'r2': 0.2867029309272766}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.06it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.04it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.03it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 6/10:  11%|█         | 8/75 [00:00<00:07,  9.55it/s]Epoch 6/10:  13%|█▎        | 10/75 [00:01<00:06,  9.79it/s]Epoch 6/10:  16%|█▌        | 12/75 [00:01<00:06,  9.86it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.85it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:06,  9.96it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.04it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.08it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.05it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.08it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.12it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.11it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.11it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.13it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.14it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.14it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.12it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.15it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.11it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.11it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.11it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.13it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.14it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.14it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.16it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.17it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:06:40,320][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0137
[2025-04-11 10:06:40,578][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0221, Metrics: {'mse': 0.021909713745117188, 'rmse': 0.14801930193429905, 'r2': 0.39178913831710815}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:15,  4.79it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.86it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.89it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.30it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:01<00:06,  9.59it/s]Epoch 7/10:  13%|█▎        | 10/75 [00:01<00:06,  9.66it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.68it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.85it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:06,  9.97it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.03it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.01it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.05it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.09it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.11it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.12it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.12it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.11it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.08it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.10it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.11it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.13it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.09it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.10it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.11it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.12it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.09it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.11it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.08it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.09it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.06it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.09it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.11it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.11it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.12it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.16it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.94it/s]
[2025-04-11 10:06:48,125][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0132
[2025-04-11 10:06:48,384][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0214, Metrics: {'mse': 0.021685205399990082, 'rmse': 0.14725897392006398, 'r2': 0.398021399974823}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:15,  4.81it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.87it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.89it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.38it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.66it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.83it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06,  9.92it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:06,  9.98it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.04it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.07it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.09it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.10it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.12it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.12it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.12it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.12it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.13it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.13it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.13it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.12it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.13it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.13it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.13it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.13it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.15it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.15it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.16it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.14it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.14it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00,  9.96it/s]
[2025-04-11 10:06:56,434][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0134
[2025-04-11 10:06:56,711][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0173, Metrics: {'mse': 0.017168929800391197, 'rmse': 0.13103026291811826, 'r2': 0.5233926773071289}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:16,  4.45it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.61it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:08,  8.67it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.24it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:01<00:06,  9.56it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.76it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06,  9.89it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:06,  9.96it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05,  9.99it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:02<00:05, 10.05it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.08it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.09it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.10it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.12it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.11it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.10it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.11it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.12it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.14it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.13it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.14it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.15it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.13it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.15it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.13it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.13it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.13it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.14it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.13it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.12it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.13it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.12it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.13it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00,  9.93it/s]
[2025-04-11 10:07:04,672][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0104
[2025-04-11 10:07:04,962][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0283, Metrics: {'mse': 0.029062125831842422, 'rmse': 0.17047617379517416, 'r2': 0.19323909282684326}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:14,  4.99it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:08,  8.00it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.69it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.85it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06,  9.92it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:06,  9.99it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.03it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.05it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.07it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.11it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.12it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.11it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.12it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.13it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.13it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.13it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.14it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.15it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.14it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.13it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.15it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.14it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.13it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.13it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.13it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.13it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00,  9.96it/s]
[2025-04-11 10:07:12,491][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0088
[2025-04-11 10:07:12,766][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0153, Metrics: {'mse': 0.015712155029177666, 'rmse': 0.1253481353238957, 'r2': 0.5638325214385986}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▄▂▂▁▁
wandb:     best_val_mse █▅▄▂▂▁▁
wandb:      best_val_r2 ▁▄▅▇▇██
wandb:    best_val_rmse █▅▄▂▃▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▂▂▂▁▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▄▂▃▃▂▁▄▁
wandb:          val_mse █▅▄▂▃▂▂▁▄▁
wandb:           val_r2 ▁▄▅▇▆▇▇█▅█
wandb:         val_rmse █▅▄▂▄▃▃▁▅▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01531
wandb:     best_val_mse 0.01571
wandb:      best_val_r2 0.56383
wandb:    best_val_rmse 0.12535
wandb:            epoch 10
wandb:   final_test_mse 0.026
wandb:    final_test_r2 -0.06594
wandb:  final_test_rmse 0.16125
wandb:  final_train_mse 0.0051
wandb:   final_train_r2 0.76064
wandb: final_train_rmse 0.07144
wandb:    final_val_mse 0.01571
wandb:     final_val_r2 0.56383
wandb:   final_val_rmse 0.12535
wandb:    learning_rate 1e-05
wandb:       train_loss 0.00878
wandb:       train_time 81.49217
wandb:         val_loss 0.01531
wandb:          val_mse 0.01571
wandb:           val_r2 0.56383
wandb:         val_rmse 0.12535
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_100542-sjzfserv
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_100542-sjzfserv/logs
Standard experiment for avg_max_depth (ru) completed successfully
Running submetric avg_subordinate_chain_len for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:07:29,560][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_subordinate_chain_len
experiment_name: avg_subordinate_chain_len_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:07:29,560][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:07:29,560][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-04-11 10:07:29,560][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:07:29,560][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:07:29,565][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:07:29,565][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-04-11 10:07:29,565][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:07:30,712][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:07:32,945][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:07:32,945][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:07:32,989][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:07:33,017][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:07:33,109][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:07:33,117][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:07:33,118][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:07:33,119][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:07:33,139][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:07:33,172][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:07:33,184][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:07:33,185][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:07:33,186][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:07:33,186][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:07:33,204][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:07:33,238][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:07:33,249][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:07:33,251][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:07:33,251][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:07:33,252][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:07:33,252][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:07:33,252][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-04-11 10:07:33,252][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-04-11 10:07:33,252][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-04-11 10:07:33,252][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:07:33,253][src.data.datasets][INFO] -   Mean: 0.0301, Std: 0.1168
[2025-04-11 10:07:33,253][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:07:33,253][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-11 10:07:33,253][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:07:33,253][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-04-11 10:07:33,253][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-04-11 10:07:33,253][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-04-11 10:07:33,253][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:07:33,254][src.data.datasets][INFO] -   Mean: 0.1273, Std: 0.2332
[2025-04-11 10:07:33,254][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:07:33,254][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-11 10:07:33,254][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:07:33,254][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-04-11 10:07:33,254][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-04-11 10:07:33,254][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-04-11 10:07:33,254][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6670
[2025-04-11 10:07:33,254][src.data.datasets][INFO] -   Mean: 0.1575, Std: 0.1943
[2025-04-11 10:07:33,254][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:07:33,254][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-04-11 10:07:33,255][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:07:33,255][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:07:33,255][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:07:33,255][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:07:37,212][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:07:37,215][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:07:37,215][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:07:37,215][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:25,  1.16s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:26,  2.68it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:16,  4.32it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:11,  5.71it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  6.84it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.30it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.18it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.79it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.16it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:06,  9.47it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.63it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.75it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05,  9.88it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04,  9.97it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.03it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.02it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.02it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:04, 10.07it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.10it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.11it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.13it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.15it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:03, 10.16it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.16it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.11it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.14it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.15it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.15it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.16it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.16it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.17it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.15it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.15it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.16it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.17it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.16it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.17it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.18it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.86it/s]
[2025-04-11 10:07:47,308][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0209
[2025-04-11 10:07:47,537][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0680, Metrics: {'mse': 0.07382526993751526, 'rmse': 0.27170806012614945, 'r2': -0.3576313257217407}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.70it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.44it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.25it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.61it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.93it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.02it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.08it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.10it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.12it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.14it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.15it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.14it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.15it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.12it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.11it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.07it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.00it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03,  9.99it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.04it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.03it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.03it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.03it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.02it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.03it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02,  9.98it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:02,  9.99it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.03it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.07it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.09it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.11it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.11it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.13it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.16it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:07:55,460][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0161
[2025-04-11 10:07:55,711][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0610, Metrics: {'mse': 0.06637398153543472, 'rmse': 0.25763148397553187, 'r2': -0.2206035852432251}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.16it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.10it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.04it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.74it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.87it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.02it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.02it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.03it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.03it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.01it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.01it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.05it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.08it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.11it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.08it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.09it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.11it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.08it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.11it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.12it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.12it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.15it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.09it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.11it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.13it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.13it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.13it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.07it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.10it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.09it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.11it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.11it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.13it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:08:03,647][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0153
[2025-04-11 10:08:03,896][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0575, Metrics: {'mse': 0.06254768371582031, 'rmse': 0.25009534924868215, 'r2': -0.1502387523651123}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.07it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.04it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.00it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.71it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.86it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.96it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:06, 10.00it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05,  9.98it/s]Epoch 4/10:  24%|██▍       | 18/75 [00:01<00:05,  9.95it/s]Epoch 4/10:  27%|██▋       | 20/75 [00:02<00:05, 10.03it/s]Epoch 4/10:  29%|██▉       | 22/75 [00:02<00:05, 10.03it/s]Epoch 4/10:  32%|███▏      | 24/75 [00:02<00:05, 10.07it/s]Epoch 4/10:  35%|███▍      | 26/75 [00:02<00:04, 10.09it/s]Epoch 4/10:  37%|███▋      | 28/75 [00:02<00:04, 10.10it/s]Epoch 4/10:  40%|████      | 30/75 [00:03<00:04, 10.12it/s]Epoch 4/10:  43%|████▎     | 32/75 [00:03<00:04, 10.12it/s]Epoch 4/10:  45%|████▌     | 34/75 [00:03<00:04, 10.13it/s]Epoch 4/10:  48%|████▊     | 36/75 [00:03<00:03, 10.13it/s]Epoch 4/10:  51%|█████     | 38/75 [00:03<00:03, 10.14it/s]Epoch 4/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.14it/s]Epoch 4/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.14it/s]Epoch 4/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.13it/s]Epoch 4/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.09it/s]Epoch 4/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.07it/s]Epoch 4/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.09it/s]Epoch 4/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.10it/s]Epoch 4/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.10it/s]Epoch 4/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.08it/s]Epoch 4/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.10it/s]Epoch 4/10:  80%|████████  | 60/75 [00:06<00:01, 10.11it/s]Epoch 4/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.11it/s]Epoch 4/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.06it/s]Epoch 4/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.09it/s]Epoch 4/10:  91%|█████████ | 68/75 [00:06<00:00, 10.10it/s]Epoch 4/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.11it/s]Epoch 4/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.13it/s]Epoch 4/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.14it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:08:11,782][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0142
[2025-04-11 10:08:12,045][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0562, Metrics: {'mse': 0.06118963286280632, 'rmse': 0.24736538331546376, 'r2': -0.12526452541351318}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.35it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.24it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.13it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.76it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.88it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.97it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.03it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.06it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.10it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.06it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.04it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.02it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04,  9.99it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.02it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.05it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.04it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.03it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.06it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.06it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.09it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.11it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.11it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.07it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.10it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.12it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.13it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.12it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.13it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.13it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.13it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.12it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.12it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.13it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.14it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:08:19,926][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0138
[2025-04-11 10:08:20,183][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0541, Metrics: {'mse': 0.05895562469959259, 'rmse': 0.24280779373733577, 'r2': -0.08418154716491699}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.28it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.20it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.73it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.87it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.02it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.06it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.09it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.11it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.12it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.12it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.13it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.14it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.14it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.13it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.14it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.14it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.13it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.08it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.10it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.04it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.07it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.10it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.11it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.11it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.11it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.13it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:08:28,092][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0117
[2025-04-11 10:08:28,357][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0522, Metrics: {'mse': 0.05518410727381706, 'rmse': 0.23491297808724204, 'r2': -0.014824151992797852}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.23it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:08,  8.15it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.06it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.71it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.86it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:06,  9.98it/s]Epoch 7/10:  21%|██▏       | 16/75 [00:01<00:05,  9.94it/s]Epoch 7/10:  24%|██▍       | 18/75 [00:01<00:05, 10.00it/s]Epoch 7/10:  27%|██▋       | 20/75 [00:02<00:05, 10.06it/s]Epoch 7/10:  29%|██▉       | 22/75 [00:02<00:05, 10.09it/s]Epoch 7/10:  32%|███▏      | 24/75 [00:02<00:05, 10.10it/s]Epoch 7/10:  35%|███▍      | 26/75 [00:02<00:04, 10.11it/s]Epoch 7/10:  37%|███▋      | 28/75 [00:02<00:04, 10.07it/s]Epoch 7/10:  40%|████      | 30/75 [00:03<00:04, 10.11it/s]Epoch 7/10:  43%|████▎     | 32/75 [00:03<00:04, 10.09it/s]Epoch 7/10:  45%|████▌     | 34/75 [00:03<00:04, 10.09it/s]Epoch 7/10:  48%|████▊     | 36/75 [00:03<00:03, 10.10it/s]Epoch 7/10:  51%|█████     | 38/75 [00:03<00:03, 10.11it/s]Epoch 7/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.13it/s]Epoch 7/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.13it/s]Epoch 7/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.10it/s]Epoch 7/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.08it/s]Epoch 7/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.10it/s]Epoch 7/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.12it/s]Epoch 7/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.06it/s]Epoch 7/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.08it/s]Epoch 7/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.08it/s]Epoch 7/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.10it/s]Epoch 7/10:  80%|████████  | 60/75 [00:06<00:01, 10.11it/s]Epoch 7/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.07it/s]Epoch 7/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.08it/s]Epoch 7/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.09it/s]Epoch 7/10:  91%|█████████ | 68/75 [00:06<00:00, 10.12it/s]Epoch 7/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.13it/s]Epoch 7/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.13it/s]Epoch 7/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.13it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.94it/s]
[2025-04-11 10:08:36,276][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0126
[2025-04-11 10:08:36,543][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0500, Metrics: {'mse': 0.05452870577573776, 'rmse': 0.23351382352173022, 'r2': -0.0027714967727661133}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:13,  5.44it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:08,  8.28it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  9.15it/s]Epoch 8/10:   8%|▊         | 6/75 [00:00<00:07,  9.32it/s]Epoch 8/10:  11%|█         | 8/75 [00:00<00:06,  9.65it/s]Epoch 8/10:  13%|█▎        | 10/75 [00:01<00:06,  9.83it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.82it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06,  9.93it/s]Epoch 8/10:  19%|█▊        | 14/75 [00:01<00:06,  9.90it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:06,  9.88it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05,  9.98it/s]Epoch 8/10:  24%|██▍       | 18/75 [00:01<00:05,  9.93it/s]Epoch 8/10:  27%|██▋       | 20/75 [00:02<00:05, 10.02it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05,  9.98it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05,  9.98it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:05,  9.98it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04,  9.98it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04,  9.97it/s]Epoch 8/10:  40%|████      | 30/75 [00:03<00:04,  9.95it/s]Epoch 8/10:  43%|████▎     | 32/75 [00:03<00:04, 10.01it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04,  9.97it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.02it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.05it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03,  9.97it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03,  9.95it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.02it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.05it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.07it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.09it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.10it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.12it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.12it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.12it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.13it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.13it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.13it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.13it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.12it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.54it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00,  9.92it/s]
[2025-04-11 10:08:44,509][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0110
[2025-04-11 10:08:44,769][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0371, Metrics: {'mse': 0.04093337059020996, 'rmse': 0.20231997081407946, 'r2': 0.24724388122558594}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:14,  5.24it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:08,  8.15it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.66it/s]Epoch 9/10:  13%|█▎        | 10/75 [00:01<00:06,  9.68it/s]Epoch 9/10:  16%|█▌        | 12/75 [00:01<00:06,  9.85it/s]Epoch 9/10:  19%|█▊        | 14/75 [00:01<00:06,  9.95it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:06,  9.93it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.00it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05,  9.98it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.04it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.04it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.07it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.07it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.07it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.08it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.04it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.02it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.06it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.09it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.09it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.10it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.12it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.12it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.07it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.08it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.09it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.07it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.10it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.11it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.07it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.08it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.11it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.12it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.12it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.12it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.14it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00,  9.91it/s]
[2025-04-11 10:08:52,734][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0079
[2025-04-11 10:08:52,994][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0413, Metrics: {'mse': 0.045338619500398636, 'rmse': 0.21292867233042767, 'r2': 0.16623228788375854}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:14,  5.03it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:08,  8.01it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.68it/s]Epoch 10/10:  13%|█▎        | 10/75 [00:01<00:06,  9.69it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.71it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06,  9.87it/s]Epoch 10/10:  19%|█▊        | 14/75 [00:01<00:06,  9.86it/s]Epoch 10/10:  21%|██▏       | 16/75 [00:01<00:05,  9.96it/s]Epoch 10/10:  24%|██▍       | 18/75 [00:01<00:05, 10.03it/s]Epoch 10/10:  27%|██▋       | 20/75 [00:02<00:05, 10.08it/s]Epoch 10/10:  29%|██▉       | 22/75 [00:02<00:05, 10.01it/s]Epoch 10/10:  32%|███▏      | 24/75 [00:02<00:05, 10.05it/s]Epoch 10/10:  35%|███▍      | 26/75 [00:02<00:04, 10.03it/s]Epoch 10/10:  37%|███▋      | 28/75 [00:02<00:04, 10.06it/s]Epoch 10/10:  40%|████      | 30/75 [00:03<00:04,  9.98it/s]Epoch 10/10:  43%|████▎     | 32/75 [00:03<00:04, 10.02it/s]Epoch 10/10:  45%|████▌     | 34/75 [00:03<00:04, 10.00it/s]Epoch 10/10:  48%|████▊     | 36/75 [00:03<00:03, 10.05it/s]Epoch 10/10:  51%|█████     | 38/75 [00:03<00:03, 10.02it/s]Epoch 10/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.03it/s]Epoch 10/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.02it/s]Epoch 10/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.03it/s]Epoch 10/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.06it/s]Epoch 10/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.08it/s]Epoch 10/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.10it/s]Epoch 10/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.08it/s]Epoch 10/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.09it/s]Epoch 10/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.10it/s]Epoch 10/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.12it/s]Epoch 10/10:  80%|████████  | 60/75 [00:06<00:01, 10.12it/s]Epoch 10/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.13it/s]Epoch 10/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.12it/s]Epoch 10/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.12it/s]Epoch 10/10:  91%|█████████ | 68/75 [00:06<00:00, 10.13it/s]Epoch 10/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.14it/s]Epoch 10/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.14it/s]Epoch 10/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.15it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00,  9.91it/s]
[2025-04-11 10:09:00,561][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0087
[2025-04-11 10:09:00,826][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0378, Metrics: {'mse': 0.041545577347278595, 'rmse': 0.2038273223767574, 'r2': 0.23598557710647583}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▆▅▅▄▄▁
wandb:     best_val_mse █▆▆▅▅▄▄▁
wandb:      best_val_r2 ▁▃▃▄▄▅▅█
wandb:    best_val_rmse █▇▆▆▅▄▄▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▅▄▄▃▄▃▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▆▅▅▄▄▁▂▁
wandb:          val_mse █▆▆▅▅▄▄▁▂▁
wandb:           val_r2 ▁▃▃▄▄▅▅█▇█
wandb:         val_rmse █▇▆▆▅▄▄▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03711
wandb:     best_val_mse 0.04093
wandb:      best_val_r2 0.24724
wandb:    best_val_rmse 0.20232
wandb:            epoch 10
wandb:   final_test_mse 0.03031
wandb:    final_test_r2 0.19747
wandb:  final_test_rmse 0.17409
wandb:  final_train_mse 0.00565
wandb:   final_train_r2 0.58627
wandb: final_train_rmse 0.07515
wandb:    final_val_mse 0.04093
wandb:     final_val_r2 0.24724
wandb:   final_val_rmse 0.20232
wandb:    learning_rate 1e-05
wandb:       train_loss 0.00867
wandb:       train_time 81.98641
wandb:         val_loss 0.03781
wandb:          val_mse 0.04155
wandb:           val_r2 0.23599
wandb:         val_rmse 0.20383
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_100729-pak8m3ji
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_100729-pak8m3ji/logs
Standard experiment for avg_subordinate_chain_len (ru) completed successfully
Running submetric avg_verb_edges for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:09:17,720][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_verb_edges
experiment_name: avg_verb_edges_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:09:17,720][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:09:17,720][__main__][INFO] - Using submetric: avg_verb_edges
[2025-04-11 10:09:17,720][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:09:17,720][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:09:17,725][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:09:17,725][__main__][INFO] - Using submetric: avg_verb_edges
[2025-04-11 10:09:17,725][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:09:19,197][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:09:21,456][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:09:21,457][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:09:21,490][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:09:21,514][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:09:21,582][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:09:21,590][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:09:21,591][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:09:21,591][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:09:21,607][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:09:21,629][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:09:21,640][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:09:21,641][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:09:21,641][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:09:21,642][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:09:21,658][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:09:21,681][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:09:21,690][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:09:21,691][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:09:21,692][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:09:21,692][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:09:21,693][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:09:21,693][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-04-11 10:09:21,693][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-04-11 10:09:21,693][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-04-11 10:09:21,693][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:09:21,693][src.data.datasets][INFO] -   Mean: 0.3433, Std: 0.2277
[2025-04-11 10:09:21,693][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:09:21,693][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-04-11 10:09:21,694][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:09:21,694][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-04-11 10:09:21,694][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-04-11 10:09:21,694][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-04-11 10:09:21,694][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8000
[2025-04-11 10:09:21,694][src.data.datasets][INFO] -   Mean: 0.3578, Std: 0.2206
[2025-04-11 10:09:21,694][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:09:21,694][src.data.datasets][INFO] - Sample label: 0.6000000238418579
[2025-04-11 10:09:21,694][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:09:21,694][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-04-11 10:09:21,695][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-04-11 10:09:21,695][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-04-11 10:09:21,695][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-04-11 10:09:21,695][src.data.datasets][INFO] -   Mean: 0.4020, Std: 0.2023
[2025-04-11 10:09:21,695][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:09:21,695][src.data.datasets][INFO] - Sample label: 0.30000001192092896
[2025-04-11 10:09:21,695][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:09:21,695][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:09:21,695][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:09:21,696][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:09:25,511][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:09:25,513][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:09:25,513][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:09:25,513][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:30,  1.22s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:27,  2.58it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:16,  4.19it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.58it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  6.73it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.64it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.34it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.86it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.24it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.52it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.71it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.85it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:05,  9.95it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.01it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.05it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.10it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.13it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.10it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.12it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.13it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.15it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.16it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.12it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.14it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.16it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.17it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.16it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.17it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.14it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.15it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.11it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.13it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.11it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.13it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.14it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.14it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.16it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.55it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.82it/s]
[2025-04-11 10:09:35,620][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1540
[2025-04-11 10:09:35,856][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1207, Metrics: {'mse': 0.1282365918159485, 'rmse': 0.3581013708657766, 'r2': -1.6346640586853027}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.30it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.21it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.12it/s]Epoch 2/10:   8%|▊         | 6/75 [00:00<00:07,  9.33it/s]Epoch 2/10:  11%|█         | 8/75 [00:00<00:06,  9.68it/s]Epoch 2/10:  13%|█▎        | 10/75 [00:01<00:06,  9.87it/s]Epoch 2/10:  16%|█▌        | 12/75 [00:01<00:06,  9.98it/s]Epoch 2/10:  19%|█▊        | 14/75 [00:01<00:06, 10.04it/s]Epoch 2/10:  21%|██▏       | 16/75 [00:01<00:05, 10.07it/s]Epoch 2/10:  24%|██▍       | 18/75 [00:01<00:05, 10.11it/s]Epoch 2/10:  27%|██▋       | 20/75 [00:02<00:05, 10.13it/s]Epoch 2/10:  29%|██▉       | 22/75 [00:02<00:05, 10.15it/s]Epoch 2/10:  32%|███▏      | 24/75 [00:02<00:05, 10.15it/s]Epoch 2/10:  35%|███▍      | 26/75 [00:02<00:04, 10.16it/s]Epoch 2/10:  37%|███▋      | 28/75 [00:02<00:04, 10.17it/s]Epoch 2/10:  40%|████      | 30/75 [00:03<00:04, 10.17it/s]Epoch 2/10:  43%|████▎     | 32/75 [00:03<00:04, 10.17it/s]Epoch 2/10:  45%|████▌     | 34/75 [00:03<00:04, 10.18it/s]Epoch 2/10:  48%|████▊     | 36/75 [00:03<00:03, 10.19it/s]Epoch 2/10:  51%|█████     | 38/75 [00:03<00:03, 10.18it/s]Epoch 2/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.19it/s]Epoch 2/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.19it/s]Epoch 2/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.19it/s]Epoch 2/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.18it/s]Epoch 2/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.19it/s]Epoch 2/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.19it/s]Epoch 2/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.19it/s]Epoch 2/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.17it/s]Epoch 2/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.17it/s]Epoch 2/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.17it/s]Epoch 2/10:  80%|████████  | 60/75 [00:05<00:01, 10.15it/s]Epoch 2/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.16it/s]Epoch 2/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.16it/s]Epoch 2/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.14it/s]Epoch 2/10:  91%|█████████ | 68/75 [00:06<00:00, 10.14it/s]Epoch 2/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.15it/s]Epoch 2/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.16it/s]Epoch 2/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.16it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.02it/s]
[2025-04-11 10:09:43,763][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0952
[2025-04-11 10:09:44,022][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0589, Metrics: {'mse': 0.057992126792669296, 'rmse': 0.2408155451640722, 'r2': -0.1914677619934082}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.20it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.15it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.08it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.75it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.90it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06,  9.98it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.03it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.08it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.11it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.13it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.15it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.17it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.16it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.16it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.17it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.12it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.12it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.13it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.14it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.10it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.12it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.12it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.08it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.09it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.12it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.16it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.16it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.16it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.17it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:09:51,955][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0680
[2025-04-11 10:09:52,218][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0540, Metrics: {'mse': 0.05450029671192169, 'rmse': 0.23345298608482543, 'r2': -0.11972689628601074}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.24it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.16it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.08it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 4/10:  11%|█         | 8/75 [00:00<00:07,  9.56it/s]Epoch 4/10:  13%|█▎        | 10/75 [00:01<00:06,  9.80it/s]Epoch 4/10:  16%|█▌        | 12/75 [00:01<00:06,  9.92it/s]Epoch 4/10:  19%|█▊        | 14/75 [00:01<00:06, 10.01it/s]Epoch 4/10:  21%|██▏       | 16/75 [00:01<00:05, 10.07it/s]Epoch 4/10:  24%|██▍       | 18/75 [00:01<00:05, 10.10it/s]Epoch 4/10:  27%|██▋       | 20/75 [00:02<00:05, 10.10it/s]Epoch 4/10:  29%|██▉       | 22/75 [00:02<00:05, 10.13it/s]Epoch 4/10:  32%|███▏      | 24/75 [00:02<00:05, 10.14it/s]Epoch 4/10:  35%|███▍      | 26/75 [00:02<00:04, 10.15it/s]Epoch 4/10:  37%|███▋      | 28/75 [00:02<00:04, 10.14it/s]Epoch 4/10:  40%|████      | 30/75 [00:03<00:04, 10.16it/s]Epoch 4/10:  43%|████▎     | 32/75 [00:03<00:04, 10.12it/s]Epoch 4/10:  45%|████▌     | 34/75 [00:03<00:04, 10.14it/s]Epoch 4/10:  48%|████▊     | 36/75 [00:03<00:03, 10.14it/s]Epoch 4/10:  51%|█████     | 38/75 [00:03<00:03, 10.15it/s]Epoch 4/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.15it/s]Epoch 4/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.16it/s]Epoch 4/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.07it/s]Epoch 4/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.01it/s]Epoch 4/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.05it/s]Epoch 4/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.01it/s]Epoch 4/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.06it/s]Epoch 4/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.04it/s]Epoch 4/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.08it/s]Epoch 4/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.10it/s]Epoch 4/10:  80%|████████  | 60/75 [00:06<00:01, 10.10it/s]Epoch 4/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.07it/s]Epoch 4/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.11it/s]Epoch 4/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.13it/s]Epoch 4/10:  91%|█████████ | 68/75 [00:06<00:00, 10.13it/s]Epoch 4/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.13it/s]Epoch 4/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.15it/s]Epoch 4/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.16it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.96it/s]
[2025-04-11 10:10:00,095][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0506
[2025-04-11 10:10:00,362][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0505, Metrics: {'mse': 0.04982338473200798, 'rmse': 0.22321152463976401, 'r2': -0.023638248443603516}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.38it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.26it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.12it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.53it/s]Epoch 5/10:  11%|█         | 8/75 [00:00<00:07,  9.57it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.64it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.78it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.87it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:06,  9.96it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05,  9.96it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.03it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05,  9.98it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.01it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.00it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.04it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.08it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.09it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.10it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.11it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.13it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.13it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.13it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.12it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.10it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.09it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.11it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.11it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.13it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.13it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.14it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.14it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.16it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.96it/s]
[2025-04-11 10:10:08,270][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0468
[2025-04-11 10:10:08,539][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0475, Metrics: {'mse': 0.04687758907675743, 'rmse': 0.2165123300801999, 'r2': 0.0368841290473938}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.15it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.08it/s]Epoch 6/10:   5%|▌         | 4/75 [00:00<00:08,  8.64it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.77it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.92it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.99it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.03it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.09it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.11it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.10it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.07it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.09it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.12it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.12it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.10it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.11it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.06it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.08it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.11it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.07it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.08it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.11it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.12it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.13it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.12it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.09it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.11it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.08it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.07it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.05it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.02it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.01it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.06it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.10it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.11it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.12it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.52it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:10:16,452][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0482
[2025-04-11 10:10:16,726][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0434, Metrics: {'mse': 0.04180152714252472, 'rmse': 0.20445421771762184, 'r2': 0.1411736011505127}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  4.94it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.96it/s]Epoch 7/10:   5%|▌         | 4/75 [00:00<00:08,  8.49it/s]Epoch 7/10:   8%|▊         | 6/75 [00:00<00:07,  9.24it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.39it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.70it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.81it/s]Epoch 7/10:  16%|█▌        | 12/75 [00:01<00:06,  9.80it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.81it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:06,  9.88it/s]Epoch 7/10:  21%|██▏       | 16/75 [00:01<00:05,  9.86it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05,  9.89it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:02<00:05,  9.99it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.06it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.10it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.12it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.12it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.10it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.13it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.13it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.14it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.13it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.15it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.14it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.14it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.14it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.11it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.10it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.07it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.06it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.07it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.08it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.05it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.04it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.04it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.07it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.09it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.11it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.53it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.92it/s]
[2025-04-11 10:10:24,663][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0432
[2025-04-11 10:10:24,927][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0390, Metrics: {'mse': 0.03806012123823166, 'rmse': 0.19509003367222955, 'r2': 0.21804195642471313}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  5.02it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:08,  8.01it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 8/10:   8%|▊         | 6/75 [00:00<00:07,  9.13it/s]Epoch 8/10:  11%|█         | 8/75 [00:00<00:07,  9.48it/s]Epoch 8/10:  13%|█▎        | 10/75 [00:01<00:06,  9.73it/s]Epoch 8/10:  16%|█▌        | 12/75 [00:01<00:06,  9.88it/s]Epoch 8/10:  19%|█▊        | 14/75 [00:01<00:06,  9.96it/s]Epoch 8/10:  21%|██▏       | 16/75 [00:01<00:05, 10.01it/s]Epoch 8/10:  24%|██▍       | 18/75 [00:01<00:05, 10.06it/s]Epoch 8/10:  27%|██▋       | 20/75 [00:02<00:05, 10.09it/s]Epoch 8/10:  29%|██▉       | 22/75 [00:02<00:05, 10.10it/s]Epoch 8/10:  32%|███▏      | 24/75 [00:02<00:05, 10.05it/s]Epoch 8/10:  35%|███▍      | 26/75 [00:02<00:04, 10.08it/s]Epoch 8/10:  37%|███▋      | 28/75 [00:02<00:04, 10.10it/s]Epoch 8/10:  40%|████      | 30/75 [00:03<00:04, 10.05it/s]Epoch 8/10:  43%|████▎     | 32/75 [00:03<00:04, 10.07it/s]Epoch 8/10:  45%|████▌     | 34/75 [00:03<00:04, 10.03it/s]Epoch 8/10:  48%|████▊     | 36/75 [00:03<00:03, 10.00it/s]Epoch 8/10:  51%|█████     | 38/75 [00:03<00:03, 10.06it/s]Epoch 8/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.04it/s]Epoch 8/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.06it/s]Epoch 8/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.03it/s]Epoch 8/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.01it/s]Epoch 8/10:  64%|██████▍   | 48/75 [00:04<00:02,  9.95it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02,  9.91it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02,  9.99it/s]Epoch 8/10:  69%|██████▉   | 52/75 [00:05<00:02,  9.97it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02,  9.95it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.04it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01,  9.98it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.03it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.01it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.02it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.02it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.07it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.09it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.11it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.13it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.54it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00,  9.89it/s]
[2025-04-11 10:10:32,895][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0417
[2025-04-11 10:10:33,155][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0399, Metrics: {'mse': 0.038786496967077255, 'rmse': 0.19694287742154387, 'r2': 0.20311832427978516}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:15,  4.87it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.92it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.93it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.39it/s]Epoch 9/10:  11%|█         | 8/75 [00:00<00:07,  9.48it/s]Epoch 9/10:  13%|█▎        | 10/75 [00:01<00:06,  9.73it/s]Epoch 9/10:  16%|█▌        | 12/75 [00:01<00:06,  9.83it/s]Epoch 9/10:  19%|█▊        | 14/75 [00:01<00:06,  9.93it/s]Epoch 9/10:  21%|██▏       | 16/75 [00:01<00:05, 10.00it/s]Epoch 9/10:  24%|██▍       | 18/75 [00:01<00:05, 10.04it/s]Epoch 9/10:  27%|██▋       | 20/75 [00:02<00:05, 10.08it/s]Epoch 9/10:  29%|██▉       | 22/75 [00:02<00:05, 10.05it/s]Epoch 9/10:  32%|███▏      | 24/75 [00:02<00:05, 10.07it/s]Epoch 9/10:  35%|███▍      | 26/75 [00:02<00:04, 10.08it/s]Epoch 9/10:  37%|███▋      | 28/75 [00:02<00:04, 10.04it/s]Epoch 9/10:  40%|████      | 30/75 [00:03<00:04, 10.07it/s]Epoch 9/10:  43%|████▎     | 32/75 [00:03<00:04,  9.98it/s]Epoch 9/10:  45%|████▌     | 34/75 [00:03<00:04, 10.04it/s]Epoch 9/10:  48%|████▊     | 36/75 [00:03<00:03, 10.03it/s]Epoch 9/10:  51%|█████     | 38/75 [00:03<00:03, 10.06it/s]Epoch 9/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.09it/s]Epoch 9/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.06it/s]Epoch 9/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.10it/s]Epoch 9/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.11it/s]Epoch 9/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.12it/s]Epoch 9/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.12it/s]Epoch 9/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.07it/s]Epoch 9/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.10it/s]Epoch 9/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.12it/s]Epoch 9/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.09it/s]Epoch 9/10:  80%|████████  | 60/75 [00:06<00:01, 10.06it/s]Epoch 9/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.08it/s]Epoch 9/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.06it/s]Epoch 9/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.04it/s]Epoch 9/10:  91%|█████████ | 68/75 [00:06<00:00, 10.08it/s]Epoch 9/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.10it/s]Epoch 9/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.11it/s]Epoch 9/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.13it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00,  9.92it/s]
[2025-04-11 10:10:40,720][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0316
[2025-04-11 10:10:40,990][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0288, Metrics: {'mse': 0.030689256265759468, 'rmse': 0.17518349313151474, 'r2': 0.36947888135910034}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:14,  5.07it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:08,  8.05it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  9.02it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.70it/s]Epoch 10/10:  13%|█▎        | 10/75 [00:01<00:06,  9.76it/s]Epoch 10/10:  16%|█▌        | 12/75 [00:01<00:06,  9.83it/s]Epoch 10/10:  19%|█▊        | 14/75 [00:01<00:06,  9.94it/s]Epoch 10/10:  21%|██▏       | 16/75 [00:01<00:05, 10.02it/s]Epoch 10/10:  24%|██▍       | 18/75 [00:01<00:05, 10.06it/s]Epoch 10/10:  27%|██▋       | 20/75 [00:02<00:05, 10.08it/s]Epoch 10/10:  29%|██▉       | 22/75 [00:02<00:05, 10.11it/s]Epoch 10/10:  32%|███▏      | 24/75 [00:02<00:05, 10.12it/s]Epoch 10/10:  35%|███▍      | 26/75 [00:02<00:04, 10.12it/s]Epoch 10/10:  37%|███▋      | 28/75 [00:02<00:04, 10.12it/s]Epoch 10/10:  40%|████      | 30/75 [00:03<00:04, 10.13it/s]Epoch 10/10:  43%|████▎     | 32/75 [00:03<00:04, 10.14it/s]Epoch 10/10:  45%|████▌     | 34/75 [00:03<00:04, 10.13it/s]Epoch 10/10:  48%|████▊     | 36/75 [00:03<00:03, 10.13it/s]Epoch 10/10:  51%|█████     | 38/75 [00:03<00:03, 10.14it/s]Epoch 10/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.14it/s]Epoch 10/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.14it/s]Epoch 10/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.13it/s]Epoch 10/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.14it/s]Epoch 10/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.08it/s]Epoch 10/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.10it/s]Epoch 10/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.07it/s]Epoch 10/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.08it/s]Epoch 10/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.05it/s]Epoch 10/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.02it/s]Epoch 10/10:  80%|████████  | 60/75 [00:06<00:01, 10.01it/s]Epoch 10/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.00it/s]Epoch 10/10:  85%|████████▌ | 64/75 [00:06<00:01,  9.99it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:01,  9.97it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.01it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.05it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.09it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.11it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.54it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00,  9.94it/s]
[2025-04-11 10:10:48,940][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0231
[2025-04-11 10:10:49,218][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0279, Metrics: {'mse': 0.029124421998858452, 'rmse': 0.17065878822626876, 'r2': 0.40162891149520874}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▃▃▂▂▂▁▁
wandb:     best_val_mse █▃▃▂▂▂▂▁▁
wandb:      best_val_r2 ▁▆▆▇▇▇▇██
wandb:    best_val_rmse █▄▃▃▃▂▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▃▂▂▂▂▂▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▃▃▂▂▂▂▁▁
wandb:          val_mse █▃▃▂▂▂▂▂▁▁
wandb:           val_r2 ▁▆▆▇▇▇▇▇██
wandb:         val_rmse █▄▃▃▃▂▂▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02793
wandb:     best_val_mse 0.02912
wandb:      best_val_r2 0.40163
wandb:    best_val_rmse 0.17066
wandb:            epoch 10
wandb:   final_test_mse 0.02942
wandb:    final_test_r2 0.28122
wandb:  final_test_rmse 0.17152
wandb:  final_train_mse 0.01226
wandb:   final_train_r2 0.76346
wandb: final_train_rmse 0.11072
wandb:    final_val_mse 0.02912
wandb:     final_val_r2 0.40163
wandb:   final_val_rmse 0.17066
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02311
wandb:       train_time 82.51716
wandb:         val_loss 0.02793
wandb:          val_mse 0.02912
wandb:           val_r2 0.40163
wandb:         val_rmse 0.17066
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_100917-pf5j1vrc
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_100917-pf5j1vrc/logs
Standard experiment for avg_verb_edges (ru) completed successfully
Running submetric lexical_density for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:11:06,399][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/lexical_density
experiment_name: lexical_density_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: lexical_density
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:11:06,400][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:11:06,400][__main__][INFO] - Using submetric: lexical_density
[2025-04-11 10:11:06,400][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:11:06,400][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:11:06,404][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:11:06,404][__main__][INFO] - Using submetric: lexical_density
[2025-04-11 10:11:06,405][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:11:07,537][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'lexical_density'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:11:09,808][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:11:09,809][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:11:09,854][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:11:09,881][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:11:09,980][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:11:09,990][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:11:09,991][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:11:09,992][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:11:10,013][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:11:10,036][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:11:10,049][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:11:10,050][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:11:10,050][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:11:10,051][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:11:10,072][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:11:10,097][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:11:10,109][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:11:10,110][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:11:10,111][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:11:10,112][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:11:10,112][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:11:10,112][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-04-11 10:11:10,113][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-04-11 10:11:10,113][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-04-11 10:11:10,113][src.data.datasets][INFO] -   Min: 0.2220, Max: 1.0000
[2025-04-11 10:11:10,113][src.data.datasets][INFO] -   Mean: 0.7345, Std: 0.1536
[2025-04-11 10:11:10,113][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:11:10,113][src.data.datasets][INFO] - Sample label: 0.6669999957084656
[2025-04-11 10:11:10,113][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:11:10,113][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-04-11 10:11:10,114][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-04-11 10:11:10,114][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-04-11 10:11:10,114][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:11:10,114][src.data.datasets][INFO] -   Mean: 0.6207, Std: 0.2095
[2025-04-11 10:11:10,114][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:11:10,114][src.data.datasets][INFO] - Sample label: 0.5509999990463257
[2025-04-11 10:11:10,114][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:11:10,114][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'lexical_density'
[2025-04-11 10:11:10,114][src.data.datasets][INFO] - Selected feature name: 'lexical_density' for task: 'single_submetric'
[2025-04-11 10:11:10,114][src.data.datasets][INFO] - Label statistics for single_submetric (feature: lexical_density):
[2025-04-11 10:11:10,115][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:11:10,115][src.data.datasets][INFO] -   Mean: 0.5581, Std: 0.2005
[2025-04-11 10:11:10,115][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:11:10,115][src.data.datasets][INFO] - Sample label: 0.8740000128746033
[2025-04-11 10:11:10,115][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:11:10,115][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:11:10,115][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:11:10,116][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:11:14,450][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:11:14,452][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:11:14,452][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:11:14,452][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:12,  1.02it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:23,  3.04it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:14,  4.76it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:11,  6.15it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  7.22it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:07,  8.03it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.63it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  9.08it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.40it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.63it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:05,  9.80it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.91it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:05,  9.98it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.04it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.09it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:03<00:04, 10.11it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.13it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.15it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.16it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.16it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.17it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.17it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.17it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.16it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.17it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.18it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.18it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.15it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.16it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.17it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.16it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.14it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.15it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.14it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.15it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.13it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.16it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.54it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.06it/s]
[2025-04-11 10:11:24,268][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.3879
[2025-04-11 10:11:24,505][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0786, Metrics: {'mse': 0.0697077065706253, 'rmse': 0.26402217060433636, 'r2': -0.5878826379776001}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.39it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.27it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.17it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.55it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.88it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06,  9.98it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.04it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.09it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.12it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.13it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.16it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.17it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.17it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.16it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.16it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.17it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.06it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.09it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.06it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.10it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.13it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.13it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.11it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.15it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.07it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.09it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.01it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.00it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01,  9.99it/s]Epoch 2/10:  85%|████████▌ | 64/75 [00:06<00:01,  9.95it/s]Epoch 2/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.03it/s]Epoch 2/10:  91%|█████████ | 68/75 [00:06<00:00, 10.05it/s]Epoch 2/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.10it/s]Epoch 2/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.11it/s]Epoch 2/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.14it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:11:32,442][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.1283
[2025-04-11 10:11:32,691][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0757, Metrics: {'mse': 0.0789719969034195, 'rmse': 0.28101956676256457, 'r2': -0.7989153861999512}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.14it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.11it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.06it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.72it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.88it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06,  9.98it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.02it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.07it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.11it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.13it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.15it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.16it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.16it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.15it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.16it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.17it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.15it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.15it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.16it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.16it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.15it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.10it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.12it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.09it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.11it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.09it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.10it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.12it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.14it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.18it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00,  9.99it/s]
[2025-04-11 10:11:40,620][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0743
[2025-04-11 10:11:40,870][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0794, Metrics: {'mse': 0.07213415205478668, 'rmse': 0.26857801856217994, 'r2': -0.6431549787521362}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.28it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.16it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.08it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.75it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.87it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.96it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.03it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.05it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.06it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.03it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05,  9.99it/s]Epoch 4/10:  32%|███▏      | 24/75 [00:02<00:05,  9.94it/s]Epoch 4/10:  35%|███▍      | 26/75 [00:02<00:04, 10.01it/s]Epoch 4/10:  37%|███▋      | 28/75 [00:02<00:04, 10.07it/s]Epoch 4/10:  40%|████      | 30/75 [00:03<00:04, 10.10it/s]Epoch 4/10:  43%|████▎     | 32/75 [00:03<00:04, 10.11it/s]Epoch 4/10:  45%|████▌     | 34/75 [00:03<00:04, 10.13it/s]Epoch 4/10:  48%|████▊     | 36/75 [00:03<00:03, 10.15it/s]Epoch 4/10:  51%|█████     | 38/75 [00:03<00:03, 10.15it/s]Epoch 4/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.10it/s]Epoch 4/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.11it/s]Epoch 4/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.13it/s]Epoch 4/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.15it/s]Epoch 4/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.16it/s]Epoch 4/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.15it/s]Epoch 4/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.16it/s]Epoch 4/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.16it/s]Epoch 4/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.15it/s]Epoch 4/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.15it/s]Epoch 4/10:  80%|████████  | 60/75 [00:06<00:01, 10.16it/s]Epoch 4/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.17it/s]Epoch 4/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.15it/s]Epoch 4/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.15it/s]Epoch 4/10:  91%|█████████ | 68/75 [00:06<00:00, 10.16it/s]Epoch 4/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.17it/s]Epoch 4/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.17it/s]Epoch 4/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.17it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:11:48,396][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0527
[2025-04-11 10:11:48,656][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0590, Metrics: {'mse': 0.05559232458472252, 'rmse': 0.23578024638362416, 'r2': -0.2663462162017822}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.24it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.17it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.53it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.67it/s]Epoch 5/10:  13%|█▎        | 10/75 [00:01<00:06,  9.67it/s]Epoch 5/10:  16%|█▌        | 12/75 [00:01<00:06,  9.84it/s]Epoch 5/10:  19%|█▊        | 14/75 [00:01<00:06,  9.87it/s]Epoch 5/10:  21%|██▏       | 16/75 [00:01<00:05,  9.97it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05,  9.93it/s]Epoch 5/10:  24%|██▍       | 18/75 [00:01<00:05,  9.90it/s]Epoch 5/10:  27%|██▋       | 20/75 [00:02<00:05, 10.01it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05,  9.95it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.02it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.00it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.05it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.00it/s]Epoch 5/10:  40%|████      | 30/75 [00:03<00:04,  9.95it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04,  9.92it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04,  9.94it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:04,  9.97it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03,  9.98it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03,  9.99it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03,  9.99it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03,  9.98it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.03it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.05it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.03it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.07it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.09it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.10it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.11it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.14it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.13it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.13it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.13it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.14it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.92it/s]
[2025-04-11 10:11:56,568][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0416
[2025-04-11 10:11:56,823][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0599, Metrics: {'mse': 0.0556567907333374, 'rmse': 0.23591691489449712, 'r2': -0.26781463623046875}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:13,  5.32it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.22it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.11it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.64it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.82it/s]Epoch 6/10:  16%|█▌        | 12/75 [00:01<00:06,  9.82it/s]Epoch 6/10:  19%|█▊        | 14/75 [00:01<00:06,  9.95it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:06,  9.92it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.00it/s]Epoch 6/10:  24%|██▍       | 18/75 [00:01<00:05,  9.96it/s]Epoch 6/10:  27%|██▋       | 20/75 [00:02<00:05, 10.02it/s]Epoch 6/10:  29%|██▉       | 22/75 [00:02<00:05,  9.99it/s]Epoch 6/10:  32%|███▏      | 24/75 [00:02<00:05, 10.03it/s]Epoch 6/10:  35%|███▍      | 26/75 [00:02<00:04, 10.01it/s]Epoch 6/10:  37%|███▋      | 28/75 [00:02<00:04, 10.06it/s]Epoch 6/10:  40%|████      | 30/75 [00:03<00:04, 10.09it/s]Epoch 6/10:  43%|████▎     | 32/75 [00:03<00:04, 10.10it/s]Epoch 6/10:  45%|████▌     | 34/75 [00:03<00:04, 10.11it/s]Epoch 6/10:  48%|████▊     | 36/75 [00:03<00:03, 10.13it/s]Epoch 6/10:  51%|█████     | 38/75 [00:03<00:03, 10.13it/s]Epoch 6/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.13it/s]Epoch 6/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.13it/s]Epoch 6/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.08it/s]Epoch 6/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.10it/s]Epoch 6/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.11it/s]Epoch 6/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.12it/s]Epoch 6/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.12it/s]Epoch 6/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.14it/s]Epoch 6/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.14it/s]Epoch 6/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.14it/s]Epoch 6/10:  80%|████████  | 60/75 [00:06<00:01, 10.14it/s]Epoch 6/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.13it/s]Epoch 6/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.15it/s]Epoch 6/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.15it/s]Epoch 6/10:  91%|█████████ | 68/75 [00:06<00:00, 10.15it/s]Epoch 6/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.14it/s]Epoch 6/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.16it/s]Epoch 6/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.16it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.94it/s]
[2025-04-11 10:12:04,368][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0358
[2025-04-11 10:12:04,628][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0489, Metrics: {'mse': 0.04644587263464928, 'rmse': 0.21551304516119035, 'r2': -0.057997703552246094}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:13,  5.30it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:08,  8.18it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.07it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.74it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.88it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.96it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.01it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.06it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.09it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.10it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.05it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.08it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.10it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.09it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.10it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.06it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.09it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.08it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.10it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.07it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.08it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.10it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.13it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.08it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.10it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.11it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.13it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.14it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.14it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:12:12,498][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0342
[2025-04-11 10:12:12,768][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0531, Metrics: {'mse': 0.05167553946375847, 'rmse': 0.2273225449966599, 'r2': -0.17712509632110596}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:15,  4.76it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.75it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.81it/s]Epoch 8/10:   8%|▊         | 6/75 [00:00<00:07,  9.06it/s]Epoch 8/10:  11%|█         | 8/75 [00:00<00:07,  9.49it/s]Epoch 8/10:  13%|█▎        | 10/75 [00:01<00:06,  9.74it/s]Epoch 8/10:  16%|█▌        | 12/75 [00:01<00:06,  9.89it/s]Epoch 8/10:  19%|█▊        | 14/75 [00:01<00:06,  9.97it/s]Epoch 8/10:  21%|██▏       | 16/75 [00:01<00:05, 10.03it/s]Epoch 8/10:  24%|██▍       | 18/75 [00:01<00:05, 10.07it/s]Epoch 8/10:  27%|██▋       | 20/75 [00:02<00:05, 10.05it/s]Epoch 8/10:  29%|██▉       | 22/75 [00:02<00:05, 10.07it/s]Epoch 8/10:  32%|███▏      | 24/75 [00:02<00:05, 10.09it/s]Epoch 8/10:  35%|███▍      | 26/75 [00:02<00:04, 10.10it/s]Epoch 8/10:  37%|███▋      | 28/75 [00:02<00:04, 10.06it/s]Epoch 8/10:  40%|████      | 30/75 [00:03<00:04, 10.09it/s]Epoch 8/10:  43%|████▎     | 32/75 [00:03<00:04, 10.10it/s]Epoch 8/10:  45%|████▌     | 34/75 [00:03<00:04, 10.12it/s]Epoch 8/10:  48%|████▊     | 36/75 [00:03<00:03, 10.12it/s]Epoch 8/10:  51%|█████     | 38/75 [00:03<00:03, 10.14it/s]Epoch 8/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.14it/s]Epoch 8/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.14it/s]Epoch 8/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.13it/s]Epoch 8/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.14it/s]Epoch 8/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.14it/s]Epoch 8/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.14it/s]Epoch 8/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.14it/s]Epoch 8/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.15it/s]Epoch 8/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.15it/s]Epoch 8/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.14it/s]Epoch 8/10:  80%|████████  | 60/75 [00:06<00:01, 10.08it/s]Epoch 8/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.10it/s]Epoch 8/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.12it/s]Epoch 8/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.13it/s]Epoch 8/10:  91%|█████████ | 68/75 [00:06<00:00, 10.13it/s]Epoch 8/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.14it/s]Epoch 8/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.15it/s]Epoch 8/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.15it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00,  9.94it/s]
[2025-04-11 10:12:20,312][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0289
[2025-04-11 10:12:20,565][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0445, Metrics: {'mse': 0.0398934930562973, 'rmse': 0.1997335551586095, 'r2': 0.09125995635986328}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:14,  5.17it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:08,  8.10it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  9.04it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.71it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.85it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.01it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.05it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.01it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.01it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.04it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.08it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.10it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.11it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.12it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.13it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.14it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.13it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.13it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.14it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.10it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.11it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.12it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.12it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.14it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.14it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:12:28,450][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0244
[2025-04-11 10:12:28,734][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0509, Metrics: {'mse': 0.049271196126937866, 'rmse': 0.22197116057483204, 'r2': -0.12235617637634277}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:15,  4.84it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.89it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.92it/s]Epoch 10/10:   8%|▊         | 6/75 [00:00<00:07,  9.15it/s]Epoch 10/10:  11%|█         | 8/75 [00:00<00:07,  9.55it/s]Epoch 10/10:  13%|█▎        | 10/75 [00:01<00:06,  9.77it/s]Epoch 10/10:  16%|█▌        | 12/75 [00:01<00:06,  9.89it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06,  9.84it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:06,  9.94it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.02it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.06it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.08it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.10it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.07it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.10it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.07it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.08it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.10it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.12it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.12it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.12it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.12it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.13it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.13it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.13it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.14it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.14it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.13it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.14it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.14it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.13it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.13it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00,  9.94it/s]
[2025-04-11 10:12:36,279][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0247
[2025-04-11 10:12:36,538][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0472, Metrics: {'mse': 0.04424240067601204, 'rmse': 0.21033877596870254, 'r2': -0.007804512977600098}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▄▂▁
wandb:     best_val_mse ▆█▄▂▁
wandb:      best_val_r2 ▃▁▅▇█
wandb:    best_val_rmse ▇█▄▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇█▄▄▂▃▁▂▂
wandb:          val_mse ▆█▇▄▄▂▃▁▃▂
wandb:           val_r2 ▃▁▂▅▅▇▆█▆▇
wandb:         val_rmse ▇█▇▄▄▂▃▁▃▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04449
wandb:     best_val_mse 0.03989
wandb:      best_val_r2 0.09126
wandb:    best_val_rmse 0.19973
wandb:            epoch 10
wandb:   final_test_mse 0.04202
wandb:    final_test_r2 -0.04533
wandb:  final_test_rmse 0.20499
wandb:  final_train_mse 0.02026
wandb:   final_train_r2 0.14135
wandb: final_train_rmse 0.14235
wandb:    final_val_mse 0.03989
wandb:     final_val_r2 0.09126
wandb:   final_val_rmse 0.19973
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02475
wandb:       train_time 80.54748
wandb:         val_loss 0.04722
wandb:          val_mse 0.04424
wandb:           val_r2 -0.0078
wandb:         val_rmse 0.21034
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_101106-i4uitwgv
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_101106-i4uitwgv/logs
Standard experiment for lexical_density (ru) completed successfully
Running submetric n_tokens for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:12:53,810][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/n_tokens
experiment_name: n_tokens_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: n_tokens
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:12:53,810][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:12:53,810][__main__][INFO] - Using submetric: n_tokens
[2025-04-11 10:12:53,810][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:12:53,810][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:12:53,814][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:12:53,815][__main__][INFO] - Using submetric: n_tokens
[2025-04-11 10:12:53,815][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:12:55,048][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'n_tokens'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:12:57,290][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:12:57,291][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:12:57,387][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:12:57,416][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:12:57,507][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:12:57,516][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:12:57,517][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:12:57,518][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:12:57,536][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:12:57,567][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:12:57,580][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:12:57,581][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:12:57,582][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:12:57,582][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:12:57,603][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:12:57,632][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:12:57,645][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:12:57,647][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:12:57,647][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:12:57,648][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:12:57,649][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:12:57,649][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-04-11 10:12:57,649][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-04-11 10:12:57,649][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-04-11 10:12:57,650][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:12:57,650][src.data.datasets][INFO] -   Mean: 0.1566, Std: 0.1207
[2025-04-11 10:12:57,650][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:12:57,650][src.data.datasets][INFO] - Sample label: 0.0729999989271164
[2025-04-11 10:12:57,650][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:12:57,650][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-04-11 10:12:57,650][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-04-11 10:12:57,650][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-04-11 10:12:57,651][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.5560
[2025-04-11 10:12:57,651][src.data.datasets][INFO] -   Mean: 0.1814, Std: 0.1403
[2025-04-11 10:12:57,651][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:12:57,651][src.data.datasets][INFO] - Sample label: 0.09300000220537186
[2025-04-11 10:12:57,651][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:12:57,651][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'n_tokens'
[2025-04-11 10:12:57,651][src.data.datasets][INFO] - Selected feature name: 'n_tokens' for task: 'single_submetric'
[2025-04-11 10:12:57,651][src.data.datasets][INFO] - Label statistics for single_submetric (feature: n_tokens):
[2025-04-11 10:12:57,651][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6050
[2025-04-11 10:12:57,651][src.data.datasets][INFO] -   Mean: 0.1327, Std: 0.1067
[2025-04-11 10:12:57,652][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:12:57,652][src.data.datasets][INFO] - Sample label: 0.04699999839067459
[2025-04-11 10:12:57,652][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:12:57,652][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:12:57,652][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:12:57,652][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:13:01,797][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:13:01,799][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:13:01,800][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:13:01,800][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:16,  1.03s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:24,  2.94it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:15,  4.64it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:11,  6.02it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  7.12it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.95it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.57it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  9.05it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.37it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.60it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:05,  9.77it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.89it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:05,  9.93it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04,  9.95it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.02it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:03<00:04, 10.07it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.06it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.04it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.03it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.08it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.11it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.13it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.15it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.14it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.15it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.17it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.16it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.17it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.18it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.18it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.17it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.16it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.12it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.14it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.15it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.16it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.17it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.56it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.01it/s]
[2025-04-11 10:13:11,958][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0428
[2025-04-11 10:13:12,182][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0280, Metrics: {'mse': 0.028269847854971886, 'rmse': 0.16813639658019286, 'r2': -0.43632423877716064}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.51it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.35it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.20it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.93it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06,  9.98it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.04it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.02it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.01it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.07it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.10it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.13it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.15it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.16it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.16it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.09it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.12it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.14it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.14it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.15it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.15it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.16it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.17it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.17it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.16it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.17it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.18it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.17it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.17it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.17it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.02it/s]
[2025-04-11 10:13:20,089][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0204
[2025-04-11 10:13:20,327][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0199, Metrics: {'mse': 0.019763639196753502, 'rmse': 0.14058321093485346, 'r2': -0.004143834114074707}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  4.96it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.01it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.99it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.71it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.87it/s]Epoch 3/10:  16%|█▌        | 12/75 [00:01<00:06,  9.88it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06,  9.85it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:06,  9.97it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05,  9.95it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.02it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.01it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.06it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.05it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.09it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.10it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.13it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.13it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.14it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.08it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.09it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.12it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.14it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.15it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.10it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.13it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.14it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.07it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.04it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.08it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.10it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.12it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.08it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.09it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.13it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.13it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.54it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00,  9.94it/s]
[2025-04-11 10:13:28,282][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0128
[2025-04-11 10:13:28,542][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0102, Metrics: {'mse': 0.00911778025329113, 'rmse': 0.0954870685134439, 'r2': 0.5367470979690552}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.32it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.22it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.12it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.77it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.82it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.92it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.00it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.06it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.06it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.08it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.10it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.12it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.15it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.16it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.16it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.15it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.15it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.15it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.15it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.16it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.11it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.11it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.13it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.16it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.99it/s]
[2025-04-11 10:13:36,419][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0103
[2025-04-11 10:13:36,671][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0099, Metrics: {'mse': 0.009133248589932919, 'rmse': 0.09556803121302081, 'r2': 0.5359611511230469}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.10it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.08it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.03it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.72it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.82it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:06, 10.00it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.05it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.09it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.11it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.10it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.13it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.13it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.12it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.13it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.09it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.10it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.11it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.11it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.13it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.10it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.10it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.12it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.13it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.13it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.08it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.09it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.11it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.12it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.12it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.12it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.14it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.54it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.96it/s]
[2025-04-11 10:13:44,563][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0089
[2025-04-11 10:13:44,827][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0104, Metrics: {'mse': 0.010349827818572521, 'rmse': 0.1017341035178102, 'r2': 0.4741497039794922}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.08it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.03it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.99it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.67it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.80it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.90it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:06,  9.97it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.02it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.05it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.03it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.02it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.05it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.08it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.10it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.12it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.12it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.11it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.11it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.13it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.13it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.13it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.09it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.06it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.09it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.10it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.11it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.12it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.12it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.12it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.13it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.12it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.13it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.11it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.13it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.14it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.53it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.94it/s]
[2025-04-11 10:13:52,374][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0066
[2025-04-11 10:13:52,637][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0140, Metrics: {'mse': 0.014211144298315048, 'rmse': 0.11921050414420303, 'r2': 0.2779653072357178}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:15,  4.89it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.93it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.93it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.39it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.66it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.82it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.92it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:06,  9.99it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.04it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.07it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.09it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.07it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.09it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.11it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.12it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.12it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.12it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.13it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.13it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.13it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.13it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.14it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.13it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.14it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.13it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.13it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.12it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.13it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.13it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.13it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.13it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.13it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.13it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.14it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:14:00,164][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0067
[2025-04-11 10:14:00,419][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0090, Metrics: {'mse': 0.008979830890893936, 'rmse': 0.09476196964444089, 'r2': 0.5437560081481934}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  5.11it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:08,  8.07it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  9.01it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.68it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.83it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06,  9.88it/s]Epoch 8/10:  19%|█▊        | 14/75 [00:01<00:06,  9.86it/s]Epoch 8/10:  21%|██▏       | 16/75 [00:01<00:05,  9.96it/s]Epoch 8/10:  24%|██▍       | 18/75 [00:01<00:05,  9.97it/s]Epoch 8/10:  27%|██▋       | 20/75 [00:02<00:05,  9.97it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05,  9.94it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.01it/s]Epoch 8/10:  32%|███▏      | 24/75 [00:02<00:05,  9.97it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:05,  9.93it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.01it/s]Epoch 8/10:  37%|███▋      | 28/75 [00:02<00:04,  9.96it/s]Epoch 8/10:  40%|████      | 30/75 [00:03<00:04, 10.03it/s]Epoch 8/10:  43%|████▎     | 32/75 [00:03<00:04, 10.05it/s]Epoch 8/10:  45%|████▌     | 34/75 [00:03<00:04, 10.08it/s]Epoch 8/10:  48%|████▊     | 36/75 [00:03<00:03, 10.04it/s]Epoch 8/10:  51%|█████     | 38/75 [00:03<00:03, 10.02it/s]Epoch 8/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.02it/s]Epoch 8/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.00it/s]Epoch 8/10:  59%|█████▊    | 44/75 [00:04<00:03,  9.99it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:03,  9.95it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.02it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.07it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.09it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.08it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:02, 10.00it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.03it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.08it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.10it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.09it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.10it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.11it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.13it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.13it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.13it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.54it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00,  9.90it/s]
[2025-04-11 10:14:08,398][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0056
[2025-04-11 10:14:08,664][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0154, Metrics: {'mse': 0.01618136838078499, 'rmse': 0.1272060076442343, 'r2': 0.17786288261413574}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:14,  5.26it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:08,  8.16it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 9/10:   8%|▊         | 6/75 [00:00<00:07,  9.22it/s]Epoch 9/10:  11%|█         | 8/75 [00:00<00:06,  9.58it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.61it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.82it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.01it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.05it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.07it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.08it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.11it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.12it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.07it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.08it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.09it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.05it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.08it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.06it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03,  9.98it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03,  9.98it/s]Epoch 9/10:  56%|█████▌    | 42/75 [00:04<00:03,  9.96it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03,  9.94it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.01it/s]Epoch 9/10:  61%|██████▏   | 46/75 [00:04<00:02,  9.96it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02,  9.94it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.01it/s]Epoch 9/10:  67%|██████▋   | 50/75 [00:05<00:02,  9.97it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02,  9.95it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.01it/s]Epoch 9/10:  72%|███████▏  | 54/75 [00:05<00:02,  9.96it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:02,  9.91it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01,  9.98it/s]Epoch 9/10:  77%|███████▋  | 58/75 [00:05<00:01,  9.94it/s]Epoch 9/10:  80%|████████  | 60/75 [00:06<00:01, 10.02it/s]Epoch 9/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.01it/s]Epoch 9/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.02it/s]Epoch 9/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.01it/s]Epoch 9/10:  91%|█████████ | 68/75 [00:06<00:00, 10.05it/s]Epoch 9/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.07it/s]Epoch 9/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.09it/s]Epoch 9/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.11it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00,  9.90it/s]
[2025-04-11 10:14:16,244][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0045
[2025-04-11 10:14:16,514][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0066, Metrics: {'mse': 0.006495583336800337, 'rmse': 0.08059518184606532, 'r2': 0.6699747443199158}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:14,  5.08it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:08,  8.04it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.99it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.70it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.85it/s]Epoch 10/10:  16%|█▌        | 12/75 [00:01<00:06,  9.84it/s]Epoch 10/10:  19%|█▊        | 14/75 [00:01<00:06,  9.90it/s]Epoch 10/10:  21%|██▏       | 16/75 [00:01<00:05,  9.97it/s]Epoch 10/10:  24%|██▍       | 18/75 [00:01<00:05, 10.03it/s]Epoch 10/10:  27%|██▋       | 20/75 [00:02<00:05, 10.01it/s]Epoch 10/10:  29%|██▉       | 22/75 [00:02<00:05, 10.05it/s]Epoch 10/10:  32%|███▏      | 24/75 [00:02<00:05, 10.07it/s]Epoch 10/10:  35%|███▍      | 26/75 [00:02<00:04, 10.08it/s]Epoch 10/10:  37%|███▋      | 28/75 [00:02<00:04, 10.11it/s]Epoch 10/10:  40%|████      | 30/75 [00:03<00:04, 10.12it/s]Epoch 10/10:  43%|████▎     | 32/75 [00:03<00:04, 10.11it/s]Epoch 10/10:  45%|████▌     | 34/75 [00:03<00:04, 10.09it/s]Epoch 10/10:  48%|████▊     | 36/75 [00:03<00:03, 10.10it/s]Epoch 10/10:  51%|█████     | 38/75 [00:03<00:03, 10.12it/s]Epoch 10/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.12it/s]Epoch 10/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.12it/s]Epoch 10/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.08it/s]Epoch 10/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.04it/s]Epoch 10/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.07it/s]Epoch 10/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.05it/s]Epoch 10/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.08it/s]Epoch 10/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.04it/s]Epoch 10/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.06it/s]Epoch 10/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.05it/s]Epoch 10/10:  80%|████████  | 60/75 [00:06<00:01, 10.07it/s]Epoch 10/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.05it/s]Epoch 10/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.08it/s]Epoch 10/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.06it/s]Epoch 10/10:  91%|█████████ | 68/75 [00:06<00:00, 10.07it/s]Epoch 10/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.08it/s]Epoch 10/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.11it/s]Epoch 10/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.12it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00,  9.93it/s]
[2025-04-11 10:14:24,444][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0049
[2025-04-11 10:14:24,710][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0084, Metrics: {'mse': 0.008705931715667248, 'rmse': 0.0933055824464284, 'r2': 0.5576721429824829}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▂▂▂▁
wandb:     best_val_mse █▅▂▂▂▁
wandb:      best_val_r2 ▁▄▇▇▇█
wandb:    best_val_rmse █▆▂▂▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▂▂▂▃▂▄▁▂
wandb:          val_mse █▅▂▂▂▃▂▄▁▂
wandb:           val_r2 ▁▄▇▇▇▆▇▅█▇
wandb:         val_rmse █▆▂▂▃▄▂▅▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.00657
wandb:     best_val_mse 0.0065
wandb:      best_val_r2 0.66997
wandb:    best_val_rmse 0.0806
wandb:            epoch 10
wandb:   final_test_mse 0.01231
wandb:    final_test_r2 -0.08202
wandb:  final_test_rmse 0.11094
wandb:  final_train_mse 0.00404
wandb:   final_train_r2 0.72252
wandb: final_train_rmse 0.06358
wandb:    final_val_mse 0.0065
wandb:     final_val_r2 0.66997
wandb:   final_val_rmse 0.0806
wandb:    learning_rate 1e-05
wandb:       train_loss 0.00494
wandb:       train_time 81.08233
wandb:         val_loss 0.00838
wandb:          val_mse 0.00871
wandb:           val_r2 0.55767
wandb:         val_rmse 0.09331
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_101253-n71yfibu
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_101253-n71yfibu/logs
Standard experiment for n_tokens (ru) completed successfully
Running submetric avg_links_len control=1 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:14:41,948][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_links_len/control1
experiment_name: avg_links_len_control1_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:14:41,948][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:14:41,948][__main__][INFO] - Using submetric: avg_links_len
[2025-04-11 10:14:41,948][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:14:41,948][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:14:41,953][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:14:41,953][__main__][INFO] - Using submetric: avg_links_len
[2025-04-11 10:14:41,953][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:14:43,145][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:14:45,334][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:14:45,334][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed1' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:14:45,384][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:14:19 2025).
[2025-04-11 10:14:45,412][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:14:19 2025).
[2025-04-11 10:14:45,587][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:14:45,596][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:14:45,596][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:14:45,597][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:14:45,617][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:14:45,647][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:14:45,660][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:14:45,661][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:14:45,662][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:14:45,663][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:14:45,682][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:14:45,710][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:14:45,723][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:14:45,724][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:14:45,725][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:14:45,725][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:14:45,726][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:14:45,727][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-04-11 10:14:45,727][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-04-11 10:14:45,727][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-04-11 10:14:45,727][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9000
[2025-04-11 10:14:45,727][src.data.datasets][INFO] -   Mean: 0.2497, Std: 0.1826
[2025-04-11 10:14:45,727][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:14:45,727][src.data.datasets][INFO] - Sample label: 0.5139999985694885
[2025-04-11 10:14:45,727][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:14:45,728][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-04-11 10:14:45,728][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-04-11 10:14:45,728][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-04-11 10:14:45,728][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8000
[2025-04-11 10:14:45,728][src.data.datasets][INFO] -   Mean: 0.2557, Std: 0.1728
[2025-04-11 10:14:45,728][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:14:45,728][src.data.datasets][INFO] - Sample label: 0.23399999737739563
[2025-04-11 10:14:45,728][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:14:45,728][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-04-11 10:14:45,728][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-04-11 10:14:45,729][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-04-11 10:14:45,729][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6270
[2025-04-11 10:14:45,729][src.data.datasets][INFO] -   Mean: 0.2617, Std: 0.1298
[2025-04-11 10:14:45,729][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:14:45,729][src.data.datasets][INFO] - Sample label: 0.14000000059604645
[2025-04-11 10:14:45,729][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:14:45,729][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:14:45,729][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:14:45,730][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:14:49,857][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:14:49,859][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:14:49,859][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:14:49,859][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:14,  1.01s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:24,  2.98it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:14,  4.69it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:11,  6.08it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  7.15it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:08,  7.99it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.61it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  9.06it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.20it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.53it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.73it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.86it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05,  9.97it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.04it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.07it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.07it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.10it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:04, 10.09it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.08it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.11it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.14it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.16it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:03, 10.16it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.12it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.11it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.14it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.15it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.17it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.17it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.16it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.17it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.18it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.18it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.18it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.19it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.19it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.19it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.20it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.03it/s]
[2025-04-11 10:14:59,728][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0968
[2025-04-11 10:15:00,047][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0434, Metrics: {'mse': 0.04663650318980217, 'rmse': 0.2159548637789901, 'r2': -0.5626283884048462}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.62it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.42it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.24it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.62it/s]Epoch 2/10:  11%|█         | 8/75 [00:00<00:06,  9.66it/s]Epoch 2/10:  13%|█▎        | 10/75 [00:01<00:06,  9.85it/s]Epoch 2/10:  16%|█▌        | 12/75 [00:01<00:06,  9.95it/s]Epoch 2/10:  19%|█▊        | 14/75 [00:01<00:06, 10.03it/s]Epoch 2/10:  21%|██▏       | 16/75 [00:01<00:05, 10.08it/s]Epoch 2/10:  24%|██▍       | 18/75 [00:01<00:05, 10.12it/s]Epoch 2/10:  27%|██▋       | 20/75 [00:02<00:05, 10.13it/s]Epoch 2/10:  29%|██▉       | 22/75 [00:02<00:05, 10.14it/s]Epoch 2/10:  32%|███▏      | 24/75 [00:02<00:05, 10.16it/s]Epoch 2/10:  35%|███▍      | 26/75 [00:02<00:04, 10.18it/s]Epoch 2/10:  37%|███▋      | 28/75 [00:02<00:04, 10.14it/s]Epoch 2/10:  40%|████      | 30/75 [00:03<00:04, 10.13it/s]Epoch 2/10:  43%|████▎     | 32/75 [00:03<00:04, 10.15it/s]Epoch 2/10:  45%|████▌     | 34/75 [00:03<00:04, 10.15it/s]Epoch 2/10:  48%|████▊     | 36/75 [00:03<00:03, 10.11it/s]Epoch 2/10:  51%|█████     | 38/75 [00:03<00:03, 10.13it/s]Epoch 2/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.15it/s]Epoch 2/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.16it/s]Epoch 2/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.17it/s]Epoch 2/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.11it/s]Epoch 2/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.13it/s]Epoch 2/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.15it/s]Epoch 2/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.16it/s]Epoch 2/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.16it/s]Epoch 2/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.17it/s]Epoch 2/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.18it/s]Epoch 2/10:  80%|████████  | 60/75 [00:05<00:01, 10.17it/s]Epoch 2/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.15it/s]Epoch 2/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.10it/s]Epoch 2/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.14it/s]Epoch 2/10:  91%|█████████ | 68/75 [00:06<00:00, 10.13it/s]Epoch 2/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.15it/s]Epoch 2/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.16it/s]Epoch 2/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.17it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.04it/s]
[2025-04-11 10:15:07,943][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0508
[2025-04-11 10:15:08,177][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0342, Metrics: {'mse': 0.032749325037002563, 'rmse': 0.18096774584716074, 'r2': -0.09731698036193848}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  4.99it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.00it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.00it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.70it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.88it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06,  9.92it/s]Epoch 3/10:  19%|█▊        | 14/75 [00:01<00:06,  9.90it/s]Epoch 3/10:  21%|██▏       | 16/75 [00:01<00:05,  9.99it/s]Epoch 3/10:  24%|██▍       | 18/75 [00:01<00:05, 10.05it/s]Epoch 3/10:  27%|██▋       | 20/75 [00:02<00:05, 10.09it/s]Epoch 3/10:  29%|██▉       | 22/75 [00:02<00:05, 10.13it/s]Epoch 3/10:  32%|███▏      | 24/75 [00:02<00:05, 10.15it/s]Epoch 3/10:  35%|███▍      | 26/75 [00:02<00:04, 10.15it/s]Epoch 3/10:  37%|███▋      | 28/75 [00:02<00:04, 10.11it/s]Epoch 3/10:  40%|████      | 30/75 [00:03<00:04, 10.10it/s]Epoch 3/10:  43%|████▎     | 32/75 [00:03<00:04, 10.09it/s]Epoch 3/10:  45%|████▌     | 34/75 [00:03<00:04, 10.11it/s]Epoch 3/10:  48%|████▊     | 36/75 [00:03<00:03, 10.03it/s]Epoch 3/10:  51%|█████     | 38/75 [00:03<00:03, 10.05it/s]Epoch 3/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.08it/s]Epoch 3/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.12it/s]Epoch 3/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.14it/s]Epoch 3/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.14it/s]Epoch 3/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.15it/s]Epoch 3/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.17it/s]Epoch 3/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.16it/s]Epoch 3/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.15it/s]Epoch 3/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.16it/s]Epoch 3/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.17it/s]Epoch 3/10:  80%|████████  | 60/75 [00:06<00:01, 10.17it/s]Epoch 3/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.17it/s]Epoch 3/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.18it/s]Epoch 3/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.18it/s]Epoch 3/10:  91%|█████████ | 68/75 [00:06<00:00, 10.18it/s]Epoch 3/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.17it/s]Epoch 3/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.18it/s]Epoch 3/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.19it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00,  9.99it/s]
[2025-04-11 10:15:16,101][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0401
[2025-04-11 10:15:16,366][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0328, Metrics: {'mse': 0.03193251043558121, 'rmse': 0.17869669956544024, 'r2': -0.06994831562042236}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.17it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.13it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.74it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.90it/s]Epoch 4/10:  16%|█▌        | 12/75 [00:01<00:06,  9.89it/s]Epoch 4/10:  19%|█▊        | 14/75 [00:01<00:06, 10.00it/s]Epoch 4/10:  21%|██▏       | 16/75 [00:01<00:05, 10.05it/s]Epoch 4/10:  24%|██▍       | 18/75 [00:01<00:05, 10.10it/s]Epoch 4/10:  27%|██▋       | 20/75 [00:02<00:05, 10.13it/s]Epoch 4/10:  29%|██▉       | 22/75 [00:02<00:05, 10.15it/s]Epoch 4/10:  32%|███▏      | 24/75 [00:02<00:05, 10.09it/s]Epoch 4/10:  35%|███▍      | 26/75 [00:02<00:04, 10.05it/s]Epoch 4/10:  37%|███▋      | 28/75 [00:02<00:04, 10.05it/s]Epoch 4/10:  40%|████      | 30/75 [00:03<00:04, 10.04it/s]Epoch 4/10:  43%|████▎     | 32/75 [00:03<00:04, 10.04it/s]Epoch 4/10:  45%|████▌     | 34/75 [00:03<00:04, 10.08it/s]Epoch 4/10:  48%|████▊     | 36/75 [00:03<00:03, 10.11it/s]Epoch 4/10:  51%|█████     | 38/75 [00:03<00:03, 10.14it/s]Epoch 4/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.11it/s]Epoch 4/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.13it/s]Epoch 4/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.10it/s]Epoch 4/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.11it/s]Epoch 4/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.13it/s]Epoch 4/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.15it/s]Epoch 4/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.15it/s]Epoch 4/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.15it/s]Epoch 4/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.16it/s]Epoch 4/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.16it/s]Epoch 4/10:  80%|████████  | 60/75 [00:06<00:01, 10.16it/s]Epoch 4/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.15it/s]Epoch 4/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.12it/s]Epoch 4/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.14it/s]Epoch 4/10:  91%|█████████ | 68/75 [00:06<00:00, 10.12it/s]Epoch 4/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.13it/s]Epoch 4/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.14it/s]Epoch 4/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.16it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:15:24,229][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0377
[2025-04-11 10:15:24,481][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0305, Metrics: {'mse': 0.031033717095851898, 'rmse': 0.17616389271315475, 'r2': -0.03983283042907715}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.24it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.19it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.03it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.66it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.82it/s]Epoch 5/10:  16%|█▌        | 12/75 [00:01<00:06,  9.82it/s]Epoch 5/10:  19%|█▊        | 14/75 [00:01<00:06,  9.95it/s]Epoch 5/10:  21%|██▏       | 16/75 [00:01<00:05, 10.04it/s]Epoch 5/10:  24%|██▍       | 18/75 [00:01<00:05, 10.08it/s]Epoch 5/10:  27%|██▋       | 20/75 [00:02<00:05, 10.10it/s]Epoch 5/10:  29%|██▉       | 22/75 [00:02<00:05, 10.13it/s]Epoch 5/10:  32%|███▏      | 24/75 [00:02<00:05, 10.14it/s]Epoch 5/10:  35%|███▍      | 26/75 [00:02<00:04, 10.14it/s]Epoch 5/10:  37%|███▋      | 28/75 [00:02<00:04, 10.08it/s]Epoch 5/10:  40%|████      | 30/75 [00:03<00:04, 10.10it/s]Epoch 5/10:  43%|████▎     | 32/75 [00:03<00:04, 10.13it/s]Epoch 5/10:  45%|████▌     | 34/75 [00:03<00:04, 10.13it/s]Epoch 5/10:  48%|████▊     | 36/75 [00:03<00:03, 10.13it/s]Epoch 5/10:  51%|█████     | 38/75 [00:03<00:03, 10.04it/s]Epoch 5/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.03it/s]Epoch 5/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.03it/s]Epoch 5/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.08it/s]Epoch 5/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.07it/s]Epoch 5/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.09it/s]Epoch 5/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.09it/s]Epoch 5/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.10it/s]Epoch 5/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.13it/s]Epoch 5/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.14it/s]Epoch 5/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.14it/s]Epoch 5/10:  80%|████████  | 60/75 [00:06<00:01, 10.15it/s]Epoch 5/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.16it/s]Epoch 5/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.12it/s]Epoch 5/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.12it/s]Epoch 5/10:  91%|█████████ | 68/75 [00:06<00:00, 10.09it/s]Epoch 5/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.12it/s]Epoch 5/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.14it/s]Epoch 5/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.14it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:15:32,356][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0369
[2025-04-11 10:15:32,622][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0405, Metrics: {'mse': 0.0373203344643116, 'rmse': 0.1931847159179825, 'r2': -0.25047576427459717}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.25it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.17it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.09it/s]Epoch 6/10:   8%|▊         | 6/75 [00:00<00:07,  9.30it/s]Epoch 6/10:  11%|█         | 8/75 [00:00<00:06,  9.64it/s]Epoch 6/10:  13%|█▎        | 10/75 [00:01<00:06,  9.77it/s]Epoch 6/10:  16%|█▌        | 12/75 [00:01<00:06,  9.92it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.90it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.01it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.00it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.05it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.07it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.11it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.12it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.12it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.13it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.14it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.14it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.15it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.16it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.15it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.09it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.12it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.13it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.13it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.13it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.15it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.14it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.16it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.16it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:15:40,149][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0375
[2025-04-11 10:15:40,412][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0297, Metrics: {'mse': 0.029384884983301163, 'rmse': 0.1714202000445139, 'r2': 0.015413880348205566}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:15,  4.79it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.85it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.39it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.67it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.83it/s]Epoch 7/10:  16%|█▌        | 12/75 [00:01<00:06,  9.80it/s]Epoch 7/10:  19%|█▊        | 14/75 [00:01<00:06,  9.90it/s]Epoch 7/10:  21%|██▏       | 16/75 [00:01<00:05, 10.00it/s]Epoch 7/10:  24%|██▍       | 18/75 [00:01<00:05, 10.06it/s]Epoch 7/10:  27%|██▋       | 20/75 [00:02<00:05, 10.09it/s]Epoch 7/10:  29%|██▉       | 22/75 [00:02<00:05, 10.06it/s]Epoch 7/10:  32%|███▏      | 24/75 [00:02<00:05, 10.09it/s]Epoch 7/10:  35%|███▍      | 26/75 [00:02<00:04, 10.06it/s]Epoch 7/10:  37%|███▋      | 28/75 [00:02<00:04, 10.09it/s]Epoch 7/10:  40%|████      | 30/75 [00:03<00:04, 10.08it/s]Epoch 7/10:  43%|████▎     | 32/75 [00:03<00:04, 10.09it/s]Epoch 7/10:  45%|████▌     | 34/75 [00:03<00:04, 10.11it/s]Epoch 7/10:  48%|████▊     | 36/75 [00:03<00:03, 10.12it/s]Epoch 7/10:  51%|█████     | 38/75 [00:03<00:03, 10.13it/s]Epoch 7/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.12it/s]Epoch 7/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.12it/s]Epoch 7/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.13it/s]Epoch 7/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.10it/s]Epoch 7/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.11it/s]Epoch 7/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.11it/s]Epoch 7/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.12it/s]Epoch 7/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.14it/s]Epoch 7/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.13it/s]Epoch 7/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.13it/s]Epoch 7/10:  80%|████████  | 60/75 [00:06<00:01, 10.14it/s]Epoch 7/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.15it/s]Epoch 7/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.10it/s]Epoch 7/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.07it/s]Epoch 7/10:  91%|█████████ | 68/75 [00:06<00:00, 10.09it/s]Epoch 7/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.11it/s]Epoch 7/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.14it/s]Epoch 7/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.14it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.94it/s]
[2025-04-11 10:15:48,343][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0346
[2025-04-11 10:15:48,601][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0341, Metrics: {'mse': 0.032350506633520126, 'rmse': 0.17986246588301888, 'r2': -0.08395397663116455}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  5.28it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:08,  8.11it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  9.03it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.70it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.86it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06,  9.96it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.02it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.05it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.09it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.11it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.11it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.11it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.14it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.08it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.10it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.07it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.09it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.06it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.08it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.10it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.10it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.11it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.10it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.11it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.13it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.13it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.13it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.08it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.10it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.07it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.10it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.11it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.12it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.14it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.54it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:15:56,127][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0353
[2025-04-11 10:15:56,392][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0324, Metrics: {'mse': 0.03135310113430023, 'rmse': 0.17706806921153298, 'r2': -0.05053436756134033}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:13,  5.37it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:08,  8.24it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  9.04it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.61it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.73it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06,  9.85it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:06,  9.96it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.01it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.05it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.05it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.09it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.11it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.11it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.12it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.14it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.08it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.09it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.11it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.12it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.12it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.13it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.13it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.13it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.12it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.13it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.13it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.16it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.16it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:16:03,918][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0351
[2025-04-11 10:16:04,179][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0310, Metrics: {'mse': 0.030822286382317543, 'rmse': 0.17556277049055002, 'r2': -0.03274857997894287}
[2025-04-11 10:16:04,180][src.training.lm_trainer][INFO] - Early stopping at epoch 9
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▃▁▁
wandb:     best_val_mse █▂▂▂▁
wandb:      best_val_r2 ▁▇▇▇█
wandb:    best_val_rmse █▃▂▂▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▃▁▇▁▃▂▂
wandb:          val_mse █▂▂▂▄▁▂▂▂
wandb:           val_r2 ▁▇▇▇▅█▇▇▇
wandb:         val_rmse █▃▂▂▄▁▂▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02972
wandb:     best_val_mse 0.02938
wandb:      best_val_r2 0.01541
wandb:    best_val_rmse 0.17142
wandb:            epoch 9
wandb:   final_test_mse 0.0169
wandb:    final_test_r2 -0.00303
wandb:  final_test_rmse 0.12999
wandb:  final_train_mse 0.03235
wandb:   final_train_r2 0.02963
wandb: final_train_rmse 0.17986
wandb:    final_val_mse 0.02938
wandb:     final_val_r2 0.01541
wandb:   final_val_rmse 0.17142
wandb:    learning_rate 1e-05
wandb:       train_loss 0.0351
wandb:       train_time 72.75775
wandb:         val_loss 0.03097
wandb:          val_mse 0.03082
wandb:           val_r2 -0.03275
wandb:         val_rmse 0.17556
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_101441-4wrerx4r
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_101441-4wrerx4r/logs
Control experiment for avg_links_len (ru, control=1) completed successfully
Running submetric avg_links_len control=2 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:16:21,853][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_links_len/control2
experiment_name: avg_links_len_control2_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:16:21,853][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:16:21,854][__main__][INFO] - Using submetric: avg_links_len
[2025-04-11 10:16:21,854][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:16:21,854][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:16:21,858][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:16:21,858][__main__][INFO] - Using submetric: avg_links_len
[2025-04-11 10:16:21,858][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:16:23,226][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:16:25,443][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:16:25,444][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed2' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:16:25,496][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:16:02 2025).
[2025-04-11 10:16:25,526][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:16:02 2025).
[2025-04-11 10:16:25,727][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:16:25,736][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:16:25,736][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:16:25,737][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:16:25,756][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:16:25,786][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:16:25,799][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:16:25,800][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:16:25,800][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:16:25,801][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:16:25,819][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:16:25,851][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:16:25,865][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:16:25,866][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:16:25,866][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:16:25,867][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:16:25,868][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:16:25,869][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-04-11 10:16:25,869][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-04-11 10:16:25,869][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-04-11 10:16:25,869][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9000
[2025-04-11 10:16:25,869][src.data.datasets][INFO] -   Mean: 0.2497, Std: 0.1826
[2025-04-11 10:16:25,869][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:16:25,869][src.data.datasets][INFO] - Sample label: 0.0560000017285347
[2025-04-11 10:16:25,870][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:16:25,870][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-04-11 10:16:25,870][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-04-11 10:16:25,870][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-04-11 10:16:25,870][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8000
[2025-04-11 10:16:25,870][src.data.datasets][INFO] -   Mean: 0.2557, Std: 0.1728
[2025-04-11 10:16:25,870][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:16:25,870][src.data.datasets][INFO] - Sample label: 0.23399999737739563
[2025-04-11 10:16:25,870][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:16:25,870][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-04-11 10:16:25,871][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-04-11 10:16:25,871][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-04-11 10:16:25,871][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6270
[2025-04-11 10:16:25,871][src.data.datasets][INFO] -   Mean: 0.2617, Std: 0.1298
[2025-04-11 10:16:25,871][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:16:25,871][src.data.datasets][INFO] - Sample label: 0.14000000059604645
[2025-04-11 10:16:25,871][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:16:25,871][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:16:25,871][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:16:25,872][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:16:30,366][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:16:30,368][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:16:30,369][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:16:30,369][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:09,  1.07it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:22,  3.17it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:14,  4.91it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:10,  6.29it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.86it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:01<00:08,  7.89it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.61it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  9.08it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.41it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.65it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.82it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.92it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.00it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.07it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.11it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.13it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:03<00:04, 10.16it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:04, 10.16it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.16it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.17it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.18it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.18it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:03, 10.18it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.18it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.18it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.18it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.18it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.19it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.15it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.15it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.11it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.09it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.07it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.06it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.07it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.11it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.14it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.16it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.11it/s]
[2025-04-11 10:16:40,362][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0911
[2025-04-11 10:16:40,593][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0453, Metrics: {'mse': 0.04182460531592369, 'rmse': 0.20451064841695576, 'r2': -0.40139830112457275}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.31it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.22it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.11it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.53it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.76it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.91it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06,  9.98it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.04it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.08it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.11it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.14it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.14it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.16it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.17it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.17it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.16it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.18it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.18it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.12it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.14it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.16it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.17it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.16it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.04it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.03it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.08it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.07it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.11it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.12it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.14it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.16it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.17it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.16it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.17it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.18it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.17it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.01it/s]
[2025-04-11 10:16:48,511][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0457
[2025-04-11 10:16:48,763][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0329, Metrics: {'mse': 0.03167818486690521, 'rmse': 0.1779836646069105, 'r2': -0.061426758766174316}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.04it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.03it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.00it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.71it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.88it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06,  9.97it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:06,  9.99it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.05it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.09it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.12it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.15it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.16it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.15it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.15it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.16it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.17it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.17it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.15it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.16it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.16it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.16it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.16it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.17it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.15it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.12it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.14it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.16it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.16it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:16:56,692][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0398
[2025-04-11 10:16:56,951][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0310, Metrics: {'mse': 0.032109811902046204, 'rmse': 0.17919210892794973, 'r2': -0.075888991355896}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.27it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.07it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.04it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 4/10:  11%|█         | 8/75 [00:00<00:06,  9.59it/s]Epoch 4/10:  13%|█▎        | 10/75 [00:01<00:06,  9.81it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.82it/s]Epoch 4/10:  16%|█▌        | 12/75 [00:01<00:06,  9.82it/s]Epoch 4/10:  19%|█▊        | 14/75 [00:01<00:06,  9.95it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:06,  9.93it/s]Epoch 4/10:  21%|██▏       | 16/75 [00:01<00:05,  9.91it/s]Epoch 4/10:  24%|██▍       | 18/75 [00:01<00:05, 10.00it/s]Epoch 4/10:  27%|██▋       | 20/75 [00:02<00:05, 10.07it/s]Epoch 4/10:  29%|██▉       | 22/75 [00:02<00:05, 10.11it/s]Epoch 4/10:  32%|███▏      | 24/75 [00:02<00:05, 10.12it/s]Epoch 4/10:  35%|███▍      | 26/75 [00:02<00:04, 10.11it/s]Epoch 4/10:  37%|███▋      | 28/75 [00:02<00:04, 10.14it/s]Epoch 4/10:  40%|████      | 30/75 [00:03<00:04, 10.16it/s]Epoch 4/10:  43%|████▎     | 32/75 [00:03<00:04, 10.11it/s]Epoch 4/10:  45%|████▌     | 34/75 [00:03<00:04, 10.12it/s]Epoch 4/10:  48%|████▊     | 36/75 [00:03<00:03, 10.11it/s]Epoch 4/10:  51%|█████     | 38/75 [00:03<00:03, 10.13it/s]Epoch 4/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.14it/s]Epoch 4/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.15it/s]Epoch 4/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.16it/s]Epoch 4/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.17it/s]Epoch 4/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.16it/s]Epoch 4/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.15it/s]Epoch 4/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.16it/s]Epoch 4/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.16it/s]Epoch 4/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.14it/s]Epoch 4/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.08it/s]Epoch 4/10:  80%|████████  | 60/75 [00:06<00:01, 10.09it/s]Epoch 4/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.13it/s]Epoch 4/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.14it/s]Epoch 4/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.14it/s]Epoch 4/10:  91%|█████████ | 68/75 [00:06<00:00, 10.15it/s]Epoch 4/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.17it/s]Epoch 4/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.17it/s]Epoch 4/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.16it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:17:04,833][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0388
[2025-04-11 10:17:05,090][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0301, Metrics: {'mse': 0.029447920620441437, 'rmse': 0.1716039644659803, 'r2': 0.0133017897605896}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  4.97it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.01it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.99it/s]Epoch 5/10:   8%|▊         | 6/75 [00:00<00:07,  9.18it/s]Epoch 5/10:  11%|█         | 8/75 [00:00<00:06,  9.58it/s]Epoch 5/10:  13%|█▎        | 10/75 [00:01<00:06,  9.80it/s]Epoch 5/10:  16%|█▌        | 12/75 [00:01<00:06,  9.92it/s]Epoch 5/10:  19%|█▊        | 14/75 [00:01<00:06, 10.01it/s]Epoch 5/10:  21%|██▏       | 16/75 [00:01<00:05, 10.05it/s]Epoch 5/10:  24%|██▍       | 18/75 [00:01<00:05, 10.09it/s]Epoch 5/10:  27%|██▋       | 20/75 [00:02<00:05, 10.12it/s]Epoch 5/10:  29%|██▉       | 22/75 [00:02<00:05, 10.13it/s]Epoch 5/10:  32%|███▏      | 24/75 [00:02<00:05, 10.13it/s]Epoch 5/10:  35%|███▍      | 26/75 [00:02<00:04, 10.15it/s]Epoch 5/10:  37%|███▋      | 28/75 [00:02<00:04, 10.16it/s]Epoch 5/10:  40%|████      | 30/75 [00:03<00:04, 10.14it/s]Epoch 5/10:  43%|████▎     | 32/75 [00:03<00:04, 10.15it/s]Epoch 5/10:  45%|████▌     | 34/75 [00:03<00:04, 10.14it/s]Epoch 5/10:  48%|████▊     | 36/75 [00:03<00:03, 10.15it/s]Epoch 5/10:  51%|█████     | 38/75 [00:03<00:03, 10.14it/s]Epoch 5/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.14it/s]Epoch 5/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.15it/s]Epoch 5/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.15it/s]Epoch 5/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.15it/s]Epoch 5/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.15it/s]Epoch 5/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.13it/s]Epoch 5/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.14it/s]Epoch 5/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.12it/s]Epoch 5/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.13it/s]Epoch 5/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.14it/s]Epoch 5/10:  80%|████████  | 60/75 [00:06<00:01, 10.14it/s]Epoch 5/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.14it/s]Epoch 5/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.14it/s]Epoch 5/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  91%|█████████ | 68/75 [00:06<00:00, 10.14it/s]Epoch 5/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.14it/s]Epoch 5/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.15it/s]Epoch 5/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.15it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:17:12,974][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0362
[2025-04-11 10:17:13,244][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0291, Metrics: {'mse': 0.028874166309833527, 'rmse': 0.16992400157080084, 'r2': 0.03252631425857544}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.90it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.95it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.68it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.83it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.00it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.04it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.10it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.11it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.12it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.13it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.14it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.15it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.15it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.15it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.16it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.16it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.17it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.17it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.15it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.16it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.15it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.16it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.17it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.17it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.17it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.17it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.99it/s]
[2025-04-11 10:17:21,205][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0362
[2025-04-11 10:17:21,477][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0299, Metrics: {'mse': 0.02903883345425129, 'rmse': 0.17040784446219395, 'r2': 0.02700883150100708}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  4.96it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.98it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.68it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.85it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.96it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.01it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.05it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.08it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.11it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.11it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.10it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.10it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.13it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.13it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.13it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.14it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.13it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.13it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.14it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.15it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.15it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.13it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.14it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.14it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.12it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.13it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:17:29,001][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0361
[2025-04-11 10:17:29,273][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0305, Metrics: {'mse': 0.02911229245364666, 'rmse': 0.17062324710790924, 'r2': 0.024547457695007324}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  4.94it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.97it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.67it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.83it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.01it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.05it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.08it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.10it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.10it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.11it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.12it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.12it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.12it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.12it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.13it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.14it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.14it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.15it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.16it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.14it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.16it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.15it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.14it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.13it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.14it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.54it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:17:36,794][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0348
[2025-04-11 10:17:37,078][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0295, Metrics: {'mse': 0.028816385194659233, 'rmse': 0.16975389596312432, 'r2': 0.0344623327255249}
[2025-04-11 10:17:37,079][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▂▁▁
wandb:     best_val_mse █▃▃▁▁
wandb:      best_val_r2 ▁▆▆██
wandb:    best_val_rmse █▃▃▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▂▁▁▁▂▁
wandb:          val_mse █▃▃▁▁▁▁▁
wandb:           val_r2 ▁▆▆█████
wandb:         val_rmse █▃▃▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02907
wandb:     best_val_mse 0.02887
wandb:      best_val_r2 0.03253
wandb:    best_val_rmse 0.16992
wandb:            epoch 8
wandb:   final_test_mse 0.01795
wandb:    final_test_r2 -0.06558
wandb:  final_test_rmse 0.13398
wandb:  final_train_mse 0.0325
wandb:   final_train_r2 0.02511
wandb: final_train_rmse 0.18028
wandb:    final_val_mse 0.02887
wandb:     final_val_r2 0.03253
wandb:   final_val_rmse 0.16992
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03479
wandb:       train_time 64.94845
wandb:         val_loss 0.02954
wandb:          val_mse 0.02882
wandb:           val_r2 0.03446
wandb:         val_rmse 0.16975
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_101621-mo40ynzm
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_101621-mo40ynzm/logs
Control experiment for avg_links_len (ru, control=2) completed successfully
Running submetric avg_links_len control=3 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:17:54,796][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_links_len/control3
experiment_name: avg_links_len_control3_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:17:54,796][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:17:54,796][__main__][INFO] - Using submetric: avg_links_len
[2025-04-11 10:17:54,796][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:17:54,796][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:17:54,801][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:17:54,801][__main__][INFO] - Using submetric: avg_links_len
[2025-04-11 10:17:54,801][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:17:56,016][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:17:58,246][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:17:58,247][src.data.datasets][INFO] - Loading 'control_avg_links_len_seed3' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:17:58,300][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_links_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 06:54:26 2025).
[2025-04-11 10:17:58,327][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_links_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_links_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 06:54:26 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 64866.60 examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 61723.77 examples/s]
[2025-04-11 10:17:58,601][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:17:58,609][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:17:58,610][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:17:58,611][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:17:58,628][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:17:58,661][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:17:58,675][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:17:58,676][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:17:58,676][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:17:58,677][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:17:58,697][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:17:58,725][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:17:58,738][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:17:58,739][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:17:58,739][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:17:58,740][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:17:58,741][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:17:58,741][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-04-11 10:17:58,741][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-04-11 10:17:58,741][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-04-11 10:17:58,741][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9000
[2025-04-11 10:17:58,741][src.data.datasets][INFO] -   Mean: 0.2497, Std: 0.1826
[2025-04-11 10:17:58,741][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:17:58,741][src.data.datasets][INFO] - Sample label: 0.4000000059604645
[2025-04-11 10:17:58,742][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:17:58,742][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-04-11 10:17:58,742][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-04-11 10:17:58,742][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-04-11 10:17:58,742][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8000
[2025-04-11 10:17:58,742][src.data.datasets][INFO] -   Mean: 0.2557, Std: 0.1728
[2025-04-11 10:17:58,742][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:17:58,742][src.data.datasets][INFO] - Sample label: 0.23399999737739563
[2025-04-11 10:17:58,742][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:17:58,742][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-04-11 10:17:58,742][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-04-11 10:17:58,743][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-04-11 10:17:58,743][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6270
[2025-04-11 10:17:58,743][src.data.datasets][INFO] -   Mean: 0.2617, Std: 0.1298
[2025-04-11 10:17:58,743][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:17:58,743][src.data.datasets][INFO] - Sample label: 0.14000000059604645
[2025-04-11 10:17:58,743][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:17:58,743][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:17:58,743][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:17:58,744][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:18:02,584][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:18:02,586][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:18:02,586][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:18:02,586][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:06,  1.11it/s]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:31,  2.31it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:15,  4.49it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:11,  6.10it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:10,  6.75it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:08,  7.82it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:07,  8.57it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:01<00:07,  8.82it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  9.29it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.58it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.79it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.91it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:02<00:05,  9.99it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.05it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.10it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.12it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.14it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:03<00:04, 10.16it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:04, 10.12it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.14it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.10it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.13it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.15it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:03, 10.16it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.17it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.17it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.17it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.16it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.12it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.14it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.16it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.16it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.17it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.18it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.18it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.18it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.19it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.19it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.19it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.15it/s]
[2025-04-11 10:18:12,392][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0922
[2025-04-11 10:18:12,618][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0721, Metrics: {'mse': 0.07843875139951706, 'rmse': 0.2800691903789438, 'r2': -1.6282119750976562}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.56it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.37it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.21it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 2/10:  11%|█         | 8/75 [00:00<00:06,  9.62it/s]Epoch 2/10:  13%|█▎        | 10/75 [00:01<00:06,  9.84it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.84it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06,  9.98it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.05it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.10it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.13it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.14it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.14it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.15it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.17it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.16it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.16it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.11it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.14it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.14it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.08it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.07it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.10it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.07it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.01it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02,  9.99it/s]Epoch 2/10:  69%|██████▉   | 52/75 [00:05<00:02,  9.97it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02,  9.95it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:02,  9.97it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.04it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.05it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.09it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.11it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.13it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:18:20,539][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0604
[2025-04-11 10:18:20,782][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0506, Metrics: {'mse': 0.05459514260292053, 'rmse': 0.23365603480954764, 'r2': -0.8292949199676514}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.14it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.99it/s]Epoch 3/10:   5%|▌         | 4/75 [00:00<00:08,  8.53it/s]Epoch 3/10:   8%|▊         | 6/75 [00:00<00:07,  9.28it/s]Epoch 3/10:  11%|█         | 8/75 [00:00<00:06,  9.63it/s]Epoch 3/10:  13%|█▎        | 10/75 [00:01<00:06,  9.82it/s]Epoch 3/10:  16%|█▌        | 12/75 [00:01<00:06,  9.95it/s]Epoch 3/10:  19%|█▊        | 14/75 [00:01<00:06, 10.03it/s]Epoch 3/10:  21%|██▏       | 16/75 [00:01<00:05, 10.06it/s]Epoch 3/10:  24%|██▍       | 18/75 [00:01<00:05, 10.10it/s]Epoch 3/10:  27%|██▋       | 20/75 [00:02<00:05, 10.13it/s]Epoch 3/10:  29%|██▉       | 22/75 [00:02<00:05, 10.14it/s]Epoch 3/10:  32%|███▏      | 24/75 [00:02<00:05, 10.15it/s]Epoch 3/10:  35%|███▍      | 26/75 [00:02<00:04, 10.13it/s]Epoch 3/10:  37%|███▋      | 28/75 [00:02<00:04, 10.15it/s]Epoch 3/10:  40%|████      | 30/75 [00:03<00:04, 10.12it/s]Epoch 3/10:  43%|████▎     | 32/75 [00:03<00:04, 10.13it/s]Epoch 3/10:  45%|████▌     | 34/75 [00:03<00:04, 10.10it/s]Epoch 3/10:  48%|████▊     | 36/75 [00:03<00:03, 10.12it/s]Epoch 3/10:  51%|█████     | 38/75 [00:03<00:03, 10.14it/s]Epoch 3/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.12it/s]Epoch 3/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.14it/s]Epoch 3/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.14it/s]Epoch 3/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.15it/s]Epoch 3/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.13it/s]Epoch 3/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.09it/s]Epoch 3/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.11it/s]Epoch 3/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.12it/s]Epoch 3/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.09it/s]Epoch 3/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.11it/s]Epoch 3/10:  80%|████████  | 60/75 [00:06<00:01, 10.13it/s]Epoch 3/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.10it/s]Epoch 3/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.12it/s]Epoch 3/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.13it/s]Epoch 3/10:  91%|█████████ | 68/75 [00:06<00:00, 10.14it/s]Epoch 3/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.16it/s]Epoch 3/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.17it/s]Epoch 3/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.17it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:18:28,742][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0499
[2025-04-11 10:18:28,998][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0398, Metrics: {'mse': 0.039030201733112335, 'rmse': 0.1975606279933133, 'r2': -0.307767391204834}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.27it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.20it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.11it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.69it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.86it/s]Epoch 4/10:  16%|█▌        | 12/75 [00:01<00:06,  9.83it/s]Epoch 4/10:  19%|█▊        | 14/75 [00:01<00:06,  9.96it/s]Epoch 4/10:  21%|██▏       | 16/75 [00:01<00:05, 10.03it/s]Epoch 4/10:  24%|██▍       | 18/75 [00:01<00:05, 10.07it/s]Epoch 4/10:  27%|██▋       | 20/75 [00:02<00:05, 10.08it/s]Epoch 4/10:  29%|██▉       | 22/75 [00:02<00:05, 10.12it/s]Epoch 4/10:  32%|███▏      | 24/75 [00:02<00:05, 10.14it/s]Epoch 4/10:  35%|███▍      | 26/75 [00:02<00:04, 10.13it/s]Epoch 4/10:  37%|███▋      | 28/75 [00:02<00:04, 10.14it/s]Epoch 4/10:  40%|████      | 30/75 [00:03<00:04, 10.15it/s]Epoch 4/10:  43%|████▎     | 32/75 [00:03<00:04, 10.16it/s]Epoch 4/10:  45%|████▌     | 34/75 [00:03<00:04, 10.16it/s]Epoch 4/10:  48%|████▊     | 36/75 [00:03<00:03, 10.15it/s]Epoch 4/10:  51%|█████     | 38/75 [00:03<00:03, 10.16it/s]Epoch 4/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.15it/s]Epoch 4/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.15it/s]Epoch 4/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.09it/s]Epoch 4/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.10it/s]Epoch 4/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.07it/s]Epoch 4/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.05it/s]Epoch 4/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.09it/s]Epoch 4/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.06it/s]Epoch 4/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.08it/s]Epoch 4/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.10it/s]Epoch 4/10:  80%|████████  | 60/75 [00:06<00:01, 10.12it/s]Epoch 4/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.09it/s]Epoch 4/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.12it/s]Epoch 4/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.13it/s]Epoch 4/10:  91%|█████████ | 68/75 [00:06<00:00, 10.11it/s]Epoch 4/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.13it/s]Epoch 4/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.13it/s]Epoch 4/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.14it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.96it/s]
[2025-04-11 10:18:36,890][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0417
[2025-04-11 10:18:37,157][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0293, Metrics: {'mse': 0.029579047113656998, 'rmse': 0.17198560147191683, 'r2': 0.00890815258026123}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  4.96it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.98it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 5/10:   8%|▊         | 6/75 [00:00<00:07,  9.16it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.34it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.68it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.81it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.02it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.06it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.04it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.06it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.10it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.11it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.13it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.12it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.13it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.09it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.11it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.11it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.07it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.10it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.13it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.13it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.14it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.16it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.15it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.14it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:18:45,078][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0380
[2025-04-11 10:18:45,337][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0325, Metrics: {'mse': 0.030425556004047394, 'rmse': 0.17442922921359078, 'r2': -0.019455432891845703}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.22it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.14it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.74it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.89it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.97it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.04it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.08it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.10it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.11it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.12it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.14it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.14it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.15it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.11it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.12it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.12it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.12it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.13it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.14it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.13it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.13it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.12it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.12it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.13it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.13it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.14it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.99it/s]
[2025-04-11 10:18:52,844][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0375
[2025-04-11 10:18:53,196][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0300, Metrics: {'mse': 0.030106196179986, 'rmse': 0.17351137190393603, 'r2': -0.008754849433898926}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.26it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:08,  8.15it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.07it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.74it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.87it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.96it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.02it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.06it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.08it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.06it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.08it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.09it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.08it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.08it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.09it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.12it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.08it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.10it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.11it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.11it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.13it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.13it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.13it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.11it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.12it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.11it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.13it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.14it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.14it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.11it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.08it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.08it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.11it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.12it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.53it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:19:00,722][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0365
[2025-04-11 10:19:01,003][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0304, Metrics: {'mse': 0.02953089028596878, 'rmse': 0.17184554194383042, 'r2': 0.010521769523620605}
[2025-04-11 10:19:01,004][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▃▁
wandb:     best_val_mse █▅▂▁
wandb:      best_val_r2 ▁▄▇█
wandb:    best_val_rmse █▅▃▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▃▁▂▁▁
wandb:          val_mse █▅▂▁▁▁▁
wandb:           val_r2 ▁▄▇████
wandb:         val_rmse █▅▃▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02934
wandb:     best_val_mse 0.02958
wandb:      best_val_r2 0.00891
wandb:    best_val_rmse 0.17199
wandb:            epoch 7
wandb:   final_test_mse 0.02148
wandb:    final_test_r2 -0.27501
wandb:  final_test_rmse 0.14655
wandb:  final_train_mse 0.03288
wandb:   final_train_r2 0.01381
wandb: final_train_rmse 0.18132
wandb:    final_val_mse 0.02958
wandb:     final_val_r2 0.00891
wandb:   final_val_rmse 0.17199
wandb:    learning_rate 1e-05
wandb:       train_loss 0.03649
wandb:       train_time 56.81448
wandb:         val_loss 0.03036
wandb:          val_mse 0.02953
wandb:           val_r2 0.01052
wandb:         val_rmse 0.17185
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_101754-57lntk2u
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_101754-57lntk2u/logs
Control experiment for avg_links_len (ru, control=3) completed successfully
Running submetric avg_max_depth control=1 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:19:19,169][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_max_depth/control1
experiment_name: avg_max_depth_control1_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:19:19,169][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:19:19,169][__main__][INFO] - Using submetric: avg_max_depth
[2025-04-11 10:19:19,169][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:19:19,169][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:19:19,173][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:19:19,173][__main__][INFO] - Using submetric: avg_max_depth
[2025-04-11 10:19:19,174][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:19:20,415][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:19:22,758][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:19:22,758][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed1' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:19:22,829][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 06:55:34 2025).
[2025-04-11 10:19:22,859][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 06:55:34 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 93032.99 examples/s]
[2025-04-11 10:19:23,136][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:19:23,150][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:19:23,151][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:19:23,152][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:19:23,172][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:19:23,205][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:19:23,219][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:19:23,221][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:19:23,222][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:19:23,222][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:19:23,243][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:19:23,277][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:19:23,293][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:19:23,295][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:19:23,296][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:19:23,296][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:19:23,298][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:19:23,298][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-04-11 10:19:23,298][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-04-11 10:19:23,298][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-04-11 10:19:23,298][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:19:23,299][src.data.datasets][INFO] -   Mean: 0.2416, Std: 0.1460
[2025-04-11 10:19:23,299][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:19:23,299][src.data.datasets][INFO] - Sample label: 0.6000000238418579
[2025-04-11 10:19:23,299][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:19:23,299][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-04-11 10:19:23,299][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-04-11 10:19:23,299][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-04-11 10:19:23,299][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-04-11 10:19:23,299][src.data.datasets][INFO] -   Mean: 0.2865, Std: 0.1898
[2025-04-11 10:19:23,300][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:19:23,300][src.data.datasets][INFO] - Sample label: 0.25
[2025-04-11 10:19:23,300][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:19:23,300][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-04-11 10:19:23,300][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-04-11 10:19:23,300][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-04-11 10:19:23,300][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-04-11 10:19:23,300][src.data.datasets][INFO] -   Mean: 0.2284, Std: 0.1562
[2025-04-11 10:19:23,300][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:19:23,300][src.data.datasets][INFO] - Sample label: 0.25
[2025-04-11 10:19:23,301][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:19:23,301][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:19:23,301][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:19:23,301][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:19:27,783][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:19:27,785][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:19:27,785][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:19:27,785][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:05,  1.14it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:21,  3.32it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:13,  5.08it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:10,  6.45it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:08,  7.48it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:01<00:07,  8.24it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.79it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  9.19it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.48it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.69it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:02<00:05,  9.84it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.95it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.02it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.06it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.10it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.14it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.15it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.18it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.17it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.16it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.17it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.19it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.18it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.18it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.18it/s]Epoch 1/10:  71%|███████   | 53/75 [00:05<00:02, 10.18it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.18it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.16it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.17it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.16it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.16it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.16it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.17it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.18it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.18it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.19it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.57it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.18it/s]
[2025-04-11 10:19:37,691][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0792
[2025-04-11 10:19:37,918][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1079, Metrics: {'mse': 0.11136027425527573, 'rmse': 0.3337068687565117, 'r2': -2.0913469791412354}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.47it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.30it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.16it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.77it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.92it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06,  9.99it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.05it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.07it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.08it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.10it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.04it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.07it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.10it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.12it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.15it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.15it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.14it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.15it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.16it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.16it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.17it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.16it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.16it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.16it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.15it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.17it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.16it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.16it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.16it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.17it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.17it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.17it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.17it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:19:45,855][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0370
[2025-04-11 10:19:46,093][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0421, Metrics: {'mse': 0.03979600593447685, 'rmse': 0.19948936296072745, 'r2': -0.10473203659057617}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.33it/s]Epoch 3/10:   3%|▎         | 2/75 [00:00<00:11,  6.43it/s]Epoch 3/10:   5%|▌         | 4/75 [00:00<00:08,  8.36it/s]Epoch 3/10:   8%|▊         | 6/75 [00:00<00:07,  9.13it/s]Epoch 3/10:  11%|█         | 8/75 [00:00<00:07,  9.53it/s]Epoch 3/10:  13%|█▎        | 10/75 [00:01<00:06,  9.75it/s]Epoch 3/10:  16%|█▌        | 12/75 [00:01<00:06,  9.87it/s]Epoch 3/10:  19%|█▊        | 14/75 [00:01<00:06,  9.97it/s]Epoch 3/10:  21%|██▏       | 16/75 [00:01<00:05, 10.04it/s]Epoch 3/10:  24%|██▍       | 18/75 [00:01<00:05, 10.07it/s]Epoch 3/10:  27%|██▋       | 20/75 [00:02<00:05, 10.07it/s]Epoch 3/10:  29%|██▉       | 22/75 [00:02<00:05, 10.09it/s]Epoch 3/10:  32%|███▏      | 24/75 [00:02<00:05, 10.09it/s]Epoch 3/10:  35%|███▍      | 26/75 [00:02<00:04, 10.12it/s]Epoch 3/10:  37%|███▋      | 28/75 [00:02<00:04, 10.13it/s]Epoch 3/10:  40%|████      | 30/75 [00:03<00:04, 10.14it/s]Epoch 3/10:  43%|████▎     | 32/75 [00:03<00:04, 10.09it/s]Epoch 3/10:  45%|████▌     | 34/75 [00:03<00:04, 10.04it/s]Epoch 3/10:  48%|████▊     | 36/75 [00:03<00:03, 10.03it/s]Epoch 3/10:  51%|█████     | 38/75 [00:03<00:03, 10.07it/s]Epoch 3/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.05it/s]Epoch 3/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.04it/s]Epoch 3/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.06it/s]Epoch 3/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.06it/s]Epoch 3/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.08it/s]Epoch 3/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.11it/s]Epoch 3/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.12it/s]Epoch 3/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.13it/s]Epoch 3/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.15it/s]Epoch 3/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.15it/s]Epoch 3/10:  80%|████████  | 60/75 [00:06<00:01, 10.15it/s]Epoch 3/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.15it/s]Epoch 3/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.11it/s]Epoch 3/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.13it/s]Epoch 3/10:  91%|█████████ | 68/75 [00:06<00:00, 10.14it/s]Epoch 3/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.13it/s]Epoch 3/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.15it/s]Epoch 3/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.15it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00,  9.90it/s]
[2025-04-11 10:19:54,207][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0256
[2025-04-11 10:19:54,461][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0391, Metrics: {'mse': 0.03577613830566406, 'rmse': 0.1891458122868811, 'r2': 0.006858944892883301}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.00it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.00it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.70it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.85it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.93it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:06,  9.98it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.04it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.09it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.10it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.08it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.12it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.13it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.13it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.16it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.16it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.13it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.08it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.09it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.13it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.14it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.04it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.08it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.11it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.12it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.13it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.09it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.12it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:20:02,356][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0245
[2025-04-11 10:20:02,634][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0406, Metrics: {'mse': 0.03808325156569481, 'rmse': 0.1951493058293952, 'r2': -0.057186245918273926}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.11it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.05it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.02it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.72it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.86it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.96it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.03it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.07it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.06it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.09it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.12it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.13it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.12it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.14it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.16it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.16it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.15it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.15it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.16it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.16it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.10it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.12it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.07it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.11it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.11it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.12it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.11it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.12it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.07it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.10it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.11it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.06it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.10it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.13it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.14it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.54it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:20:10,153][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0239
[2025-04-11 10:20:10,441][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0407, Metrics: {'mse': 0.036973290145397186, 'rmse': 0.19228439912119025, 'r2': -0.026373863220214844}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  4.98it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.98it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.71it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.87it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.97it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.03it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.05it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.09it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.12it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.13it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.13it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.15it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.08it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.11it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.08it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.06it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.03it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.07it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.08it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.10it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.11it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.12it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.14it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.13it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.13it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.14it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.12it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.12it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.12it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.13it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.93it/s]
[2025-04-11 10:20:17,993][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0230
[2025-04-11 10:20:18,266][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0384, Metrics: {'mse': 0.03536364808678627, 'rmse': 0.18805224828963432, 'r2': 0.01830965280532837}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  4.97it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  8.00it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.95it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.68it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.86it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.95it/s]Epoch 7/10:  19%|█▊        | 14/75 [00:01<00:06,  9.94it/s]Epoch 7/10:  21%|██▏       | 16/75 [00:01<00:05, 10.01it/s]Epoch 7/10:  24%|██▍       | 18/75 [00:01<00:05, 10.02it/s]Epoch 7/10:  27%|██▋       | 20/75 [00:02<00:05, 10.08it/s]Epoch 7/10:  29%|██▉       | 22/75 [00:02<00:05, 10.10it/s]Epoch 7/10:  32%|███▏      | 24/75 [00:02<00:05, 10.12it/s]Epoch 7/10:  35%|███▍      | 26/75 [00:02<00:04, 10.12it/s]Epoch 7/10:  37%|███▋      | 28/75 [00:02<00:04, 10.08it/s]Epoch 7/10:  40%|████      | 30/75 [00:03<00:04, 10.04it/s]Epoch 7/10:  43%|████▎     | 32/75 [00:03<00:04, 10.09it/s]Epoch 7/10:  45%|████▌     | 34/75 [00:03<00:04, 10.11it/s]Epoch 7/10:  48%|████▊     | 36/75 [00:03<00:03, 10.12it/s]Epoch 7/10:  51%|█████     | 38/75 [00:03<00:03, 10.11it/s]Epoch 7/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.13it/s]Epoch 7/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.15it/s]Epoch 7/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.15it/s]Epoch 7/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.08it/s]Epoch 7/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.09it/s]Epoch 7/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.07it/s]Epoch 7/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.09it/s]Epoch 7/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.11it/s]Epoch 7/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.11it/s]Epoch 7/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.13it/s]Epoch 7/10:  80%|████████  | 60/75 [00:06<00:01, 10.12it/s]Epoch 7/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.13it/s]Epoch 7/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.08it/s]Epoch 7/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.09it/s]Epoch 7/10:  91%|█████████ | 68/75 [00:06<00:00, 10.12it/s]Epoch 7/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.13it/s]Epoch 7/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.13it/s]Epoch 7/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.14it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:20:26,191][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0226
[2025-04-11 10:20:26,465][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0408, Metrics: {'mse': 0.036976490169763565, 'rmse': 0.1922927200123904, 'r2': -0.026462674140930176}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  5.11it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:08,  8.07it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  9.00it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.68it/s]Epoch 8/10:  13%|█▎        | 10/75 [00:01<00:06,  9.73it/s]Epoch 8/10:  16%|█▌        | 12/75 [00:01<00:06,  9.83it/s]Epoch 8/10:  19%|█▊        | 14/75 [00:01<00:06,  9.91it/s]Epoch 8/10:  21%|██▏       | 16/75 [00:01<00:05,  9.98it/s]Epoch 8/10:  24%|██▍       | 18/75 [00:01<00:05, 10.03it/s]Epoch 8/10:  27%|██▋       | 20/75 [00:02<00:05, 10.06it/s]Epoch 8/10:  29%|██▉       | 22/75 [00:02<00:05, 10.08it/s]Epoch 8/10:  32%|███▏      | 24/75 [00:02<00:05, 10.11it/s]Epoch 8/10:  35%|███▍      | 26/75 [00:02<00:04, 10.11it/s]Epoch 8/10:  37%|███▋      | 28/75 [00:02<00:04, 10.11it/s]Epoch 8/10:  40%|████      | 30/75 [00:03<00:04, 10.11it/s]Epoch 8/10:  43%|████▎     | 32/75 [00:03<00:04, 10.12it/s]Epoch 8/10:  45%|████▌     | 34/75 [00:03<00:04, 10.08it/s]Epoch 8/10:  48%|████▊     | 36/75 [00:03<00:03, 10.08it/s]Epoch 8/10:  51%|█████     | 38/75 [00:03<00:03, 10.09it/s]Epoch 8/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.12it/s]Epoch 8/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.12it/s]Epoch 8/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.12it/s]Epoch 8/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.12it/s]Epoch 8/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.13it/s]Epoch 8/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.14it/s]Epoch 8/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.14it/s]Epoch 8/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.15it/s]Epoch 8/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.14it/s]Epoch 8/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.14it/s]Epoch 8/10:  80%|████████  | 60/75 [00:06<00:01, 10.14it/s]Epoch 8/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.14it/s]Epoch 8/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.13it/s]Epoch 8/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.13it/s]Epoch 8/10:  91%|█████████ | 68/75 [00:06<00:00, 10.14it/s]Epoch 8/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.15it/s]Epoch 8/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.13it/s]Epoch 8/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.15it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:20:34,005][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0228
[2025-04-11 10:20:34,281][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0410, Metrics: {'mse': 0.0389016829431057, 'rmse': 0.19723509561714847, 'r2': -0.07990574836730957}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:15,  4.89it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.93it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.94it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.67it/s]Epoch 9/10:  13%|█▎        | 10/75 [00:01<00:06,  9.70it/s]Epoch 9/10:  16%|█▌        | 12/75 [00:01<00:06,  9.87it/s]Epoch 9/10:  19%|█▊        | 14/75 [00:01<00:06,  9.96it/s]Epoch 9/10:  21%|██▏       | 16/75 [00:01<00:05,  9.97it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05,  9.94it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.01it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.07it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.06it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.08it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.11it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.12it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.11it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.11it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.11it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.13it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.13it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.13it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.13it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.09it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.09it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.12it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.12it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.12it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.13it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.14it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.14it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00,  9.93it/s]
[2025-04-11 10:20:41,836][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0225
[2025-04-11 10:20:42,107][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0395, Metrics: {'mse': 0.03633522614836693, 'rmse': 0.19061801108071327, 'r2': -0.008661270141601562}
[2025-04-11 10:20:42,108][src.training.lm_trainer][INFO] - Early stopping at epoch 9
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁▁
wandb:     best_val_mse █▁▁▁
wandb:      best_val_r2 ▁███
wandb:    best_val_rmse █▂▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▁▁▁▁▁▁▁
wandb:          val_mse █▁▁▁▁▁▁▁▁
wandb:           val_r2 ▁████████
wandb:         val_rmse █▂▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03839
wandb:     best_val_mse 0.03536
wandb:      best_val_r2 0.01831
wandb:    best_val_rmse 0.18805
wandb:            epoch 9
wandb:   final_test_mse 0.02552
wandb:    final_test_r2 -0.04618
wandb:  final_test_rmse 0.15974
wandb:  final_train_mse 0.02149
wandb:   final_train_r2 -0.00761
wandb: final_train_rmse 0.14658
wandb:    final_val_mse 0.03536
wandb:     final_val_r2 0.01831
wandb:   final_val_rmse 0.18805
wandb:    learning_rate 1e-05
wandb:       train_loss 0.0225
wandb:       train_time 72.59188
wandb:         val_loss 0.03953
wandb:          val_mse 0.03634
wandb:           val_r2 -0.00866
wandb:         val_rmse 0.19062
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_101919-tjtkkrm5
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_101919-tjtkkrm5/logs
Control experiment for avg_max_depth (ru, control=1) completed successfully
Running submetric avg_max_depth control=2 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:21:00,713][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_max_depth/control2
experiment_name: avg_max_depth_control2_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:21:00,714][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:21:00,714][__main__][INFO] - Using submetric: avg_max_depth
[2025-04-11 10:21:00,714][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:21:00,714][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:21:00,718][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:21:00,718][__main__][INFO] - Using submetric: avg_max_depth
[2025-04-11 10:21:00,718][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:21:02,323][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:21:04,615][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:21:04,616][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed2' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:21:04,673][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 06:56:58 2025).
[2025-04-11 10:21:04,701][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 06:56:58 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 78085.36 examples/s]
[2025-04-11 10:21:04,966][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:21:04,975][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:21:04,975][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:21:04,976][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:21:04,996][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:21:05,024][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:21:05,037][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:21:05,039][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:21:05,039][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:21:05,040][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:21:05,059][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:21:05,088][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:21:05,100][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:21:05,101][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:21:05,102][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:21:05,102][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:21:05,103][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:21:05,103][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-04-11 10:21:05,103][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-04-11 10:21:05,104][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-04-11 10:21:05,104][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:21:05,104][src.data.datasets][INFO] -   Mean: 0.2416, Std: 0.1460
[2025-04-11 10:21:05,104][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:21:05,104][src.data.datasets][INFO] - Sample label: 0.125
[2025-04-11 10:21:05,104][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:21:05,104][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-04-11 10:21:05,104][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-04-11 10:21:05,105][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-04-11 10:21:05,105][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-04-11 10:21:05,105][src.data.datasets][INFO] -   Mean: 0.2865, Std: 0.1898
[2025-04-11 10:21:05,105][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:21:05,105][src.data.datasets][INFO] - Sample label: 0.25
[2025-04-11 10:21:05,105][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:21:05,105][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-04-11 10:21:05,105][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-04-11 10:21:05,105][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-04-11 10:21:05,105][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-04-11 10:21:05,106][src.data.datasets][INFO] -   Mean: 0.2284, Std: 0.1562
[2025-04-11 10:21:05,106][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:21:05,106][src.data.datasets][INFO] - Sample label: 0.25
[2025-04-11 10:21:05,106][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:21:05,106][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:21:05,106][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:21:05,107][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:21:09,434][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:21:09,436][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:21:09,436][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:21:09,436][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:16,  1.03s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:24,  2.93it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:18,  3.88it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.61it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:10,  6.32it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:08,  7.55it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:07,  8.39it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:06,  8.94it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  9.32it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.60it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.78it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.89it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.98it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.05it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.08it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.10it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.15it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.11it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.11it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.07it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.12it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.14it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.15it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.11it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.13it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.10it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.12it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.09it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.11it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.08it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.12it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.14it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.16it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.17it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.18it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.18it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.16it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.55it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.99it/s]
[2025-04-11 10:21:19,701][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0729
[2025-04-11 10:21:19,938][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0520, Metrics: {'mse': 0.05080040171742439, 'rmse': 0.2253894445563598, 'r2': -0.41021275520324707}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.35it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.25it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.14it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.55it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.92it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06,  9.99it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.04it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.08it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.12it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.13it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.15it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.16it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.17it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.17it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.18it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.18it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.18it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.12it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.16it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.16it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.16it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.17it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.11it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.13it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.14it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.17it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.16it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.11it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.14it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.16it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.16it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.17it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.58it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.01it/s]
[2025-04-11 10:21:27,847][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0336
[2025-04-11 10:21:28,098][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0364, Metrics: {'mse': 0.03364994749426842, 'rmse': 0.1834392201637055, 'r2': 0.06588172912597656}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.89it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.94it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.94it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.65it/s]Epoch 3/10:  13%|█▎        | 10/75 [00:01<00:06,  9.69it/s]Epoch 3/10:  16%|█▌        | 12/75 [00:01<00:06,  9.87it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06,  9.89it/s]Epoch 3/10:  19%|█▊        | 14/75 [00:01<00:06,  9.86it/s]Epoch 3/10:  21%|██▏       | 16/75 [00:01<00:05,  9.97it/s]Epoch 3/10:  24%|██▍       | 18/75 [00:01<00:05,  9.95it/s]Epoch 3/10:  27%|██▋       | 20/75 [00:02<00:05, 10.03it/s]Epoch 3/10:  29%|██▉       | 22/75 [00:02<00:05,  9.96it/s]Epoch 3/10:  32%|███▏      | 24/75 [00:02<00:05, 10.04it/s]Epoch 3/10:  35%|███▍      | 26/75 [00:02<00:04, 10.04it/s]Epoch 3/10:  37%|███▋      | 28/75 [00:02<00:04, 10.08it/s]Epoch 3/10:  40%|████      | 30/75 [00:03<00:04, 10.06it/s]Epoch 3/10:  43%|████▎     | 32/75 [00:03<00:04, 10.08it/s]Epoch 3/10:  45%|████▌     | 34/75 [00:03<00:04, 10.12it/s]Epoch 3/10:  48%|████▊     | 36/75 [00:03<00:03, 10.14it/s]Epoch 3/10:  51%|█████     | 38/75 [00:03<00:03, 10.14it/s]Epoch 3/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.14it/s]Epoch 3/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.16it/s]Epoch 3/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.12it/s]Epoch 3/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.13it/s]Epoch 3/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.13it/s]Epoch 3/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.14it/s]Epoch 3/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.07it/s]Epoch 3/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.11it/s]Epoch 3/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.12it/s]Epoch 3/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.13it/s]Epoch 3/10:  80%|████████  | 60/75 [00:06<00:01, 10.15it/s]Epoch 3/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.14it/s]Epoch 3/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.14it/s]Epoch 3/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.14it/s]Epoch 3/10:  91%|█████████ | 68/75 [00:06<00:00, 10.16it/s]Epoch 3/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.16it/s]Epoch 3/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.15it/s]Epoch 3/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.16it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00,  9.94it/s]
[2025-04-11 10:21:36,191][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0261
[2025-04-11 10:21:36,454][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0399, Metrics: {'mse': 0.03840745985507965, 'rmse': 0.19597821270508528, 'r2': -0.06618630886077881}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  5.20it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.13it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.07it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.73it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.86it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.97it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:06,  9.99it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.05it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.02it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.06it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05,  9.99it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.05it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.01it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.05it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.08it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.10it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.13it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.09it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.10it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.12it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.14it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.15it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.13it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.15it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.14it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.13it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.14it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.13it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.14it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.16it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.17it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:21:43,976][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0256
[2025-04-11 10:21:44,253][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0369, Metrics: {'mse': 0.03443048521876335, 'rmse': 0.18555453435247374, 'r2': 0.04421412944793701}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.07it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.98it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.70it/s]Epoch 5/10:  13%|█▎        | 10/75 [00:01<00:06,  9.73it/s]Epoch 5/10:  16%|█▌        | 12/75 [00:01<00:06,  9.90it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.89it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:06, 10.00it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.04it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.09it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.07it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.11it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.12it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.13it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.15it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.15it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.15it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.08it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.11it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.09it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.11it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.13it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.13it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.09it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.12it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.13it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.09it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.01it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.06it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.01it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.07it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.02it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.02it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.02it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.06it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.10it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.13it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.54it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.93it/s]
[2025-04-11 10:21:51,805][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0239
[2025-04-11 10:21:52,078][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0376, Metrics: {'mse': 0.03488990664482117, 'rmse': 0.18678840072344205, 'r2': 0.03146064281463623}
[2025-04-11 10:21:52,079][src.training.lm_trainer][INFO] - Early stopping at epoch 5
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁
wandb:     best_val_mse █▁
wandb:      best_val_r2 ▁█
wandb:    best_val_rmse █▁
wandb:            epoch ▁▁▃▃▅▅▆▆██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁
wandb:       train_loss █▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▃▁▂
wandb:          val_mse █▁▃▁▂
wandb:           val_r2 ▁█▆█▇
wandb:         val_rmse █▁▃▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03644
wandb:     best_val_mse 0.03365
wandb:      best_val_r2 0.06588
wandb:    best_val_rmse 0.18344
wandb:            epoch 5
wandb:   final_test_mse 0.0233
wandb:    final_test_r2 0.04492
wandb:  final_test_rmse 0.15263
wandb:  final_train_mse 0.02395
wandb:   final_train_r2 -0.12327
wandb: final_train_rmse 0.15477
wandb:    final_val_mse 0.03365
wandb:     final_val_r2 0.06588
wandb:   final_val_rmse 0.18344
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02393
wandb:       train_time 40.71895
wandb:         val_loss 0.03764
wandb:          val_mse 0.03489
wandb:           val_r2 0.03146
wandb:         val_rmse 0.18679
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_102100-l7l2bif6
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_102100-l7l2bif6/logs
Control experiment for avg_max_depth (ru, control=2) completed successfully
Running submetric avg_max_depth control=3 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:22:09,016][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_max_depth/control3
experiment_name: avg_max_depth_control3_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_max_depth
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:22:09,016][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:22:09,016][__main__][INFO] - Using submetric: avg_max_depth
[2025-04-11 10:22:09,017][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:22:09,017][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:22:09,021][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:22:09,021][__main__][INFO] - Using submetric: avg_max_depth
[2025-04-11 10:22:09,021][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:22:10,525][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_max_depth'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:22:12,870][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:22:12,870][src.data.datasets][INFO] - Loading 'control_avg_max_depth_seed3' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:22:12,954][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_max_depth_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 06:57:54 2025).
[2025-04-11 10:22:12,982][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_max_depth_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_max_depth_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 06:57:54 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 110546.44 examples/s]
[2025-04-11 10:22:13,227][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:22:13,235][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:22:13,235][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:22:13,237][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:22:13,254][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:22:13,282][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:22:13,294][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:22:13,295][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:22:13,295][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:22:13,296][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:22:13,312][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:22:13,339][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:22:13,351][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:22:13,353][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:22:13,353][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:22:13,354][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:22:13,355][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:22:13,355][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-04-11 10:22:13,355][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-04-11 10:22:13,355][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-04-11 10:22:13,355][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:22:13,355][src.data.datasets][INFO] -   Mean: 0.2416, Std: 0.1460
[2025-04-11 10:22:13,355][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:22:13,356][src.data.datasets][INFO] - Sample label: 0.20000000298023224
[2025-04-11 10:22:13,356][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:22:13,356][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-04-11 10:22:13,356][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-04-11 10:22:13,356][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-04-11 10:22:13,356][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-04-11 10:22:13,356][src.data.datasets][INFO] -   Mean: 0.2865, Std: 0.1898
[2025-04-11 10:22:13,356][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:22:13,356][src.data.datasets][INFO] - Sample label: 0.25
[2025-04-11 10:22:13,357][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:22:13,357][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_max_depth'
[2025-04-11 10:22:13,357][src.data.datasets][INFO] - Selected feature name: 'avg_max_depth' for task: 'single_submetric'
[2025-04-11 10:22:13,357][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_max_depth):
[2025-04-11 10:22:13,357][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.7500
[2025-04-11 10:22:13,357][src.data.datasets][INFO] -   Mean: 0.2284, Std: 0.1562
[2025-04-11 10:22:13,357][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:22:13,357][src.data.datasets][INFO] - Sample label: 0.25
[2025-04-11 10:22:13,357][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:22:13,357][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:22:13,358][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:22:13,358][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:22:17,361][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:22:17,363][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:22:17,364][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:22:17,364][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:30,  1.22s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:27,  2.57it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:16,  4.18it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.58it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.20it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.35it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.17it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.77it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.20it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:06,  9.47it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.68it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.78it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05,  9.85it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04,  9.91it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04,  9.96it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04,  9.98it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.00it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:04, 10.00it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.06it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.11it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.13it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.10it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:03, 10.12it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.15it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.17it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.17it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.10it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.05it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.10it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.08it/s]Epoch 1/10:  80%|████████  | 60/75 [00:07<00:01, 10.11it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.13it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.14it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.15it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.13it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:08<00:00, 10.13it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.15it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.17it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.79it/s]
[2025-04-11 10:22:27,589][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0761
[2025-04-11 10:22:27,840][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0667, Metrics: {'mse': 0.06731501966714859, 'rmse': 0.2594513820875668, 'r2': -0.8686565160751343}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.49it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.34it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.19it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.58it/s]Epoch 2/10:  11%|█         | 8/75 [00:00<00:06,  9.63it/s]Epoch 2/10:  13%|█▎        | 10/75 [00:01<00:06,  9.84it/s]Epoch 2/10:  16%|█▌        | 12/75 [00:01<00:06,  9.97it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06,  9.95it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.04it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.07it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.11it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.13it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.15it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.16it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.17it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.17it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.16it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.16it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.17it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.16it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.16it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.17it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.17it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.16it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.16it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.16it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.15it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.16it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.16it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.16it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.16it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.16it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.18it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.02it/s]
[2025-04-11 10:22:35,738][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0324
[2025-04-11 10:22:35,982][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0420, Metrics: {'mse': 0.038647036999464035, 'rmse': 0.19658849661021377, 'r2': -0.07283675670623779}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.67it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.79it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.35it/s]Epoch 3/10:  11%|█         | 8/75 [00:00<00:07,  9.42it/s]Epoch 3/10:  13%|█▎        | 10/75 [00:01<00:06,  9.71it/s]Epoch 3/10:  16%|█▌        | 12/75 [00:01<00:06,  9.86it/s]Epoch 3/10:  19%|█▊        | 14/75 [00:01<00:06,  9.89it/s]Epoch 3/10:  21%|██▏       | 16/75 [00:01<00:05,  9.97it/s]Epoch 3/10:  24%|██▍       | 18/75 [00:01<00:05,  9.95it/s]Epoch 3/10:  27%|██▋       | 20/75 [00:02<00:05, 10.02it/s]Epoch 3/10:  29%|██▉       | 22/75 [00:02<00:05, 10.07it/s]Epoch 3/10:  32%|███▏      | 24/75 [00:02<00:05, 10.11it/s]Epoch 3/10:  35%|███▍      | 26/75 [00:02<00:04, 10.12it/s]Epoch 3/10:  37%|███▋      | 28/75 [00:02<00:04, 10.13it/s]Epoch 3/10:  40%|████      | 30/75 [00:03<00:04, 10.10it/s]Epoch 3/10:  43%|████▎     | 32/75 [00:03<00:04, 10.12it/s]Epoch 3/10:  45%|████▌     | 34/75 [00:03<00:04, 10.13it/s]Epoch 3/10:  48%|████▊     | 36/75 [00:03<00:03, 10.13it/s]Epoch 3/10:  51%|█████     | 38/75 [00:03<00:03, 10.15it/s]Epoch 3/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.15it/s]Epoch 3/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.15it/s]Epoch 3/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.15it/s]Epoch 3/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.16it/s]Epoch 3/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.16it/s]Epoch 3/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.15it/s]Epoch 3/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.15it/s]Epoch 3/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.16it/s]Epoch 3/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.16it/s]Epoch 3/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.15it/s]Epoch 3/10:  80%|████████  | 60/75 [00:06<00:01, 10.10it/s]Epoch 3/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.08it/s]Epoch 3/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.11it/s]Epoch 3/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.06it/s]Epoch 3/10:  91%|█████████ | 68/75 [00:06<00:00, 10.08it/s]Epoch 3/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.11it/s]Epoch 3/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.14it/s]Epoch 3/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.14it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00,  9.93it/s]
[2025-04-11 10:22:44,092][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0261
[2025-04-11 10:22:44,381][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0412, Metrics: {'mse': 0.038646150380373, 'rmse': 0.19658624158463633, 'r2': -0.07281231880187988}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.68it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.78it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.35it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.64it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.81it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.93it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:06, 10.00it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.05it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.08it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.12it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.13it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.15it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.15it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.15it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.16it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.15it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.10it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.12it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.15it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.15it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.15it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.16it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.16it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.14it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.16it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.16it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.17it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:22:52,268][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0248
[2025-04-11 10:22:52,541][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0427, Metrics: {'mse': 0.04075711965560913, 'rmse': 0.2018839261942593, 'r2': -0.13141250610351562}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.03it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.02it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.00it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.38it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.66it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.79it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.92it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:06,  9.99it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.06it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.10it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.12it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.14it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.15it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.14it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.15it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.15it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.14it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.15it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.16it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.16it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.15it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.09it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.12it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.13it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.14it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.13it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.14it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.99it/s]
[2025-04-11 10:23:00,048][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0243
[2025-04-11 10:23:00,319][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0422, Metrics: {'mse': 0.03892146050930023, 'rmse': 0.19728522628240625, 'r2': -0.08045470714569092}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.91it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.95it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.94it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.68it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.78it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.89it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.03it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.08it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.05it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.08it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.01it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.01it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04,  9.99it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04,  9.98it/s]Epoch 6/10:  43%|████▎     | 32/75 [00:03<00:04,  9.95it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04,  9.92it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:04, 10.00it/s]Epoch 6/10:  48%|████▊     | 36/75 [00:03<00:03,  9.95it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03,  9.91it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.00it/s]Epoch 6/10:  53%|█████▎    | 40/75 [00:04<00:03,  9.97it/s]Epoch 6/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.04it/s]Epoch 6/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.08it/s]Epoch 6/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.10it/s]Epoch 6/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.06it/s]Epoch 6/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.10it/s]Epoch 6/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.11it/s]Epoch 6/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.12it/s]Epoch 6/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.13it/s]Epoch 6/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.14it/s]Epoch 6/10:  80%|████████  | 60/75 [00:06<00:01, 10.15it/s]Epoch 6/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.14it/s]Epoch 6/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.14it/s]Epoch 6/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.14it/s]Epoch 6/10:  91%|█████████ | 68/75 [00:06<00:00, 10.15it/s]Epoch 6/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.15it/s]Epoch 6/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.15it/s]Epoch 6/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.16it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.89it/s]
[2025-04-11 10:23:07,907][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0236
[2025-04-11 10:23:08,177][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0429, Metrics: {'mse': 0.04067022353410721, 'rmse': 0.20166859828467892, 'r2': -0.12900018692016602}
[2025-04-11 10:23:08,177][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁
wandb:     best_val_mse █▁▁
wandb:      best_val_r2 ▁██
wandb:    best_val_rmse █▁▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▁▁▁▁
wandb:          val_mse █▁▁▂▁▁
wandb:           val_r2 ▁██▇██
wandb:         val_rmse █▁▁▂▁▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04118
wandb:     best_val_mse 0.03865
wandb:      best_val_r2 -0.07281
wandb:    best_val_rmse 0.19659
wandb:            epoch 6
wandb:   final_test_mse 0.02497
wandb:    final_test_r2 -0.02353
wandb:  final_test_rmse 0.15801
wandb:  final_train_mse 0.02099
wandb:   final_train_r2 0.0157
wandb: final_train_rmse 0.14488
wandb:    final_val_mse 0.03865
wandb:     final_val_r2 -0.07281
wandb:   final_val_rmse 0.19659
wandb:    learning_rate 1e-05
wandb:       train_loss 0.02365
wandb:       train_time 49.12648
wandb:         val_loss 0.04292
wandb:          val_mse 0.04067
wandb:           val_r2 -0.129
wandb:         val_rmse 0.20167
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_102209-ytq7sb4c
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_102209-ytq7sb4c/logs
Control experiment for avg_max_depth (ru, control=3) completed successfully
Running submetric avg_subordinate_chain_len control=1 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:23:26,814][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_subordinate_chain_len/control1
experiment_name: avg_subordinate_chain_len_control1_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:23:26,814][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:23:26,814][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-04-11 10:23:26,814][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:23:26,814][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:23:26,818][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:23:26,818][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-04-11 10:23:26,819][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:23:28,129][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:23:30,496][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:23:30,497][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed1' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:23:30,581][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 06:59:17 2025).
[2025-04-11 10:23:30,611][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 06:59:17 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 59028.90 examples/s]
[2025-04-11 10:23:31,071][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:23:31,081][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:23:31,081][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:23:31,082][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:23:31,109][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:23:31,140][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:23:31,155][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:23:31,156][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:23:31,156][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:23:31,157][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:23:31,178][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:23:31,209][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:23:31,222][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:23:31,224][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:23:31,224][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:23:31,225][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:23:31,226][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:23:31,226][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-04-11 10:23:31,226][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-04-11 10:23:31,226][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-04-11 10:23:31,226][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:23:31,226][src.data.datasets][INFO] -   Mean: 0.0301, Std: 0.1168
[2025-04-11 10:23:31,226][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:23:31,226][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-11 10:23:31,227][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:23:31,227][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-04-11 10:23:31,227][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-04-11 10:23:31,227][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-04-11 10:23:31,227][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:23:31,227][src.data.datasets][INFO] -   Mean: 0.1273, Std: 0.2332
[2025-04-11 10:23:31,227][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:23:31,227][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-11 10:23:31,227][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:23:31,227][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-04-11 10:23:31,228][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-04-11 10:23:31,228][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-04-11 10:23:31,228][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6670
[2025-04-11 10:23:31,228][src.data.datasets][INFO] -   Mean: 0.1575, Std: 0.1943
[2025-04-11 10:23:31,228][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:23:31,228][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-04-11 10:23:31,228][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:23:31,228][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:23:31,228][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:23:31,229][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:23:35,368][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:23:35,371][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:23:35,371][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:23:35,371][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:21,  1.11s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:25,  2.78it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:15,  4.44it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:11,  5.84it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  6.97it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.83it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.16it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.80it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.23it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.53it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.73it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.87it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05,  9.97it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.04it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.08it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.11it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.15it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:04, 10.16it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.17it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.18it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.19it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.19it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:03, 10.19it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.19it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.19it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.19it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.19it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.19it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.14it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.16it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.18it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.18it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.18it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.19it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.19it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.18it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.19it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.20it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.95it/s]
[2025-04-11 10:23:45,520][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0200
[2025-04-11 10:23:45,754][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0690, Metrics: {'mse': 0.07495254278182983, 'rmse': 0.27377462041217376, 'r2': -0.3783615827560425}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.34it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.23it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.12it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.55it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.73it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.87it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06,  9.97it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:06,  9.97it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05,  9.97it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.05it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.09it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.12it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.14it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.16it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.16it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.16it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.17it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.17it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.17it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.17it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.18it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.13it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.13it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.08it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.11it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.14it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.16it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.17it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.17it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.18it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.13it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.16it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.00it/s]
[2025-04-11 10:23:53,682][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0177
[2025-04-11 10:23:53,924][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0631, Metrics: {'mse': 0.06869445741176605, 'rmse': 0.2620962750818219, 'r2': -0.26327669620513916}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.89it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.94it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 3/10:  11%|█         | 8/75 [00:00<00:07,  9.50it/s]Epoch 3/10:  13%|█▎        | 10/75 [00:01<00:06,  9.73it/s]Epoch 3/10:  16%|█▌        | 12/75 [00:01<00:06,  9.87it/s]Epoch 3/10:  19%|█▊        | 14/75 [00:01<00:06,  9.98it/s]Epoch 3/10:  21%|██▏       | 16/75 [00:01<00:05, 10.04it/s]Epoch 3/10:  24%|██▍       | 18/75 [00:01<00:05, 10.08it/s]Epoch 3/10:  27%|██▋       | 20/75 [00:02<00:05, 10.11it/s]Epoch 3/10:  29%|██▉       | 22/75 [00:02<00:05, 10.13it/s]Epoch 3/10:  32%|███▏      | 24/75 [00:02<00:05, 10.14it/s]Epoch 3/10:  35%|███▍      | 26/75 [00:02<00:04, 10.16it/s]Epoch 3/10:  37%|███▋      | 28/75 [00:02<00:04, 10.16it/s]Epoch 3/10:  40%|████      | 30/75 [00:03<00:04, 10.15it/s]Epoch 3/10:  43%|████▎     | 32/75 [00:03<00:04, 10.16it/s]Epoch 3/10:  45%|████▌     | 34/75 [00:03<00:04, 10.12it/s]Epoch 3/10:  48%|████▊     | 36/75 [00:03<00:03, 10.14it/s]Epoch 3/10:  51%|█████     | 38/75 [00:03<00:03, 10.08it/s]Epoch 3/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.09it/s]Epoch 3/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.06it/s]Epoch 3/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.10it/s]Epoch 3/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.11it/s]Epoch 3/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.12it/s]Epoch 3/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.07it/s]Epoch 3/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.10it/s]Epoch 3/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.07it/s]Epoch 3/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.06it/s]Epoch 3/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.04it/s]Epoch 3/10:  80%|████████  | 60/75 [00:06<00:01, 10.08it/s]Epoch 3/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.10it/s]Epoch 3/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.13it/s]Epoch 3/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.14it/s]Epoch 3/10:  91%|█████████ | 68/75 [00:06<00:00, 10.14it/s]Epoch 3/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.17it/s]Epoch 3/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.17it/s]Epoch 3/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.17it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00,  9.93it/s]
[2025-04-11 10:24:02,019][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0157
[2025-04-11 10:24:02,287][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0621, Metrics: {'mse': 0.06759382784366608, 'rmse': 0.25998813019764205, 'r2': -0.24303627014160156}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  4.98it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.01it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.90it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.38it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.67it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.83it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.95it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.01it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.07it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.10it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.13it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.09it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.12it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.13it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.13it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.14it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.14it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.15it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.14it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.16it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.16it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.16it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.16it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.17it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.16it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.17it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.58it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:24:10,154][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0150
[2025-04-11 10:24:10,441][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0613, Metrics: {'mse': 0.06667634844779968, 'rmse': 0.25821763775505285, 'r2': -0.2261641025543213}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.93it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.98it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.36it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.64it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.82it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.88it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:06,  9.97it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.03it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.06it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.10it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.13it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.13it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.15it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.15it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.15it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.15it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.16it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.15it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.16it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.11it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.09it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.10it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.11it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.07it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.08it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.11it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.12it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.13it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.18it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.96it/s]
[2025-04-11 10:24:18,347][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0143
[2025-04-11 10:24:18,623][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0614, Metrics: {'mse': 0.06681791692972183, 'rmse': 0.2584916186837048, 'r2': -0.2287675142288208}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.09it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.07it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.02it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.71it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.85it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.95it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.02it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.06it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.10it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.12it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.14it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.14it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.16it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.17it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.16it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.16it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.12it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.13it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.13it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.13it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.15it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.15it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.13it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.16it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.15it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.17it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.17it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.16it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.16it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.17it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:24:26,143][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0144
[2025-04-11 10:24:26,423][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0589, Metrics: {'mse': 0.06410864740610123, 'rmse': 0.25319685504780903, 'r2': -0.17894458770751953}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:15,  4.90it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.92it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.93it/s]Epoch 7/10:   8%|▊         | 6/75 [00:00<00:07,  9.15it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.32it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.68it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.86it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.95it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.03it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.08it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.11it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.11it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.14it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.14it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.14it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.14it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.14it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.14it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.15it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.11it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.12it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.12it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.13it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.11it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.12it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.12it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.16it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.16it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.16it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.16it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.17it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:24:34,340][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0141
[2025-04-11 10:24:34,605][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0586, Metrics: {'mse': 0.06379588693380356, 'rmse': 0.2525784767825706, 'r2': -0.17319297790527344}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:14,  4.98it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  8.00it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.70it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.86it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06,  9.95it/s]Epoch 8/10:  19%|█▊        | 14/75 [00:01<00:06,  9.90it/s]Epoch 8/10:  21%|██▏       | 16/75 [00:01<00:05, 10.00it/s]Epoch 8/10:  24%|██▍       | 18/75 [00:01<00:05, 10.02it/s]Epoch 8/10:  27%|██▋       | 20/75 [00:02<00:05, 10.07it/s]Epoch 8/10:  29%|██▉       | 22/75 [00:02<00:05, 10.10it/s]Epoch 8/10:  32%|███▏      | 24/75 [00:02<00:05, 10.11it/s]Epoch 8/10:  35%|███▍      | 26/75 [00:02<00:04, 10.13it/s]Epoch 8/10:  37%|███▋      | 28/75 [00:02<00:04, 10.15it/s]Epoch 8/10:  40%|████      | 30/75 [00:03<00:04, 10.16it/s]Epoch 8/10:  43%|████▎     | 32/75 [00:03<00:04, 10.15it/s]Epoch 8/10:  45%|████▌     | 34/75 [00:03<00:04, 10.16it/s]Epoch 8/10:  48%|████▊     | 36/75 [00:03<00:03, 10.16it/s]Epoch 8/10:  51%|█████     | 38/75 [00:03<00:03, 10.16it/s]Epoch 8/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.15it/s]Epoch 8/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.16it/s]Epoch 8/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.15it/s]Epoch 8/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.14it/s]Epoch 8/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.14it/s]Epoch 8/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.14it/s]Epoch 8/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.14it/s]Epoch 8/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.09it/s]Epoch 8/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.11it/s]Epoch 8/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.12it/s]Epoch 8/10:  80%|████████  | 60/75 [00:06<00:01, 10.14it/s]Epoch 8/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.15it/s]Epoch 8/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.16it/s]Epoch 8/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.16it/s]Epoch 8/10:  91%|█████████ | 68/75 [00:06<00:00, 10.16it/s]Epoch 8/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.17it/s]Epoch 8/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.16it/s]Epoch 8/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.16it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:24:42,520][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0141
[2025-04-11 10:24:42,906][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0601, Metrics: {'mse': 0.06544241309165955, 'rmse': 0.2558171477670321, 'r2': -0.20347225666046143}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:15,  4.90it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.91it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.92it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.63it/s]Epoch 9/10:  13%|█▎        | 10/75 [00:01<00:06,  9.67it/s]Epoch 9/10:  16%|█▌        | 12/75 [00:01<00:06,  9.85it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06,  9.84it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:06,  9.89it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05,  9.98it/s]Epoch 9/10:  24%|██▍       | 18/75 [00:01<00:05,  9.94it/s]Epoch 9/10:  27%|██▋       | 20/75 [00:02<00:05, 10.03it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.00it/s]Epoch 9/10:  29%|██▉       | 22/75 [00:02<00:05,  9.97it/s]Epoch 9/10:  32%|███▏      | 24/75 [00:02<00:05, 10.04it/s]Epoch 9/10:  35%|███▍      | 26/75 [00:02<00:04, 10.03it/s]Epoch 9/10:  37%|███▋      | 28/75 [00:02<00:04, 10.05it/s]Epoch 9/10:  40%|████      | 30/75 [00:03<00:04,  9.99it/s]Epoch 9/10:  43%|████▎     | 32/75 [00:03<00:04, 10.03it/s]Epoch 9/10:  45%|████▌     | 34/75 [00:03<00:04, 10.00it/s]Epoch 9/10:  48%|████▊     | 36/75 [00:03<00:03, 10.00it/s]Epoch 9/10:  51%|█████     | 38/75 [00:03<00:03,  9.99it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03,  9.96it/s]Epoch 9/10:  53%|█████▎    | 40/75 [00:04<00:03,  9.94it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03,  9.93it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.01it/s]Epoch 9/10:  59%|█████▊    | 44/75 [00:04<00:03,  9.99it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:03,  9.97it/s]Epoch 9/10:  61%|██████▏   | 46/75 [00:04<00:02,  9.94it/s]Epoch 9/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.03it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02,  9.99it/s]Epoch 9/10:  67%|██████▋   | 50/75 [00:05<00:02,  9.95it/s]Epoch 9/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.03it/s]Epoch 9/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.08it/s]Epoch 9/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.05it/s]Epoch 9/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.09it/s]Epoch 9/10:  80%|████████  | 60/75 [00:06<00:01, 10.11it/s]Epoch 9/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.12it/s]Epoch 9/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.13it/s]Epoch 9/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.14it/s]Epoch 9/10:  91%|█████████ | 68/75 [00:06<00:00, 10.13it/s]Epoch 9/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.14it/s]Epoch 9/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.16it/s]Epoch 9/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.16it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00,  9.89it/s]
[2025-04-11 10:24:50,493][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0140
[2025-04-11 10:24:50,768][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0598, Metrics: {'mse': 0.06510848551988602, 'rmse': 0.25516364458889124, 'r2': -0.19733142852783203}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:15,  4.89it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.90it/s]Epoch 10/10:   5%|▌         | 4/75 [00:00<00:08,  8.52it/s]Epoch 10/10:   8%|▊         | 6/75 [00:00<00:07,  9.25it/s]Epoch 10/10:  11%|█         | 8/75 [00:00<00:06,  9.60it/s]Epoch 10/10:  13%|█▎        | 10/75 [00:01<00:06,  9.81it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.81it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06,  9.92it/s]Epoch 10/10:  19%|█▊        | 14/75 [00:01<00:06,  9.90it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:06,  9.87it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05,  9.98it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.00it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.06it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.08it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.10it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.13it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.08it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.10it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.10it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.11it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.09it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.11it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.08it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.10it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.12it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.13it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.14it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.11it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.13it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.13it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.12it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.14it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.14it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:24:58,312][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0142
[2025-04-11 10:24:58,594][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0578, Metrics: {'mse': 0.06292887777090073, 'rmse': 0.250856289079825, 'r2': -0.157248854637146}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▄▃▂▂▁
wandb:     best_val_mse █▄▄▃▂▂▁
wandb:      best_val_r2 ▁▅▅▆▇▇█
wandb:    best_val_rmse █▄▄▃▂▂▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▃▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▄▃▃▂▂▂▂▁
wandb:          val_mse █▄▄▃▃▂▂▂▂▁
wandb:           val_r2 ▁▅▅▆▆▇▇▇▇█
wandb:         val_rmse █▄▄▃▃▂▂▃▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05782
wandb:     best_val_mse 0.06293
wandb:      best_val_r2 -0.15725
wandb:    best_val_rmse 0.25086
wandb:            epoch 10
wandb:   final_test_mse 0.05261
wandb:    final_test_r2 -0.39289
wandb:  final_test_rmse 0.22936
wandb:  final_train_mse 0.0135
wandb:   final_train_r2 0.01127
wandb: final_train_rmse 0.11617
wandb:    final_val_mse 0.06293
wandb:     final_val_r2 -0.15725
wandb:   final_val_rmse 0.25086
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01418
wandb:       train_time 81.8422
wandb:         val_loss 0.05782
wandb:          val_mse 0.06293
wandb:           val_r2 -0.15725
wandb:         val_rmse 0.25086
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_102326-4vdtjgii
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_102326-4vdtjgii/logs
Control experiment for avg_subordinate_chain_len (ru, control=1) completed successfully
Running submetric avg_subordinate_chain_len control=2 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:25:16,965][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_subordinate_chain_len/control2
experiment_name: avg_subordinate_chain_len_control2_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:25:16,965][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:25:16,965][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-04-11 10:25:16,965][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:25:16,965][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:25:16,972][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:25:16,972][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-04-11 10:25:16,972][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:25:18,220][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:25:20,517][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:25:20,518][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed2' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:25:20,582][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:00:25 2025).
[2025-04-11 10:25:20,611][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:00:25 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 86990.69 examples/s]
[2025-04-11 10:25:20,988][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:25:21,002][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:25:21,003][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:25:21,004][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:25:21,024][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:25:21,052][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:25:21,064][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:25:21,065][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:25:21,065][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:25:21,066][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:25:21,083][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:25:21,112][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:25:21,124][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:25:21,125][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:25:21,126][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:25:21,126][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:25:21,127][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:25:21,128][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-04-11 10:25:21,128][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-04-11 10:25:21,128][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-04-11 10:25:21,128][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:25:21,128][src.data.datasets][INFO] -   Mean: 0.0301, Std: 0.1168
[2025-04-11 10:25:21,128][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:25:21,128][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-11 10:25:21,128][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:25:21,129][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-04-11 10:25:21,129][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-04-11 10:25:21,129][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-04-11 10:25:21,129][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:25:21,129][src.data.datasets][INFO] -   Mean: 0.1273, Std: 0.2332
[2025-04-11 10:25:21,129][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:25:21,129][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-11 10:25:21,129][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:25:21,129][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-04-11 10:25:21,129][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-04-11 10:25:21,129][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-04-11 10:25:21,130][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6670
[2025-04-11 10:25:21,130][src.data.datasets][INFO] -   Mean: 0.1575, Std: 0.1943
[2025-04-11 10:25:21,130][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:25:21,130][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-04-11 10:25:21,130][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:25:21,130][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:25:21,130][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:25:21,131][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:25:25,289][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:25:25,291][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:25:25,291][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:25:25,291][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:17,  1.04s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:24,  2.92it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:15,  4.61it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:11,  6.00it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  7.10it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:01<00:08,  7.55it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.37it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.92it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.29it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.38it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:06,  9.48it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.72it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.88it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05,  9.97it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.03it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.08it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.12it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.13it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:04, 10.14it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.15it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.16it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.17it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.12it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:03, 10.14it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.16it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.16it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.11it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.15it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.16it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.10it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.12it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.13it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.15it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.14it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.15it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.15it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.16it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.16it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.17it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.98it/s]
[2025-04-11 10:25:35,218][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0213
[2025-04-11 10:25:35,442][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0701, Metrics: {'mse': 0.07610267400741577, 'rmse': 0.27586713107475447, 'r2': -0.3995121717453003}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.65it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.44it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.24it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.60it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.94it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.02it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.06it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.10it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.12it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.14it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.14it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.15it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.15it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.15it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.16it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.17it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.17it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.16it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.17it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.17it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.16it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.16it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.17it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.17it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.17it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.17it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.18it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.18it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.17it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.17it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.17it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.17it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.16it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.17it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.03it/s]
[2025-04-11 10:25:43,326][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0177
[2025-04-11 10:25:43,581][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0630, Metrics: {'mse': 0.06849757581949234, 'rmse': 0.26172041536626894, 'r2': -0.25965607166290283}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.74it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.83it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.81it/s]Epoch 3/10:   8%|▊         | 6/75 [00:00<00:07,  9.02it/s]Epoch 3/10:  11%|█         | 8/75 [00:00<00:07,  9.48it/s]Epoch 3/10:  13%|█▎        | 10/75 [00:01<00:06,  9.73it/s]Epoch 3/10:  16%|█▌        | 12/75 [00:01<00:06,  9.89it/s]Epoch 3/10:  19%|█▊        | 14/75 [00:01<00:06,  9.98it/s]Epoch 3/10:  21%|██▏       | 16/75 [00:01<00:05, 10.03it/s]Epoch 3/10:  24%|██▍       | 18/75 [00:01<00:05, 10.00it/s]Epoch 3/10:  27%|██▋       | 20/75 [00:02<00:05, 10.05it/s]Epoch 3/10:  29%|██▉       | 22/75 [00:02<00:05, 10.09it/s]Epoch 3/10:  32%|███▏      | 24/75 [00:02<00:05, 10.11it/s]Epoch 3/10:  35%|███▍      | 26/75 [00:02<00:04, 10.12it/s]Epoch 3/10:  37%|███▋      | 28/75 [00:02<00:04, 10.14it/s]Epoch 3/10:  40%|████      | 30/75 [00:03<00:04, 10.15it/s]Epoch 3/10:  43%|████▎     | 32/75 [00:03<00:04, 10.15it/s]Epoch 3/10:  45%|████▌     | 34/75 [00:03<00:04, 10.15it/s]Epoch 3/10:  48%|████▊     | 36/75 [00:03<00:03, 10.15it/s]Epoch 3/10:  51%|█████     | 38/75 [00:03<00:03, 10.16it/s]Epoch 3/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.16it/s]Epoch 3/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.15it/s]Epoch 3/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.15it/s]Epoch 3/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.15it/s]Epoch 3/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.14it/s]Epoch 3/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.13it/s]Epoch 3/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.14it/s]Epoch 3/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.09it/s]Epoch 3/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.12it/s]Epoch 3/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.13it/s]Epoch 3/10:  80%|████████  | 60/75 [00:06<00:01, 10.13it/s]Epoch 3/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.14it/s]Epoch 3/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.15it/s]Epoch 3/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.15it/s]Epoch 3/10:  91%|█████████ | 68/75 [00:06<00:00, 10.10it/s]Epoch 3/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.12it/s]Epoch 3/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.14it/s]Epoch 3/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.15it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00,  9.93it/s]
[2025-04-11 10:25:51,683][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0159
[2025-04-11 10:25:51,953][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0643, Metrics: {'mse': 0.0699862539768219, 'rmse': 0.2645491522889875, 'r2': -0.2870326042175293}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  4.99it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  8.00it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.71it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.85it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.01it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.06it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.08it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.10it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.13it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.14it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.15it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.15it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.14it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.15it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.15it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.14it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.13it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.13it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.15it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.16it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.16it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.17it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:25:59,477][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0161
[2025-04-11 10:25:59,740][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0620, Metrics: {'mse': 0.06747975200414658, 'rmse': 0.2597686509264476, 'r2': -0.24093842506408691}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.73it/s]Epoch 5/10:   3%|▎         | 2/75 [00:00<00:10,  6.78it/s]Epoch 5/10:   5%|▌         | 4/75 [00:00<00:08,  8.56it/s]Epoch 5/10:   8%|▊         | 6/75 [00:00<00:07,  9.26it/s]Epoch 5/10:  11%|█         | 8/75 [00:00<00:06,  9.60it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.65it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.84it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.95it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.02it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.06it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.08it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.10it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.11it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.11it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.07it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.00it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.04it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.09it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.09it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.06it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.05it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.07it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.11it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.13it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.14it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.15it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.15it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.13it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.16it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.93it/s]
[2025-04-11 10:26:07,671][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0146
[2025-04-11 10:26:07,933][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0604, Metrics: {'mse': 0.06570696085691452, 'rmse': 0.25633369044453463, 'r2': -0.2083371877670288}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.80it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.88it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.81it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.33it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:06,  9.59it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.77it/s]Epoch 6/10:  16%|█▌        | 12/75 [00:01<00:06,  9.76it/s]Epoch 6/10:  19%|█▊        | 14/75 [00:01<00:06,  9.90it/s]Epoch 6/10:  21%|██▏       | 16/75 [00:01<00:05,  9.99it/s]Epoch 6/10:  24%|██▍       | 18/75 [00:01<00:05, 10.03it/s]Epoch 6/10:  27%|██▋       | 20/75 [00:02<00:05, 10.07it/s]Epoch 6/10:  29%|██▉       | 22/75 [00:02<00:05, 10.10it/s]Epoch 6/10:  32%|███▏      | 24/75 [00:02<00:05, 10.12it/s]Epoch 6/10:  35%|███▍      | 26/75 [00:02<00:04, 10.12it/s]Epoch 6/10:  37%|███▋      | 28/75 [00:02<00:04, 10.13it/s]Epoch 6/10:  40%|████      | 30/75 [00:03<00:04, 10.14it/s]Epoch 6/10:  43%|████▎     | 32/75 [00:03<00:04, 10.14it/s]Epoch 6/10:  45%|████▌     | 34/75 [00:03<00:04, 10.16it/s]Epoch 6/10:  48%|████▊     | 36/75 [00:03<00:03, 10.16it/s]Epoch 6/10:  51%|█████     | 38/75 [00:03<00:03, 10.15it/s]Epoch 6/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.16it/s]Epoch 6/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.15it/s]Epoch 6/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.15it/s]Epoch 6/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.14it/s]Epoch 6/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.14it/s]Epoch 6/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.14it/s]Epoch 6/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.14it/s]Epoch 6/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.13it/s]Epoch 6/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.09it/s]Epoch 6/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.11it/s]Epoch 6/10:  80%|████████  | 60/75 [00:06<00:01, 10.11it/s]Epoch 6/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.11it/s]Epoch 6/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.13it/s]Epoch 6/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.13it/s]Epoch 6/10:  91%|█████████ | 68/75 [00:06<00:00, 10.14it/s]Epoch 6/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.13it/s]Epoch 6/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.15it/s]Epoch 6/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.16it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:26:15,854][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0143
[2025-04-11 10:26:16,146][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0599, Metrics: {'mse': 0.06520220637321472, 'rmse': 0.2553472270717165, 'r2': -0.19905495643615723}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.43it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.60it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:08,  8.73it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.28it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:01<00:06,  9.59it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.79it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.89it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:06,  9.97it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.03it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.08it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.11it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.12it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.13it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.14it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.13it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.15it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.15it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.15it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.09it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.11it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.13it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.13it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.13it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.15it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.15it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.14it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.14it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.13it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.89it/s]
[2025-04-11 10:26:24,135][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0146
[2025-04-11 10:26:24,414][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0593, Metrics: {'mse': 0.06450895220041275, 'rmse': 0.2539861259998521, 'r2': -0.18630611896514893}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:15,  4.88it/s]Epoch 8/10:   3%|▎         | 2/75 [00:00<00:10,  6.90it/s]Epoch 8/10:   5%|▌         | 4/75 [00:00<00:08,  8.63it/s]Epoch 8/10:   8%|▊         | 6/75 [00:00<00:07,  9.28it/s]Epoch 8/10:  11%|█         | 8/75 [00:00<00:06,  9.62it/s]Epoch 8/10:  13%|█▎        | 10/75 [00:01<00:06,  9.81it/s]Epoch 8/10:  16%|█▌        | 12/75 [00:01<00:06,  9.92it/s]Epoch 8/10:  19%|█▊        | 14/75 [00:01<00:06,  9.98it/s]Epoch 8/10:  21%|██▏       | 16/75 [00:01<00:05, 10.03it/s]Epoch 8/10:  24%|██▍       | 18/75 [00:01<00:05, 10.02it/s]Epoch 8/10:  27%|██▋       | 20/75 [00:02<00:05, 10.01it/s]Epoch 8/10:  29%|██▉       | 22/75 [00:02<00:05, 10.06it/s]Epoch 8/10:  32%|███▏      | 24/75 [00:02<00:05, 10.08it/s]Epoch 8/10:  35%|███▍      | 26/75 [00:02<00:04, 10.10it/s]Epoch 8/10:  37%|███▋      | 28/75 [00:02<00:04, 10.12it/s]Epoch 8/10:  40%|████      | 30/75 [00:03<00:04, 10.13it/s]Epoch 8/10:  43%|████▎     | 32/75 [00:03<00:04, 10.13it/s]Epoch 8/10:  45%|████▌     | 34/75 [00:03<00:04, 10.14it/s]Epoch 8/10:  48%|████▊     | 36/75 [00:03<00:03, 10.14it/s]Epoch 8/10:  51%|█████     | 38/75 [00:03<00:03, 10.14it/s]Epoch 8/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.13it/s]Epoch 8/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.13it/s]Epoch 8/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.15it/s]Epoch 8/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.15it/s]Epoch 8/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.10it/s]Epoch 8/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.07it/s]Epoch 8/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.05it/s]Epoch 8/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.08it/s]Epoch 8/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.11it/s]Epoch 8/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.12it/s]Epoch 8/10:  80%|████████  | 60/75 [00:06<00:01, 10.12it/s]Epoch 8/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.07it/s]Epoch 8/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.09it/s]Epoch 8/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.02it/s]Epoch 8/10:  91%|█████████ | 68/75 [00:06<00:00, 10.03it/s]Epoch 8/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.07it/s]Epoch 8/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.09it/s]Epoch 8/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.10it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00,  9.93it/s]
[2025-04-11 10:26:32,356][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0144
[2025-04-11 10:26:32,632][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0593, Metrics: {'mse': 0.06452704966068268, 'rmse': 0.2540217503692994, 'r2': -0.18663883209228516}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:15,  4.79it/s]Epoch 9/10:   3%|▎         | 2/75 [00:00<00:10,  6.83it/s]Epoch 9/10:   5%|▌         | 4/75 [00:00<00:08,  8.58it/s]Epoch 9/10:   8%|▊         | 6/75 [00:00<00:07,  9.25it/s]Epoch 9/10:  11%|█         | 8/75 [00:00<00:06,  9.59it/s]Epoch 9/10:  13%|█▎        | 10/75 [00:01<00:06,  9.79it/s]Epoch 9/10:  16%|█▌        | 12/75 [00:01<00:06,  9.90it/s]Epoch 9/10:  19%|█▊        | 14/75 [00:01<00:06,  9.98it/s]Epoch 9/10:  21%|██▏       | 16/75 [00:01<00:05, 10.04it/s]Epoch 9/10:  24%|██▍       | 18/75 [00:01<00:05, 10.07it/s]Epoch 9/10:  27%|██▋       | 20/75 [00:02<00:05, 10.09it/s]Epoch 9/10:  29%|██▉       | 22/75 [00:02<00:05, 10.11it/s]Epoch 9/10:  32%|███▏      | 24/75 [00:02<00:05, 10.12it/s]Epoch 9/10:  35%|███▍      | 26/75 [00:02<00:04, 10.13it/s]Epoch 9/10:  37%|███▋      | 28/75 [00:02<00:04, 10.13it/s]Epoch 9/10:  40%|████      | 30/75 [00:03<00:04, 10.14it/s]Epoch 9/10:  43%|████▎     | 32/75 [00:03<00:04, 10.14it/s]Epoch 9/10:  45%|████▌     | 34/75 [00:03<00:04, 10.14it/s]Epoch 9/10:  48%|████▊     | 36/75 [00:03<00:03, 10.14it/s]Epoch 9/10:  51%|█████     | 38/75 [00:03<00:03, 10.14it/s]Epoch 9/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.14it/s]Epoch 9/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.15it/s]Epoch 9/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.14it/s]Epoch 9/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.14it/s]Epoch 9/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.15it/s]Epoch 9/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.14it/s]Epoch 9/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.14it/s]Epoch 9/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.14it/s]Epoch 9/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.15it/s]Epoch 9/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.10it/s]Epoch 9/10:  80%|████████  | 60/75 [00:06<00:01, 10.11it/s]Epoch 9/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.11it/s]Epoch 9/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.11it/s]Epoch 9/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.13it/s]Epoch 9/10:  91%|█████████ | 68/75 [00:06<00:00, 10.13it/s]Epoch 9/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.14it/s]Epoch 9/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.13it/s]Epoch 9/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.15it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:26:40,172][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0141
[2025-04-11 10:26:40,474][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0591, Metrics: {'mse': 0.06433161348104477, 'rmse': 0.2536367747016287, 'r2': -0.18304479122161865}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:14,  5.02it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:08,  8.01it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  9.00it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.70it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.85it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06,  9.95it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.01it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.05it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.09it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.11it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.11it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.12it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.12it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.13it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.13it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.13it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.09it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.11it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.07it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.05it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.04it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.02it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.03it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.07it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.10it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.11it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.11it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.11it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.13it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.14it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.13it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.13it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00,  9.96it/s]
[2025-04-11 10:26:48,396][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0141
[2025-04-11 10:26:48,703][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0590, Metrics: {'mse': 0.06415263563394547, 'rmse': 0.2532837058200655, 'r2': -0.17975354194641113}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▃▂▂▁▁▁
wandb:     best_val_mse █▄▃▂▂▁▁▁
wandb:      best_val_r2 ▁▅▆▇▇███
wandb:    best_val_rmse █▄▃▂▂▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▅▃▃▂▁▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▄▃▂▂▁▁▁▁
wandb:          val_mse █▄▄▃▂▂▁▁▁▁
wandb:           val_r2 ▁▅▅▆▇▇████
wandb:         val_rmse █▄▄▃▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05897
wandb:     best_val_mse 0.06415
wandb:      best_val_r2 -0.17975
wandb:    best_val_rmse 0.25328
wandb:            epoch 10
wandb:   final_test_mse 0.05421
wandb:    final_test_r2 -0.43549
wandb:  final_test_rmse 0.23284
wandb:  final_train_mse 0.01363
wandb:   final_train_r2 0.0016
wandb: final_train_rmse 0.11674
wandb:    final_val_mse 0.06415
wandb:     final_val_r2 -0.17975
wandb:   final_val_rmse 0.25328
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01407
wandb:       train_time 82.22639
wandb:         val_loss 0.05897
wandb:          val_mse 0.06415
wandb:           val_r2 -0.17975
wandb:         val_rmse 0.25328
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_102517-adswinn8
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_102517-adswinn8/logs
Control experiment for avg_subordinate_chain_len (ru, control=2) completed successfully
Running submetric avg_subordinate_chain_len control=3 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:27:07,906][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_subordinate_chain_len/control3
experiment_name: avg_subordinate_chain_len_control3_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_subordinate_chain_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:27:07,906][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:27:07,906][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-04-11 10:27:07,906][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:27:07,906][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:27:07,911][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:27:07,911][__main__][INFO] - Using submetric: avg_subordinate_chain_len
[2025-04-11 10:27:07,911][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:27:09,137][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:27:11,372][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:27:11,372][src.data.datasets][INFO] - Loading 'control_avg_subordinate_chain_len_seed3' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:27:11,426][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:01:41 2025).
[2025-04-11 10:27:11,454][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_subordinate_chain_len_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_subordinate_chain_len_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:01:41 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 66264.94 examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 28858.84 examples/s]
[2025-04-11 10:27:11,959][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:27:11,967][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:27:11,968][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:27:11,969][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:27:11,990][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:27:12,020][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:27:12,034][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:27:12,035][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:27:12,035][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:27:12,036][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:27:12,053][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:27:12,079][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:27:12,092][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:27:12,093][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:27:12,093][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:27:12,094][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:27:12,094][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:27:12,094][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-04-11 10:27:12,094][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-04-11 10:27:12,095][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-04-11 10:27:12,095][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:27:12,095][src.data.datasets][INFO] -   Mean: 0.0301, Std: 0.1168
[2025-04-11 10:27:12,095][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:27:12,095][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-11 10:27:12,095][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:27:12,095][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-04-11 10:27:12,095][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-04-11 10:27:12,095][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-04-11 10:27:12,096][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:27:12,096][src.data.datasets][INFO] -   Mean: 0.1273, Std: 0.2332
[2025-04-11 10:27:12,096][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:27:12,096][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-11 10:27:12,096][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:27:12,096][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_subordinate_chain_len'
[2025-04-11 10:27:12,096][src.data.datasets][INFO] - Selected feature name: 'avg_subordinate_chain_len' for task: 'single_submetric'
[2025-04-11 10:27:12,096][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_subordinate_chain_len):
[2025-04-11 10:27:12,096][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.6670
[2025-04-11 10:27:12,097][src.data.datasets][INFO] -   Mean: 0.1575, Std: 0.1943
[2025-04-11 10:27:12,097][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:27:12,097][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-04-11 10:27:12,097][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:27:12,097][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:27:12,097][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:27:12,097][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:27:16,432][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:27:16,435][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:27:16,435][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:27:16,435][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:19,  1.07s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:25,  2.86it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:15,  4.54it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:11,  5.93it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  7.05it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.91it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.54it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  9.02it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.37it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.61it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.78it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.91it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:05, 10.00it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.05it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.10it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.12it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.14it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.14it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.16it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.16it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.17it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.18it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.19it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.19it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.18it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.15it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.14it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.15it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.10it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.13it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.15it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.16it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.17it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.18it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.17it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.56it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.97it/s]
[2025-04-11 10:27:26,951][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0212
[2025-04-11 10:27:27,187][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0695, Metrics: {'mse': 0.07550694793462753, 'rmse': 0.2747852760513699, 'r2': -0.3885568380355835}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  4.99it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.03it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.01it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.72it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.82it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 2/10:  19%|█▊        | 14/75 [00:01<00:06,  9.91it/s]Epoch 2/10:  21%|██▏       | 16/75 [00:01<00:05, 10.00it/s]Epoch 2/10:  24%|██▍       | 18/75 [00:01<00:05, 10.05it/s]Epoch 2/10:  27%|██▋       | 20/75 [00:02<00:05, 10.10it/s]Epoch 2/10:  29%|██▉       | 22/75 [00:02<00:05, 10.09it/s]Epoch 2/10:  32%|███▏      | 24/75 [00:02<00:05, 10.11it/s]Epoch 2/10:  35%|███▍      | 26/75 [00:02<00:04, 10.14it/s]Epoch 2/10:  37%|███▋      | 28/75 [00:02<00:04, 10.16it/s]Epoch 2/10:  40%|████      | 30/75 [00:03<00:04, 10.18it/s]Epoch 2/10:  43%|████▎     | 32/75 [00:03<00:04, 10.17it/s]Epoch 2/10:  45%|████▌     | 34/75 [00:03<00:04, 10.17it/s]Epoch 2/10:  48%|████▊     | 36/75 [00:03<00:03, 10.18it/s]Epoch 2/10:  51%|█████     | 38/75 [00:03<00:03, 10.18it/s]Epoch 2/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.17it/s]Epoch 2/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.17it/s]Epoch 2/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.18it/s]Epoch 2/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.16it/s]Epoch 2/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.16it/s]Epoch 2/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.17it/s]Epoch 2/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.15it/s]Epoch 2/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.16it/s]Epoch 2/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.11it/s]Epoch 2/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.12it/s]Epoch 2/10:  80%|████████  | 60/75 [00:06<00:01, 10.14it/s]Epoch 2/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.14it/s]Epoch 2/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.14it/s]Epoch 2/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.15it/s]Epoch 2/10:  91%|█████████ | 68/75 [00:06<00:00, 10.16it/s]Epoch 2/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.17it/s]Epoch 2/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.16it/s]Epoch 2/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.17it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00,  9.99it/s]
[2025-04-11 10:27:35,102][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0174
[2025-04-11 10:27:35,362][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0630, Metrics: {'mse': 0.06853731721639633, 'rmse': 0.2617963277366517, 'r2': -0.2603868246078491}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.92it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.98it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 3/10:  11%|█         | 8/75 [00:00<00:07,  9.51it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.57it/s]Epoch 3/10:  13%|█▎        | 10/75 [00:01<00:06,  9.62it/s]Epoch 3/10:  16%|█▌        | 12/75 [00:01<00:06,  9.85it/s]Epoch 3/10:  19%|█▊        | 14/75 [00:01<00:06,  9.96it/s]Epoch 3/10:  21%|██▏       | 16/75 [00:01<00:05, 10.04it/s]Epoch 3/10:  24%|██▍       | 18/75 [00:01<00:05, 10.09it/s]Epoch 3/10:  27%|██▋       | 20/75 [00:02<00:05, 10.11it/s]Epoch 3/10:  29%|██▉       | 22/75 [00:02<00:05, 10.13it/s]Epoch 3/10:  32%|███▏      | 24/75 [00:02<00:05, 10.15it/s]Epoch 3/10:  35%|███▍      | 26/75 [00:02<00:04, 10.16it/s]Epoch 3/10:  37%|███▋      | 28/75 [00:02<00:04, 10.15it/s]Epoch 3/10:  40%|████      | 30/75 [00:03<00:04, 10.17it/s]Epoch 3/10:  43%|████▎     | 32/75 [00:03<00:04, 10.16it/s]Epoch 3/10:  45%|████▌     | 34/75 [00:03<00:04, 10.16it/s]Epoch 3/10:  48%|████▊     | 36/75 [00:03<00:03, 10.16it/s]Epoch 3/10:  51%|█████     | 38/75 [00:03<00:03, 10.17it/s]Epoch 3/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.16it/s]Epoch 3/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.12it/s]Epoch 3/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.12it/s]Epoch 3/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.14it/s]Epoch 3/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.14it/s]Epoch 3/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.14it/s]Epoch 3/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.14it/s]Epoch 3/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.15it/s]Epoch 3/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.15it/s]Epoch 3/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.16it/s]Epoch 3/10:  80%|████████  | 60/75 [00:06<00:01, 10.15it/s]Epoch 3/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.17it/s]Epoch 3/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.14it/s]Epoch 3/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.13it/s]Epoch 3/10:  91%|█████████ | 68/75 [00:06<00:00, 10.13it/s]Epoch 3/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.14it/s]Epoch 3/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.16it/s]Epoch 3/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.17it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:27:43,418][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0153
[2025-04-11 10:27:43,701][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0607, Metrics: {'mse': 0.06597399711608887, 'rmse': 0.25685403854346706, 'r2': -0.21324801445007324}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:32,  2.26it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:13,  5.32it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:09,  7.05it/s]Epoch 4/10:   9%|▉         | 7/75 [00:01<00:08,  8.10it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:07,  8.78it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.21it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.51it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:06,  9.71it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:02<00:05,  9.85it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:02<00:05,  9.94it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.02it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.07it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.10it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:03<00:04, 10.12it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:03<00:04, 10.14it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.14it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.09it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.11it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.08it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.11it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.12it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.12it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.14it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.10it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.11it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.08it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.11it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.09it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.10it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.12it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.15it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.65it/s]
[2025-04-11 10:27:51,855][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0151
[2025-04-11 10:27:52,129][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0600, Metrics: {'mse': 0.06519309431314468, 'rmse': 0.25532938395951355, 'r2': -0.19888734817504883}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.05it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.03it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.00it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.72it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.85it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.96it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.02it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.06it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.08it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.10it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.14it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.09it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.06it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.04it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.08it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.10it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.11it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.13it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.13it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.13it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.13it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.15it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.15it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.15it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.09it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.12it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.14it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.13it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.13it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:28:00,071][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0149
[2025-04-11 10:28:00,354][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0606, Metrics: {'mse': 0.06594501435756683, 'rmse': 0.25679761361345793, 'r2': -0.2127150297164917}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.57it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.59it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:08,  8.65it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.25it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:06,  9.56it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.76it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.89it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.04it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.07it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.10it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.12it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.13it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.12it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.13it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.15it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.10it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.08it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.11it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.09it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.07it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.09it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.10it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.08it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.12it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.08it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.07it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.09it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.11it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.12it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.13it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.13it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.13it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.90it/s]
[2025-04-11 10:28:07,930][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0144
[2025-04-11 10:28:08,211][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0594, Metrics: {'mse': 0.06456533819437027, 'rmse': 0.25409710386852163, 'r2': -0.18734312057495117}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  4.96it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.97it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.69it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.84it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.95it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.00it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05,  9.99it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.03it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.00it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.05it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.09it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.11it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.12it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.11it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.13it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.14it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.14it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.16it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.11it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.12it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.13it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.16it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.16it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.15it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.16it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.14it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.16it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.16it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:28:16,108][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0143
[2025-04-11 10:28:16,394][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0591, Metrics: {'mse': 0.06419059634208679, 'rmse': 0.25335863186812246, 'r2': -0.1804516315460205}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:15,  4.71it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.79it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.35it/s]Epoch 8/10:  11%|█         | 8/75 [00:00<00:07,  9.44it/s]Epoch 8/10:  13%|█▎        | 10/75 [00:01<00:06,  9.70it/s]Epoch 8/10:  16%|█▌        | 12/75 [00:01<00:06,  9.85it/s]Epoch 8/10:  19%|█▊        | 14/75 [00:01<00:06,  9.97it/s]Epoch 8/10:  21%|██▏       | 16/75 [00:01<00:05, 10.03it/s]Epoch 8/10:  24%|██▍       | 18/75 [00:01<00:05, 10.06it/s]Epoch 8/10:  27%|██▋       | 20/75 [00:02<00:05, 10.09it/s]Epoch 8/10:  29%|██▉       | 22/75 [00:02<00:05, 10.12it/s]Epoch 8/10:  32%|███▏      | 24/75 [00:02<00:05, 10.12it/s]Epoch 8/10:  35%|███▍      | 26/75 [00:02<00:04, 10.13it/s]Epoch 8/10:  37%|███▋      | 28/75 [00:02<00:04, 10.14it/s]Epoch 8/10:  40%|████      | 30/75 [00:03<00:04, 10.14it/s]Epoch 8/10:  43%|████▎     | 32/75 [00:03<00:04, 10.14it/s]Epoch 8/10:  45%|████▌     | 34/75 [00:03<00:04, 10.14it/s]Epoch 8/10:  48%|████▊     | 36/75 [00:03<00:03, 10.15it/s]Epoch 8/10:  51%|█████     | 38/75 [00:03<00:03, 10.09it/s]Epoch 8/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.12it/s]Epoch 8/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.07it/s]Epoch 8/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.10it/s]Epoch 8/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.06it/s]Epoch 8/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.04it/s]Epoch 8/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.07it/s]Epoch 8/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.08it/s]Epoch 8/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.10it/s]Epoch 8/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.12it/s]Epoch 8/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.13it/s]Epoch 8/10:  80%|████████  | 60/75 [00:06<00:01, 10.12it/s]Epoch 8/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.13it/s]Epoch 8/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.08it/s]Epoch 8/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.01it/s]Epoch 8/10:  91%|█████████ | 68/75 [00:06<00:00, 10.03it/s]Epoch 8/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.07it/s]Epoch 8/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.09it/s]Epoch 8/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.12it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00,  9.92it/s]
[2025-04-11 10:28:24,336][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0140
[2025-04-11 10:28:24,621][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0600, Metrics: {'mse': 0.06521301716566086, 'rmse': 0.2553683950015367, 'r2': -0.19925367832183838}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:14,  5.26it/s]Epoch 9/10:   3%|▎         | 2/75 [00:00<00:10,  7.29it/s]Epoch 9/10:   5%|▌         | 4/75 [00:00<00:08,  8.84it/s]Epoch 9/10:   8%|▊         | 6/75 [00:00<00:07,  9.42it/s]Epoch 9/10:  11%|█         | 8/75 [00:00<00:06,  9.70it/s]Epoch 9/10:  13%|█▎        | 10/75 [00:01<00:06,  9.86it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.85it/s]Epoch 9/10:  16%|█▌        | 12/75 [00:01<00:06,  9.82it/s]Epoch 9/10:  19%|█▊        | 14/75 [00:01<00:06,  9.96it/s]Epoch 9/10:  21%|██▏       | 16/75 [00:01<00:05, 10.01it/s]Epoch 9/10:  24%|██▍       | 18/75 [00:01<00:05, 10.05it/s]Epoch 9/10:  27%|██▋       | 20/75 [00:02<00:05, 10.07it/s]Epoch 9/10:  29%|██▉       | 22/75 [00:02<00:05, 10.09it/s]Epoch 9/10:  32%|███▏      | 24/75 [00:02<00:05, 10.12it/s]Epoch 9/10:  35%|███▍      | 26/75 [00:02<00:04, 10.13it/s]Epoch 9/10:  37%|███▋      | 28/75 [00:02<00:04, 10.13it/s]Epoch 9/10:  40%|████      | 30/75 [00:03<00:04, 10.13it/s]Epoch 9/10:  43%|████▎     | 32/75 [00:03<00:04, 10.13it/s]Epoch 9/10:  45%|████▌     | 34/75 [00:03<00:04, 10.07it/s]Epoch 9/10:  48%|████▊     | 36/75 [00:03<00:03, 10.09it/s]Epoch 9/10:  51%|█████     | 38/75 [00:03<00:03, 10.11it/s]Epoch 9/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.08it/s]Epoch 9/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.10it/s]Epoch 9/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.11it/s]Epoch 9/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.12it/s]Epoch 9/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.14it/s]Epoch 9/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.14it/s]Epoch 9/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.14it/s]Epoch 9/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.08it/s]Epoch 9/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.12it/s]Epoch 9/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.14it/s]Epoch 9/10:  80%|████████  | 60/75 [00:06<00:01, 10.14it/s]Epoch 9/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.13it/s]Epoch 9/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.15it/s]Epoch 9/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.15it/s]Epoch 9/10:  91%|█████████ | 68/75 [00:06<00:00, 10.15it/s]Epoch 9/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.14it/s]Epoch 9/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.15it/s]Epoch 9/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.16it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00,  9.96it/s]
[2025-04-11 10:28:32,152][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0140
[2025-04-11 10:28:32,432][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0590, Metrics: {'mse': 0.06407749652862549, 'rmse': 0.25313533243825426, 'r2': -0.17837166786193848}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:16,  4.56it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.69it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:08,  8.73it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.29it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:01<00:06,  9.61it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.78it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06,  9.90it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:06,  9.99it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.04it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.07it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.09it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.11it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.12it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.11it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.11it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.13it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.13it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.13it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.13it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.15it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.14it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.13it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.14it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.14it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:28:40,495][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0139
[2025-04-11 10:28:40,786][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0591, Metrics: {'mse': 0.0642482191324234, 'rmse': 0.25347232419422716, 'r2': -0.1815112829208374}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▂▂▁▁▁
wandb:     best_val_mse █▄▂▂▁▁▁
wandb:      best_val_r2 ▁▅▇▇███
wandb:    best_val_rmse █▄▂▂▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▂▂▂▁▁▂▁▁
wandb:          val_mse █▄▂▂▂▁▁▂▁▁
wandb:           val_r2 ▁▅▇▇▇██▇██
wandb:         val_rmse █▄▂▂▂▁▁▂▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05898
wandb:     best_val_mse 0.06408
wandb:      best_val_r2 -0.17837
wandb:    best_val_rmse 0.25314
wandb:            epoch 10
wandb:   final_test_mse 0.05319
wandb:    final_test_r2 -0.40826
wandb:  final_test_rmse 0.23062
wandb:  final_train_mse 0.01338
wandb:   final_train_r2 0.01942
wandb: final_train_rmse 0.11569
wandb:    final_val_mse 0.06408
wandb:     final_val_r2 -0.17837
wandb:   final_val_rmse 0.25314
wandb:    learning_rate 1e-05
wandb:       train_loss 0.0139
wandb:       train_time 82.19695
wandb:         val_loss 0.05911
wandb:          val_mse 0.06425
wandb:           val_r2 -0.18151
wandb:         val_rmse 0.25347
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_102707-bd5vpodt
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_102707-bd5vpodt/logs
Control experiment for avg_subordinate_chain_len (ru, control=3) completed successfully
Running submetric avg_verb_edges control=1 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:28:59,052][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_verb_edges/control1
experiment_name: avg_verb_edges_control1_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:28:59,052][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:28:59,052][__main__][INFO] - Using submetric: avg_verb_edges
[2025-04-11 10:28:59,052][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:28:59,053][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:28:59,196][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:28:59,196][__main__][INFO] - Using submetric: avg_verb_edges
[2025-04-11 10:28:59,196][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:29:00,314][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:29:02,537][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:29:02,537][src.data.datasets][INFO] - Loading 'control_avg_verb_edges_seed1' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:29:02,590][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_verb_edges_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:03:03 2025).
[2025-04-11 10:29:02,618][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_verb_edges_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:03:03 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 76201.20 examples/s]
[2025-04-11 10:29:02,934][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:29:02,943][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:29:02,944][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:29:02,960][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:29:03,059][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:29:03,185][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:29:03,212][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:29:03,213][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:29:03,213][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:29:03,215][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:29:03,302][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:29:03,427][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:29:03,442][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:29:03,444][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:29:03,445][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:29:03,449][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:29:03,450][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:29:03,450][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-04-11 10:29:03,450][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-04-11 10:29:03,450][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-04-11 10:29:03,450][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:29:03,450][src.data.datasets][INFO] -   Mean: 0.3433, Std: 0.2277
[2025-04-11 10:29:03,450][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:29:03,450][src.data.datasets][INFO] - Sample label: 0.4000000059604645
[2025-04-11 10:29:03,451][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:29:03,451][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-04-11 10:29:03,451][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-04-11 10:29:03,451][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-04-11 10:29:03,451][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8000
[2025-04-11 10:29:03,451][src.data.datasets][INFO] -   Mean: 0.3578, Std: 0.2206
[2025-04-11 10:29:03,451][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:29:03,451][src.data.datasets][INFO] - Sample label: 0.6000000238418579
[2025-04-11 10:29:03,451][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:29:03,451][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-04-11 10:29:03,452][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-04-11 10:29:03,452][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-04-11 10:29:03,452][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-04-11 10:29:03,452][src.data.datasets][INFO] -   Mean: 0.4020, Std: 0.2023
[2025-04-11 10:29:03,452][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:29:03,452][src.data.datasets][INFO] - Sample label: 0.30000001192092896
[2025-04-11 10:29:03,452][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:29:03,452][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:29:03,453][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:29:03,453][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:29:07,561][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:29:07,563][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:29:07,563][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:29:07,563][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:12,  1.03it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:23,  3.06it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:14,  4.78it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.55it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.92it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:01<00:08,  7.89it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.57it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  9.06it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.40it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.64it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.81it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.92it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05,  9.99it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.05it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.10it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.12it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.14it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:04, 10.11it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.14it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.15it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.14it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.11it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:03, 10.13it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.14it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.15it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.16it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.18it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.18it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.18it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.19it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.13it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.14it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.15it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.11it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.11it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.13it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.15it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.17it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.07it/s]
[2025-04-11 10:29:17,457][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1511
[2025-04-11 10:29:17,698][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0671, Metrics: {'mse': 0.06612755358219147, 'rmse': 0.257152782567468, 'r2': -0.35861289501190186}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.08it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.09it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.06it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.76it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.90it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.00it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.07it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.11it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.12it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.14it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.16it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.17it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.17it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.18it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.18it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.19it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.18it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.19it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.19it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.19it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.19it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.16it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.16it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.17it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.17it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.18it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.17it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.18it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.19it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.18it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.18it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.18it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.18it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.18it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.58it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.01it/s]
[2025-04-11 10:29:25,614][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0861
[2025-04-11 10:29:25,854][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0692, Metrics: {'mse': 0.0692644864320755, 'rmse': 0.26318147053331, 'r2': -0.42306220531463623}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:13,  5.49it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.30it/s]Epoch 3/10:   5%|▌         | 4/75 [00:00<00:08,  8.76it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.08it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.93it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.02it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.08it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.10it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.12it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.14it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.15it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.16it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.17it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.17it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.16it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.16it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.12it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.13it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.09it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.10it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.09it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.11it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.13it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.13it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.15it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.16it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.16it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.17it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.17it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.17it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.18it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.17it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.12it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.12it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.00it/s]
[2025-04-11 10:29:33,358][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0678
[2025-04-11 10:29:33,617][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0553, Metrics: {'mse': 0.052413150668144226, 'rmse': 0.2289391855234578, 'r2': -0.07684588432312012}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  4.95it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.97it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 4/10:   8%|▊         | 6/75 [00:00<00:07,  9.15it/s]Epoch 4/10:  11%|█         | 8/75 [00:00<00:07,  9.56it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.58it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.82it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.92it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.01it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.04it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.09it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.05it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.08it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.10it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.12it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.07it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.10it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.07it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.09it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.12it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.10it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.11it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.09it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.12it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.13it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.13it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.14it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.15it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.10it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.12it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.12it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.13it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.10it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.12it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.12it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:29:41,693][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0585
[2025-04-11 10:29:41,953][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0712, Metrics: {'mse': 0.06530553102493286, 'rmse': 0.2555494688410306, 'r2': -0.341724157333374}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  5.17it/s]Epoch 5/10:   3%|▎         | 2/75 [00:00<00:10,  7.08it/s]Epoch 5/10:   5%|▌         | 4/75 [00:00<00:08,  8.66it/s]Epoch 5/10:   8%|▊         | 6/75 [00:00<00:07,  9.31it/s]Epoch 5/10:  11%|█         | 8/75 [00:00<00:06,  9.64it/s]Epoch 5/10:  13%|█▎        | 10/75 [00:01<00:06,  9.83it/s]Epoch 5/10:  16%|█▌        | 12/75 [00:01<00:06,  9.93it/s]Epoch 5/10:  19%|█▊        | 14/75 [00:01<00:06, 10.01it/s]Epoch 5/10:  21%|██▏       | 16/75 [00:01<00:05,  9.94it/s]Epoch 5/10:  24%|██▍       | 18/75 [00:01<00:05, 10.01it/s]Epoch 5/10:  27%|██▋       | 20/75 [00:02<00:05, 10.07it/s]Epoch 5/10:  29%|██▉       | 22/75 [00:02<00:05, 10.09it/s]Epoch 5/10:  32%|███▏      | 24/75 [00:02<00:05, 10.11it/s]Epoch 5/10:  35%|███▍      | 26/75 [00:02<00:04, 10.12it/s]Epoch 5/10:  37%|███▋      | 28/75 [00:02<00:04, 10.08it/s]Epoch 5/10:  40%|████      | 30/75 [00:03<00:04, 10.10it/s]Epoch 5/10:  43%|████▎     | 32/75 [00:03<00:04, 10.10it/s]Epoch 5/10:  45%|████▌     | 34/75 [00:03<00:04, 10.12it/s]Epoch 5/10:  48%|████▊     | 36/75 [00:03<00:03, 10.13it/s]Epoch 5/10:  51%|█████     | 38/75 [00:03<00:03, 10.13it/s]Epoch 5/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.09it/s]Epoch 5/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.11it/s]Epoch 5/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.13it/s]Epoch 5/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.15it/s]Epoch 5/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.09it/s]Epoch 5/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.11it/s]Epoch 5/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.14it/s]Epoch 5/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.15it/s]Epoch 5/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.15it/s]Epoch 5/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.16it/s]Epoch 5/10:  80%|████████  | 60/75 [00:06<00:01, 10.16it/s]Epoch 5/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.17it/s]Epoch 5/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.16it/s]Epoch 5/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.16it/s]Epoch 5/10:  91%|█████████ | 68/75 [00:06<00:00, 10.17it/s]Epoch 5/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.17it/s]Epoch 5/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.16it/s]Epoch 5/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.17it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.96it/s]
[2025-04-11 10:29:49,485][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0573
[2025-04-11 10:29:49,761][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0544, Metrics: {'mse': 0.05270926281809807, 'rmse': 0.22958497951324705, 'r2': -0.08292949199676514}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  4.97it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.98it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.68it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.84it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.95it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:06,  9.99it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.04it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.08it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.11it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.12it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.09it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.11it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.13it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.14it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.14it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.14it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.16it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.14it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.14it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.15it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.14it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.16it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.16it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.16it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.16it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.17it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:29:57,627][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0558
[2025-04-11 10:29:57,904][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0546, Metrics: {'mse': 0.0530368834733963, 'rmse': 0.23029738051787801, 'r2': -0.08966064453125}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.03it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:08,  8.04it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.01it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.36it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.64it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.79it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.91it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:06,  9.95it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.03it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.06it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.09it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.12it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.13it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.14it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.15it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.15it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.16it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.16it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.17it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.16it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.16it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.15it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.16it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.16it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.15it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.16it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.16it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.14it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.14it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.16it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:30:05,423][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0546
[2025-04-11 10:30:05,694][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0608, Metrics: {'mse': 0.05584564432501793, 'rmse': 0.23631683038881918, 'r2': -0.14736759662628174}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:13,  5.53it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:08,  8.34it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  9.18it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.90it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06,  9.98it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.01it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.05it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.07it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.09it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.11it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.13it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.15it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.15it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.16it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.16it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.15it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.15it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.16it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.15it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.11it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.07it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.09it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.11it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.14it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.14it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.15it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.16it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.16it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.00it/s]
[2025-04-11 10:30:13,194][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0551
[2025-04-11 10:30:13,474][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0540, Metrics: {'mse': 0.0519888736307621, 'rmse': 0.22801068753626902, 'r2': -0.06812894344329834}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:15,  4.90it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.96it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.94it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.68it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.84it/s]Epoch 9/10:  16%|█▌        | 12/75 [00:01<00:06,  9.83it/s]Epoch 9/10:  19%|█▊        | 14/75 [00:01<00:06,  9.91it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:06,  9.89it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05,  9.97it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.03it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.06it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.09it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.11it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.12it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.14it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.15it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.15it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.15it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.15it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.14it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.15it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.16it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.13it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.12it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.04it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.04it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.02it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.07it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.06it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.09it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.10it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.12it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.14it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.14it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.56it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:30:21,396][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0526
[2025-04-11 10:30:21,752][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0574, Metrics: {'mse': 0.053251173347234726, 'rmse': 0.23076215752855736, 'r2': -0.09406328201293945}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:14,  5.02it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:08,  8.01it/s]Epoch 10/10:   5%|▌         | 4/75 [00:00<00:08,  8.59it/s]Epoch 10/10:   8%|▊         | 6/75 [00:00<00:07,  9.29it/s]Epoch 10/10:  11%|█         | 8/75 [00:00<00:06,  9.63it/s]Epoch 10/10:  13%|█▎        | 10/75 [00:01<00:06,  9.82it/s]Epoch 10/10:  16%|█▌        | 12/75 [00:01<00:06,  9.93it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.03it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.07it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.10it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.11it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.14it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.13it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.09it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.09it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.11it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.13it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.13it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.13it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.15it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.11it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.05it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.08it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.06it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.08it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.04it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.06it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.08it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.10it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.11it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.13it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:30:29,290][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0522
[2025-04-11 10:30:29,565][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0655, Metrics: {'mse': 0.05983457341790199, 'rmse': 0.24461106560804233, 'r2': -0.22932147979736328}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁▁
wandb:     best_val_mse █▁▁▁
wandb:      best_val_r2 ▁███
wandb:    best_val_rmse █▁▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆▇▂█▁▁▄▁▂▆
wandb:          val_mse ▇█▁▆▁▁▃▁▂▄
wandb:           val_r2 ▂▁█▃██▆█▇▅
wandb:         val_rmse ▇█▁▆▁▁▃▁▂▄
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.054
wandb:     best_val_mse 0.05199
wandb:      best_val_r2 -0.06813
wandb:    best_val_rmse 0.22801
wandb:            epoch 10
wandb:   final_test_mse 0.04814
wandb:    final_test_r2 -0.1762
wandb:  final_test_rmse 0.21941
wandb:  final_train_mse 0.04792
wandb:   final_train_r2 0.07539
wandb: final_train_rmse 0.21891
wandb:    final_val_mse 0.05199
wandb:     final_val_r2 -0.06813
wandb:   final_val_rmse 0.22801
wandb:    learning_rate 1e-05
wandb:       train_loss 0.05223
wandb:       train_time 80.38112
wandb:         val_loss 0.06555
wandb:          val_mse 0.05983
wandb:           val_r2 -0.22932
wandb:         val_rmse 0.24461
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_102859-1ek3lswj
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_102859-1ek3lswj/logs
Control experiment for avg_verb_edges (ru, control=1) completed successfully
Running submetric avg_verb_edges control=2 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:30:47,467][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_verb_edges/control2
experiment_name: avg_verb_edges_control2_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 2
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:30:47,467][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:30:47,467][__main__][INFO] - Using submetric: avg_verb_edges
[2025-04-11 10:30:47,467][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:30:47,467][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:30:47,471][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:30:47,471][__main__][INFO] - Using submetric: avg_verb_edges
[2025-04-11 10:30:47,472][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:30:49,089][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:30:51,426][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:30:51,427][src.data.datasets][INFO] - Loading 'control_avg_verb_edges_seed2' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:30:51,477][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_verb_edges_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:04:00 2025).
[2025-04-11 10:30:51,504][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_verb_edges_seed2' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed2/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:04:00 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 78670.42 examples/s]
[2025-04-11 10:30:51,800][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:30:51,809][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:30:51,810][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:30:51,811][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:30:51,833][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:30:51,863][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:30:51,878][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:30:51,879][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:30:51,879][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:30:51,880][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:30:51,897][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:30:51,926][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:30:51,938][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:30:51,940][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:30:51,940][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:30:51,941][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:30:51,942][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:30:51,942][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-04-11 10:30:51,942][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-04-11 10:30:51,942][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-04-11 10:30:51,942][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:30:51,943][src.data.datasets][INFO] -   Mean: 0.3433, Std: 0.2277
[2025-04-11 10:30:51,943][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:30:51,943][src.data.datasets][INFO] - Sample label: 0.3330000042915344
[2025-04-11 10:30:51,943][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:30:51,943][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-04-11 10:30:51,943][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-04-11 10:30:51,943][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-04-11 10:30:51,943][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8000
[2025-04-11 10:30:51,943][src.data.datasets][INFO] -   Mean: 0.3578, Std: 0.2206
[2025-04-11 10:30:51,944][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:30:51,944][src.data.datasets][INFO] - Sample label: 0.6000000238418579
[2025-04-11 10:30:51,944][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:30:51,944][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-04-11 10:30:51,944][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-04-11 10:30:51,944][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-04-11 10:30:51,944][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-04-11 10:30:51,944][src.data.datasets][INFO] -   Mean: 0.4020, Std: 0.2023
[2025-04-11 10:30:51,944][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:30:51,944][src.data.datasets][INFO] - Sample label: 0.30000001192092896
[2025-04-11 10:30:51,945][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:30:51,945][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:30:51,945][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:30:51,945][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:30:56,283][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:30:56,285][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:30:56,285][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:30:56,285][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:16,  1.03s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:24,  2.93it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:15,  4.62it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.40it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.78it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:01<00:08,  7.77it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.49it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.99it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.35it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.60it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.77it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.84it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05,  9.89it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04,  9.94it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04,  9.98it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.00it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.00it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:04, 10.06it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.09it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.12it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.14it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.15it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:03, 10.16it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.17it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.18it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.17it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.18it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.18it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.18it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.16it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.18it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.16it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.12it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.07it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.07it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.10it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.13it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.15it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.99it/s]
[2025-04-11 10:31:06,392][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1609
[2025-04-11 10:31:06,628][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1125, Metrics: {'mse': 0.11931334435939789, 'rmse': 0.34541763759165206, 'r2': -1.4513328075408936}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:13,  5.53it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.35it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.21it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.60it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.93it/s]Epoch 2/10:  16%|█▌        | 12/75 [00:01<00:06,  9.95it/s]Epoch 2/10:  19%|█▊        | 14/75 [00:01<00:06, 10.04it/s]Epoch 2/10:  21%|██▏       | 16/75 [00:01<00:05, 10.03it/s]Epoch 2/10:  24%|██▍       | 18/75 [00:01<00:05, 10.02it/s]Epoch 2/10:  27%|██▋       | 20/75 [00:02<00:05, 10.06it/s]Epoch 2/10:  29%|██▉       | 22/75 [00:02<00:05, 10.10it/s]Epoch 2/10:  32%|███▏      | 24/75 [00:02<00:05, 10.13it/s]Epoch 2/10:  35%|███▍      | 26/75 [00:02<00:04, 10.12it/s]Epoch 2/10:  37%|███▋      | 28/75 [00:02<00:04, 10.13it/s]Epoch 2/10:  40%|████      | 30/75 [00:03<00:04, 10.15it/s]Epoch 2/10:  43%|████▎     | 32/75 [00:03<00:04, 10.12it/s]Epoch 2/10:  45%|████▌     | 34/75 [00:03<00:04, 10.14it/s]Epoch 2/10:  48%|████▊     | 36/75 [00:03<00:03, 10.14it/s]Epoch 2/10:  51%|█████     | 38/75 [00:03<00:03, 10.14it/s]Epoch 2/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.11it/s]Epoch 2/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.14it/s]Epoch 2/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.15it/s]Epoch 2/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.15it/s]Epoch 2/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.16it/s]Epoch 2/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.16it/s]Epoch 2/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.16it/s]Epoch 2/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.16it/s]Epoch 2/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.17it/s]Epoch 2/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.17it/s]Epoch 2/10:  80%|████████  | 60/75 [00:05<00:01, 10.16it/s]Epoch 2/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.16it/s]Epoch 2/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.17it/s]Epoch 2/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.17it/s]Epoch 2/10:  91%|█████████ | 68/75 [00:06<00:00, 10.16it/s]Epoch 2/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.17it/s]Epoch 2/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.17it/s]Epoch 2/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.15it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.01it/s]
[2025-04-11 10:31:14,542][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0941
[2025-04-11 10:31:14,786][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1094, Metrics: {'mse': 0.11093607544898987, 'rmse': 0.33307067635712073, 'r2': -1.279219150543213}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.82it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.90it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.92it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.69it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.86it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06,  9.96it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.03it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.07it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.10it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.12it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.14it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.15it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.15it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.16it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.16it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.16it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.15it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.15it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.15it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.14it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.15it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.16it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.16it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.13it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.13it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.15it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.16it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.16it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.16it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.16it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00,  9.99it/s]
[2025-04-11 10:31:22,836][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0799
[2025-04-11 10:31:23,108][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0556, Metrics: {'mse': 0.058267269283533096, 'rmse': 0.24138614144878554, 'r2': -0.19712066650390625}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.90it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.95it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.95it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.69it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.85it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.01it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.06it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.09it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.11it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.14it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.15it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.14it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.15it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.16it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.16it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.10it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.11it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.13it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.14it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.09it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.11it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.13it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.13it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.10it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.12it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.14it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.14it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.16it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.16it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:31:30,979][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0653
[2025-04-11 10:31:31,249][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0493, Metrics: {'mse': 0.051120560616254807, 'rmse': 0.22609856394115999, 'r2': -0.050289154052734375}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:14,  4.99it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.00it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.70it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.85it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.01it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.06it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.09it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.11it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.08it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.05it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.07it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.09it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.11it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.13it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.14it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.14it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.16it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.15it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.16it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.15it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.17it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.16it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.16it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.16it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.16it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.17it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:31:39,163][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0572
[2025-04-11 10:31:39,434][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0520, Metrics: {'mse': 0.05098351463675499, 'rmse': 0.2257952936550162, 'r2': -0.04747354984283447}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.86it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.90it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.89it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.38it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.65it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.82it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.94it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.01it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.05it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.10it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.12it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.14it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.16it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.15it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.15it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.15it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.15it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.14it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.14it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.15it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.14it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.14it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.13it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.13it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.14it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.14it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.14it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:31:46,960][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0550
[2025-04-11 10:31:47,236][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0515, Metrics: {'mse': 0.04994601756334305, 'rmse': 0.223486056753756, 'r2': -0.026157736778259277}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.18it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:08,  8.11it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.06it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.73it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.88it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.98it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.03it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.06it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.11it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.12it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.12it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.14it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.13it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.15it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.15it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.15it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.15it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.16it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.15it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.16it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.16it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.16it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.17it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.17it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.16it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.16it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.16it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.17it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.58it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.99it/s]
[2025-04-11 10:31:54,743][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0538
[2025-04-11 10:31:55,005][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0508, Metrics: {'mse': 0.04952806979417801, 'rmse': 0.22254902784370462, 'r2': -0.017570853233337402}
[2025-04-11 10:31:55,005][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ██▂▁
wandb:     best_val_mse █▇▂▁
wandb:      best_val_r2 ▁▂▇█
wandb:    best_val_rmse █▇▂▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss ██▂▁▁▁▁
wandb:          val_mse █▇▂▁▁▁▁
wandb:           val_r2 ▁▂▇████
wandb:         val_rmse █▇▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.0493
wandb:     best_val_mse 0.05112
wandb:      best_val_r2 -0.05029
wandb:    best_val_rmse 0.2261
wandb:            epoch 7
wandb:   final_test_mse 0.05292
wandb:    final_test_r2 -0.29306
wandb:  final_test_rmse 0.23005
wandb:  final_train_mse 0.05322
wandb:   final_train_r2 -0.02694
wandb: final_train_rmse 0.2307
wandb:    final_val_mse 0.05112
wandb:     final_val_r2 -0.05029
wandb:   final_val_rmse 0.2261
wandb:    learning_rate 1e-05
wandb:       train_loss 0.05378
wandb:       train_time 56.95858
wandb:         val_loss 0.05084
wandb:          val_mse 0.04953
wandb:           val_r2 -0.01757
wandb:         val_rmse 0.22255
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_103047-dsc9azt1
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_103047-dsc9azt1/logs
Control experiment for avg_verb_edges (ru, control=2) completed successfully
Running submetric avg_verb_edges control=3 for ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-11 10:32:12,168][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/submetric_output/ru/avg_verb_edges/control3
experiment_name: avg_verb_edges_control3_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ru
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_verb_edges
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 3
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-04-11 10:32:12,168][__main__][INFO] - Normalized task: single_submetric
[2025-04-11 10:32:12,168][__main__][INFO] - Using submetric: avg_verb_edges
[2025-04-11 10:32:12,168][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-11 10:32:12,169][__main__][INFO] - Determined Task Type: regression
[2025-04-11 10:32:12,173][__main__][INFO] - Running LM probe experiment for task 'single_submetric' (type: regression) on languages: ['ru']
[2025-04-11 10:32:12,173][__main__][INFO] - Using submetric: avg_verb_edges
[2025-04-11 10:32:12,174][__main__][INFO] - Processing language: ru
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-11 10:32:13,825][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'single_submetric', submetric: 'avg_verb_edges'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-11 10:32:16,087][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-11 10:32:16,088][src.data.datasets][INFO] - Loading 'control_avg_verb_edges_seed3' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:32:16,130][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_avg_verb_edges_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:04:59 2025).
[2025-04-11 10:32:16,155][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_avg_verb_edges_seed3' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_avg_verb_edges_seed3/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Fri Apr 11 07:04:59 2025).
Filter:   0%|          | 0/7460 [00:00<?, ? examples/s]Filter: 100%|██████████| 7460/7460 [00:00<00:00, 101250.06 examples/s]
[2025-04-11 10:32:16,385][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-11 10:32:16,395][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:32:16,396][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-11 10:32:16,397][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:32:16,421][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:32:16,447][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:32:16,460][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-11 10:32:16,461][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:32:16,461][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-11 10:32:16,462][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-11 10:32:16,480][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:32:16,505][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-11 10:32:16,517][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-11 10:32:16,519][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-11 10:32:16,519][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-11 10:32:16,520][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-11 10:32:16,520][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:32:16,520][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-04-11 10:32:16,520][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-04-11 10:32:16,521][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-04-11 10:32:16,521][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-11 10:32:16,521][src.data.datasets][INFO] -   Mean: 0.3433, Std: 0.2277
[2025-04-11 10:32:16,521][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-11 10:32:16,521][src.data.datasets][INFO] - Sample label: 0.4000000059604645
[2025-04-11 10:32:16,521][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:32:16,521][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-04-11 10:32:16,521][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-04-11 10:32:16,522][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-04-11 10:32:16,522][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8000
[2025-04-11 10:32:16,522][src.data.datasets][INFO] -   Mean: 0.3578, Std: 0.2206
[2025-04-11 10:32:16,522][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-11 10:32:16,522][src.data.datasets][INFO] - Sample label: 0.6000000238418579
[2025-04-11 10:32:16,522][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-04-11 10:32:16,522][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_verb_edges'
[2025-04-11 10:32:16,522][src.data.datasets][INFO] - Selected feature name: 'avg_verb_edges' for task: 'single_submetric'
[2025-04-11 10:32:16,522][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_verb_edges):
[2025-04-11 10:32:16,522][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8330
[2025-04-11 10:32:16,523][src.data.datasets][INFO] -   Mean: 0.4020, Std: 0.2023
[2025-04-11 10:32:16,523][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-11 10:32:16,523][src.data.datasets][INFO] - Sample label: 0.30000001192092896
[2025-04-11 10:32:16,523][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-11 10:32:16,523][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-11 10:32:16,523][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-11 10:32:16,524][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-11 10:32:20,671][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-11 10:32:20,673][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-11 10:32:20,673][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-11 10:32:20,673][__main__][INFO] - Successfully created model for ru
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:00<01:11,  1.04it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:23,  3.10it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:14,  4.83it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:10,  6.19it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  7.25it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:01<00:08,  7.69it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.47it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.73it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  9.22it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.55it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.61it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.82it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.94it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05,  9.96it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.02it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.07it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.07it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.10it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:04, 10.13it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.15it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.13it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.14it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.16it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:03, 10.17it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.14it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.15it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.16it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.18it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.16it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.17it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.18it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.19it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.15it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.15it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.15it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.17it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.18it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.19it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.19it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  9.08it/s]
[2025-04-11 10:32:30,546][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1474
[2025-04-11 10:32:30,809][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1033, Metrics: {'mse': 0.10980384796857834, 'rmse': 0.3313666367765143, 'r2': -1.2559571266174316}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:12,  5.80it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.51it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.31it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.65it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.02it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.04it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.07it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.07it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.11it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.13it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.16it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.15it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.15it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.17it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.16it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.16it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.11it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.12it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.12it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.13it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.13it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.08it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.12it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.14it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.15it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.16it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.17it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.17it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.17it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.18it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.18it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.18it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.18it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.59it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.04it/s]
[2025-04-11 10:32:38,687][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0838
[2025-04-11 10:32:38,914][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0564, Metrics: {'mse': 0.05764539912343025, 'rmse': 0.24009456287769254, 'r2': -0.18434417247772217}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:14,  5.15it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.12it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.75it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.88it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06,  9.98it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.03it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.07it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.08it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.10it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.13it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.14it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.16it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.16it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.16it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.17it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.17it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.16it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.11it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.14it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.15it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.15it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.16it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.15it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.14it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.16it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.17it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.16it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.17it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.18it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.18it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.58it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.01it/s]
[2025-04-11 10:32:46,966][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0753
[2025-04-11 10:32:47,225][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0529, Metrics: {'mse': 0.05125400051474571, 'rmse': 0.22639346393998594, 'r2': -0.05303072929382324}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:13,  5.29it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.19it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.09it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.70it/s]Epoch 4/10:  13%|█▎        | 10/75 [00:01<00:06,  9.76it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.81it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.93it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:06,  9.98it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.05it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.09it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.12it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.14it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.15it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.15it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.16it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.16it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.16it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.16it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.17it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.16it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.15it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.15it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.16it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.12it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.07it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.11it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.12it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.13it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.15it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.15it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.17it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.16it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.17it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.17it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.99it/s]
[2025-04-11 10:32:55,083][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0810
[2025-04-11 10:32:55,335][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0700, Metrics: {'mse': 0.0684598982334137, 'rmse': 0.26164842486323836, 'r2': -0.40653157234191895}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:13,  5.31it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.21it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.11it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.77it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.91it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.99it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.05it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.10it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.12it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.11it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.12it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.14it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.10it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.07it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.09it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.09it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.12it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.12it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.14it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.15it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.15it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.14it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.15it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.16it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.16it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.16it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.17it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.17it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.15it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.16it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.16it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.15it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.15it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.16it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.57it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.99it/s]
[2025-04-11 10:33:02,841][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0617
[2025-04-11 10:33:03,109][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0510, Metrics: {'mse': 0.04916903004050255, 'rmse': 0.22174090745846275, 'r2': -0.01019430160522461}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:14,  5.22it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:08,  8.15it/s]Epoch 6/10:   5%|▌         | 4/75 [00:00<00:08,  8.65it/s]Epoch 6/10:   8%|▊         | 6/75 [00:00<00:07,  9.32it/s]Epoch 6/10:  11%|█         | 8/75 [00:00<00:06,  9.65it/s]Epoch 6/10:  13%|█▎        | 10/75 [00:01<00:06,  9.85it/s]Epoch 6/10:  16%|█▌        | 12/75 [00:01<00:06,  9.94it/s]Epoch 6/10:  19%|█▊        | 14/75 [00:01<00:06,  9.99it/s]Epoch 6/10:  21%|██▏       | 16/75 [00:01<00:05, 10.04it/s]Epoch 6/10:  24%|██▍       | 18/75 [00:01<00:05, 10.08it/s]Epoch 6/10:  27%|██▋       | 20/75 [00:02<00:05, 10.10it/s]Epoch 6/10:  29%|██▉       | 22/75 [00:02<00:05, 10.11it/s]Epoch 6/10:  32%|███▏      | 24/75 [00:02<00:05, 10.14it/s]Epoch 6/10:  35%|███▍      | 26/75 [00:02<00:04, 10.15it/s]Epoch 6/10:  37%|███▋      | 28/75 [00:02<00:04, 10.15it/s]Epoch 6/10:  40%|████      | 30/75 [00:03<00:04, 10.09it/s]Epoch 6/10:  43%|████▎     | 32/75 [00:03<00:04, 10.05it/s]Epoch 6/10:  45%|████▌     | 34/75 [00:03<00:04, 10.07it/s]Epoch 6/10:  48%|████▊     | 36/75 [00:03<00:03, 10.10it/s]Epoch 6/10:  51%|█████     | 38/75 [00:03<00:03, 10.11it/s]Epoch 6/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.12it/s]Epoch 6/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.14it/s]Epoch 6/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.09it/s]Epoch 6/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.11it/s]Epoch 6/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.12it/s]Epoch 6/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.12it/s]Epoch 6/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.14it/s]Epoch 6/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.15it/s]Epoch 6/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.14it/s]Epoch 6/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.15it/s]Epoch 6/10:  80%|████████  | 60/75 [00:06<00:01, 10.16it/s]Epoch 6/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.14it/s]Epoch 6/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.13it/s]Epoch 6/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.12it/s]Epoch 6/10:  91%|█████████ | 68/75 [00:06<00:00, 10.13it/s]Epoch 6/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.15it/s]Epoch 6/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.15it/s]Epoch 6/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.15it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-11 10:33:11,046][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0596
[2025-04-11 10:33:11,303][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0518, Metrics: {'mse': 0.048957888036966324, 'rmse': 0.22126429453702268, 'r2': -0.00585627555847168}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.22it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:08,  8.15it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.08it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.74it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.88it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.97it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.03it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.07it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.10it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.12it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.12it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.13it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.14it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.12it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.12it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.11it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.13it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.14it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.13it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.09it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.11it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.12it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.09it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.10it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.07it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.06it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.06it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.09it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.11it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.10it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.12it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.13it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.13it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.12it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.13it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.15it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.55it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.97it/s]
[2025-04-11 10:33:18,827][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0563
[2025-04-11 10:33:19,106][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0525, Metrics: {'mse': 0.05019998922944069, 'rmse': 0.2240535409884001, 'r2': -0.03137564659118652}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.60it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.72it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.81it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.34it/s]Epoch 8/10:  11%|█         | 8/75 [00:00<00:07,  9.48it/s]Epoch 8/10:  13%|█▎        | 10/75 [00:01<00:06,  9.74it/s]Epoch 8/10:  16%|█▌        | 12/75 [00:01<00:06,  9.86it/s]Epoch 8/10:  19%|█▊        | 14/75 [00:01<00:06,  9.96it/s]Epoch 8/10:  21%|██▏       | 16/75 [00:01<00:05, 10.03it/s]Epoch 8/10:  24%|██▍       | 18/75 [00:01<00:05, 10.08it/s]Epoch 8/10:  27%|██▋       | 20/75 [00:02<00:05, 10.10it/s]Epoch 8/10:  29%|██▉       | 22/75 [00:02<00:05, 10.12it/s]Epoch 8/10:  32%|███▏      | 24/75 [00:02<00:05, 10.14it/s]Epoch 8/10:  35%|███▍      | 26/75 [00:02<00:04, 10.14it/s]Epoch 8/10:  37%|███▋      | 28/75 [00:02<00:04, 10.14it/s]Epoch 8/10:  40%|████      | 30/75 [00:03<00:04, 10.15it/s]Epoch 8/10:  43%|████▎     | 32/75 [00:03<00:04, 10.16it/s]Epoch 8/10:  45%|████▌     | 34/75 [00:03<00:04, 10.15it/s]Epoch 8/10:  48%|████▊     | 36/75 [00:03<00:03, 10.16it/s]Epoch 8/10:  51%|█████     | 38/75 [00:03<00:03, 10.16it/s]Epoch 8/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.16it/s]Epoch 8/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.16it/s]Epoch 8/10:  59%|█████▊    | 44/75 [00:04<00:03, 10.16it/s]Epoch 8/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.16it/s]Epoch 8/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.15it/s]Epoch 8/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.14it/s]Epoch 8/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.15it/s]Epoch 8/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.15it/s]Epoch 8/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.16it/s]Epoch 8/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.16it/s]Epoch 8/10:  80%|████████  | 60/75 [00:06<00:01, 10.16it/s]Epoch 8/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.16it/s]Epoch 8/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.15it/s]Epoch 8/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.15it/s]Epoch 8/10:  91%|█████████ | 68/75 [00:06<00:00, 10.16it/s]Epoch 8/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.17it/s]Epoch 8/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.17it/s]Epoch 8/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.16it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00,  9.95it/s]
[2025-04-11 10:33:26,644][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0564
[2025-04-11 10:33:26,905][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0540, Metrics: {'mse': 0.05094850808382034, 'rmse': 0.22571776200339294, 'r2': -0.04675424098968506}
[2025-04-11 10:33:26,906][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁▁
wandb:     best_val_mse █▂▁▁
wandb:      best_val_r2 ▁▇██
wandb:    best_val_rmse █▂▁▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▃▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▂▁▄▁▁▁▁
wandb:          val_mse █▂▁▃▁▁▁▁
wandb:           val_r2 ▁▇█▆████
wandb:         val_rmse █▂▁▄▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05102
wandb:     best_val_mse 0.04917
wandb:      best_val_r2 -0.01019
wandb:    best_val_rmse 0.22174
wandb:            epoch 8
wandb:   final_test_mse 0.04585
wandb:    final_test_r2 -0.12032
wandb:  final_test_rmse 0.21413
wandb:  final_train_mse 0.05118
wandb:   final_train_r2 0.01255
wandb: final_train_rmse 0.22622
wandb:    final_val_mse 0.04917
wandb:     final_val_r2 -0.01019
wandb:   final_val_rmse 0.22174
wandb:    learning_rate 1e-05
wandb:       train_loss 0.05644
wandb:       train_time 64.62432
wandb:         val_loss 0.05401
wandb:          val_mse 0.05095
wandb:           val_r2 -0.04675
wandb:         val_rmse 0.22572
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_103212-lxyeg5sp
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250411_103212-lxyeg5sp/logs
Control experiment for avg_verb_edges (ru, control=3) completed successfully
Running submetric lexical_density control=1 for ru
slurmstepd: error: *** JOB 64315171 ON k28i22 CANCELLED AT 2025-04-11T10:33:37 DUE TO TIME LIMIT ***
