SLURM_JOB_ID: 64333899
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: xling_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100
SLURM_NNODES: 1
SLURM_NODELIST: k28g27
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Sat Apr 12 17:15:17 CEST 2025
Walltime: 00-12:00:00
========================================================================
Retrieving notices: - \ | done
Channels:
 - pytorch
 - nvidia
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): ...working... done
Solving environment: ...working... done


==> WARNING: A newer version of conda exists. <==
    current version: 25.1.1
    latest version: 25.3.1

Please update conda by running

    $ conda update -n base -c defaults conda



# All requested packages already installed.

Requirement already satisfied: hydra-core in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (1.3.2)
Requirement already satisfied: hydra-submitit-launcher in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (1.2.0)
Requirement already satisfied: omegaconf<2.4,>=2.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (2.3.0)
Requirement already satisfied: antlr4-python3-runtime==4.9.* in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (4.9.3)
Requirement already satisfied: packaging in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (24.2)
Requirement already satisfied: submitit>=1.3.3 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-submitit-launcher) (1.5.2)
Requirement already satisfied: PyYAML>=5.1.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.2)
Requirement already satisfied: cloudpickle>=1.2.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from submitit>=1.3.3->hydra-submitit-launcher) (3.1.1)
Requirement already satisfied: typing_extensions>=3.7.4.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from submitit>=1.3.3->hydra-submitit-launcher) (4.12.2)
Requirement already satisfied: transformers<4.36.0,>=4.30.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (4.35.2)
Requirement already satisfied: torch in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (2.5.1)
Requirement already satisfied: datasets in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (3.5.0)
Requirement already satisfied: wandb in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (0.19.9)
Requirement already satisfied: filelock in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (3.18.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.30.1)
Requirement already satisfied: numpy>=1.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (1.26.4)
Requirement already satisfied: packaging>=20.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (24.2)
Requirement already satisfied: pyyaml>=5.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (2024.11.6)
Requirement already satisfied: requests in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (2.32.3)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.15.2)
Requirement already satisfied: safetensors>=0.3.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.5.3)
Requirement already satisfied: tqdm>=4.27 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (4.67.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (4.12.2)
Requirement already satisfied: networkx in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (3.2.1)
Requirement already satisfied: jinja2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (3.1.6)
Requirement already satisfied: fsspec in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (2024.12.0)
Requirement already satisfied: sympy==1.13.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)
Requirement already satisfied: pyarrow>=15.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (19.0.1)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (0.3.8)
Requirement already satisfied: pandas in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (2.2.3)
Requirement already satisfied: xxhash in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (3.5.0)
Requirement already satisfied: multiprocess<0.70.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (0.70.16)
Requirement already satisfied: aiohttp in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (3.11.16)
Requirement already satisfied: click!=8.0.0,>=7.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (8.1.8)
Requirement already satisfied: docker-pycreds>=0.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (0.4.0)
Requirement already satisfied: eval-type-backport in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (0.2.2)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (3.1.44)
Requirement already satisfied: platformdirs in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (4.3.7)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.15.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (5.29.4)
Requirement already satisfied: psutil>=5.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (7.0.0)
Requirement already satisfied: pydantic<3 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (2.11.1)
Requirement already satisfied: sentry-sdk>=2.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (2.25.0)
Requirement already satisfied: setproctitle in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (1.3.5)
Requirement already satisfied: setuptools in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (78.1.0)
Requirement already satisfied: six>=1.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)
Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (2.6.1)
Requirement already satisfied: aiosignal>=1.1.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.2)
Requirement already satisfied: async-timeout<6.0,>=4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (5.0.1)
Requirement already satisfied: attrs>=17.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (6.3.1)
Requirement already satisfied: propcache>=0.2.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (0.3.1)
Requirement already satisfied: yarl<2.0,>=1.17.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.18.3)
Requirement already satisfied: gitdb<5,>=4.0.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)
Requirement already satisfied: annotated-types>=0.6.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (0.7.0)
Requirement already satisfied: pydantic-core==2.33.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (2.33.0)
Requirement already satisfied: typing-inspection>=0.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (0.4.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (2025.1.31)
Requirement already satisfied: MarkupSafe>=2.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)
Requirement already satisfied: python-dateutil>=2.8.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: smmap<6,>=3.0.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)
Environment variables:
PYTHONPATH=:/data/leuven/371/vsc37132/qtype-eval:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval
HF_HOME=/data/leuven/371/vsc37132/qtype-eval/data/cache
TRANSFORMERS_OFFLINE=1
HF_DATASETS_OFFLINE=1
HYDRA_JOB_CHDIR=False
GPU information:
Sat Apr 12 17:16:29 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:18:00.0 Off |                    0 |
| N/A   35C    P0             69W /  500W |       1MiB /  81920MiB |      0%   E. Process |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Python executable: /data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/bin/python
PyTorch CUDA available: True
Running cross-lingual question_type from ar to en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:17:10,558][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ar_to_en/question_type
experiment_name: cross_lingual_question_type_ar_to_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ar
  eval_language: en
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:17:10,558][__main__][INFO] - Normalized task: question_type
[2025-04-12 17:17:10,558][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 17:17:10,558][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:17:14,553][__main__][INFO] - Running cross-lingual experiment: ar -> en
[2025-04-12 17:17:14,553][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 17:17:14,554][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:17:17,725][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:17:17,725][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:17:18,199][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:17:18,367][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:17:18,870][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:17:18,995][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:17:18,995][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:17:18,997][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:17:19,025][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:17:19,064][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:17:19,142][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:17:19,144][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:17:19,144][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:17:19,145][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:17:19,168][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:17:19,202][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:17:19,253][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:17:19,254][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:17:19,254][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:17:19,255][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:17:19,261][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:17:19,261][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:17:19,261][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:17:19,262][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:17:19,283][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-12 17:17:19,284][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-12 17:17:19,284][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:17:19,284][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:17:19,285][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:17:19,285][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:17:19,285][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:17:19,285][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:17:19,285][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-12 17:17:19,285][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-12 17:17:19,285][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:17:19,285][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:17:19,286][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:17:19,286][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:17:19,286][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:17:19,286][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:17:19,286][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-12 17:17:19,286][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-12 17:17:19,286][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:17:19,286][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:17:19,286][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:17:19,287][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:17:19,287][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:17:19,287][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
[2025-04-12 17:17:22,104][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:17:22,106][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:17:22,148][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:17:22,174][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:17:22,202][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:17:22,211][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:17:22,212][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:17:22,213][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:17:22,235][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:17:22,264][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:17:22,294][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:17:22,295][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:17:22,296][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:17:22,297][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:17:22,321][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:17:22,350][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:17:22,364][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:17:22,365][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:17:22,365][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:17:22,366][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:17:22,367][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:17:22,367][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:17:22,367][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:17:22,367][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:17:22,367][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-12 17:17:22,367][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 17:17:22,367][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:17:22,368][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:17:22,368][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:17:22,368][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:17:22,368][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:17:22,368][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:17:22,368][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 17:17:22,368][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 17:17:22,368][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:17:22,369][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:17:22,369][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:17:22,369][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:17:22,369][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:17:22,369][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:17:22,369][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:17:22,369][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:17:22,369][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:17:22,370][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:17:22,370][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:17:22,370][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:17:22,370][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:17:22,370][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:17:44,378][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:17:44,381][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 17:17:44,381][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:17:44,381][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:03<04:00,  3.89s/it]Epoch 1/10:   3%|▎         | 2/63 [00:03<01:41,  1.66s/it]Epoch 1/10:   6%|▋         | 4/63 [00:04<00:40,  1.47it/s]Epoch 1/10:  10%|▉         | 6/63 [00:04<00:23,  2.47it/s]Epoch 1/10:  13%|█▎        | 8/63 [00:04<00:15,  3.57it/s]Epoch 1/10:  16%|█▌        | 10/63 [00:04<00:11,  4.69it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:04<00:11,  4.72it/s]Epoch 1/10:  21%|██        | 13/63 [00:05<00:08,  5.92it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:05<00:06,  6.96it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:05<00:05,  7.81it/s]Epoch 1/10:  30%|███       | 19/63 [00:05<00:05,  8.49it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:05<00:04,  9.01it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:06<00:04,  9.39it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:06<00:03,  9.68it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:06<00:03,  9.88it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:06<00:03, 10.03it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:06<00:03, 10.14it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:07<00:02, 10.21it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:07<00:02, 10.26it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:07<00:02, 10.30it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:07<00:02, 10.33it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:07<00:02, 10.35it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:08<00:01, 10.36it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:08<00:01, 10.37it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:08<00:01, 10.38it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:08<00:01, 10.38it/s]Epoch 1/10:  81%|████████  | 51/63 [00:08<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:09<00:00, 10.39it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:09<00:00, 10.38it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:09<00:00, 10.39it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:09<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:09<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 63/63 [00:10<00:00,  8.60it/s]Epoch 1/10: 100%|██████████| 63/63 [00:10<00:00,  6.21it/s]
[2025-04-12 17:18:01,342][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6786
[2025-04-12 17:18:01,549][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6689, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9473684210526315}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:13,  4.63it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  7.85it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  8.97it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.52it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.83it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:18:08,201][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.4403
[2025-04-12 17:18:08,409][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2506, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:13,  4.54it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  7.77it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  8.93it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.49it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.81it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:18:15,064][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1303
[2025-04-12 17:18:15,295][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1745, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:13,  4.51it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  7.75it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  8.92it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.49it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.81it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.00it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.27it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:18:21,873][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0490
[2025-04-12 17:18:22,210][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2095, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:13,  4.68it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  7.88it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  8.99it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.54it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.84it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.38it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.38it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:18:28,391][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0335
[2025-04-12 17:18:28,615][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2242, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:13,  4.53it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  7.77it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  8.92it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.49it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.81it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05, 10.00it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.26it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.30it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.27it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.17it/s]
[2025-04-12 17:18:34,812][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0204
[2025-04-12 17:18:35,045][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2147, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
[2025-04-12 17:18:35,046][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▂▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▅▂▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁██▁▁▁
wandb:               val_f1 ▁██▂▂▂
wandb:             val_loss █▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.97727
wandb:          best_val_f1 0.97561
wandb:        best_val_loss 0.17453
wandb:                epoch 6
wandb:  final_test_accuracy 0.62727
wandb:        final_test_f1 0.4058
wandb: final_train_accuracy 1
wandb:       final_train_f1 1
wandb:   final_val_accuracy 0.97727
wandb:         final_val_f1 0.97561
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02043
wandb:           train_time 43.84734
wandb:         val_accuracy 0.95455
wandb:               val_f1 0.95238
wandb:             val_loss 0.21467
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_171710-qx5htaiv
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_171710-qx5htaiv/logs
Cross-lingual experiment for question_type (ar → en) completed successfully
Running cross-lingual complexity from ar to en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:18:56,343][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ar_to_en/complexity
experiment_name: cross_lingual_complexity_ar_to_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ar
  eval_language: en
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:18:56,344][__main__][INFO] - Normalized task: complexity
[2025-04-12 17:18:56,344][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 17:18:56,344][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:18:58,280][__main__][INFO] - Running cross-lingual experiment: ar -> en
[2025-04-12 17:18:58,281][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 17:18:58,281][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:19:01,123][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:19:01,123][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:19:01,232][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:19:01,273][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:19:01,380][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:19:01,389][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:19:01,389][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:19:01,391][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:19:01,416][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:19:01,450][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:19:01,464][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:19:01,465][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:19:01,466][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:19:01,467][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:19:01,488][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:19:01,519][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:19:01,532][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:19:01,534][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:19:01,534][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:19:01,535][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:19:01,535][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:19:01,536][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:19:01,536][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:19:01,536][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:19:01,536][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:19:01,554][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-12 17:19:01,554][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:19:01,554][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-12 17:19:01,555][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:19:01,555][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:19:01,555][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:19:01,555][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:19:01,555][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:19:01,555][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-12 17:19:01,555][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:19:01,555][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-12 17:19:01,556][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:19:01,556][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:19:01,556][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:19:01,556][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:19:01,556][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:19:01,556][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-12 17:19:01,556][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:19:01,556][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-12 17:19:01,557][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:19:01,557][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:19:01,557][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:19:01,557][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
[2025-04-12 17:19:04,329][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:19:04,329][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:19:04,350][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:19:04,384][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:19:04,399][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:19:04,408][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:19:04,409][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:19:04,410][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:19:04,432][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:19:04,467][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:19:04,480][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:19:04,482][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:19:04,482][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:19:04,483][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:19:04,509][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:19:04,544][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:19:04,559][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:19:04,561][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:19:04,561][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:19:04,562][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:19:04,563][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:19:04,563][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:19:04,563][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:19:04,563][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:19:04,563][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:19:04,563][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-12 17:19:04,564][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:19:04,564][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-12 17:19:04,564][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:19:04,564][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:19:04,564][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:19:04,564][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:19:04,564][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:19:04,565][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-12 17:19:04,565][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:19:04,565][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-12 17:19:04,565][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:19:04,565][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:19:04,565][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:19:04,565][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:19:04,565][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:19:04,566][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-12 17:19:04,566][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:19:04,566][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-12 17:19:04,566][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:19:04,566][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:19:04,566][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:19:04,567][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:19:10,016][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:19:10,019][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 17:19:10,019][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:19:10,019][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:32,  1.50s/it]Epoch 1/10:   3%|▎         | 2/63 [00:01<00:41,  1.48it/s]Epoch 1/10:   6%|▋         | 4/63 [00:01<00:18,  3.19it/s]Epoch 1/10:  10%|▉         | 6/63 [00:01<00:12,  4.74it/s]Epoch 1/10:  13%|█▎        | 8/63 [00:02<00:09,  6.07it/s]Epoch 1/10:  16%|█▌        | 10/63 [00:02<00:07,  7.16it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:02<00:07,  6.51it/s]Epoch 1/10:  21%|██        | 13/63 [00:02<00:06,  7.55it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:02<00:05,  8.34it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:03<00:05,  8.92it/s]Epoch 1/10:  30%|███       | 19/63 [00:03<00:04,  9.35it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:03<00:04,  9.65it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:03<00:04,  9.87it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:03<00:03, 10.02it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:04<00:03, 10.13it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:04<00:03, 10.20it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:04<00:03, 10.26it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:04<00:02, 10.30it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:04<00:02, 10.33it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:05<00:02, 10.35it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:05<00:02, 10.36it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:05<00:02, 10.37it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  81%|████████  | 51/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:07<00:00, 10.39it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:07<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:07<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00, 11.09it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.31it/s]
[2025-04-12 17:19:19,984][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1408
[2025-04-12 17:19:20,189][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0773, Metrics: {'mse': 0.08030864596366882, 'rmse': 0.2833878013670822, 'r2': -0.23782336711883545}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:13,  4.60it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  7.81it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  8.96it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.51it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.83it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:19:26,839][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0611
[2025-04-12 17:19:27,042][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0713, Metrics: {'mse': 0.07381942123174667, 'rmse': 0.2716972970637483, 'r2': -0.1378028392791748}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:14,  4.39it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  7.66it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  8.85it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.45it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.78it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05,  9.99it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.12it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.26it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.30it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.38it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.38it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.37it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.38it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.38it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:06<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.18it/s]
[2025-04-12 17:19:33,700][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0445
[2025-04-12 17:19:33,919][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0524, Metrics: {'mse': 0.053010594099760056, 'rmse': 0.23024029642910046, 'r2': 0.18293046951293945}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:13,  4.71it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  7.90it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  9.01it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.55it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.85it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.04it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.15it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.38it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.38it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:19:40,488][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0288
[2025-04-12 17:19:40,712][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0425, Metrics: {'mse': 0.04322616755962372, 'rmse': 0.20790903674353292, 'r2': 0.3337409496307373}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:13,  4.64it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  7.85it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  8.98it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.53it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.17it/s]
[2025-04-12 17:19:47,327][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0247
[2025-04-12 17:19:47,554][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0417, Metrics: {'mse': 0.04257378354668617, 'rmse': 0.20633415506572383, 'r2': 0.34379637241363525}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:13,  4.46it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  7.72it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  8.89it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.47it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.80it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05,  9.99it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.12it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.20it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.26it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.30it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.34it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.37it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.37it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.38it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.38it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.38it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.38it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.38it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.38it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:06<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.18it/s]
[2025-04-12 17:19:54,169][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0217
[2025-04-12 17:19:54,400][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0399, Metrics: {'mse': 0.0406080037355423, 'rmse': 0.20151427675363923, 'r2': 0.374095618724823}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:13,  4.58it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:07,  7.81it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:06,  8.95it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:05,  9.51it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:05,  9.83it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 7/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 10.18it/s]
[2025-04-12 17:20:01,004][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0167
[2025-04-12 17:20:01,235][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0395, Metrics: {'mse': 0.040268708020448685, 'rmse': 0.200670645637195, 'r2': 0.379325270652771}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:33,  1.86it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:12,  4.73it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:08,  6.57it/s]Epoch 8/10:  11%|█         | 7/63 [00:01<00:07,  7.78it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:01<00:06,  8.60it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:01<00:05,  9.15it/s]Epoch 8/10:  21%|██        | 13/63 [00:01<00:05,  9.53it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:01<00:04,  9.79it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:02<00:04,  9.97it/s]Epoch 8/10:  30%|███       | 19/63 [00:02<00:04, 10.10it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:02<00:04, 10.18it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:02<00:03, 10.25it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:02<00:03, 10.29it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:03<00:03, 10.32it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:03<00:03, 10.34it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:03<00:03, 10.35it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.36it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.37it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:04<00:02, 10.37it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:04<00:02, 10.38it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.38it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.38it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.38it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.38it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:05<00:01, 10.38it/s]Epoch 8/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.38it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.38it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:06<00:00, 10.39it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:06<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00,  9.67it/s]
[2025-04-12 17:20:08,188][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0162
[2025-04-12 17:20:08,409][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0300, Metrics: {'mse': 0.02911883033812046, 'rmse': 0.17064240486502896, 'r2': 0.551181972026825}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:13,  4.74it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:07,  7.92it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:06,  9.02it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:05,  9.56it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:05,  9.86it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:01<00:05, 10.03it/s]Epoch 9/10:  21%|██        | 13/63 [00:01<00:04, 10.15it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.38it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 9/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:20:15,016][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0140
[2025-04-12 17:20:15,248][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0316, Metrics: {'mse': 0.03138638287782669, 'rmse': 0.17716202436703724, 'r2': 0.5162314176559448}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:12,  4.86it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:07,  8.00it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:06,  9.07it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:05,  9.59it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:05,  9.88it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:01<00:05, 10.05it/s]Epoch 10/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.38it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.38it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 10/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 10.22it/s]
[2025-04-12 17:20:21,415][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0137
[2025-04-12 17:20:21,644][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0280, Metrics: {'mse': 0.02759411372244358, 'rmse': 0.1661147607000762, 'r2': 0.5746829509735107}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▄▃▃▃▃▁▁
wandb:     best_val_mse █▇▄▃▃▃▃▁▁
wandb:      best_val_r2 ▁▂▅▆▆▆▆██
wandb:    best_val_rmse █▇▅▃▃▃▃▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▄▃▃▃▃▁▂▁
wandb:          val_mse █▇▄▃▃▃▃▁▂▁
wandb:           val_r2 ▁▂▅▆▆▆▆█▇█
wandb:         val_rmse █▇▅▃▃▃▃▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.028
wandb:     best_val_mse 0.02759
wandb:      best_val_r2 0.57468
wandb:    best_val_rmse 0.16611
wandb:            epoch 10
wandb:   final_test_mse 0.09239
wandb:    final_test_r2 -1.39735
wandb:  final_test_rmse 0.30396
wandb:  final_train_mse 0.00782
wandb:   final_train_r2 0.7453
wandb: final_train_rmse 0.08842
wandb:    final_val_mse 0.02759
wandb:     final_val_r2 0.57468
wandb:   final_val_rmse 0.16611
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01374
wandb:       train_time 69.67364
wandb:         val_loss 0.028
wandb:          val_mse 0.02759
wandb:           val_r2 0.57468
wandb:         val_rmse 0.16611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_171856-a83mw10z
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_171856-a83mw10z/logs
Cross-lingual experiment for complexity (ar → en) completed successfully
Running cross-lingual question_type from ar to fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:20:41,569][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ar_to_fi/question_type
experiment_name: cross_lingual_question_type_ar_to_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ar
  eval_language: fi
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:20:41,569][__main__][INFO] - Normalized task: question_type
[2025-04-12 17:20:41,569][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 17:20:41,569][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:20:42,830][__main__][INFO] - Running cross-lingual experiment: ar -> fi
[2025-04-12 17:20:42,830][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 17:20:42,830][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:20:45,687][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:20:45,687][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:20:45,770][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:20:45,803][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:20:45,916][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:20:45,925][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:20:45,925][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:20:45,927][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:20:45,956][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:20:45,991][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:20:46,006][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:20:46,008][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:20:46,008][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:20:46,009][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:20:46,033][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:20:46,066][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:20:46,082][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:20:46,084][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:20:46,084][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:20:46,085][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:20:46,086][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:20:46,086][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:20:46,086][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:20:46,086][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:20:46,087][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-12 17:20:46,087][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-12 17:20:46,087][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:20:46,087][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:20:46,087][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:20:46,087][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:20:46,087][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:20:46,087][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:20:46,088][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-12 17:20:46,088][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-12 17:20:46,088][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:20:46,088][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:20:46,088][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:20:46,088][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:20:46,088][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:20:46,088][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:20:46,089][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-12 17:20:46,089][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-12 17:20:46,089][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:20:46,089][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:20:46,089][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:20:46,089][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:20:46,090][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:20:46,090][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
[2025-04-12 17:20:48,901][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:20:48,901][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:20:48,928][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:20:48,963][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:20:48,980][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 17:20:48,989][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:20:48,990][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 17:20:48,991][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:20:49,016][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:20:49,054][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:20:49,069][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 17:20:49,070][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:20:49,070][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 17:20:49,072][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:20:49,098][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:20:49,133][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:20:49,149][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 17:20:49,151][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:20:49,151][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 17:20:49,152][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 17:20:49,152][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:20:49,153][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:20:49,153][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:20:49,153][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:20:49,153][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 17:20:49,153][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-12 17:20:49,153][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 17:20:49,153][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:20:49,154][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:20:49,154][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:20:49,154][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:20:49,154][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:20:49,154][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-12 17:20:49,154][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-12 17:20:49,154][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 17:20:49,154][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:20:49,155][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:20:49,155][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:20:49,155][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:20:49,155][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:20:49,155][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:20:49,155][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:20:49,155][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 17:20:49,155][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:20:49,155][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 17:20:49,156][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:20:49,156][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:20:49,156][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:20:54,266][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:20:54,269][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 17:20:54,270][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:20:54,270][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:25,  1.39s/it]Epoch 1/10:   3%|▎         | 2/63 [00:01<00:38,  1.59it/s]Epoch 1/10:   6%|▋         | 4/63 [00:01<00:17,  3.38it/s]Epoch 1/10:  10%|▉         | 6/63 [00:01<00:11,  4.96it/s]Epoch 1/10:  13%|█▎        | 8/63 [00:02<00:08,  6.29it/s]Epoch 1/10:  16%|█▌        | 10/63 [00:02<00:07,  7.35it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:02<00:07,  6.65it/s]Epoch 1/10:  21%|██        | 13/63 [00:02<00:06,  7.67it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:02<00:05,  8.44it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:03<00:05,  9.00it/s]Epoch 1/10:  30%|███       | 19/63 [00:03<00:04,  9.41it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:03<00:04,  9.70it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:03<00:04,  9.91it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:03<00:03, 10.05it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:04<00:03, 10.15it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:04<00:03, 10.23it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:04<00:03, 10.28it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:04<00:02, 10.31it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:04<00:02, 10.34it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:04<00:02, 10.35it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:05<00:02, 10.37it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:05<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  81%|████████  | 51/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:07<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:07<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00, 11.09it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.44it/s]
[2025-04-12 17:21:03,796][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6786
[2025-04-12 17:21:03,992][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6689, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9473684210526315}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:13,  4.66it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  7.87it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  9.00it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.54it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.85it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.04it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.15it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.26it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:21:10,637][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.4403
[2025-04-12 17:21:10,845][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2506, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:13,  4.71it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  7.91it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  9.02it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.56it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.85it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.04it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.22it/s]
[2025-04-12 17:21:17,475][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1303
[2025-04-12 17:21:17,698][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1745, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:13,  4.71it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  7.90it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  9.02it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.56it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.86it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.04it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.22it/s]
[2025-04-12 17:21:24,254][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0490
[2025-04-12 17:21:24,472][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2095, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:12,  5.07it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  8.15it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  9.17it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.65it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.92it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.08it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.18it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.25it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.18it/s]
[2025-04-12 17:21:30,661][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0335
[2025-04-12 17:21:30,919][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2242, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:13,  4.63it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  7.85it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  8.97it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.53it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.84it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05, 10.03it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.15it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:21:37,102][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0204
[2025-04-12 17:21:37,326][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2147, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
[2025-04-12 17:21:37,327][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▂▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▅▂▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁██▁▁▁
wandb:               val_f1 ▁██▂▂▂
wandb:             val_loss █▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.97727
wandb:          best_val_f1 0.97561
wandb:        best_val_loss 0.17453
wandb:                epoch 6
wandb:  final_test_accuracy 0.8
wandb:        final_test_f1 0.77083
wandb: final_train_accuracy 1
wandb:       final_train_f1 1
wandb:   final_val_accuracy 0.97727
wandb:         final_val_f1 0.97561
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02043
wandb:           train_time 40.99666
wandb:         val_accuracy 0.95455
wandb:               val_f1 0.95238
wandb:             val_loss 0.21467
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_172041-a5xg65zf
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_172041-a5xg65zf/logs
Cross-lingual experiment for question_type (ar → fi) completed successfully
Running cross-lingual complexity from ar to fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:21:57,424][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ar_to_fi/complexity
experiment_name: cross_lingual_complexity_ar_to_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ar
  eval_language: fi
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:21:57,424][__main__][INFO] - Normalized task: complexity
[2025-04-12 17:21:57,425][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 17:21:57,425][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:21:58,658][__main__][INFO] - Running cross-lingual experiment: ar -> fi
[2025-04-12 17:21:58,659][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 17:21:58,659][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:22:01,496][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:22:01,496][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:22:01,597][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:22:01,630][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:22:01,733][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:22:01,742][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:22:01,743][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:22:01,744][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:22:01,769][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:22:01,800][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:22:01,812][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:22:01,814][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:22:01,814][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:22:01,815][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:22:01,837][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:22:01,867][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:22:01,880][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:22:01,882][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:22:01,882][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:22:01,883][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:22:01,883][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:22:01,884][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:22:01,884][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:22:01,884][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:22:01,884][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:22:01,884][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-12 17:22:01,884][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:22:01,885][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-12 17:22:01,885][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:22:01,885][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:22:01,885][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:22:01,885][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:22:01,885][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:22:01,885][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-12 17:22:01,886][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:22:01,886][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-12 17:22:01,886][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:22:01,886][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:22:01,886][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:22:01,886][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:22:01,886][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:22:01,887][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-12 17:22:01,887][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:22:01,887][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-12 17:22:01,887][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:22:01,887][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:22:01,887][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:22:01,889][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
[2025-04-12 17:22:04,695][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:22:04,695][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:22:04,721][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:22:04,759][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:22:04,777][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 17:22:04,786][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:22:04,787][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 17:22:04,788][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:22:04,816][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:22:04,853][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:22:04,869][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 17:22:04,870][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:22:04,870][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 17:22:04,872][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:22:04,900][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:22:04,936][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:22:04,951][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 17:22:04,953][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:22:04,953][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 17:22:04,954][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 17:22:04,954][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:22:04,955][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:22:04,955][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:22:04,955][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:22:04,955][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:22:04,955][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-12 17:22:04,955][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 17:22:04,955][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-12 17:22:04,956][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:22:04,956][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:22:04,956][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:22:04,956][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:22:04,956][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:22:04,956][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-12 17:22:04,956][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 17:22:04,957][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-12 17:22:04,957][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:22:04,957][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:22:04,957][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:22:04,957][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:22:04,957][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:22:04,957][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-12 17:22:04,958][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 17:22:04,958][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-12 17:22:04,958][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 17:22:04,958][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:22:04,958][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:22:04,959][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:22:10,045][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:22:10,049][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 17:22:10,049][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:22:10,049][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:27,  1.41s/it]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:26,  2.28it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:15,  3.81it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:10,  5.21it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:02<00:08,  6.41it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:02<00:08,  6.47it/s]Epoch 1/10:  21%|██        | 13/63 [00:02<00:06,  7.39it/s]Epoch 1/10:  22%|██▏       | 14/63 [00:02<00:06,  7.77it/s]Epoch 1/10:  25%|██▌       | 16/63 [00:02<00:05,  8.54it/s]Epoch 1/10:  29%|██▊       | 18/63 [00:03<00:04,  9.08it/s]Epoch 1/10:  32%|███▏      | 20/63 [00:03<00:04,  9.47it/s]Epoch 1/10:  35%|███▍      | 22/63 [00:03<00:04,  9.74it/s]Epoch 1/10:  38%|███▊      | 24/63 [00:03<00:03,  9.94it/s]Epoch 1/10:  41%|████▏     | 26/63 [00:03<00:03, 10.07it/s]Epoch 1/10:  44%|████▍     | 28/63 [00:04<00:03, 10.17it/s]Epoch 1/10:  48%|████▊     | 30/63 [00:04<00:03, 10.24it/s]Epoch 1/10:  51%|█████     | 32/63 [00:04<00:03, 10.28it/s]Epoch 1/10:  54%|█████▍    | 34/63 [00:04<00:02, 10.32it/s]Epoch 1/10:  57%|█████▋    | 36/63 [00:04<00:02, 10.34it/s]Epoch 1/10:  60%|██████    | 38/63 [00:05<00:02, 10.35it/s]Epoch 1/10:  63%|██████▎   | 40/63 [00:05<00:02, 10.36it/s]Epoch 1/10:  67%|██████▋   | 42/63 [00:05<00:02, 10.37it/s]Epoch 1/10:  70%|██████▉   | 44/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  73%|███████▎  | 46/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  76%|███████▌  | 48/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  79%|███████▉  | 50/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 52/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  86%|████████▌ | 54/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 56/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 58/63 [00:07<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▌| 60/63 [00:07<00:00, 10.40it/s]Epoch 1/10:  98%|█████████▊| 62/63 [00:07<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.39it/s]
[2025-04-12 17:22:19,894][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1408
[2025-04-12 17:22:20,094][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0773, Metrics: {'mse': 0.08030864596366882, 'rmse': 0.2833878013670822, 'r2': -0.23782336711883545}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:13,  4.55it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  7.78it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  8.93it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.50it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.37it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.38it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.27it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:22:26,732][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0611
[2025-04-12 17:22:26,935][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0713, Metrics: {'mse': 0.07381942123174667, 'rmse': 0.2716972970637483, 'r2': -0.1378028392791748}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:12,  4.95it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  8.06it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  9.11it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.61it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.89it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.06it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.37it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.38it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.38it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.23it/s]
[2025-04-12 17:22:33,554][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0445
[2025-04-12 17:22:33,779][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0524, Metrics: {'mse': 0.053010594099760056, 'rmse': 0.23024029642910046, 'r2': 0.18293046951293945}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:13,  4.58it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  7.81it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  8.95it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.51it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-12 17:22:40,344][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0288
[2025-04-12 17:22:40,566][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0425, Metrics: {'mse': 0.04322616755962372, 'rmse': 0.20790903674353292, 'r2': 0.3337409496307373}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:13,  4.54it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  7.77it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  8.92it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.49it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.81it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.00it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.26it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.30it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.34it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.37it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.37it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.38it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.17it/s]
[2025-04-12 17:22:47,154][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0247
[2025-04-12 17:22:47,370][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0417, Metrics: {'mse': 0.04257378354668617, 'rmse': 0.20633415506572383, 'r2': 0.34379637241363525}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:12,  4.85it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  8.00it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  9.06it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.58it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.87it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05, 10.05it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.37it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.37it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.37it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.38it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.38it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:22:53,975][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0217
[2025-04-12 17:22:54,199][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0399, Metrics: {'mse': 0.0406080037355423, 'rmse': 0.20151427675363923, 'r2': 0.374095618724823}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:13,  4.45it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:07,  7.70it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:06,  8.88it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:05,  9.46it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:05,  9.79it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:01<00:05, 10.00it/s]Epoch 7/10:  21%|██        | 13/63 [00:01<00:04, 10.12it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:04, 10.30it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 7/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 10.17it/s]
[2025-04-12 17:23:00,820][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0167
[2025-04-12 17:23:01,040][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0395, Metrics: {'mse': 0.040268708020448685, 'rmse': 0.200670645637195, 'r2': 0.379325270652771}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:12,  4.82it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:07,  7.98it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:06,  9.06it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:05,  9.58it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:05,  9.87it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:01<00:05, 10.05it/s]Epoch 8/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.38it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.38it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.38it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 8/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.38it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:23:07,644][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0162
[2025-04-12 17:23:07,875][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0300, Metrics: {'mse': 0.02911883033812046, 'rmse': 0.17064240486502896, 'r2': 0.551181972026825}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:14,  4.40it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:07,  7.67it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:06,  8.86it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:05,  9.45it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:05,  9.78it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:01<00:05,  9.98it/s]Epoch 9/10:  21%|██        | 13/63 [00:01<00:04, 10.11it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:01<00:04, 10.20it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:04, 10.26it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:04, 10.30it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:02<00:03, 10.34it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:02<00:03, 10.37it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.36it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.37it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.38it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.38it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.38it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.38it/s]Epoch 9/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:06<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 11.27it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 10.16it/s]
[2025-04-12 17:23:14,510][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0140
[2025-04-12 17:23:14,743][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0316, Metrics: {'mse': 0.03138638287782669, 'rmse': 0.17716202436703724, 'r2': 0.5162314176559448}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:13,  4.59it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:07,  7.81it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:06,  8.95it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:05,  9.51it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 10/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:04, 10.30it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:02<00:03, 10.37it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 10/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.38it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.38it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:23:20,927][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0137
[2025-04-12 17:23:21,162][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0280, Metrics: {'mse': 0.02759411372244358, 'rmse': 0.1661147607000762, 'r2': 0.5746829509735107}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▄▃▃▃▃▁▁
wandb:     best_val_mse █▇▄▃▃▃▃▁▁
wandb:      best_val_r2 ▁▂▅▆▆▆▆██
wandb:    best_val_rmse █▇▅▃▃▃▃▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▄▃▃▃▃▁▂▁
wandb:          val_mse █▇▄▃▃▃▃▁▂▁
wandb:           val_r2 ▁▂▅▆▆▆▆█▇█
wandb:         val_rmse █▇▅▃▃▃▃▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.028
wandb:     best_val_mse 0.02759
wandb:      best_val_r2 0.57468
wandb:    best_val_rmse 0.16611
wandb:            epoch 10
wandb:   final_test_mse 0.04157
wandb:    final_test_r2 -0.05274
wandb:  final_test_rmse 0.20389
wandb:  final_train_mse 0.00782
wandb:   final_train_r2 0.7453
wandb: final_train_rmse 0.08842
wandb:    final_val_mse 0.02759
wandb:     final_val_r2 0.57468
wandb:   final_val_rmse 0.16611
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01374
wandb:       train_time 69.21336
wandb:         val_loss 0.028
wandb:          val_mse 0.02759
wandb:           val_r2 0.57468
wandb:         val_rmse 0.16611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_172157-dexjo14w
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_172157-dexjo14w/logs
Cross-lingual experiment for complexity (ar → fi) completed successfully
Running cross-lingual question_type from ar to id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:23:42,116][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ar_to_id/question_type
experiment_name: cross_lingual_question_type_ar_to_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ar
  eval_language: id
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:23:42,116][__main__][INFO] - Normalized task: question_type
[2025-04-12 17:23:42,116][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 17:23:42,116][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:23:43,640][__main__][INFO] - Running cross-lingual experiment: ar -> id
[2025-04-12 17:23:43,640][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 17:23:43,641][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:23:46,723][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:23:46,724][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:23:46,814][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:23:46,850][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:23:46,968][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:23:46,978][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:23:46,978][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:23:46,980][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:23:47,007][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:23:47,044][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:23:47,059][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:23:47,061][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:23:47,061][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:23:47,062][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:23:47,090][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:23:47,127][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:23:47,143][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:23:47,144][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:23:47,144][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:23:47,146][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:23:47,147][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:23:47,147][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:23:47,147][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:23:47,147][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:23:47,148][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-12 17:23:47,148][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-12 17:23:47,148][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:23:47,148][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:23:47,148][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:23:47,148][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:23:47,148][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:23:47,149][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:23:47,149][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-12 17:23:47,149][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-12 17:23:47,149][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:23:47,149][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:23:47,149][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:23:47,149][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:23:47,149][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:23:47,150][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:23:47,150][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-12 17:23:47,150][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-12 17:23:47,150][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:23:47,150][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:23:47,150][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:23:47,150][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:23:47,151][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:23:47,151][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
[2025-04-12 17:23:49,873][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:23:49,873][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:23:49,901][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:23:49,936][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:23:49,962][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 17:23:49,969][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:23:49,970][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 17:23:49,971][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:23:49,994][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:23:50,027][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:23:50,044][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 17:23:50,045][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:23:50,046][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 17:23:50,047][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:23:50,072][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:23:50,109][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:23:50,124][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 17:23:50,126][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:23:50,126][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 17:23:50,127][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 17:23:50,127][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:23:50,127][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:23:50,127][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:23:50,128][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:23:50,128][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-12 17:23:50,128][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-12 17:23:50,128][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 17:23:50,128][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:23:50,128][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:23:50,128][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:23:50,128][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:23:50,129][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:23:50,129][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 17:23:50,129][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 17:23:50,129][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 17:23:50,129][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:23:50,129][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:23:50,129][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:23:50,129][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:23:50,130][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:23:50,130][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:23:50,130][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:23:50,130][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 17:23:50,130][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:23:50,130][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 17:23:50,130][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:23:50,130][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:23:50,131][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:23:55,255][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:23:55,258][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 17:23:55,258][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:23:55,258][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:16,  1.24s/it]Epoch 1/10:   3%|▎         | 2/63 [00:01<00:34,  1.76it/s]Epoch 1/10:   6%|▋         | 4/63 [00:01<00:16,  3.65it/s]Epoch 1/10:  10%|▉         | 6/63 [00:01<00:10,  5.26it/s]Epoch 1/10:  13%|█▎        | 8/63 [00:01<00:08,  6.57it/s]Epoch 1/10:  16%|█▌        | 10/63 [00:02<00:06,  7.59it/s]Epoch 1/10:  19%|█▉        | 12/63 [00:02<00:06,  8.37it/s]Epoch 1/10:  22%|██▏       | 14/63 [00:02<00:05,  8.94it/s]Epoch 1/10:  25%|██▌       | 16/63 [00:02<00:05,  9.36it/s]Epoch 1/10:  29%|██▊       | 18/63 [00:02<00:04,  9.66it/s]Epoch 1/10:  32%|███▏      | 20/63 [00:03<00:04,  9.88it/s]Epoch 1/10:  35%|███▍      | 22/63 [00:03<00:04, 10.03it/s]Epoch 1/10:  38%|███▊      | 24/63 [00:03<00:03, 10.14it/s]Epoch 1/10:  41%|████▏     | 26/63 [00:03<00:03, 10.22it/s]Epoch 1/10:  44%|████▍     | 28/63 [00:03<00:03, 10.26it/s]Epoch 1/10:  48%|████▊     | 30/63 [00:04<00:03, 10.30it/s]Epoch 1/10:  51%|█████     | 32/63 [00:04<00:03, 10.33it/s]Epoch 1/10:  54%|█████▍    | 34/63 [00:04<00:02, 10.33it/s]Epoch 1/10:  57%|█████▋    | 36/63 [00:04<00:02, 10.35it/s]Epoch 1/10:  60%|██████    | 38/63 [00:04<00:02, 10.36it/s]Epoch 1/10:  63%|██████▎   | 40/63 [00:04<00:02, 10.37it/s]Epoch 1/10:  67%|██████▋   | 42/63 [00:05<00:02, 10.38it/s]Epoch 1/10:  70%|██████▉   | 44/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  73%|███████▎  | 46/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 48/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  79%|███████▉  | 50/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  83%|████████▎ | 52/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  86%|████████▌ | 54/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 56/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 58/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▌| 60/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  98%|█████████▊| 62/63 [00:07<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.74it/s]
[2025-04-12 17:24:05,021][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6786
[2025-04-12 17:24:05,221][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6689, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9473684210526315}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:13,  4.52it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  7.76it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  8.92it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.49it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.81it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:24:12,020][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.4403
[2025-04-12 17:24:12,234][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2506, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:13,  4.54it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  7.77it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  8.92it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.49it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.81it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.27it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:24:18,880][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1303
[2025-04-12 17:24:19,115][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1745, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:13,  4.46it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  7.71it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  8.89it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.47it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.80it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.00it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.12it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.35it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.38it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:24:25,688][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0490
[2025-04-12 17:24:25,910][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2095, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:12,  4.79it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  7.96it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  9.05it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.57it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.86it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.04it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.38it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-12 17:24:32,083][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0335
[2025-04-12 17:24:32,311][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2242, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:23,  2.66it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:10,  5.91it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:07,  7.59it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:06,  8.56it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:01<00:05,  9.17it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05,  9.56it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:05,  9.82it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.00it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.12it/s]Epoch 6/10:  30%|███       | 19/63 [00:02<00:04, 10.20it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.26it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.30it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.32it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.34it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:03<00:03, 10.36it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.36it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.37it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:04<00:02, 10.38it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.38it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.38it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.38it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.38it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.38it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:06<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.27it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00,  9.94it/s]
[2025-04-12 17:24:38,653][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0204
[2025-04-12 17:24:38,877][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2147, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
[2025-04-12 17:24:38,878][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▂▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▅▂▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁██▁▁▁
wandb:               val_f1 ▁██▂▂▂
wandb:             val_loss █▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.97727
wandb:          best_val_f1 0.97561
wandb:        best_val_loss 0.17453
wandb:                epoch 6
wandb:  final_test_accuracy 0.72727
wandb:        final_test_f1 0.63415
wandb: final_train_accuracy 1
wandb:       final_train_f1 1
wandb:   final_val_accuracy 0.97727
wandb:         final_val_f1 0.97561
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02043
wandb:           train_time 41.06955
wandb:         val_accuracy 0.95455
wandb:               val_f1 0.95238
wandb:             val_loss 0.21467
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_172342-vsiru2mo
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_172342-vsiru2mo/logs
Cross-lingual experiment for question_type (ar → id) completed successfully
Running cross-lingual complexity from ar to id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:24:59,431][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ar_to_id/complexity
experiment_name: cross_lingual_complexity_ar_to_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ar
  eval_language: id
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:24:59,431][__main__][INFO] - Normalized task: complexity
[2025-04-12 17:24:59,431][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 17:24:59,431][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:25:01,253][__main__][INFO] - Running cross-lingual experiment: ar -> id
[2025-04-12 17:25:01,254][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 17:25:01,254][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:25:04,113][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:25:04,113][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:25:04,184][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:25:04,219][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:25:04,331][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:25:04,340][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:25:04,341][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:25:04,342][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:25:04,373][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:25:04,415][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:25:04,433][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:25:04,436][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:25:04,437][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:25:04,438][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:25:04,467][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:25:04,503][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:25:04,518][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:25:04,520][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:25:04,520][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:25:04,522][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:25:04,523][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:25:04,523][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:25:04,523][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:25:04,523][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:25:04,523][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:25:04,523][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-12 17:25:04,524][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:25:04,524][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-12 17:25:04,524][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:25:04,524][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:25:04,524][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:25:04,524][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:25:04,524][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:25:04,525][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-12 17:25:04,525][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:25:04,525][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-12 17:25:04,525][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:25:04,525][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:25:04,525][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:25:04,525][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:25:04,525][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:25:04,526][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-12 17:25:04,526][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:25:04,526][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-12 17:25:04,526][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:25:04,526][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:25:04,526][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:25:04,527][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
[2025-04-12 17:25:07,295][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:25:07,296][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:25:07,321][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:25:07,356][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:25:07,372][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 17:25:07,379][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:25:07,380][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 17:25:07,381][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:25:07,408][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:25:07,447][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:25:07,461][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 17:25:07,462][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:25:07,463][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 17:25:07,464][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:25:07,495][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:25:07,537][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:25:07,555][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 17:25:07,556][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:25:07,556][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 17:25:07,558][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 17:25:07,558][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:25:07,558][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:25:07,558][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:25:07,558][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:25:07,559][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:25:07,559][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-12 17:25:07,559][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 17:25:07,559][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-12 17:25:07,559][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:25:07,559][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:25:07,559][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:25:07,560][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:25:07,560][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:25:07,560][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-12 17:25:07,560][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 17:25:07,560][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-12 17:25:07,560][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:25:07,560][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:25:07,560][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:25:07,561][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:25:07,561][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:25:07,561][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-12 17:25:07,561][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 17:25:07,561][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-12 17:25:07,561][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 17:25:07,561][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:25:07,561][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:25:07,562][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:25:13,142][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:25:13,144][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 17:25:13,145][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:25:13,145][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:17,  1.25s/it]Epoch 1/10:   3%|▎         | 2/63 [00:01<00:35,  1.73it/s]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:21,  2.78it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:12,  4.74it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:08,  6.27it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:02<00:07,  7.41it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:02<00:06,  8.26it/s]Epoch 1/10:  21%|██        | 13/63 [00:02<00:05,  8.88it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:02<00:05,  9.33it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:02<00:04,  9.64it/s]Epoch 1/10:  30%|███       | 19/63 [00:02<00:04,  9.87it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:03<00:04, 10.03it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:03<00:03, 10.14it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:03<00:03, 10.22it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:03<00:03, 10.28it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:03<00:03, 10.31it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:04<00:03, 10.34it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:04<00:02, 10.36it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:04<00:02, 10.37it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:04<00:02, 10.38it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:04<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:05<00:02, 10.40it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:05<00:01, 10.40it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:05<00:01, 10.40it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:05<00:01, 10.40it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:05<00:01, 10.40it/s]Epoch 1/10:  81%|████████  | 51/63 [00:06<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:06<00:00, 10.41it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:06<00:00, 10.41it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:06<00:00, 10.41it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:06<00:00, 10.41it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:07<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00, 11.11it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.69it/s]
[2025-04-12 17:25:22,689][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1408
[2025-04-12 17:25:22,893][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0773, Metrics: {'mse': 0.08030864596366882, 'rmse': 0.2833878013670822, 'r2': -0.23782336711883545}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:13,  4.70it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  7.89it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  9.01it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.55it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.86it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.04it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.22it/s]
[2025-04-12 17:25:29,657][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0611
[2025-04-12 17:25:29,871][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0713, Metrics: {'mse': 0.07381942123174667, 'rmse': 0.2716972970637483, 'r2': -0.1378028392791748}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:13,  4.48it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  7.73it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  8.91it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.49it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.81it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.41it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.41it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.41it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-12 17:25:36,511][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0445
[2025-04-12 17:25:36,730][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0524, Metrics: {'mse': 0.053010594099760056, 'rmse': 0.23024029642910046, 'r2': 0.18293046951293945}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:13,  4.68it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  7.89it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  9.01it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.55it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.85it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.04it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.24it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.41it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.41it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.41it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.41it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.41it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.41it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.42it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.17it/s]
[2025-04-12 17:25:43,314][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0288
[2025-04-12 17:25:43,527][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0425, Metrics: {'mse': 0.04322616755962372, 'rmse': 0.20790903674353292, 'r2': 0.3337409496307373}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:13,  4.52it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  7.76it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  8.93it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.50it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.41it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.41it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.41it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-12 17:25:50,084][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0247
[2025-04-12 17:25:50,312][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0417, Metrics: {'mse': 0.04257378354668617, 'rmse': 0.20633415506572383, 'r2': 0.34379637241363525}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:13,  4.43it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  7.70it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  8.88it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.47it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.80it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.41it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.41it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.41it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.41it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:25:56,903][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0217
[2025-04-12 17:25:57,136][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0399, Metrics: {'mse': 0.0406080037355423, 'rmse': 0.20151427675363923, 'r2': 0.374095618724823}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:13,  4.48it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:07,  7.74it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:06,  8.92it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:05,  9.49it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 7/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:02<00:03, 10.37it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.41it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.41it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.41it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.41it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.41it/s]Epoch 7/10:  81%|████████  | 51/63 [00:05<00:01, 10.41it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.41it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.41it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:26:03,740][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0167
[2025-04-12 17:26:03,971][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0395, Metrics: {'mse': 0.040268708020448685, 'rmse': 0.200670645637195, 'r2': 0.379325270652771}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:13,  4.48it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:07,  7.73it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:06,  8.91it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:05,  9.48it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:05,  9.81it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 8/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.41it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.41it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 10.18it/s]
[2025-04-12 17:26:10,583][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0162
[2025-04-12 17:26:10,813][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0300, Metrics: {'mse': 0.02911883033812046, 'rmse': 0.17064240486502896, 'r2': 0.551181972026825}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:14,  4.31it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:07,  7.60it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:06,  8.82it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:05,  9.43it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:01<00:05,  9.77it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:01<00:05,  9.98it/s]Epoch 9/10:  21%|██        | 13/63 [00:01<00:04, 10.12it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.41it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.41it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.41it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.41it/s]Epoch 9/10:  81%|████████  | 51/63 [00:05<00:01, 10.41it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 11.27it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 10.16it/s]
[2025-04-12 17:26:17,451][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0140
[2025-04-12 17:26:17,673][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0316, Metrics: {'mse': 0.03138638287782669, 'rmse': 0.17716202436703724, 'r2': 0.5162314176559448}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:13,  4.61it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:07,  7.84it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:06,  8.98it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:05,  9.53it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:05,  9.84it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:01<00:05, 10.03it/s]Epoch 10/10:  21%|██        | 13/63 [00:01<00:04, 10.15it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:02<00:03, 10.38it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:02<00:03, 10.39it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:03<00:03, 10.40it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 10/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:05<00:00, 10.41it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-12 17:26:23,844][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0137
[2025-04-12 17:26:24,067][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0280, Metrics: {'mse': 0.02759411372244358, 'rmse': 0.1661147607000762, 'r2': 0.5746829509735107}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▄▃▃▃▃▁▁
wandb:     best_val_mse █▇▄▃▃▃▃▁▁
wandb:      best_val_r2 ▁▂▅▆▆▆▆██
wandb:    best_val_rmse █▇▅▃▃▃▃▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▄▃▃▃▃▁▂▁
wandb:          val_mse █▇▄▃▃▃▃▁▂▁
wandb:           val_r2 ▁▂▅▆▆▆▆█▇█
wandb:         val_rmse █▇▅▃▃▃▃▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.028
wandb:     best_val_mse 0.02759
wandb:      best_val_r2 0.57468
wandb:    best_val_rmse 0.16611
wandb:            epoch 10
wandb:   final_test_mse 0.10586
wandb:    final_test_r2 -1.59628
wandb:  final_test_rmse 0.32536
wandb:  final_train_mse 0.00782
wandb:   final_train_r2 0.7453
wandb: final_train_rmse 0.08842
wandb:    final_val_mse 0.02759
wandb:     final_val_r2 0.57468
wandb:   final_val_rmse 0.16611
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01374
wandb:       train_time 69.05948
wandb:         val_loss 0.028
wandb:          val_mse 0.02759
wandb:           val_r2 0.57468
wandb:         val_rmse 0.16611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_172459-87aln8va
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_172459-87aln8va/logs
Cross-lingual experiment for complexity (ar → id) completed successfully
Running cross-lingual question_type from ar to ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:26:46,069][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ar_to_ja/question_type
experiment_name: cross_lingual_question_type_ar_to_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ar
  eval_language: ja
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:26:46,069][__main__][INFO] - Normalized task: question_type
[2025-04-12 17:26:46,069][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 17:26:46,069][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:26:47,674][__main__][INFO] - Running cross-lingual experiment: ar -> ja
[2025-04-12 17:26:47,675][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 17:26:47,675][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:26:50,856][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:26:50,857][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:26:50,936][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:26:50,972][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:26:51,088][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:26:51,098][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:26:51,099][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:26:51,100][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:26:51,126][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:26:51,162][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:26:51,177][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:26:51,179][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:26:51,179][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:26:51,180][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:26:51,203][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:26:51,236][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:26:51,251][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:26:51,253][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:26:51,253][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:26:51,254][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:26:51,255][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:26:51,255][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:26:51,255][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:26:51,255][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:26:51,256][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-12 17:26:51,256][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-12 17:26:51,256][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:26:51,256][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:26:51,256][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:26:51,256][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:26:51,256][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:26:51,256][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:26:51,257][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-12 17:26:51,257][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-12 17:26:51,257][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:26:51,257][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:26:51,257][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:26:51,257][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:26:51,257][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:26:51,258][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:26:51,258][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-12 17:26:51,258][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-12 17:26:51,258][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:26:51,258][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:26:51,258][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:26:51,258][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:26:51,259][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:26:51,259][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
[2025-04-12 17:26:54,002][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:26:54,002][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:26:54,029][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:26:54,067][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:26:54,113][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 17:26:54,123][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:26:54,123][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 17:26:54,125][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:26:54,152][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:26:54,189][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:26:54,205][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 17:26:54,206][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:26:54,207][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 17:26:54,208][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:26:54,235][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:26:54,273][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:26:54,290][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 17:26:54,291][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:26:54,291][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 17:26:54,293][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 17:26:54,293][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:26:54,293][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:26:54,293][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:26:54,293][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:26:54,293][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-12 17:26:54,294][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 17:26:54,294][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 17:26:54,294][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:26:54,294][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:26:54,294][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:26:54,294][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:26:54,294][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:26:54,294][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-12 17:26:54,295][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-12 17:26:54,295][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 17:26:54,295][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:26:54,295][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:26:54,295][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:26:54,295][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:26:54,295][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:26:54,295][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-12 17:26:54,296][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-12 17:26:54,296][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 17:26:54,296][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:26:54,296][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 17:26:54,296][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:26:54,296][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:26:54,296][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:26:59,566][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:26:59,569][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 17:26:59,569][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:26:59,569][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:32,  1.49s/it]Epoch 1/10:   3%|▎         | 2/63 [00:01<00:41,  1.48it/s]Epoch 1/10:   6%|▋         | 4/63 [00:01<00:18,  3.20it/s]Epoch 1/10:  10%|▉         | 6/63 [00:01<00:11,  4.75it/s]Epoch 1/10:  13%|█▎        | 8/63 [00:02<00:09,  6.09it/s]Epoch 1/10:  16%|█▌        | 10/63 [00:02<00:08,  6.34it/s]Epoch 1/10:  19%|█▉        | 12/63 [00:02<00:06,  7.31it/s]Epoch 1/10:  22%|██▏       | 14/63 [00:02<00:06,  8.10it/s]Epoch 1/10:  25%|██▌       | 16/63 [00:03<00:05,  8.72it/s]Epoch 1/10:  29%|██▊       | 18/63 [00:03<00:04,  9.18it/s]Epoch 1/10:  32%|███▏      | 20/63 [00:03<00:04,  9.52it/s]Epoch 1/10:  35%|███▍      | 22/63 [00:03<00:04,  9.77it/s]Epoch 1/10:  38%|███▊      | 24/63 [00:03<00:03,  9.95it/s]Epoch 1/10:  41%|████▏     | 26/63 [00:04<00:03, 10.08it/s]Epoch 1/10:  44%|████▍     | 28/63 [00:04<00:03, 10.17it/s]Epoch 1/10:  48%|████▊     | 30/63 [00:04<00:03, 10.24it/s]Epoch 1/10:  51%|█████     | 32/63 [00:04<00:03, 10.29it/s]Epoch 1/10:  54%|█████▍    | 34/63 [00:04<00:02, 10.32it/s]Epoch 1/10:  57%|█████▋    | 36/63 [00:04<00:02, 10.34it/s]Epoch 1/10:  60%|██████    | 38/63 [00:05<00:02, 10.36it/s]Epoch 1/10:  63%|██████▎   | 40/63 [00:05<00:02, 10.37it/s]Epoch 1/10:  67%|██████▋   | 42/63 [00:05<00:02, 10.36it/s]Epoch 1/10:  70%|██████▉   | 44/63 [00:05<00:01, 10.37it/s]Epoch 1/10:  73%|███████▎  | 46/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  76%|███████▌  | 48/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  79%|███████▉  | 50/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 52/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  86%|████████▌ | 54/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 56/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  92%|█████████▏| 58/63 [00:07<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▌| 60/63 [00:07<00:00, 10.40it/s]Epoch 1/10:  98%|█████████▊| 62/63 [00:07<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.32it/s]
[2025-04-12 17:27:09,040][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6786
[2025-04-12 17:27:09,244][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6689, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9473684210526315}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:13,  4.76it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  7.94it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  9.04it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.57it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.86it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.04it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.22it/s]
[2025-04-12 17:27:15,873][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.4403
[2025-04-12 17:27:16,088][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2506, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:13,  4.58it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  7.81it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  8.95it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.51it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.83it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:27:22,720][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1303
[2025-04-12 17:27:22,946][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1745, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:13,  4.47it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  7.72it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  8.90it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.47it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.80it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.00it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.18it/s]
[2025-04-12 17:27:29,533][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0490
[2025-04-12 17:27:29,755][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2095, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:13,  4.56it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  7.79it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  8.94it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.50it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-12 17:27:35,930][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0335
[2025-04-12 17:27:36,156][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2242, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:13,  4.60it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  7.82it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  8.96it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.52it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.83it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.38it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:27:42,338][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0204
[2025-04-12 17:27:42,571][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2147, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
[2025-04-12 17:27:42,572][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▂▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▅▂▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁██▁▁▁
wandb:               val_f1 ▁██▂▂▂
wandb:             val_loss █▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.97727
wandb:          best_val_f1 0.97561
wandb:        best_val_loss 0.17453
wandb:                epoch 6
wandb:  final_test_accuracy 0.40217
wandb:        final_test_f1 0
wandb: final_train_accuracy 1
wandb:       final_train_f1 1
wandb:   final_val_accuracy 0.97727
wandb:         final_val_f1 0.97561
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02043
wandb:           train_time 41.10331
wandb:         val_accuracy 0.95455
wandb:               val_f1 0.95238
wandb:             val_loss 0.21467
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_172646-1mxmkmmd
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_172646-1mxmkmmd/logs
Cross-lingual experiment for question_type (ar → ja) completed successfully
Running cross-lingual complexity from ar to ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:28:02,609][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ar_to_ja/complexity
experiment_name: cross_lingual_complexity_ar_to_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ar
  eval_language: ja
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:28:02,609][__main__][INFO] - Normalized task: complexity
[2025-04-12 17:28:02,609][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 17:28:02,609][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:28:04,020][__main__][INFO] - Running cross-lingual experiment: ar -> ja
[2025-04-12 17:28:04,020][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 17:28:04,021][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:28:06,831][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:28:06,832][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:28:06,900][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:28:06,933][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:28:07,048][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:28:07,057][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:28:07,058][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:28:07,059][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:28:07,085][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:28:07,123][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:28:07,138][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:28:07,140][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:28:07,140][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:28:07,141][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:28:07,168][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:28:07,205][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:28:07,220][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:28:07,221][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:28:07,221][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:28:07,223][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:28:07,223][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:28:07,223][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:28:07,223][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:28:07,223][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:28:07,224][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:28:07,224][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-12 17:28:07,224][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:28:07,224][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-12 17:28:07,224][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:28:07,224][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:28:07,225][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:28:07,225][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:28:07,225][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:28:07,225][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-12 17:28:07,225][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:28:07,225][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-12 17:28:07,225][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:28:07,225][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:28:07,226][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:28:07,226][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:28:07,226][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:28:07,226][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-12 17:28:07,226][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:28:07,226][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-12 17:28:07,226][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:28:07,226][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:28:07,227][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:28:07,227][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
[2025-04-12 17:28:10,002][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:28:10,002][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:28:10,026][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:28:10,055][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:28:10,069][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 17:28:10,078][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:28:10,079][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 17:28:10,080][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:28:10,103][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:28:10,131][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:28:10,144][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 17:28:10,145][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:28:10,145][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 17:28:10,146][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:28:10,168][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:28:10,197][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:28:10,209][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 17:28:10,210][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:28:10,211][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 17:28:10,212][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 17:28:10,212][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:28:10,212][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:28:10,212][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:28:10,213][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:28:10,213][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:28:10,213][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-12 17:28:10,213][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 17:28:10,213][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-04-12 17:28:10,213][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:28:10,213][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:28:10,214][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:28:10,214][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:28:10,214][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:28:10,214][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-12 17:28:10,214][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 17:28:10,214][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-12 17:28:10,214][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:28:10,215][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:28:10,215][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:28:10,215][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:28:10,215][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:28:10,215][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-12 17:28:10,215][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 17:28:10,215][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-12 17:28:10,215][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 17:28:10,215][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:28:10,216][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:28:10,216][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:28:14,838][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:28:14,840][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 17:28:14,841][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:28:14,841][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:25,  1.38s/it]Epoch 1/10:   3%|▎         | 2/63 [00:01<00:38,  1.59it/s]Epoch 1/10:   6%|▋         | 4/63 [00:01<00:17,  3.37it/s]Epoch 1/10:  10%|▉         | 6/63 [00:01<00:11,  4.95it/s]Epoch 1/10:  13%|█▎        | 8/63 [00:02<00:08,  6.28it/s]Epoch 1/10:  16%|█▌        | 10/63 [00:02<00:07,  7.34it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:02<00:07,  6.62it/s]Epoch 1/10:  21%|██        | 13/63 [00:02<00:06,  7.65it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:02<00:05,  8.42it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:03<00:05,  8.99it/s]Epoch 1/10:  30%|███       | 19/63 [00:03<00:04,  9.40it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:03<00:04,  9.69it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:03<00:04,  9.90it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:03<00:03, 10.05it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:04<00:03, 10.15it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:04<00:03, 10.23it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:04<00:03, 10.28it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:04<00:02, 10.32it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:04<00:02, 10.34it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:04<00:02, 10.36it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:05<00:02, 10.37it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:05<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  81%|████████  | 51/63 [00:06<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:07<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:07<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00, 11.09it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.44it/s]
[2025-04-12 17:28:24,385][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1408
[2025-04-12 17:28:24,584][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0773, Metrics: {'mse': 0.08030864596366882, 'rmse': 0.2833878013670822, 'r2': -0.23782336711883545}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:12,  4.94it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  8.07it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  9.12it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.62it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.90it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.07it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.18it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.25it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.30it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.33it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.35it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.25it/s]
[2025-04-12 17:28:31,186][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0611
[2025-04-12 17:28:31,392][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0713, Metrics: {'mse': 0.07381942123174667, 'rmse': 0.2716972970637483, 'r2': -0.1378028392791748}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:12,  4.87it/s]Epoch 3/10:   3%|▎         | 2/63 [00:00<00:10,  5.60it/s]Epoch 3/10:   6%|▋         | 4/63 [00:00<00:07,  7.87it/s]Epoch 3/10:  10%|▉         | 6/63 [00:00<00:06,  8.90it/s]Epoch 3/10:  13%|█▎        | 8/63 [00:00<00:05,  9.45it/s]Epoch 3/10:  16%|█▌        | 10/63 [00:01<00:05,  9.78it/s]Epoch 3/10:  19%|█▉        | 12/63 [00:01<00:05,  9.98it/s]Epoch 3/10:  22%|██▏       | 14/63 [00:01<00:04, 10.12it/s]Epoch 3/10:  25%|██▌       | 16/63 [00:01<00:04, 10.21it/s]Epoch 3/10:  29%|██▊       | 18/63 [00:01<00:04, 10.27it/s]Epoch 3/10:  32%|███▏      | 20/63 [00:02<00:04, 10.31it/s]Epoch 3/10:  35%|███▍      | 22/63 [00:02<00:03, 10.34it/s]Epoch 3/10:  38%|███▊      | 24/63 [00:02<00:03, 10.36it/s]Epoch 3/10:  41%|████▏     | 26/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  44%|████▍     | 28/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  48%|████▊     | 30/63 [00:03<00:03, 10.39it/s]Epoch 3/10:  51%|█████     | 32/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  54%|█████▍    | 34/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  57%|█████▋    | 36/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  60%|██████    | 38/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 40/63 [00:04<00:02, 10.40it/s]Epoch 3/10:  67%|██████▋   | 42/63 [00:04<00:02, 10.40it/s]Epoch 3/10:  70%|██████▉   | 44/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  73%|███████▎  | 46/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 48/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  79%|███████▉  | 50/63 [00:04<00:01, 10.40it/s]Epoch 3/10:  83%|████████▎ | 52/63 [00:05<00:01, 10.40it/s]Epoch 3/10:  86%|████████▌ | 54/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 56/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 58/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▌| 60/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  98%|█████████▊| 62/63 [00:06<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.12it/s]
[2025-04-12 17:28:38,085][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0445
[2025-04-12 17:28:38,310][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0524, Metrics: {'mse': 0.053010594099760056, 'rmse': 0.23024029642910046, 'r2': 0.18293046951293945}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:13,  4.47it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  7.73it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  8.90it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.48it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.81it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.00it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.17it/s]
[2025-04-12 17:28:44,901][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0288
[2025-04-12 17:28:45,149][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0425, Metrics: {'mse': 0.04322616755962372, 'rmse': 0.20790903674353292, 'r2': 0.3337409496307373}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:13,  4.67it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  7.88it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  9.00it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.54it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.85it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.03it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.15it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-12 17:28:51,712][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0247
[2025-04-12 17:28:51,937][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0417, Metrics: {'mse': 0.04257378354668617, 'rmse': 0.20633415506572383, 'r2': 0.34379637241363525}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:13,  4.47it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  7.72it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  8.90it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.48it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.80it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:28:58,530][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0217
[2025-04-12 17:28:58,758][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0399, Metrics: {'mse': 0.0406080037355423, 'rmse': 0.20151427675363923, 'r2': 0.374095618724823}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:13,  4.58it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:07,  7.81it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:06,  8.96it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:05,  9.51it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:05,  9.83it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 7/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-12 17:29:05,342][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0167
[2025-04-12 17:29:05,576][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0395, Metrics: {'mse': 0.040268708020448685, 'rmse': 0.200670645637195, 'r2': 0.379325270652771}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:13,  4.53it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:07,  7.76it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:06,  8.92it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:05,  9.49it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 8/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 10.14it/s]
[2025-04-12 17:29:12,212][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0162
[2025-04-12 17:29:12,452][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0300, Metrics: {'mse': 0.02911883033812046, 'rmse': 0.17064240486502896, 'r2': 0.551181972026825}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:13,  4.47it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:07,  7.72it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:06,  8.90it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:05,  9.48it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:05,  9.81it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 9/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:29:19,061][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0140
[2025-04-12 17:29:19,290][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0316, Metrics: {'mse': 0.03138638287782669, 'rmse': 0.17716202436703724, 'r2': 0.5162314176559448}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:13,  4.52it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:07,  7.76it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:06,  8.92it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:05,  9.49it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 10/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 10/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:29:25,475][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0137
[2025-04-12 17:29:25,699][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0280, Metrics: {'mse': 0.02759411372244358, 'rmse': 0.1661147607000762, 'r2': 0.5746829509735107}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▄▃▃▃▃▁▁
wandb:     best_val_mse █▇▄▃▃▃▃▁▁
wandb:      best_val_r2 ▁▂▅▆▆▆▆██
wandb:    best_val_rmse █▇▅▃▃▃▃▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▄▃▃▃▃▁▂▁
wandb:          val_mse █▇▄▃▃▃▃▁▂▁
wandb:           val_r2 ▁▂▅▆▆▆▆█▇█
wandb:         val_rmse █▇▅▃▃▃▃▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.028
wandb:     best_val_mse 0.02759
wandb:      best_val_r2 0.57468
wandb:    best_val_rmse 0.16611
wandb:            epoch 10
wandb:   final_test_mse 0.12008
wandb:    final_test_r2 -1.30655
wandb:  final_test_rmse 0.34652
wandb:  final_train_mse 0.00782
wandb:   final_train_r2 0.7453
wandb: final_train_rmse 0.08842
wandb:    final_val_mse 0.02759
wandb:     final_val_r2 0.57468
wandb:   final_val_rmse 0.16611
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01374
wandb:       train_time 69.20807
wandb:         val_loss 0.028
wandb:          val_mse 0.02759
wandb:           val_r2 0.57468
wandb:         val_rmse 0.16611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_172802-37sz0f8i
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_172802-37sz0f8i/logs
Cross-lingual experiment for complexity (ar → ja) completed successfully
Running cross-lingual question_type from ar to ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:29:46,845][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ar_to_ko/question_type
experiment_name: cross_lingual_question_type_ar_to_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ar
  eval_language: ko
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:29:46,845][__main__][INFO] - Normalized task: question_type
[2025-04-12 17:29:46,845][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 17:29:46,845][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:29:48,286][__main__][INFO] - Running cross-lingual experiment: ar -> ko
[2025-04-12 17:29:48,286][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 17:29:48,287][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:29:51,148][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:29:51,148][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:29:51,257][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:29:51,289][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:29:51,395][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:29:51,404][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:29:51,405][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:29:51,406][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:29:51,427][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:29:51,460][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:29:51,476][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:29:51,477][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:29:51,477][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:29:51,478][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:29:51,503][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:29:51,535][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:29:51,550][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:29:51,552][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:29:51,552][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:29:51,553][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:29:51,554][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:29:51,554][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:29:51,554][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:29:51,554][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:29:51,554][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-12 17:29:51,554][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-12 17:29:51,555][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:29:51,555][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:29:51,555][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:29:51,555][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:29:51,555][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:29:51,555][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:29:51,555][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-12 17:29:51,555][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-12 17:29:51,556][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:29:51,556][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:29:51,556][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:29:51,556][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:29:51,556][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:29:51,556][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:29:51,556][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-12 17:29:51,557][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-12 17:29:51,557][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:29:51,557][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:29:51,557][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:29:51,557][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:29:51,557][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:29:51,558][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
[2025-04-12 17:29:54,378][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:29:54,379][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:29:54,407][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:29:54,448][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:29:54,467][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 17:29:54,474][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:29:54,474][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 17:29:54,476][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:29:54,510][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:29:54,550][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:29:54,569][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 17:29:54,571][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:29:54,571][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 17:29:54,572][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:29:54,599][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:29:54,640][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:29:54,659][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 17:29:54,661][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:29:54,661][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 17:29:54,663][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 17:29:54,663][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:29:54,663][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:29:54,663][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:29:54,664][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:29:54,664][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-12 17:29:54,664][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-12 17:29:54,664][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 17:29:54,664][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:29:54,664][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:29:54,664][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:29:54,664][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:29:54,665][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:29:54,665][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 17:29:54,665][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 17:29:54,665][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 17:29:54,665][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:29:54,665][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:29:54,665][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:29:54,665][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:29:54,666][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:29:54,666][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:29:54,666][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:29:54,666][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 17:29:54,666][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:29:54,666][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 17:29:54,666][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:29:54,666][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:29:54,667][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:30:00,047][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:30:00,050][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 17:30:00,050][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:30:00,050][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:25,  1.38s/it]Epoch 1/10:   3%|▎         | 2/63 [00:01<00:38,  1.60it/s]Epoch 1/10:   6%|▋         | 4/63 [00:01<00:17,  3.40it/s]Epoch 1/10:  10%|▉         | 6/63 [00:01<00:11,  4.98it/s]Epoch 1/10:  13%|█▎        | 8/63 [00:02<00:08,  6.30it/s]Epoch 1/10:  16%|█▌        | 10/63 [00:02<00:07,  7.36it/s]Epoch 1/10:  19%|█▉        | 12/63 [00:02<00:06,  8.18it/s]Epoch 1/10:  22%|██▏       | 14/63 [00:02<00:05,  8.80it/s]Epoch 1/10:  25%|██▌       | 16/63 [00:02<00:05,  9.25it/s]Epoch 1/10:  29%|██▊       | 18/63 [00:03<00:04,  9.58it/s]Epoch 1/10:  32%|███▏      | 20/63 [00:03<00:04,  9.82it/s]Epoch 1/10:  35%|███▍      | 22/63 [00:03<00:04,  9.99it/s]Epoch 1/10:  38%|███▊      | 24/63 [00:03<00:03, 10.11it/s]Epoch 1/10:  41%|████▏     | 26/63 [00:03<00:03, 10.19it/s]Epoch 1/10:  44%|████▍     | 28/63 [00:03<00:03, 10.25it/s]Epoch 1/10:  48%|████▊     | 30/63 [00:04<00:03, 10.28it/s]Epoch 1/10:  51%|█████     | 32/63 [00:04<00:03, 10.32it/s]Epoch 1/10:  54%|█████▍    | 34/63 [00:04<00:02, 10.34it/s]Epoch 1/10:  57%|█████▋    | 36/63 [00:04<00:02, 10.35it/s]Epoch 1/10:  60%|██████    | 38/63 [00:04<00:02, 10.35it/s]Epoch 1/10:  63%|██████▎   | 40/63 [00:05<00:02, 10.36it/s]Epoch 1/10:  67%|██████▋   | 42/63 [00:05<00:02, 10.37it/s]Epoch 1/10:  70%|██████▉   | 44/63 [00:05<00:01, 10.37it/s]Epoch 1/10:  73%|███████▎  | 46/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  76%|███████▌  | 48/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  79%|███████▉  | 50/63 [00:06<00:01, 10.38it/s]Epoch 1/10:  83%|████████▎ | 52/63 [00:06<00:01, 10.38it/s]Epoch 1/10:  86%|████████▌ | 54/63 [00:06<00:00, 10.38it/s]Epoch 1/10:  89%|████████▉ | 56/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 58/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▌| 60/63 [00:07<00:00, 10.39it/s]Epoch 1/10:  98%|█████████▊| 62/63 [00:07<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.57it/s]
[2025-04-12 17:30:09,961][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6786
[2025-04-12 17:30:10,167][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6689, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9473684210526315}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:13,  4.65it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  7.86it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  8.99it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.53it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.84it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.06it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.15it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.23it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.28it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:06<00:00, 10.31it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.21it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.16it/s]
[2025-04-12 17:30:16,978][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.4403
[2025-04-12 17:30:17,186][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2506, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:13,  4.53it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  7.77it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  8.92it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.49it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.81it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.38it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.38it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.38it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.38it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:30:23,830][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1303
[2025-04-12 17:30:24,042][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1745, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:13,  4.63it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  7.84it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  8.97it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.52it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.83it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-12 17:30:30,604][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0490
[2025-04-12 17:30:30,834][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2095, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:13,  4.76it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  7.94it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  9.03it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.56it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.86it/s]Epoch 5/10:  16%|█▌        | 10/63 [00:01<00:05,  9.87it/s]Epoch 5/10:  19%|█▉        | 12/63 [00:01<00:05, 10.06it/s]Epoch 5/10:  22%|██▏       | 14/63 [00:01<00:04, 10.18it/s]Epoch 5/10:  25%|██▌       | 16/63 [00:01<00:04, 10.25it/s]Epoch 5/10:  29%|██▊       | 18/63 [00:01<00:04, 10.29it/s]Epoch 5/10:  32%|███▏      | 20/63 [00:02<00:04, 10.32it/s]Epoch 5/10:  35%|███▍      | 22/63 [00:02<00:03, 10.34it/s]Epoch 5/10:  38%|███▊      | 24/63 [00:02<00:03, 10.36it/s]Epoch 5/10:  41%|████▏     | 26/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  44%|████▍     | 28/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  48%|████▊     | 30/63 [00:03<00:03, 10.37it/s]Epoch 5/10:  51%|█████     | 32/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  54%|█████▍    | 34/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  57%|█████▋    | 36/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  60%|██████    | 38/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 40/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  67%|██████▋   | 42/63 [00:04<00:02, 10.39it/s]Epoch 5/10:  70%|██████▉   | 44/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  73%|███████▎  | 46/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 48/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  79%|███████▉  | 50/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  83%|████████▎ | 52/63 [00:05<00:01, 10.39it/s]Epoch 5/10:  86%|████████▌ | 54/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 56/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 58/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▌| 60/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  98%|█████████▊| 62/63 [00:06<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.13it/s]
[2025-04-12 17:30:37,058][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0335
[2025-04-12 17:30:37,283][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2242, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:13,  4.65it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  7.86it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  8.99it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.53it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.84it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05, 10.03it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.37it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.38it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:30:43,463][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0204
[2025-04-12 17:30:43,696][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2147, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
[2025-04-12 17:30:43,697][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▂▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▅▂▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁██▁▁▁
wandb:               val_f1 ▁██▂▂▂
wandb:             val_loss █▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.97727
wandb:          best_val_f1 0.97561
wandb:        best_val_loss 0.17453
wandb:                epoch 6
wandb:  final_test_accuracy 0.52727
wandb:        final_test_f1 0.16129
wandb: final_train_accuracy 1
wandb:       final_train_f1 1
wandb:   final_val_accuracy 0.97727
wandb:         final_val_f1 0.97561
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02043
wandb:           train_time 41.08481
wandb:         val_accuracy 0.95455
wandb:               val_f1 0.95238
wandb:             val_loss 0.21467
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_172946-zhtl01lk
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_172946-zhtl01lk/logs
Cross-lingual experiment for question_type (ar → ko) completed successfully
Running cross-lingual complexity from ar to ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:31:03,818][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ar_to_ko/complexity
experiment_name: cross_lingual_complexity_ar_to_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ar
  eval_language: ko
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:31:03,818][__main__][INFO] - Normalized task: complexity
[2025-04-12 17:31:03,819][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 17:31:03,819][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:31:05,312][__main__][INFO] - Running cross-lingual experiment: ar -> ko
[2025-04-12 17:31:05,312][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 17:31:05,312][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:31:08,142][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:31:08,142][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:31:08,246][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:31:08,278][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:31:08,377][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:31:08,386][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:31:08,387][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:31:08,388][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:31:08,411][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:31:08,446][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:31:08,462][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:31:08,463][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:31:08,463][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:31:08,464][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:31:08,486][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:31:08,518][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:31:08,531][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:31:08,533][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:31:08,533][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:31:08,534][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:31:08,535][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:31:08,535][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:31:08,535][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:31:08,536][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:31:08,536][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:31:08,536][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-12 17:31:08,536][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:31:08,536][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-12 17:31:08,536][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:31:08,536][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:31:08,537][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:31:08,537][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:31:08,537][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:31:08,537][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-12 17:31:08,537][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:31:08,537][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-12 17:31:08,537][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:31:08,538][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:31:08,538][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:31:08,538][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:31:08,538][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:31:08,538][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-12 17:31:08,538][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:31:08,538][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-12 17:31:08,538][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:31:08,539][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:31:08,539][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:31:08,539][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
[2025-04-12 17:31:11,318][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:31:11,319][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:31:11,342][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:31:11,371][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:31:11,385][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 17:31:11,391][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:31:11,392][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 17:31:11,393][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:31:11,413][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:31:11,444][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:31:11,458][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 17:31:11,460][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:31:11,460][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 17:31:11,461][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:31:11,482][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:31:11,518][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:31:11,532][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 17:31:11,533][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:31:11,534][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 17:31:11,535][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 17:31:11,535][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:31:11,535][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:31:11,535][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:31:11,536][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:31:11,536][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:31:11,536][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-12 17:31:11,536][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 17:31:11,536][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-04-12 17:31:11,537][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:31:11,537][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:31:11,537][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:31:11,537][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:31:11,537][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:31:11,537][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-12 17:31:11,537][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 17:31:11,537][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-12 17:31:11,538][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:31:11,538][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:31:11,538][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:31:11,538][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:31:11,538][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:31:11,538][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-12 17:31:11,538][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 17:31:11,538][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-12 17:31:11,539][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 17:31:11,539][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:31:11,539][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:31:11,539][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:31:16,765][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:31:16,768][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 17:31:16,768][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:31:16,768][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:15,  1.22s/it]Epoch 1/10:   5%|▍         | 3/63 [00:01<00:23,  2.58it/s]Epoch 1/10:   8%|▊         | 5/63 [00:01<00:13,  4.21it/s]Epoch 1/10:  11%|█         | 7/63 [00:01<00:09,  5.63it/s]Epoch 1/10:  14%|█▍        | 9/63 [00:01<00:07,  6.80it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:02<00:06,  7.74it/s]Epoch 1/10:  21%|██        | 13/63 [00:02<00:05,  8.46it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:02<00:05,  9.00it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:02<00:04,  9.40it/s]Epoch 1/10:  30%|███       | 19/63 [00:02<00:04,  9.69it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:03<00:04,  9.90it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:03<00:03, 10.04it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:03<00:03, 10.15it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:03<00:03, 10.22it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:03<00:03, 10.27it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:04<00:03, 10.31it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:04<00:02, 10.34it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:04<00:02, 10.35it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:04<00:02, 10.37it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:04<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:05<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:05<00:01, 10.40it/s]Epoch 1/10:  81%|████████  | 51/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:06<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00, 11.10it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.78it/s]
[2025-04-12 17:31:26,188][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1408
[2025-04-12 17:31:26,381][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0773, Metrics: {'mse': 0.08030864596366882, 'rmse': 0.2833878013670822, 'r2': -0.23782336711883545}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:13,  4.64it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  7.85it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  8.98it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.53it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.84it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.38it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-12 17:31:33,153][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0611
[2025-04-12 17:31:33,357][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0713, Metrics: {'mse': 0.07381942123174667, 'rmse': 0.2716972970637483, 'r2': -0.1378028392791748}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:13,  4.69it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  7.89it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  9.01it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.55it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.86it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.04it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.29it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.33it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.35it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.37it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.38it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-12 17:31:39,989][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0445
[2025-04-12 17:31:40,212][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0524, Metrics: {'mse': 0.053010594099760056, 'rmse': 0.23024029642910046, 'r2': 0.18293046951293945}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:14,  4.39it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  7.66it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  8.86it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.46it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.79it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05,  9.99it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:31:46,783][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0288
[2025-04-12 17:31:46,998][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0425, Metrics: {'mse': 0.04322616755962372, 'rmse': 0.20790903674353292, 'r2': 0.3337409496307373}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:13,  4.52it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  7.77it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  8.92it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.49it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.20it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.26it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.24it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.29it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.32it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.34it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.35it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:31:53,569][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0247
[2025-04-12 17:31:53,796][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0417, Metrics: {'mse': 0.04257378354668617, 'rmse': 0.20633415506572383, 'r2': 0.34379637241363525}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:13,  4.53it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  7.76it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  8.92it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.49it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.81it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.30it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.18it/s]
[2025-04-12 17:32:00,400][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0217
[2025-04-12 17:32:00,631][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0399, Metrics: {'mse': 0.0406080037355423, 'rmse': 0.20151427675363923, 'r2': 0.374095618724823}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:13,  4.75it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:07,  7.93it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:06,  9.03it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:05,  9.56it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:05,  9.86it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:01<00:05, 10.04it/s]Epoch 7/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-12 17:32:07,222][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0167
[2025-04-12 17:32:07,447][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0395, Metrics: {'mse': 0.040268708020448685, 'rmse': 0.200670645637195, 'r2': 0.379325270652771}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:13,  4.52it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:07,  7.76it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:06,  8.93it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:05,  9.49it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:05,  9.81it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 8/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-12 17:32:14,046][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0162
[2025-04-12 17:32:14,275][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0300, Metrics: {'mse': 0.02911883033812046, 'rmse': 0.17064240486502896, 'r2': 0.551181972026825}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:14,  4.42it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:07,  7.69it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:06,  8.88it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:05,  9.46it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:05,  9.79it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:01<00:05, 10.00it/s]Epoch 9/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 9/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 10.18it/s]
[2025-04-12 17:32:20,883][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0140
[2025-04-12 17:32:21,112][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0316, Metrics: {'mse': 0.03138638287782669, 'rmse': 0.17716202436703724, 'r2': 0.5162314176559448}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:13,  4.48it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:07,  7.73it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:06,  8.90it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:05,  9.48it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:05,  9.81it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 10/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.38it/s]Epoch 10/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:32:27,295][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0137
[2025-04-12 17:32:27,527][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0280, Metrics: {'mse': 0.02759411372244358, 'rmse': 0.1661147607000762, 'r2': 0.5746829509735107}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▄▃▃▃▃▁▁
wandb:     best_val_mse █▇▄▃▃▃▃▁▁
wandb:      best_val_r2 ▁▂▅▆▆▆▆██
wandb:    best_val_rmse █▇▅▃▃▃▃▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▄▃▃▃▃▁▂▁
wandb:          val_mse █▇▄▃▃▃▃▁▂▁
wandb:           val_r2 ▁▂▅▆▆▆▆█▇█
wandb:         val_rmse █▇▅▃▃▃▃▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.028
wandb:     best_val_mse 0.02759
wandb:      best_val_r2 0.57468
wandb:    best_val_rmse 0.16611
wandb:            epoch 10
wandb:   final_test_mse 0.07184
wandb:    final_test_r2 -1.23077
wandb:  final_test_rmse 0.26804
wandb:  final_train_mse 0.00782
wandb:   final_train_r2 0.7453
wandb: final_train_rmse 0.08842
wandb:    final_val_mse 0.02759
wandb:     final_val_r2 0.57468
wandb:   final_val_rmse 0.16611
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01374
wandb:       train_time 68.99021
wandb:         val_loss 0.028
wandb:          val_mse 0.02759
wandb:           val_r2 0.57468
wandb:         val_rmse 0.16611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_173103-751pka3q
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_173103-751pka3q/logs
Cross-lingual experiment for complexity (ar → ko) completed successfully
Running cross-lingual question_type from ar to ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:32:49,580][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ar_to_ru/question_type
experiment_name: cross_lingual_question_type_ar_to_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ar
  eval_language: ru
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:32:49,580][__main__][INFO] - Normalized task: question_type
[2025-04-12 17:32:49,581][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 17:32:49,581][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:32:51,292][__main__][INFO] - Running cross-lingual experiment: ar -> ru
[2025-04-12 17:32:51,293][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 17:32:51,293][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:32:54,186][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:32:54,187][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:32:54,284][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:32:54,314][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:32:54,415][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:32:54,424][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:32:54,425][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:32:54,426][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:32:54,454][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:32:54,487][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:32:54,504][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:32:54,505][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:32:54,505][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:32:54,506][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:32:54,534][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:32:54,569][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:32:54,587][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:32:54,590][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:32:54,590][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:32:54,591][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:32:54,592][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:32:54,592][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:32:54,592][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:32:54,592][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:32:54,592][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-12 17:32:54,592][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-12 17:32:54,592][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:32:54,593][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:32:54,593][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:32:54,593][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:32:54,593][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:32:54,593][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:32:54,593][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-12 17:32:54,593][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-12 17:32:54,593][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:32:54,594][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:32:54,594][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:32:54,594][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:32:54,594][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:32:54,594][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:32:54,594][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-12 17:32:54,594][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-12 17:32:54,595][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:32:54,595][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:32:54,595][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:32:54,595][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:32:54,595][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:32:54,596][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
[2025-04-12 17:32:57,435][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:32:57,435][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:32:57,464][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:32:57,508][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:32:57,532][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 17:32:57,541][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:32:57,542][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 17:32:57,544][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:32:57,574][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:32:57,616][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:32:57,650][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 17:32:57,651][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:32:57,651][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 17:32:57,653][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:32:57,686][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:32:57,726][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:32:57,755][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 17:32:57,756][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:32:57,756][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 17:32:57,758][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 17:32:57,759][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:32:57,759][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:32:57,759][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:32:57,759][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:32:57,760][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 17:32:57,760][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-12 17:32:57,760][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 17:32:57,760][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:32:57,760][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:32:57,760][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:32:57,760][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:32:57,760][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:32:57,761][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 17:32:57,761][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 17:32:57,761][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 17:32:57,761][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:32:57,761][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:32:57,761][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:32:57,761][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:32:57,761][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:32:57,762][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:32:57,762][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:32:57,762][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 17:32:57,762][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:32:57,762][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 17:32:57,762][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:32:57,762][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:32:57,763][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:33:03,108][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:33:03,111][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 17:33:03,111][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:33:03,111][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:17,  1.25s/it]Epoch 1/10:   3%|▎         | 2/63 [00:01<00:35,  1.74it/s]Epoch 1/10:   6%|▋         | 4/63 [00:01<00:16,  3.62it/s]Epoch 1/10:  10%|▉         | 6/63 [00:01<00:10,  5.23it/s]Epoch 1/10:  13%|█▎        | 8/63 [00:01<00:08,  6.54it/s]Epoch 1/10:  16%|█▌        | 10/63 [00:02<00:07,  7.56it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:02<00:07,  6.79it/s]Epoch 1/10:  21%|██        | 13/63 [00:02<00:06,  7.79it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:02<00:05,  8.53it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:02<00:05,  9.07it/s]Epoch 1/10:  30%|███       | 19/63 [00:03<00:04,  9.45it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:03<00:04,  9.73it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:03<00:04,  9.92it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:03<00:03, 10.06it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:03<00:03, 10.16it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:04<00:03, 10.23it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:04<00:03, 10.28it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:04<00:02, 10.31it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:04<00:02, 10.33it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:04<00:02, 10.35it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:05<00:02, 10.36it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:05<00:02, 10.37it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  81%|████████  | 51/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:06<00:00, 10.38it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:07<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00, 11.08it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.57it/s]
[2025-04-12 17:33:12,982][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6786
[2025-04-12 17:33:13,175][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6689, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9473684210526315}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:13,  4.63it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  7.84it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  8.97it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.51it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.38it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.38it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:33:19,828][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.4403
[2025-04-12 17:33:20,043][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2506, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:13,  4.59it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  7.81it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  8.95it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.51it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.21it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:33:26,688][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1303
[2025-04-12 17:33:26,913][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1745, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:13,  4.68it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  7.89it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  9.00it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.54it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.84it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.03it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:33:33,488][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0490
[2025-04-12 17:33:33,727][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2095, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:13,  4.71it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  7.90it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  9.01it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.54it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.84it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.38it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.38it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.38it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.38it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.19it/s]
[2025-04-12 17:33:39,915][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0335
[2025-04-12 17:33:40,141][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2242, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:13,  4.72it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  7.91it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  9.01it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.55it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.85it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05, 10.03it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.15it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.38it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.38it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:33:46,321][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0204
[2025-04-12 17:33:46,549][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2147, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523}
[2025-04-12 17:33:46,550][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▂▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▅▂▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁██▁▁▁
wandb:               val_f1 ▁██▂▂▂
wandb:             val_loss █▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.97727
wandb:          best_val_f1 0.97561
wandb:        best_val_loss 0.17453
wandb:                epoch 6
wandb:  final_test_accuracy 0.9
wandb:        final_test_f1 0.88889
wandb: final_train_accuracy 1
wandb:       final_train_f1 1
wandb:   final_val_accuracy 0.97727
wandb:         final_val_f1 0.97561
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02043
wandb:           train_time 40.92505
wandb:         val_accuracy 0.95455
wandb:               val_f1 0.95238
wandb:             val_loss 0.21467
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_173249-2v9qxa0b
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_173249-2v9qxa0b/logs
Cross-lingual experiment for question_type (ar → ru) completed successfully
Running cross-lingual complexity from ar to ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:34:06,578][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ar_to_ru/complexity
experiment_name: cross_lingual_complexity_ar_to_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ar
  eval_language: ru
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:34:06,579][__main__][INFO] - Normalized task: complexity
[2025-04-12 17:34:06,579][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 17:34:06,579][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:34:08,302][__main__][INFO] - Running cross-lingual experiment: ar -> ru
[2025-04-12 17:34:08,302][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 17:34:08,303][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:34:11,166][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:34:11,166][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:34:11,243][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:34:11,270][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:34:11,366][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:34:11,376][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:34:11,376][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:34:11,378][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:34:11,397][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:34:11,438][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:34:11,453][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:34:11,455][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:34:11,455][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:34:11,456][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:34:11,478][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:34:11,511][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:34:11,526][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:34:11,527][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:34:11,527][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:34:11,528][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:34:11,529][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:34:11,529][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:34:11,529][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:34:11,529][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:34:11,529][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:34:11,530][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-12 17:34:11,530][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:34:11,530][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-12 17:34:11,530][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:34:11,530][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:34:11,530][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:34:11,530][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:34:11,531][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:34:11,531][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-12 17:34:11,531][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:34:11,531][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-12 17:34:11,531][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:34:11,531][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:34:11,531][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:34:11,531][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:34:11,532][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:34:11,532][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-12 17:34:11,532][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:34:11,532][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-12 17:34:11,532][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:34:11,532][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:34:11,533][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:34:11,533][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
[2025-04-12 17:34:14,358][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:34:14,359][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:34:14,433][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:34:14,474][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:34:14,492][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 17:34:14,501][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:34:14,502][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 17:34:14,503][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:34:14,532][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:34:14,568][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:34:14,582][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 17:34:14,584][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:34:14,584][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 17:34:14,585][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:34:14,610][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:34:14,645][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:34:14,661][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 17:34:14,663][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:34:14,663][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 17:34:14,664][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 17:34:14,664][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:34:14,665][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:34:14,665][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:34:14,665][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:34:14,665][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:34:14,665][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-12 17:34:14,665][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 17:34:14,665][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-04-12 17:34:14,666][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:34:14,666][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:34:14,666][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:34:14,666][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:34:14,666][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:34:14,666][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-12 17:34:14,666][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 17:34:14,667][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-12 17:34:14,667][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:34:14,667][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:34:14,667][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:34:14,667][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:34:14,667][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:34:14,667][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-12 17:34:14,667][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 17:34:14,668][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-12 17:34:14,668][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 17:34:14,668][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:34:14,668][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:34:14,668][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:34:20,165][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:34:20,167][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 17:34:20,168][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:34:20,168][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/63 [00:01<01:14,  1.21s/it]Epoch 1/10:   3%|▎         | 2/63 [00:01<00:34,  1.79it/s]Epoch 1/10:   6%|▋         | 4/63 [00:01<00:15,  3.72it/s]Epoch 1/10:  10%|▉         | 6/63 [00:01<00:10,  5.35it/s]Epoch 1/10:  13%|█▎        | 8/63 [00:01<00:08,  6.65it/s]Epoch 1/10:  16%|█▌        | 10/63 [00:02<00:06,  7.66it/s]Epoch 1/10:  17%|█▋        | 11/63 [00:02<00:07,  6.82it/s]Epoch 1/10:  21%|██        | 13/63 [00:02<00:06,  7.82it/s]Epoch 1/10:  24%|██▍       | 15/63 [00:02<00:05,  8.55it/s]Epoch 1/10:  27%|██▋       | 17/63 [00:02<00:05,  9.08it/s]Epoch 1/10:  30%|███       | 19/63 [00:03<00:04,  9.47it/s]Epoch 1/10:  33%|███▎      | 21/63 [00:03<00:04,  9.74it/s]Epoch 1/10:  37%|███▋      | 23/63 [00:03<00:04,  9.94it/s]Epoch 1/10:  40%|███▉      | 25/63 [00:03<00:03, 10.07it/s]Epoch 1/10:  43%|████▎     | 27/63 [00:03<00:03, 10.17it/s]Epoch 1/10:  46%|████▌     | 29/63 [00:04<00:03, 10.24it/s]Epoch 1/10:  49%|████▉     | 31/63 [00:04<00:03, 10.28it/s]Epoch 1/10:  52%|█████▏    | 33/63 [00:04<00:02, 10.31it/s]Epoch 1/10:  56%|█████▌    | 35/63 [00:04<00:02, 10.33it/s]Epoch 1/10:  59%|█████▊    | 37/63 [00:04<00:02, 10.35it/s]Epoch 1/10:  62%|██████▏   | 39/63 [00:04<00:02, 10.36it/s]Epoch 1/10:  65%|██████▌   | 41/63 [00:05<00:02, 10.37it/s]Epoch 1/10:  68%|██████▊   | 43/63 [00:05<00:01, 10.38it/s]Epoch 1/10:  71%|███████▏  | 45/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  75%|███████▍  | 47/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  78%|███████▊  | 49/63 [00:05<00:01, 10.39it/s]Epoch 1/10:  81%|████████  | 51/63 [00:06<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 53/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  87%|████████▋ | 55/63 [00:06<00:00, 10.39it/s]Epoch 1/10:  90%|█████████ | 57/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  94%|█████████▎| 59/63 [00:06<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 61/63 [00:07<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00, 11.10it/s]Epoch 1/10: 100%|██████████| 63/63 [00:07<00:00,  8.65it/s]
[2025-04-12 17:34:29,491][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1408
[2025-04-12 17:34:29,685][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0773, Metrics: {'mse': 0.08030864596366882, 'rmse': 0.2833878013670822, 'r2': -0.23782336711883545}
Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/63 [00:00<00:13,  4.76it/s]Epoch 2/10:   5%|▍         | 3/63 [00:00<00:07,  7.94it/s]Epoch 2/10:   8%|▊         | 5/63 [00:00<00:06,  9.03it/s]Epoch 2/10:  11%|█         | 7/63 [00:00<00:05,  9.56it/s]Epoch 2/10:  14%|█▍        | 9/63 [00:00<00:05,  9.86it/s]Epoch 2/10:  17%|█▋        | 11/63 [00:01<00:05, 10.04it/s]Epoch 2/10:  21%|██        | 13/63 [00:01<00:04, 10.16it/s]Epoch 2/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 2/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 2/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 2/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 2/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 2/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 2/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 2/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 2/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 2/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 2/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 2/10: 100%|██████████| 63/63 [00:06<00:00, 10.23it/s]
[2025-04-12 17:34:36,304][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0611
[2025-04-12 17:34:36,514][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0713, Metrics: {'mse': 0.07381942123174667, 'rmse': 0.2716972970637483, 'r2': -0.1378028392791748}
Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/63 [00:00<00:13,  4.53it/s]Epoch 3/10:   5%|▍         | 3/63 [00:00<00:07,  7.77it/s]Epoch 3/10:   8%|▊         | 5/63 [00:00<00:06,  8.93it/s]Epoch 3/10:  11%|█         | 7/63 [00:00<00:05,  9.50it/s]Epoch 3/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 3/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 3/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 3/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 3/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 3/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 3/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 3/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 3/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 3/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 3/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 3/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 3/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 3/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:34:43,159][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0445
[2025-04-12 17:34:43,374][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0524, Metrics: {'mse': 0.053010594099760056, 'rmse': 0.23024029642910046, 'r2': 0.18293046951293945}
Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/63 [00:00<00:13,  4.58it/s]Epoch 4/10:   5%|▍         | 3/63 [00:00<00:07,  7.81it/s]Epoch 4/10:   8%|▊         | 5/63 [00:00<00:06,  8.95it/s]Epoch 4/10:  11%|█         | 7/63 [00:00<00:05,  9.51it/s]Epoch 4/10:  14%|█▍        | 9/63 [00:00<00:05,  9.83it/s]Epoch 4/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 4/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 4/10:  24%|██▍       | 15/63 [00:01<00:04, 10.23it/s]Epoch 4/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 4/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 4/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 4/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 4/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 4/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 4/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 4/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 4/10:  81%|████████  | 51/63 [00:05<00:01, 10.33it/s]Epoch 4/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.35it/s]Epoch 4/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.36it/s]Epoch 4/10:  90%|█████████ | 57/63 [00:05<00:00, 10.38it/s]Epoch 4/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 4/10: 100%|██████████| 63/63 [00:06<00:00, 10.21it/s]
[2025-04-12 17:34:49,945][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0288
[2025-04-12 17:34:50,200][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0425, Metrics: {'mse': 0.04322616755962372, 'rmse': 0.20790903674353292, 'r2': 0.3337409496307373}
Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/63 [00:00<00:13,  4.57it/s]Epoch 5/10:   5%|▍         | 3/63 [00:00<00:07,  7.80it/s]Epoch 5/10:   8%|▊         | 5/63 [00:00<00:06,  8.95it/s]Epoch 5/10:  11%|█         | 7/63 [00:00<00:05,  9.50it/s]Epoch 5/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 5/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 5/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 5/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 5/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 5/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 5/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 5/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 5/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 5/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 5/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 5/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 5/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 5/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 5/10: 100%|██████████| 63/63 [00:06<00:00, 10.17it/s]
[2025-04-12 17:34:56,790][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0247
[2025-04-12 17:34:57,023][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0417, Metrics: {'mse': 0.04257378354668617, 'rmse': 0.20633415506572383, 'r2': 0.34379637241363525}
Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/63 [00:00<00:13,  4.58it/s]Epoch 6/10:   5%|▍         | 3/63 [00:00<00:07,  7.81it/s]Epoch 6/10:   8%|▊         | 5/63 [00:00<00:06,  8.95it/s]Epoch 6/10:  11%|█         | 7/63 [00:00<00:05,  9.51it/s]Epoch 6/10:  14%|█▍        | 9/63 [00:00<00:05,  9.83it/s]Epoch 6/10:  17%|█▋        | 11/63 [00:01<00:05, 10.02it/s]Epoch 6/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 6/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 6/10:  27%|██▋       | 17/63 [00:01<00:04, 10.28it/s]Epoch 6/10:  30%|███       | 19/63 [00:01<00:04, 10.32it/s]Epoch 6/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 6/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 6/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 6/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 6/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 6/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 6/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 6/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 6/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:35:03,613][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0217
[2025-04-12 17:35:03,838][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0399, Metrics: {'mse': 0.0406080037355423, 'rmse': 0.20151427675363923, 'r2': 0.374095618724823}
Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/63 [00:00<00:13,  4.57it/s]Epoch 7/10:   5%|▍         | 3/63 [00:00<00:07,  7.79it/s]Epoch 7/10:   8%|▊         | 5/63 [00:00<00:06,  8.94it/s]Epoch 7/10:  11%|█         | 7/63 [00:00<00:05,  9.50it/s]Epoch 7/10:  14%|█▍        | 9/63 [00:00<00:05,  9.82it/s]Epoch 7/10:  17%|█▋        | 11/63 [00:01<00:05, 10.01it/s]Epoch 7/10:  21%|██        | 13/63 [00:01<00:04, 10.14it/s]Epoch 7/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 7/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 7/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 7/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 7/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 7/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 7/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 7/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 7/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 7/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 7/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 7/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.39it/s]Epoch 7/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 11.28it/s]Epoch 7/10: 100%|██████████| 63/63 [00:06<00:00, 10.20it/s]
[2025-04-12 17:35:10,423][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0167
[2025-04-12 17:35:10,652][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0395, Metrics: {'mse': 0.040268708020448685, 'rmse': 0.200670645637195, 'r2': 0.379325270652771}
Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/63 [00:00<00:13,  4.44it/s]Epoch 8/10:   5%|▍         | 3/63 [00:00<00:07,  7.70it/s]Epoch 8/10:   8%|▊         | 5/63 [00:00<00:06,  8.88it/s]Epoch 8/10:  11%|█         | 7/63 [00:00<00:05,  9.46it/s]Epoch 8/10:  14%|█▍        | 9/63 [00:00<00:05,  9.79it/s]Epoch 8/10:  17%|█▋        | 11/63 [00:01<00:05, 10.00it/s]Epoch 8/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 8/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 8/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 8/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 8/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 8/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 8/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 8/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 8/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 8/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 8/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 8/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 8/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 8/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 8/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 8/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 8/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 8/10: 100%|██████████| 63/63 [00:06<00:00, 10.17it/s]
[2025-04-12 17:35:17,272][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0162
[2025-04-12 17:35:17,504][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0300, Metrics: {'mse': 0.02911883033812046, 'rmse': 0.17064240486502896, 'r2': 0.551181972026825}
Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/63 [00:00<00:13,  4.43it/s]Epoch 9/10:   5%|▍         | 3/63 [00:00<00:07,  7.70it/s]Epoch 9/10:   8%|▊         | 5/63 [00:00<00:06,  8.88it/s]Epoch 9/10:  11%|█         | 7/63 [00:00<00:05,  9.46it/s]Epoch 9/10:  14%|█▍        | 9/63 [00:00<00:05,  9.79it/s]Epoch 9/10:  17%|█▋        | 11/63 [00:01<00:05, 10.00it/s]Epoch 9/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 9/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 9/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 9/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 9/10:  33%|███▎      | 21/63 [00:02<00:04, 10.33it/s]Epoch 9/10:  37%|███▋      | 23/63 [00:02<00:03, 10.35it/s]Epoch 9/10:  40%|███▉      | 25/63 [00:02<00:03, 10.36it/s]Epoch 9/10:  43%|████▎     | 27/63 [00:02<00:03, 10.37it/s]Epoch 9/10:  46%|████▌     | 29/63 [00:02<00:03, 10.38it/s]Epoch 9/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 9/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 9/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.39it/s]Epoch 9/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.39it/s]Epoch 9/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 9/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.40it/s]Epoch 9/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.40it/s]Epoch 9/10:  81%|████████  | 51/63 [00:05<00:01, 10.40it/s]Epoch 9/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 9/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.41it/s]Epoch 9/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 11.30it/s]Epoch 9/10: 100%|██████████| 63/63 [00:06<00:00, 10.17it/s]
[2025-04-12 17:35:24,132][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0140
[2025-04-12 17:35:24,360][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0316, Metrics: {'mse': 0.03138638287782669, 'rmse': 0.17716202436703724, 'r2': 0.5162314176559448}
Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/63 [00:00<00:14,  4.42it/s]Epoch 10/10:   5%|▍         | 3/63 [00:00<00:07,  7.69it/s]Epoch 10/10:   8%|▊         | 5/63 [00:00<00:06,  8.87it/s]Epoch 10/10:  11%|█         | 7/63 [00:00<00:05,  9.46it/s]Epoch 10/10:  14%|█▍        | 9/63 [00:00<00:05,  9.79it/s]Epoch 10/10:  17%|█▋        | 11/63 [00:01<00:05, 10.00it/s]Epoch 10/10:  21%|██        | 13/63 [00:01<00:04, 10.13it/s]Epoch 10/10:  24%|██▍       | 15/63 [00:01<00:04, 10.22it/s]Epoch 10/10:  27%|██▋       | 17/63 [00:01<00:04, 10.27it/s]Epoch 10/10:  30%|███       | 19/63 [00:01<00:04, 10.31it/s]Epoch 10/10:  33%|███▎      | 21/63 [00:02<00:04, 10.34it/s]Epoch 10/10:  37%|███▋      | 23/63 [00:02<00:03, 10.36it/s]Epoch 10/10:  40%|███▉      | 25/63 [00:02<00:03, 10.37it/s]Epoch 10/10:  43%|████▎     | 27/63 [00:02<00:03, 10.38it/s]Epoch 10/10:  46%|████▌     | 29/63 [00:02<00:03, 10.39it/s]Epoch 10/10:  49%|████▉     | 31/63 [00:03<00:03, 10.39it/s]Epoch 10/10:  52%|█████▏    | 33/63 [00:03<00:02, 10.39it/s]Epoch 10/10:  56%|█████▌    | 35/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  59%|█████▊    | 37/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  62%|██████▏   | 39/63 [00:03<00:02, 10.40it/s]Epoch 10/10:  65%|██████▌   | 41/63 [00:04<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 43/63 [00:04<00:01, 10.39it/s]Epoch 10/10:  71%|███████▏  | 45/63 [00:04<00:01, 10.39it/s]Epoch 10/10:  75%|███████▍  | 47/63 [00:04<00:01, 10.39it/s]Epoch 10/10:  78%|███████▊  | 49/63 [00:04<00:01, 10.39it/s]Epoch 10/10:  81%|████████  | 51/63 [00:05<00:01, 10.39it/s]Epoch 10/10:  84%|████████▍ | 53/63 [00:05<00:00, 10.39it/s]Epoch 10/10:  87%|████████▋ | 55/63 [00:05<00:00, 10.40it/s]Epoch 10/10:  90%|█████████ | 57/63 [00:05<00:00, 10.40it/s]Epoch 10/10:  94%|█████████▎| 59/63 [00:05<00:00, 10.40it/s]Epoch 10/10:  97%|█████████▋| 61/63 [00:05<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 11.29it/s]Epoch 10/10: 100%|██████████| 63/63 [00:06<00:00, 10.18it/s]
[2025-04-12 17:35:30,551][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0137
[2025-04-12 17:35:30,774][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0280, Metrics: {'mse': 0.02759411372244358, 'rmse': 0.1661147607000762, 'r2': 0.5746829509735107}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▄▃▃▃▃▁▁
wandb:     best_val_mse █▇▄▃▃▃▃▁▁
wandb:      best_val_r2 ▁▂▅▆▆▆▆██
wandb:    best_val_rmse █▇▅▃▃▃▃▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▄▃▃▃▃▁▂▁
wandb:          val_mse █▇▄▃▃▃▃▁▂▁
wandb:           val_r2 ▁▂▅▆▆▆▆█▇█
wandb:         val_rmse █▇▅▃▃▃▃▁▂▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.028
wandb:     best_val_mse 0.02759
wandb:      best_val_r2 0.57468
wandb:    best_val_rmse 0.16611
wandb:            epoch 10
wandb:   final_test_mse 0.02686
wandb:    final_test_r2 0.32056
wandb:  final_test_rmse 0.16389
wandb:  final_train_mse 0.00782
wandb:   final_train_r2 0.7453
wandb: final_train_rmse 0.08842
wandb:    final_val_mse 0.02759
wandb:     final_val_r2 0.57468
wandb:   final_val_rmse 0.16611
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01374
wandb:       train_time 68.99783
wandb:         val_loss 0.028
wandb:          val_mse 0.02759
wandb:           val_r2 0.57468
wandb:         val_rmse 0.16611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_173406-7e01489q
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_173406-7e01489q/logs
Cross-lingual experiment for complexity (ar → ru) completed successfully
Running cross-lingual question_type from en to ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:35:52,764][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/en_to_ar/question_type
experiment_name: cross_lingual_question_type_en_to_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: en
  eval_language: ar
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:35:52,764][__main__][INFO] - Normalized task: question_type
[2025-04-12 17:35:52,765][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 17:35:52,765][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:35:54,296][__main__][INFO] - Running cross-lingual experiment: en -> ar
[2025-04-12 17:35:54,296][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 17:35:54,296][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:35:57,160][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:35:57,160][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:35:57,336][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:35:57,377][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:35:57,492][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:35:57,502][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:35:57,503][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:35:57,504][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:35:57,531][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:35:57,567][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:35:57,584][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:35:57,586][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:35:57,586][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:35:57,587][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:35:57,612][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:35:57,650][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:35:57,666][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:35:57,668][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:35:57,668][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:35:57,669][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:35:57,669][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:35:57,670][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:35:57,670][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:35:57,670][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:35:57,670][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-12 17:35:57,670][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 17:35:57,670][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:35:57,670][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:35:57,671][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:35:57,671][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:35:57,671][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:35:57,671][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:35:57,671][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 17:35:57,671][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 17:35:57,671][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:35:57,671][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:35:57,672][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:35:57,672][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:35:57,672][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:35:57,672][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:35:57,672][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:35:57,672][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:35:57,672][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:35:57,672][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:35:57,672][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:35:57,673][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:35:57,673][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:35:57,673][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
[2025-04-12 17:36:00,466][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:36:00,467][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:36:00,491][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:36:00,521][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:36:00,535][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:36:00,543][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:36:00,544][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:36:00,545][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:36:00,568][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:36:00,601][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:36:00,614][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:36:00,615][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:36:00,616][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:36:00,617][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:36:00,639][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:36:00,668][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:36:00,680][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:36:00,682][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:36:00,682][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:36:00,683][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:36:00,683][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:36:00,683][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:36:00,683][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:36:00,684][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:36:00,684][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-12 17:36:00,684][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-12 17:36:00,684][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:36:00,684][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:36:00,684][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:36:00,684][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:36:00,684][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:36:00,685][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:36:00,685][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-12 17:36:00,685][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-12 17:36:00,685][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:36:00,685][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:36:00,685][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:36:00,685][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:36:00,685][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:36:00,686][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:36:00,686][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-12 17:36:00,686][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-12 17:36:00,686][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:36:00,686][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:36:00,686][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:36:00,686][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:36:00,686][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:36:00,687][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:36:05,539][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:36:05,541][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 17:36:05,542][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:36:05,542][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:32,  1.26s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:42,  1.73it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:19,  3.63it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.24it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.55it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:09,  6.57it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:08,  7.52it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:07,  8.27it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  8.85it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:03<00:06,  9.28it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.60it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.84it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.00it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.12it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.20it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.26it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  80%|████████  | 60/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.41it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.82it/s]
[2025-04-12 17:36:16,092][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6845
[2025-04-12 17:36:16,361][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6831, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.80it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.96it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.57it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.87it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-12 17:36:24,157][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5806
[2025-04-12 17:36:24,440][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.4622, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.45it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.71it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.89it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 17:36:32,254][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.2258
[2025-04-12 17:36:32,786][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1580, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:43,  1.69it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:16,  4.45it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:11,  6.31it/s]Epoch 4/10:   9%|▉         | 7/75 [00:01<00:08,  7.57it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:07,  8.44it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:07,  9.04it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.45it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:06,  9.74it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:02<00:05,  9.94it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:02<00:05, 10.08it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.18it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.24it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.29it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:03<00:04, 10.33it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:03<00:04, 10.35it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.72it/s]
[2025-04-12 17:36:40,906][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0656
[2025-04-12 17:36:41,195][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1744, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9459459459459459}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.72it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.92it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.03it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.57it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.35it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.36it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 17:36:48,542][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0410
[2025-04-12 17:36:48,819][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1984, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.76it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.94it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.04it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.57it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 17:36:56,166][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0294
[2025-04-12 17:36:56,559][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2083, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-12 17:36:56,560][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▅▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▇▃▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████
wandb:               val_f1 ▁█████
wandb:             val_loss █▅▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94737
wandb:        best_val_loss 0.15796
wandb:                epoch 6
wandb:  final_test_accuracy 0.61039
wandb:        final_test_f1 0.55882
wandb: final_train_accuracy 0.99832
wandb:       final_train_f1 0.99832
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94737
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02937
wandb:           train_time 48.97351
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94737
wandb:             val_loss 0.20829
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_173552-1nildpxq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_173552-1nildpxq/logs
Cross-lingual experiment for question_type (en → ar) completed successfully
Running cross-lingual complexity from en to ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:37:17,474][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/en_to_ar/complexity
experiment_name: cross_lingual_complexity_en_to_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: en
  eval_language: ar
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:37:17,474][__main__][INFO] - Normalized task: complexity
[2025-04-12 17:37:17,474][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 17:37:17,474][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:37:19,182][__main__][INFO] - Running cross-lingual experiment: en -> ar
[2025-04-12 17:37:19,182][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 17:37:19,183][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:37:22,097][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:37:22,097][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:37:22,167][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:37:22,199][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:37:22,306][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:37:22,317][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:37:22,318][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:37:22,319][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:37:22,342][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:37:22,375][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:37:22,389][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:37:22,390][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:37:22,391][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:37:22,392][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:37:22,415][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:37:22,449][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:37:22,463][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:37:22,465][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:37:22,465][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:37:22,466][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:37:22,467][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:37:22,467][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:37:22,467][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:37:22,467][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:37:22,467][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:37:22,467][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-12 17:37:22,468][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:37:22,468][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-12 17:37:22,468][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:37:22,468][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:37:22,468][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:37:22,468][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:37:22,468][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:37:22,469][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-12 17:37:22,469][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:37:22,469][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-12 17:37:22,469][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:37:22,469][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:37:22,469][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:37:22,469][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:37:22,469][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:37:22,470][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-12 17:37:22,470][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:37:22,470][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-12 17:37:22,470][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:37:22,470][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:37:22,470][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:37:22,471][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
[2025-04-12 17:37:25,263][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:37:25,263][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:37:25,294][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:37:25,333][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:37:25,350][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:37:25,359][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:37:25,359][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:37:25,360][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:37:25,386][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:37:25,423][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:37:25,438][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:37:25,439][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:37:25,439][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:37:25,441][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:37:25,463][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:37:25,496][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:37:25,510][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:37:25,512][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:37:25,512][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:37:25,513][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:37:25,513][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:37:25,514][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:37:25,514][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:37:25,514][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:37:25,514][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:37:25,514][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-12 17:37:25,514][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:37:25,514][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-12 17:37:25,515][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:37:25,515][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:37:25,515][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:37:25,515][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:37:25,515][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:37:25,515][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-12 17:37:25,515][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:37:25,516][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-12 17:37:25,516][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:37:25,516][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:37:25,516][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:37:25,516][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:37:25,516][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:37:25,516][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-12 17:37:25,516][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:37:25,517][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-12 17:37:25,517][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:37:25,517][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:37:25,517][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:37:25,517][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:37:32,134][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:37:32,138][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 17:37:32,138][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:37:32,138][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:35,  1.29s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:43,  1.70it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:19,  3.57it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.17it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.48it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:09,  6.53it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:08,  7.48it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:07,  8.23it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  8.82it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:03<00:06,  9.26it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.58it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.82it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05,  9.98it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.11it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:04<00:04, 10.19it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.25it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.32it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  80%|████████  | 60/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.78it/s]
[2025-04-12 17:37:43,173][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1401
[2025-04-12 17:37:43,419][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0726, Metrics: {'mse': 0.07804504036903381, 'rmse': 0.2793654244337223, 'r2': -0.8648216724395752}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.90it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.04it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.60it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 2/10:  13%|█▎        | 10/75 [00:01<00:06,  9.90it/s]Epoch 2/10:  16%|█▌        | 12/75 [00:01<00:06, 10.07it/s]Epoch 2/10:  19%|█▊        | 14/75 [00:01<00:05, 10.18it/s]Epoch 2/10:  21%|██▏       | 16/75 [00:01<00:05, 10.25it/s]Epoch 2/10:  24%|██▍       | 18/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  27%|██▋       | 20/75 [00:02<00:05, 10.32it/s]Epoch 2/10:  29%|██▉       | 22/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  32%|███▏      | 24/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  35%|███▍      | 26/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  37%|███▋      | 28/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  40%|████      | 30/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  43%|████▎     | 32/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  45%|████▌     | 34/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  48%|████▊     | 36/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  51%|█████     | 38/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  53%|█████▎    | 40/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  59%|█████▊    | 44/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  67%|██████▋   | 50/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  80%|████████  | 60/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.38it/s]Epoch 2/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.38it/s]Epoch 2/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  91%|█████████ | 68/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.39it/s]Epoch 2/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-12 17:37:51,213][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0610
[2025-04-12 17:37:51,485][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0362, Metrics: {'mse': 0.03731273114681244, 'rmse': 0.1931650360360602, 'r2': 0.10844314098358154}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.48it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.72it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.89it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 17:37:59,307][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0333
[2025-04-12 17:37:59,590][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0358, Metrics: {'mse': 0.03637943044304848, 'rmse': 0.190733925778946, 'r2': 0.1307436227798462}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  4.95it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.07it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.11it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.60it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-12 17:38:07,312][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0281
[2025-04-12 17:38:07,607][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0374, Metrics: {'mse': 0.03771558031439781, 'rmse': 0.19420499559588525, 'r2': 0.09881740808486938}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.61it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.83it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 17:38:14,956][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0225
[2025-04-12 17:38:15,237][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0380, Metrics: {'mse': 0.037830520421266556, 'rmse': 0.19450069516910873, 'r2': 0.09607100486755371}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.66it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.86it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 17:38:22,585][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0215
[2025-04-12 17:38:22,857][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0365, Metrics: {'mse': 0.03609136864542961, 'rmse': 0.18997728455115262, 'r2': 0.13762664794921875}
[2025-04-12 17:38:22,858][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁
wandb:     best_val_mse █▁▁
wandb:      best_val_r2 ▁██
wandb:    best_val_rmse █▁▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▁▁▁▁
wandb:          val_mse █▁▁▁▁▁
wandb:           val_r2 ▁█████
wandb:         val_rmse █▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03582
wandb:     best_val_mse 0.03638
wandb:      best_val_r2 0.13074
wandb:    best_val_rmse 0.19073
wandb:            epoch 6
wandb:   final_test_mse 0.19343
wandb:    final_test_r2 -2.33473
wandb:  final_test_rmse 0.43981
wandb:  final_train_mse 0.02187
wandb:   final_train_r2 0.18488
wandb: final_train_rmse 0.14788
wandb:    final_val_mse 0.03638
wandb:     final_val_r2 0.13074
wandb:   final_val_rmse 0.19073
wandb:    learning_rate 1e-05
wandb:       train_loss 0.0215
wandb:       train_time 48.22834
wandb:         val_loss 0.03649
wandb:          val_mse 0.03609
wandb:           val_r2 0.13763
wandb:         val_rmse 0.18998
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_173717-asq3h9im
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_173717-asq3h9im/logs
Cross-lingual experiment for complexity (en → ar) completed successfully
Running cross-lingual question_type from en to fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:38:45,849][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/en_to_fi/question_type
experiment_name: cross_lingual_question_type_en_to_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: en
  eval_language: fi
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:38:45,849][__main__][INFO] - Normalized task: question_type
[2025-04-12 17:38:45,849][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 17:38:45,849][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:38:47,591][__main__][INFO] - Running cross-lingual experiment: en -> fi
[2025-04-12 17:38:47,591][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 17:38:47,592][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:38:50,533][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:38:50,533][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:38:50,677][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:38:50,711][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:38:50,833][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:38:50,847][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:38:50,848][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:38:50,849][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:38:50,877][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:38:50,916][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:38:50,934][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:38:50,935][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:38:50,935][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:38:50,937][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:38:50,963][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:38:51,001][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:38:51,017][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:38:51,019][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:38:51,019][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:38:51,021][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:38:51,022][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:38:51,022][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:38:51,022][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:38:51,022][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:38:51,022][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-12 17:38:51,022][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 17:38:51,023][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:38:51,023][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:38:51,023][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:38:51,023][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:38:51,023][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:38:51,023][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:38:51,023][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 17:38:51,023][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 17:38:51,024][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:38:51,024][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:38:51,024][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:38:51,024][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:38:51,024][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:38:51,024][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:38:51,024][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:38:51,024][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:38:51,025][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:38:51,025][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:38:51,025][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:38:51,025][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:38:51,025][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:38:51,026][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
[2025-04-12 17:38:53,839][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:38:53,839][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:38:53,886][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:38:53,923][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:38:53,939][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 17:38:53,949][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:38:53,949][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 17:38:53,950][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:38:53,975][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:38:54,007][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:38:54,022][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 17:38:54,024][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:38:54,024][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 17:38:54,025][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:38:54,048][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:38:54,082][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:38:54,095][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 17:38:54,097][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:38:54,097][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 17:38:54,099][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 17:38:54,099][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:38:54,099][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:38:54,099][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:38:54,099][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:38:54,099][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 17:38:54,100][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-12 17:38:54,100][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 17:38:54,100][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:38:54,100][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:38:54,100][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:38:54,100][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:38:54,100][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:38:54,100][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-12 17:38:54,101][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-12 17:38:54,101][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 17:38:54,101][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:38:54,101][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:38:54,101][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:38:54,101][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:38:54,101][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:38:54,101][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:38:54,101][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:38:54,102][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 17:38:54,102][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:38:54,102][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 17:38:54,102][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:38:54,102][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:38:54,102][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:38:59,854][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:38:59,857][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 17:38:59,857][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:38:59,857][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:57,  1.59s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:52,  1.40it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:23,  3.06it/s]Epoch 1/10:   8%|▊         | 6/75 [00:02<00:15,  4.58it/s]Epoch 1/10:  11%|█         | 8/75 [00:02<00:11,  5.92it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:09,  7.02it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.49it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:08,  7.53it/s]Epoch 1/10:  20%|██        | 15/75 [00:03<00:07,  8.32it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:03<00:06,  8.91it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:06,  9.33it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.64it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.86it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:04<00:04, 10.01it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:04<00:04, 10.12it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.20it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.25it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.29it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.32it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:05<00:03, 10.34it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.35it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:08<00:00, 10.38it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:08<00:00, 10.38it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.88it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.48it/s]
[2025-04-12 17:39:11,017][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6845
[2025-04-12 17:39:11,281][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6831, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.75it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.93it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.02it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.55it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.97it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 17:39:19,100][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5806
[2025-04-12 17:39:19,378][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.4622, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.87it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:08,  8.02it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.08it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.87it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-12 17:39:27,184][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.2258
[2025-04-12 17:39:27,462][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1580, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.53it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.77it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.92it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 17:39:35,209][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0656
[2025-04-12 17:39:35,487][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1744, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9459459459459459}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.74it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.92it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.03it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.97it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 17:39:42,843][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0410
[2025-04-12 17:39:43,125][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1984, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.42it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.67it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 17:39:50,485][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0294
[2025-04-12 17:39:50,778][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2083, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-12 17:39:50,779][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▅▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▇▃▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████
wandb:               val_f1 ▁█████
wandb:             val_loss █▅▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94737
wandb:        best_val_loss 0.15796
wandb:                epoch 6
wandb:  final_test_accuracy 0.76364
wandb:        final_test_f1 0.80882
wandb: final_train_accuracy 0.99832
wandb:       final_train_f1 0.99832
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94737
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02937
wandb:           train_time 48.6063
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94737
wandb:             val_loss 0.20829
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_173845-m4dll961
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_173845-m4dll961/logs
Cross-lingual experiment for question_type (en → fi) completed successfully
Running cross-lingual complexity from en to fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:40:13,617][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/en_to_fi/complexity
experiment_name: cross_lingual_complexity_en_to_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: en
  eval_language: fi
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:40:13,617][__main__][INFO] - Normalized task: complexity
[2025-04-12 17:40:13,618][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 17:40:13,618][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:40:15,245][__main__][INFO] - Running cross-lingual experiment: en -> fi
[2025-04-12 17:40:15,245][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 17:40:15,246][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:40:18,134][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:40:18,134][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:40:18,235][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:40:18,269][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:40:18,391][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:40:18,402][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:40:18,402][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:40:18,404][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:40:18,428][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:40:18,462][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:40:18,476][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:40:18,478][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:40:18,478][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:40:18,479][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:40:18,502][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:40:18,539][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:40:18,555][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:40:18,557][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:40:18,557][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:40:18,558][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:40:18,560][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:40:18,560][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:40:18,560][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:40:18,560][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:40:18,560][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:40:18,561][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-12 17:40:18,561][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:40:18,561][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-12 17:40:18,561][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:40:18,561][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:40:18,561][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:40:18,561][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:40:18,562][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:40:18,562][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-12 17:40:18,562][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:40:18,562][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-12 17:40:18,562][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:40:18,562][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:40:18,562][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:40:18,563][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:40:18,563][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:40:18,563][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-12 17:40:18,563][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:40:18,563][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-12 17:40:18,563][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:40:18,563][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:40:18,564][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:40:18,564][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
[2025-04-12 17:40:21,398][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:40:21,399][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:40:21,432][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:40:21,473][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:40:21,492][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 17:40:21,501][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:40:21,502][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 17:40:21,503][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:40:21,528][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:40:21,564][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:40:21,580][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 17:40:21,581][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:40:21,581][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 17:40:21,582][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:40:21,607][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:40:21,643][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:40:21,658][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 17:40:21,660][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:40:21,660][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 17:40:21,662][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 17:40:21,662][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:40:21,662][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:40:21,662][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:40:21,662][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:40:21,663][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:40:21,663][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-12 17:40:21,663][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 17:40:21,663][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-12 17:40:21,663][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:40:21,663][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:40:21,663][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:40:21,663][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:40:21,664][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:40:21,664][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-12 17:40:21,664][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 17:40:21,664][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-12 17:40:21,664][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:40:21,664][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:40:21,664][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:40:21,665][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:40:21,665][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:40:21,665][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-12 17:40:21,665][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 17:40:21,665][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-12 17:40:21,665][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 17:40:21,665][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:40:21,665][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:40:21,666][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:40:27,085][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:40:27,088][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 17:40:27,088][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:40:27,088][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:33,  1.26s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:42,  1.72it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:19,  3.61it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.22it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.53it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.56it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.79it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  7.79it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:07,  8.53it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.07it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.45it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.73it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.92it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.06it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.16it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.23it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.89it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.82it/s]
[2025-04-12 17:40:37,846][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1401
[2025-04-12 17:40:38,102][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0726, Metrics: {'mse': 0.07804504036903381, 'rmse': 0.2793654244337223, 'r2': -0.8648216724395752}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.77it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.94it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.03it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-12 17:40:45,898][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0610
[2025-04-12 17:40:46,170][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0362, Metrics: {'mse': 0.03731273114681244, 'rmse': 0.1931650360360602, 'r2': 0.10844314098358154}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.40it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.67it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.37it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 17:40:53,999][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0333
[2025-04-12 17:40:54,287][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0358, Metrics: {'mse': 0.03637943044304848, 'rmse': 0.190733925778946, 'r2': 0.1307436227798462}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.61it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.83it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 17:41:02,025][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0281
[2025-04-12 17:41:02,357][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0374, Metrics: {'mse': 0.03771558031439781, 'rmse': 0.19420499559588525, 'r2': 0.09881740808486938}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:26,  2.78it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:11,  6.07it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:09,  7.72it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  8.66it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:07,  9.24it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.62it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.86it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.03it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.14it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:02<00:05, 10.22it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.27it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.31it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.33it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:03<00:04, 10.36it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.00it/s]
[2025-04-12 17:41:09,859][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0225
[2025-04-12 17:41:10,148][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0380, Metrics: {'mse': 0.037830520421266556, 'rmse': 0.19450069516910873, 'r2': 0.09607100486755371}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.77it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.94it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.03it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.33it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.35it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.36it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.36it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 17:41:17,505][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0215
[2025-04-12 17:41:18,015][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0365, Metrics: {'mse': 0.03609136864542961, 'rmse': 0.18997728455115262, 'r2': 0.13762664794921875}
[2025-04-12 17:41:18,016][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁
wandb:     best_val_mse █▁▁
wandb:      best_val_r2 ▁██
wandb:    best_val_rmse █▁▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▁▁▁▁
wandb:          val_mse █▁▁▁▁▁
wandb:           val_r2 ▁█████
wandb:         val_rmse █▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03582
wandb:     best_val_mse 0.03638
wandb:      best_val_r2 0.13074
wandb:    best_val_rmse 0.19073
wandb:            epoch 6
wandb:   final_test_mse 0.11182
wandb:    final_test_r2 -1.83194
wandb:  final_test_rmse 0.3344
wandb:  final_train_mse 0.02187
wandb:   final_train_r2 0.18488
wandb: final_train_rmse 0.14788
wandb:    final_val_mse 0.03638
wandb:     final_val_r2 0.13074
wandb:   final_val_rmse 0.19073
wandb:    learning_rate 1e-05
wandb:       train_loss 0.0215
wandb:       train_time 48.67692
wandb:         val_loss 0.03649
wandb:          val_mse 0.03609
wandb:           val_r2 0.13763
wandb:         val_rmse 0.18998
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_174013-1o90icxk
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_174013-1o90icxk/logs
Cross-lingual experiment for complexity (en → fi) completed successfully
Running cross-lingual question_type from en to id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:41:41,658][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/en_to_id/question_type
experiment_name: cross_lingual_question_type_en_to_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: en
  eval_language: id
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:41:41,658][__main__][INFO] - Normalized task: question_type
[2025-04-12 17:41:41,658][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 17:41:41,658][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:41:43,380][__main__][INFO] - Running cross-lingual experiment: en -> id
[2025-04-12 17:41:43,381][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 17:41:43,381][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:41:46,231][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:41:46,231][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:41:46,337][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:41:46,371][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:41:46,498][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:41:46,509][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:41:46,510][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:41:46,511][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:41:46,537][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:41:46,572][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:41:46,587][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:41:46,589][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:41:46,589][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:41:46,591][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:41:46,615][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:41:46,653][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:41:46,668][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:41:46,669][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:41:46,669][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:41:46,671][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:41:46,671][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:41:46,671][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:41:46,671][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:41:46,672][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:41:46,672][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-12 17:41:46,672][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 17:41:46,672][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:41:46,672][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:41:46,672][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:41:46,673][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:41:46,673][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:41:46,673][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:41:46,673][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 17:41:46,673][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 17:41:46,673][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:41:46,673][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:41:46,673][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:41:46,674][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:41:46,674][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:41:46,674][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:41:46,674][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:41:46,674][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:41:46,674][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:41:46,674][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:41:46,674][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:41:46,674][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:41:46,675][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:41:46,675][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
[2025-04-12 17:41:49,515][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:41:49,515][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:41:49,576][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:41:49,614][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:41:49,640][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 17:41:49,648][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:41:49,648][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 17:41:49,650][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:41:49,671][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:41:49,705][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:41:49,721][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 17:41:49,723][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:41:49,723][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 17:41:49,724][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:41:49,749][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:41:49,789][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:41:49,803][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 17:41:49,804][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:41:49,804][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 17:41:49,806][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 17:41:49,806][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:41:49,806][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:41:49,806][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:41:49,806][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:41:49,807][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-12 17:41:49,807][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-12 17:41:49,807][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 17:41:49,807][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:41:49,807][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:41:49,807][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:41:49,807][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:41:49,808][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:41:49,808][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 17:41:49,808][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 17:41:49,808][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 17:41:49,808][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:41:49,808][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:41:49,808][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:41:49,808][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:41:49,809][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:41:49,809][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:41:49,809][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:41:49,809][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 17:41:49,809][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:41:49,809][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 17:41:49,809][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:41:49,809][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:41:49,810][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:41:55,624][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:41:55,627][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 17:41:55,627][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:41:55,628][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:31,  1.24s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:28,  2.54it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:16,  4.15it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.57it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  6.75it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.70it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.43it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.98it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.39it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.68it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.88it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.03it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.14it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.22it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.28it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.34it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00,  8.98it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00,  9.36it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00,  9.65it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00,  9.87it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.48it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.86it/s]
[2025-04-12 17:42:06,281][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6845
[2025-04-12 17:42:06,592][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6831, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  4.96it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.08it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.12it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.62it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.32it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.35it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-12 17:42:14,383][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5806
[2025-04-12 17:42:14,668][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.4622, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.56it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.79it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.94it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 17:42:22,483][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.2258
[2025-04-12 17:42:22,760][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1580, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.72it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.91it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.02it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-12 17:42:30,487][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0656
[2025-04-12 17:42:30,773][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1744, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9459459459459459}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.57it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.80it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.95it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 17:42:38,120][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0410
[2025-04-12 17:42:38,416][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1984, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.65it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.86it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.53it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-12 17:42:45,758][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0294
[2025-04-12 17:42:46,045][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2083, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-12 17:42:46,046][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▅▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▇▃▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████
wandb:               val_f1 ▁█████
wandb:             val_loss █▅▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94737
wandb:        best_val_loss 0.15796
wandb:                epoch 6
wandb:  final_test_accuracy 0.72727
wandb:        final_test_f1 0.72222
wandb: final_train_accuracy 0.99832
wandb:       final_train_f1 0.99832
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94737
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02937
wandb:           train_time 48.23533
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94737
wandb:             val_loss 0.20829
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_174141-zsgg0f7m
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_174141-zsgg0f7m/logs
Cross-lingual experiment for question_type (en → id) completed successfully
Running cross-lingual complexity from en to id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:43:10,160][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/en_to_id/complexity
experiment_name: cross_lingual_complexity_en_to_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: en
  eval_language: id
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:43:10,160][__main__][INFO] - Normalized task: complexity
[2025-04-12 17:43:10,160][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 17:43:10,160][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:43:11,803][__main__][INFO] - Running cross-lingual experiment: en -> id
[2025-04-12 17:43:11,803][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 17:43:11,803][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:43:14,662][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:43:14,663][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:43:14,747][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:43:14,784][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:43:14,897][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:43:14,907][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:43:14,908][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:43:14,910][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:43:14,935][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:43:14,973][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:43:14,989][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:43:14,990][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:43:14,991][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:43:14,992][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:43:15,020][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:43:15,060][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:43:15,075][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:43:15,076][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:43:15,077][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:43:15,078][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:43:15,078][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:43:15,079][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:43:15,079][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:43:15,079][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:43:15,079][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:43:15,079][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-12 17:43:15,079][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:43:15,079][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-12 17:43:15,080][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:43:15,080][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:43:15,080][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:43:15,080][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:43:15,080][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:43:15,080][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-12 17:43:15,080][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:43:15,080][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-12 17:43:15,081][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:43:15,081][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:43:15,081][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:43:15,081][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:43:15,081][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:43:15,081][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-12 17:43:15,081][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:43:15,082][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-12 17:43:15,082][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:43:15,082][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:43:15,082][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:43:15,082][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
[2025-04-12 17:43:17,912][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:43:17,912][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:43:17,941][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:43:17,977][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:43:18,005][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 17:43:18,012][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:43:18,013][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 17:43:18,014][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:43:18,037][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:43:18,071][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:43:18,085][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 17:43:18,087][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:43:18,087][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 17:43:18,088][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:43:18,110][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:43:18,141][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:43:18,156][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 17:43:18,158][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:43:18,158][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 17:43:18,159][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 17:43:18,160][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:43:18,160][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:43:18,160][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:43:18,160][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:43:18,160][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:43:18,160][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-12 17:43:18,161][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 17:43:18,161][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-12 17:43:18,161][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:43:18,161][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:43:18,161][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:43:18,161][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:43:18,161][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:43:18,162][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-12 17:43:18,162][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 17:43:18,162][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-12 17:43:18,162][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:43:18,162][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:43:18,162][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:43:18,162][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:43:18,162][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:43:18,163][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-12 17:43:18,163][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 17:43:18,163][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-12 17:43:18,163][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 17:43:18,163][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:43:18,163][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:43:18,163][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:43:23,886][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:43:23,888][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 17:43:23,889][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:43:23,889][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:39,  1.35s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:30,  2.37it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:17,  3.92it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.33it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:10,  6.53it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.51it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.28it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.86it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.30it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.61it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.84it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.01it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.13it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.21it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.27it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.33it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.41it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00,  8.96it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00,  9.34it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00,  9.64it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00,  9.86it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.47it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.74it/s]
[2025-04-12 17:43:34,540][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1401
[2025-04-12 17:43:34,788][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0726, Metrics: {'mse': 0.07804504036903381, 'rmse': 0.2793654244337223, 'r2': -0.8648216724395752}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.75it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.93it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.04it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.57it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.87it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.41it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-12 17:43:42,590][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0610
[2025-04-12 17:43:42,876][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0362, Metrics: {'mse': 0.03731273114681244, 'rmse': 0.1931650360360602, 'r2': 0.10844314098358154}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.56it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.79it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.95it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 17:43:50,689][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0333
[2025-04-12 17:43:50,968][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0358, Metrics: {'mse': 0.03637943044304848, 'rmse': 0.190733925778946, 'r2': 0.1307436227798462}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.63it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.85it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.53it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.99it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.12it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.21it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 17:43:58,717][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0281
[2025-04-12 17:43:59,007][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0374, Metrics: {'mse': 0.03771558031439781, 'rmse': 0.19420499559588525, 'r2': 0.09881740808486938}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.68it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.88it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.00it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.55it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-12 17:44:06,342][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0225
[2025-04-12 17:44:06,625][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0380, Metrics: {'mse': 0.037830520421266556, 'rmse': 0.19450069516910873, 'r2': 0.09607100486755371}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.69it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.89it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.01it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.55it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.41it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 17:44:13,970][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0215
[2025-04-12 17:44:14,257][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0365, Metrics: {'mse': 0.03609136864542961, 'rmse': 0.18997728455115262, 'r2': 0.13762664794921875}
[2025-04-12 17:44:14,258][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁
wandb:     best_val_mse █▁▁
wandb:      best_val_r2 ▁██
wandb:    best_val_rmse █▁▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▁▁▁▁
wandb:          val_mse █▁▁▁▁▁
wandb:           val_r2 ▁█████
wandb:         val_rmse █▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03582
wandb:     best_val_mse 0.03638
wandb:      best_val_r2 0.13074
wandb:    best_val_rmse 0.19073
wandb:            epoch 6
wandb:   final_test_mse 0.13574
wandb:    final_test_r2 -2.32915
wandb:  final_test_rmse 0.36843
wandb:  final_train_mse 0.02187
wandb:   final_train_r2 0.18488
wandb: final_train_rmse 0.14788
wandb:    final_val_mse 0.03638
wandb:     final_val_r2 0.13074
wandb:   final_val_rmse 0.19073
wandb:    learning_rate 1e-05
wandb:       train_loss 0.0215
wandb:       train_time 48.30515
wandb:         val_loss 0.03649
wandb:          val_mse 0.03609
wandb:           val_r2 0.13763
wandb:         val_rmse 0.18998
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_174310-9ngfy9kv
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_174310-9ngfy9kv/logs
Cross-lingual experiment for complexity (en → id) completed successfully
Running cross-lingual question_type from en to ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:44:37,695][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/en_to_ja/question_type
experiment_name: cross_lingual_question_type_en_to_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: en
  eval_language: ja
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:44:37,696][__main__][INFO] - Normalized task: question_type
[2025-04-12 17:44:37,696][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 17:44:37,696][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:44:39,153][__main__][INFO] - Running cross-lingual experiment: en -> ja
[2025-04-12 17:44:39,153][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 17:44:39,153][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:44:42,295][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:44:42,296][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:44:42,396][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:44:42,432][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:44:42,560][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:44:42,571][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:44:42,572][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:44:42,574][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:44:42,605][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:44:42,647][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:44:42,665][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:44:42,666][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:44:42,666][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:44:42,668][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:44:42,700][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:44:42,739][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:44:42,755][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:44:42,756][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:44:42,756][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:44:42,758][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:44:42,758][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:44:42,758][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:44:42,758][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:44:42,759][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:44:42,759][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-12 17:44:42,759][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 17:44:42,759][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:44:42,759][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:44:42,759][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:44:42,760][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:44:42,760][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:44:42,760][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:44:42,760][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 17:44:42,760][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 17:44:42,760][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:44:42,760][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:44:42,760][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:44:42,761][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:44:42,761][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:44:42,761][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:44:42,761][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:44:42,761][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:44:42,761][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:44:42,761][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:44:42,761][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:44:42,761][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:44:42,762][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:44:42,762][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
[2025-04-12 17:44:45,443][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:44:45,443][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:44:45,466][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:44:45,496][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:44:45,520][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 17:44:45,529][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:44:45,530][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 17:44:45,531][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:44:45,551][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:44:45,582][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:44:45,596][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 17:44:45,598][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:44:45,598][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 17:44:45,599][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:44:45,619][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:44:45,650][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:44:45,663][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 17:44:45,665][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:44:45,665][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 17:44:45,666][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 17:44:45,667][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:44:45,667][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:44:45,667][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:44:45,667][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:44:45,667][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-12 17:44:45,667][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 17:44:45,667][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 17:44:45,668][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:44:45,668][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:44:45,668][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:44:45,668][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:44:45,668][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:44:45,668][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-12 17:44:45,668][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-12 17:44:45,668][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 17:44:45,669][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:44:45,669][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:44:45,669][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:44:45,669][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:44:45,669][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:44:45,669][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-12 17:44:45,669][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-12 17:44:45,669][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 17:44:45,670][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:44:45,670][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 17:44:45,670][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:44:45,670][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:44:45,670][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:44:50,994][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:44:50,997][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 17:44:50,998][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:44:50,998][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:36,  1.31s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:43,  1.67it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:20,  3.53it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.13it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.44it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:09,  6.57it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:08,  7.51it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:07,  8.27it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  8.85it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:03<00:06,  9.28it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.60it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.83it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.00it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.11it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:04<00:04, 10.19it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.25it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  80%|████████  | 60/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.78it/s]
[2025-04-12 17:45:01,744][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6845
[2025-04-12 17:45:02,288][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6831, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  4.94it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.05it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.60it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.33it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.35it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.36it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-12 17:45:10,090][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5806
[2025-04-12 17:45:10,390][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.4622, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.40it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.67it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 17:45:18,409][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.2258
[2025-04-12 17:45:18,704][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1580, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.65it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.87it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.99it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 17:45:26,457][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0656
[2025-04-12 17:45:26,741][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1744, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9459459459459459}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.58it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.81it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.95it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 17:45:34,098][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0410
[2025-04-12 17:45:34,401][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1984, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.43it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.69it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 17:45:41,765][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0294
[2025-04-12 17:45:42,054][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2083, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-12 17:45:42,054][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▅▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▇▃▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████
wandb:               val_f1 ▁█████
wandb:             val_loss █▅▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94737
wandb:        best_val_loss 0.15796
wandb:                epoch 6
wandb:  final_test_accuracy 0.76087
wandb:        final_test_f1 0.83077
wandb: final_train_accuracy 0.99832
wandb:       final_train_f1 0.99832
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94737
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02937
wandb:           train_time 48.8564
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94737
wandb:             val_loss 0.20829
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_174437-m7f5y9cs
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_174437-m7f5y9cs/logs
Cross-lingual experiment for question_type (en → ja) completed successfully
Running cross-lingual complexity from en to ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:46:04,566][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/en_to_ja/complexity
experiment_name: cross_lingual_complexity_en_to_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: en
  eval_language: ja
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:46:04,566][__main__][INFO] - Normalized task: complexity
[2025-04-12 17:46:04,566][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 17:46:04,566][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:46:06,233][__main__][INFO] - Running cross-lingual experiment: en -> ja
[2025-04-12 17:46:06,234][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 17:46:06,234][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:46:09,112][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:46:09,113][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:46:09,180][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:46:09,217][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:46:09,326][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:46:09,337][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:46:09,338][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:46:09,339][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:46:09,369][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:46:09,414][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:46:09,433][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:46:09,434][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:46:09,434][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:46:09,436][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:46:09,466][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:46:09,503][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:46:09,519][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:46:09,520][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:46:09,521][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:46:09,522][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:46:09,522][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:46:09,523][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:46:09,523][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:46:09,523][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:46:09,523][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:46:09,523][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-12 17:46:09,523][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:46:09,523][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-12 17:46:09,524][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:46:09,524][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:46:09,524][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:46:09,524][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:46:09,524][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:46:09,524][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-12 17:46:09,524][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:46:09,525][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-12 17:46:09,525][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:46:09,525][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:46:09,525][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:46:09,525][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:46:09,525][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:46:09,525][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-12 17:46:09,526][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:46:09,526][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-12 17:46:09,526][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:46:09,526][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:46:09,526][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:46:09,526][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
[2025-04-12 17:46:12,336][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:46:12,336][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:46:12,361][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:46:12,394][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:46:12,421][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 17:46:12,431][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:46:12,431][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 17:46:12,432][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:46:12,453][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:46:12,484][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:46:12,496][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 17:46:12,498][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:46:12,498][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 17:46:12,499][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:46:12,519][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:46:12,552][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:46:12,566][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 17:46:12,567][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:46:12,567][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 17:46:12,568][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 17:46:12,569][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:46:12,569][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:46:12,569][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:46:12,569][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:46:12,569][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:46:12,570][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-12 17:46:12,570][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 17:46:12,570][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-04-12 17:46:12,570][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:46:12,570][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:46:12,570][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:46:12,570][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:46:12,571][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:46:12,571][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-12 17:46:12,571][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 17:46:12,571][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-12 17:46:12,571][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:46:12,571][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:46:12,571][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:46:12,571][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:46:12,572][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:46:12,572][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-12 17:46:12,572][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 17:46:12,572][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-12 17:46:12,572][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 17:46:12,572][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:46:12,572][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:46:12,573][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:46:18,236][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:46:18,239][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 17:46:18,239][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:46:18,239][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:27,  1.18s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:27,  2.65it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:16,  4.30it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:11,  5.72it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  6.88it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.76it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:08,  7.65it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:07,  8.36it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  8.91it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:06,  9.33it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.63it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.85it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.01it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.12it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.21it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.26it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.37it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.89it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.90it/s]
[2025-04-12 17:46:28,709][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1401
[2025-04-12 17:46:28,966][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0726, Metrics: {'mse': 0.07804504036903381, 'rmse': 0.2793654244337223, 'r2': -0.8648216724395752}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.64it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.85it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.53it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 17:46:36,771][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0610
[2025-04-12 17:46:37,051][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0362, Metrics: {'mse': 0.03731273114681244, 'rmse': 0.1931650360360602, 'r2': 0.10844314098358154}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.14it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.46it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:08,  8.73it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.36it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.73it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 17:46:45,065][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0333
[2025-04-12 17:46:45,371][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0358, Metrics: {'mse': 0.03637943044304848, 'rmse': 0.190733925778946, 'r2': 0.1307436227798462}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.38it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.65it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.83it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.77it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 17:46:53,146][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0281
[2025-04-12 17:46:53,454][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0374, Metrics: {'mse': 0.03771558031439781, 'rmse': 0.19420499559588525, 'r2': 0.09881740808486938}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.59it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.81it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 17:47:00,809][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0225
[2025-04-12 17:47:01,109][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0380, Metrics: {'mse': 0.037830520421266556, 'rmse': 0.19450069516910873, 'r2': 0.09607100486755371}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.49it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.74it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.90it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 17:47:08,491][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0215
[2025-04-12 17:47:08,782][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0365, Metrics: {'mse': 0.03609136864542961, 'rmse': 0.18997728455115262, 'r2': 0.13762664794921875}
[2025-04-12 17:47:08,783][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁
wandb:     best_val_mse █▁▁
wandb:      best_val_r2 ▁██
wandb:    best_val_rmse █▁▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▁▁▁▁
wandb:          val_mse █▁▁▁▁▁
wandb:           val_r2 ▁█████
wandb:         val_rmse █▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03582
wandb:     best_val_mse 0.03638
wandb:      best_val_r2 0.13074
wandb:    best_val_rmse 0.19073
wandb:            epoch 6
wandb:   final_test_mse 0.19901
wandb:    final_test_r2 -2.82285
wandb:  final_test_rmse 0.44611
wandb:  final_train_mse 0.02187
wandb:   final_train_r2 0.18488
wandb: final_train_rmse 0.14788
wandb:    final_val_mse 0.03638
wandb:     final_val_r2 0.13074
wandb:   final_val_rmse 0.19073
wandb:    learning_rate 1e-05
wandb:       train_loss 0.0215
wandb:       train_time 48.50168
wandb:         val_loss 0.03649
wandb:          val_mse 0.03609
wandb:           val_r2 0.13763
wandb:         val_rmse 0.18998
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_174604-0znulcyz
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_174604-0znulcyz/logs
Cross-lingual experiment for complexity (en → ja) completed successfully
Running cross-lingual question_type from en to ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:47:28,833][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/en_to_ko/question_type
experiment_name: cross_lingual_question_type_en_to_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: en
  eval_language: ko
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:47:28,833][__main__][INFO] - Normalized task: question_type
[2025-04-12 17:47:28,833][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 17:47:28,833][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:47:30,252][__main__][INFO] - Running cross-lingual experiment: en -> ko
[2025-04-12 17:47:30,253][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 17:47:30,253][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:47:33,315][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:47:33,316][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:47:33,382][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:47:33,412][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:47:33,499][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:47:33,510][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:47:33,511][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:47:33,512][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:47:33,535][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:47:33,565][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:47:33,578][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:47:33,580][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:47:33,580][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:47:33,581][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:47:33,604][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:47:33,632][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:47:33,647][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:47:33,650][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:47:33,650][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:47:33,651][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:47:33,651][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:47:33,652][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:47:33,652][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:47:33,652][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:47:33,652][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-12 17:47:33,652][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 17:47:33,652][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:47:33,652][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:47:33,653][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:47:33,653][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:47:33,653][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:47:33,653][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:47:33,653][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 17:47:33,653][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 17:47:33,653][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:47:33,653][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:47:33,654][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:47:33,654][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:47:33,654][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:47:33,654][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:47:33,654][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:47:33,654][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:47:33,654][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:47:33,654][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:47:33,655][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:47:33,655][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:47:33,655][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:47:33,655][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
[2025-04-12 17:47:36,334][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:47:36,334][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:47:36,359][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:47:36,394][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:47:36,409][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 17:47:36,415][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:47:36,415][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 17:47:36,417][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:47:36,440][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:47:36,475][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:47:36,489][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 17:47:36,491][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:47:36,491][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 17:47:36,492][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:47:36,517][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:47:36,551][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:47:36,566][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 17:47:36,568][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:47:36,568][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 17:47:36,569][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 17:47:36,569][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:47:36,570][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:47:36,570][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:47:36,570][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:47:36,570][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-12 17:47:36,570][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-12 17:47:36,570][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 17:47:36,570][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:47:36,571][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:47:36,571][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:47:36,571][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:47:36,571][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:47:36,571][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 17:47:36,571][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 17:47:36,571][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 17:47:36,571][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:47:36,572][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:47:36,572][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:47:36,572][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:47:36,572][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:47:36,572][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:47:36,572][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:47:36,572][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 17:47:36,572][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:47:36,572][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 17:47:36,573][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:47:36,573][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:47:36,573][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:47:41,542][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:47:41,545][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 17:47:41,545][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:47:41,545][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:29,  1.21s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:27,  2.59it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:16,  4.22it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.63it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  6.81it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.74it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.46it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  9.00it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.40it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.69it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.89it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.04it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.15it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.22it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.28it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.34it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00,  8.92it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00,  9.32it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00,  9.62it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00,  9.84it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.44it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.88it/s]
[2025-04-12 17:47:51,954][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6845
[2025-04-12 17:47:52,211][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6831, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.07it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.15it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.16it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.64it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.91it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.18it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-12 17:48:00,007][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5806
[2025-04-12 17:48:00,304][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.4622, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.18it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.49it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:08,  8.75it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.38it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.74it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 17:48:08,313][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.2258
[2025-04-12 17:48:08,610][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1580, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.29it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.58it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.81it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 17:48:16,376][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0656
[2025-04-12 17:48:16,761][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1744, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9459459459459459}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.59it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.80it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.95it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 17:48:24,145][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0410
[2025-04-12 17:48:24,457][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1984, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.46it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.71it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.89it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 17:48:31,822][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0294
[2025-04-12 17:48:32,312][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2083, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-12 17:48:32,313][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▅▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▇▃▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████
wandb:               val_f1 ▁█████
wandb:             val_loss █▅▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94737
wandb:        best_val_loss 0.15796
wandb:                epoch 6
wandb:  final_test_accuracy 0.68182
wandb:        final_test_f1 0.74074
wandb: final_train_accuracy 0.99832
wandb:       final_train_f1 0.99832
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94737
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02937
wandb:           train_time 48.81139
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94737
wandb:             val_loss 0.20829
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_174728-2pd8fpio
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_174728-2pd8fpio/logs
Cross-lingual experiment for question_type (en → ko) completed successfully
Running cross-lingual complexity from en to ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:48:53,514][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/en_to_ko/complexity
experiment_name: cross_lingual_complexity_en_to_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: en
  eval_language: ko
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:48:53,515][__main__][INFO] - Normalized task: complexity
[2025-04-12 17:48:53,515][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 17:48:53,515][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:48:55,112][__main__][INFO] - Running cross-lingual experiment: en -> ko
[2025-04-12 17:48:55,112][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 17:48:55,113][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:48:58,034][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:48:58,034][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:48:58,103][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:48:58,129][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:48:58,216][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:48:58,227][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:48:58,227][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:48:58,228][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:48:58,249][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:48:58,276][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:48:58,288][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:48:58,290][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:48:58,290][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:48:58,291][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:48:58,314][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:48:58,343][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:48:58,355][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:48:58,357][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:48:58,357][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:48:58,358][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:48:58,359][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:48:58,359][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:48:58,359][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:48:58,359][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:48:58,359][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:48:58,360][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-12 17:48:58,360][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:48:58,360][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-12 17:48:58,360][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:48:58,360][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:48:58,360][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:48:58,360][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:48:58,361][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:48:58,361][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-12 17:48:58,361][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:48:58,361][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-12 17:48:58,361][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:48:58,361][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:48:58,361][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:48:58,361][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:48:58,362][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:48:58,362][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-12 17:48:58,362][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:48:58,362][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-12 17:48:58,362][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:48:58,362][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:48:58,362][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:48:58,363][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
[2025-04-12 17:49:01,189][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:49:01,189][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:49:01,213][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:49:01,247][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:49:01,263][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 17:49:01,269][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:49:01,270][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 17:49:01,271][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:49:01,293][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:49:01,327][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:49:01,342][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 17:49:01,343][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:49:01,343][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 17:49:01,344][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:49:01,365][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:49:01,396][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:49:01,408][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 17:49:01,410][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:49:01,410][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 17:49:01,411][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 17:49:01,412][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:49:01,412][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:49:01,412][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:49:01,412][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:49:01,412][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:49:01,413][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-12 17:49:01,413][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 17:49:01,413][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-04-12 17:49:01,413][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:49:01,413][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:49:01,413][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:49:01,413][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:49:01,414][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:49:01,414][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-12 17:49:01,414][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 17:49:01,414][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-12 17:49:01,414][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:49:01,414][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:49:01,414][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:49:01,414][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:49:01,415][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:49:01,415][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-12 17:49:01,415][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 17:49:01,415][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-12 17:49:01,415][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 17:49:01,415][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:49:01,415][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:49:01,416][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:49:06,435][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:49:06,439][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 17:49:06,439][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:49:06,439][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:25,  1.15s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:39,  1.86it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:18,  3.83it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.46it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.75it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.74it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.48it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  9.03it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.42it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.71it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:02<00:05,  9.91it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.05it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.15it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.22it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.27it/s]Epoch 1/10:  40%|████      | 30/75 [00:03<00:04, 10.31it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.34it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00,  8.73it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00,  9.17it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00,  9.51it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00,  9.76it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.92it/s]
[2025-04-12 17:49:16,953][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1401
[2025-04-12 17:49:17,224][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0726, Metrics: {'mse': 0.07804504036903381, 'rmse': 0.2793654244337223, 'r2': -0.8648216724395752}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.66it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.85it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.53it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 17:49:25,028][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0610
[2025-04-12 17:49:25,301][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0362, Metrics: {'mse': 0.03731273114681244, 'rmse': 0.1931650360360602, 'r2': 0.10844314098358154}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.39it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.66it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 17:49:33,311][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0333
[2025-04-12 17:49:33,845][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0358, Metrics: {'mse': 0.03637943044304848, 'rmse': 0.190733925778946, 'r2': 0.1307436227798462}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.42it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.68it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 17:49:41,596][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0281
[2025-04-12 17:49:41,896][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0374, Metrics: {'mse': 0.03771558031439781, 'rmse': 0.19420499559588525, 'r2': 0.09881740808486938}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.52it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.75it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.91it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 17:49:49,264][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0225
[2025-04-12 17:49:49,544][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0380, Metrics: {'mse': 0.037830520421266556, 'rmse': 0.19450069516910873, 'r2': 0.09607100486755371}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.49it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.74it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.90it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 17:49:56,928][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0215
[2025-04-12 17:49:57,231][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0365, Metrics: {'mse': 0.03609136864542961, 'rmse': 0.18997728455115262, 'r2': 0.13762664794921875}
[2025-04-12 17:49:57,232][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁
wandb:     best_val_mse █▁▁
wandb:      best_val_r2 ▁██
wandb:    best_val_rmse █▁▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▁▁▁▁
wandb:          val_mse █▁▁▁▁▁
wandb:           val_r2 ▁█████
wandb:         val_rmse █▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03582
wandb:     best_val_mse 0.03638
wandb:      best_val_r2 0.13074
wandb:    best_val_rmse 0.19073
wandb:            epoch 6
wandb:   final_test_mse 0.18262
wandb:    final_test_r2 -4.67032
wandb:  final_test_rmse 0.42734
wandb:  final_train_mse 0.02187
wandb:   final_train_r2 0.18488
wandb: final_train_rmse 0.14788
wandb:    final_val_mse 0.03638
wandb:     final_val_r2 0.13074
wandb:   final_val_rmse 0.19073
wandb:    learning_rate 1e-05
wandb:       train_loss 0.0215
wandb:       train_time 48.69329
wandb:         val_loss 0.03649
wandb:          val_mse 0.03609
wandb:           val_r2 0.13763
wandb:         val_rmse 0.18998
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_174853-ivqtdeq1
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_174853-ivqtdeq1/logs
Cross-lingual experiment for complexity (en → ko) completed successfully
Running cross-lingual question_type from en to ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:50:20,436][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/en_to_ru/question_type
experiment_name: cross_lingual_question_type_en_to_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: en
  eval_language: ru
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:50:20,436][__main__][INFO] - Normalized task: question_type
[2025-04-12 17:50:20,436][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 17:50:20,436][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:50:22,148][__main__][INFO] - Running cross-lingual experiment: en -> ru
[2025-04-12 17:50:22,148][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 17:50:22,149][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:50:25,139][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:50:25,139][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:50:25,354][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:50:25,388][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:50:25,497][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:50:25,508][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:50:25,509][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:50:25,511][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:50:25,540][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:50:25,579][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:50:25,596][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:50:25,597][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:50:25,597][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:50:25,599][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:50:25,623][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:50:25,658][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:50:25,674][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:50:25,676][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:50:25,676][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:50:25,677][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:50:25,678][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:50:25,678][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:50:25,678][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:50:25,678][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:50:25,678][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-12 17:50:25,679][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 17:50:25,679][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:50:25,679][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:50:25,679][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:50:25,679][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:50:25,679][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:50:25,679][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:50:25,679][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 17:50:25,680][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 17:50:25,680][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:50:25,680][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:50:25,680][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:50:25,680][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:50:25,680][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:50:25,680][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:50:25,680][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:50:25,681][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:50:25,681][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:50:25,681][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:50:25,681][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:50:25,681][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:50:25,681][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:50:25,682][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
[2025-04-12 17:50:28,485][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:50:28,485][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:50:28,512][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:50:28,550][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:50:28,567][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 17:50:28,576][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:50:28,577][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 17:50:28,578][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:50:28,603][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:50:28,641][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:50:28,656][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 17:50:28,657][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:50:28,657][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 17:50:28,658][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:50:28,683][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:50:28,719][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:50:28,734][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 17:50:28,735][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:50:28,736][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 17:50:28,737][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 17:50:28,737][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:50:28,737][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:50:28,737][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:50:28,737][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:50:28,738][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 17:50:28,738][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-12 17:50:28,738][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 17:50:28,738][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:50:28,738][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:50:28,738][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:50:28,738][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:50:28,738][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:50:28,739][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 17:50:28,739][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 17:50:28,739][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 17:50:28,739][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:50:28,739][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:50:28,739][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:50:28,739][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:50:28,739][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:50:28,740][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:50:28,740][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:50:28,740][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 17:50:28,740][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:50:28,740][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 17:50:28,740][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:50:28,740][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:50:28,741][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:50:34,342][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:50:34,345][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 17:50:34,345][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:50:34,345][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:30,  1.23s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:28,  2.56it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:16,  4.17it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.59it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  6.77it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.78it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:08,  7.67it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:07,  8.38it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  8.93it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.34it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.64it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.86it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.01it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.12it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.21it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.20it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.25it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.30it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.41it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.90it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.85it/s]
[2025-04-12 17:50:45,136][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6845
[2025-04-12 17:50:45,406][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6831, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.84it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  8.00it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.07it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-12 17:50:53,205][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5806
[2025-04-12 17:50:53,505][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.4622, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.69it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.89it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  9.00it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 17:51:01,502][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.2258
[2025-04-12 17:51:01,825][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.1580, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.75it/s]Epoch 4/10:   3%|▎         | 2/75 [00:00<00:17,  4.21it/s]Epoch 4/10:   5%|▌         | 4/75 [00:00<00:10,  6.71it/s]Epoch 4/10:   8%|▊         | 6/75 [00:00<00:08,  8.06it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:08,  7.98it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:07,  8.84it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.37it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06,  9.71it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:06,  9.93it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.08it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:02<00:05, 10.18it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.25it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.30it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.33it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.00it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.96it/s]
[2025-04-12 17:51:09,756][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0656
[2025-04-12 17:51:10,066][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1744, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9459459459459459}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:18,  3.97it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.31it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:08,  8.63it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.30it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.69it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.92it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.07it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.18it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 17:51:17,456][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0410
[2025-04-12 17:51:17,799][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.1984, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.39it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.66it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 17:51:25,163][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0294
[2025-04-12 17:51:25,469][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.2083, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9473684210526315}
[2025-04-12 17:51:25,470][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▅▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▇▃▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████
wandb:               val_f1 ▁█████
wandb:             val_loss █▅▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94737
wandb:        best_val_loss 0.15796
wandb:                epoch 6
wandb:  final_test_accuracy 0.92727
wandb:        final_test_f1 0.9322
wandb: final_train_accuracy 0.99832
wandb:       final_train_f1 0.99832
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94737
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02937
wandb:           train_time 48.80964
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94737
wandb:             val_loss 0.20829
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_175020-lay6qcnr
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_175020-lay6qcnr/logs
Cross-lingual experiment for question_type (en → ru) completed successfully
Running cross-lingual complexity from en to ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:51:47,786][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/en_to_ru/complexity
experiment_name: cross_lingual_complexity_en_to_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: en
  eval_language: ru
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:51:47,787][__main__][INFO] - Normalized task: complexity
[2025-04-12 17:51:47,787][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 17:51:47,787][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:51:49,477][__main__][INFO] - Running cross-lingual experiment: en -> ru
[2025-04-12 17:51:49,477][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 17:51:49,477][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:51:52,331][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:51:52,331][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:51:52,395][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:51:52,432][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:51:52,540][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:51:52,552][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:51:52,552][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:51:52,554][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:51:52,577][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:51:52,613][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:51:52,629][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:51:52,630][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:51:52,631][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:51:52,632][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:51:52,654][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:51:52,687][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:51:52,702][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:51:52,704][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:51:52,704][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:51:52,705][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:51:52,705][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:51:52,705][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:51:52,706][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:51:52,706][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:51:52,706][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:51:52,706][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-12 17:51:52,706][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:51:52,706][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-12 17:51:52,707][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:51:52,707][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:51:52,707][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:51:52,707][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:51:52,707][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:51:52,707][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-12 17:51:52,707][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:51:52,707][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-12 17:51:52,708][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:51:52,708][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:51:52,708][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:51:52,708][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:51:52,708][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:51:52,708][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-12 17:51:52,708][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:51:52,709][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-12 17:51:52,709][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:51:52,709][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:51:52,709][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:51:52,709][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
[2025-04-12 17:51:55,507][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:51:55,507][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:51:55,539][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:51:55,582][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:51:55,601][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 17:51:55,611][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:51:55,612][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 17:51:55,613][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:51:55,642][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:51:55,682][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:51:55,699][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 17:51:55,700][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:51:55,700][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 17:51:55,702][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:51:55,733][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:51:55,770][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:51:55,786][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 17:51:55,787][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:51:55,788][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 17:51:55,789][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 17:51:55,789][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:51:55,790][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:51:55,790][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:51:55,790][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:51:55,790][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:51:55,790][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-12 17:51:55,790][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 17:51:55,790][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-04-12 17:51:55,791][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:51:55,791][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:51:55,791][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:51:55,791][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:51:55,791][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:51:55,791][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-12 17:51:55,791][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 17:51:55,792][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-12 17:51:55,792][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:51:55,792][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:51:55,792][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:51:55,792][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:51:55,792][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:51:55,792][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-12 17:51:55,793][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 17:51:55,793][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-12 17:51:55,793][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 17:51:55,793][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:51:55,793][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:51:55,793][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:52:00,935][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:52:00,938][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 17:52:00,938][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:52:00,939][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:28,  1.19s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:40,  1.81it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:18,  3.75it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.37it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.67it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.68it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.87it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  7.86it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.58it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.11it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.48it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.75it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.94it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.08it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.17it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.24it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.89it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.89it/s]
[2025-04-12 17:52:11,525][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1401
[2025-04-12 17:52:11,793][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0726, Metrics: {'mse': 0.07804504036903381, 'rmse': 0.2793654244337223, 'r2': -0.8648216724395752}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.90it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.02it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.09it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.60it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 17:52:19,630][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0610
[2025-04-12 17:52:19,905][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0362, Metrics: {'mse': 0.03731273114681244, 'rmse': 0.1931650360360602, 'r2': 0.10844314098358154}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.32it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.60it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 17:52:27,936][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0333
[2025-04-12 17:52:28,227][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0358, Metrics: {'mse': 0.03637943044304848, 'rmse': 0.190733925778946, 'r2': 0.1307436227798462}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.35it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.62it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.83it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.99it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 17:52:35,986][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0281
[2025-04-12 17:52:36,284][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0374, Metrics: {'mse': 0.03771558031439781, 'rmse': 0.19420499559588525, 'r2': 0.09881740808486938}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.45it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.71it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.88it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 17:52:43,653][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0225
[2025-04-12 17:52:43,951][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0380, Metrics: {'mse': 0.037830520421266556, 'rmse': 0.19450069516910873, 'r2': 0.09607100486755371}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.60it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.82it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.98it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 17:52:51,302][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0215
[2025-04-12 17:52:51,586][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0365, Metrics: {'mse': 0.03609136864542961, 'rmse': 0.18997728455115262, 'r2': 0.13762664794921875}
[2025-04-12 17:52:51,587][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▁▁
wandb:     best_val_mse █▁▁
wandb:      best_val_r2 ▁██
wandb:    best_val_rmse █▁▁
wandb:            epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▁▁▁▁▁
wandb:          val_mse █▁▁▁▁▁
wandb:           val_r2 ▁█████
wandb:         val_rmse █▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.03582
wandb:     best_val_mse 0.03638
wandb:      best_val_r2 0.13074
wandb:    best_val_rmse 0.19073
wandb:            epoch 6
wandb:   final_test_mse 0.17436
wandb:    final_test_r2 -3.41023
wandb:  final_test_rmse 0.41756
wandb:  final_train_mse 0.02187
wandb:   final_train_r2 0.18488
wandb: final_train_rmse 0.14788
wandb:    final_val_mse 0.03638
wandb:     final_val_r2 0.13074
wandb:   final_val_rmse 0.19073
wandb:    learning_rate 1e-05
wandb:       train_loss 0.0215
wandb:       train_time 48.50511
wandb:         val_loss 0.03649
wandb:          val_mse 0.03609
wandb:           val_r2 0.13763
wandb:         val_rmse 0.18998
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_175147-2ord09hd
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_175147-2ord09hd/logs
Cross-lingual experiment for complexity (en → ru) completed successfully
Running cross-lingual question_type from fi to ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:53:13,092][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/fi_to_ar/question_type
experiment_name: cross_lingual_question_type_fi_to_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: fi
  eval_language: ar
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:53:13,114][__main__][INFO] - Normalized task: question_type
[2025-04-12 17:53:13,114][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 17:53:13,115][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:53:15,136][__main__][INFO] - Running cross-lingual experiment: fi -> ar
[2025-04-12 17:53:15,136][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 17:53:15,137][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:53:18,025][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:53:18,026][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:53:18,102][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:53:18,137][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:53:18,245][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 17:53:18,257][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:53:18,257][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 17:53:18,259][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:53:18,286][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:53:18,321][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:53:18,334][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 17:53:18,336][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:53:18,336][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 17:53:18,337][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:53:18,362][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:53:18,396][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:53:18,411][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 17:53:18,412][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:53:18,413][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 17:53:18,413][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 17:53:18,414][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:53:18,414][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:53:18,414][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:53:18,414][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:53:18,415][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 17:53:18,415][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-12 17:53:18,415][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 17:53:18,415][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:53:18,415][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:53:18,415][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:53:18,415][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:53:18,416][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:53:18,416][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-12 17:53:18,416][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-12 17:53:18,416][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 17:53:18,416][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:53:18,416][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:53:18,416][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:53:18,416][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:53:18,417][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:53:18,417][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:53:18,417][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:53:18,417][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 17:53:18,417][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:53:18,417][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 17:53:18,417][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:53:18,418][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:53:18,418][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
[2025-04-12 17:53:21,285][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:53:21,286][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:53:21,317][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:53:21,359][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:53:21,419][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:53:21,427][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:53:21,427][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:53:21,429][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:53:21,460][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:53:21,503][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:53:21,519][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:53:21,521][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:53:21,521][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:53:21,523][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:53:21,555][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:53:21,593][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:53:21,610][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:53:21,611][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:53:21,611][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:53:21,612][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:53:21,613][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:53:21,613][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:53:21,613][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:53:21,613][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:53:21,614][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-12 17:53:21,614][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-12 17:53:21,614][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:53:21,614][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:53:21,614][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:53:21,614][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:53:21,614][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:53:21,615][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:53:21,615][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-12 17:53:21,615][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-12 17:53:21,615][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:53:21,615][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:53:21,615][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:53:21,615][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:53:21,615][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:53:21,615][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:53:21,616][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-12 17:53:21,616][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-12 17:53:21,616][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:53:21,616][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:53:21,616][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:53:21,616][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:53:21,616][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:53:21,617][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:53:26,877][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:53:26,881][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 17:53:26,881][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:53:26,881][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:33,  1.26s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:28,  2.51it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:17,  4.11it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.53it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  6.71it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.75it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:08,  7.63it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:07,  8.35it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  8.90it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:06,  9.32it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.62it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.84it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:05, 10.00it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.10it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.19it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.25it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.29it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.32it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.35it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.71it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.82it/s]
[2025-04-12 17:53:37,837][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6911
[2025-04-12 17:53:38,073][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6877, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.71it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.90it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.01it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 17:53:45,903][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6242
[2025-04-12 17:53:46,148][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5929, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8771929824561403}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.29it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.58it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.80it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 17:53:54,175][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.3684
[2025-04-12 17:53:54,438][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2901, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.44it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.69it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.88it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 17:54:02,224][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1253
[2025-04-12 17:54:02,745][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1434, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9473684210526315}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:42,  1.74it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:15,  4.53it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:10,  6.38it/s]Epoch 5/10:   9%|▉         | 7/75 [00:01<00:08,  7.62it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:07,  8.47it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:07,  9.05it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06,  9.45it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:06,  9.73it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:02<00:05,  9.93it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:02<00:05, 10.07it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.16it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.23it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.28it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:03<00:04, 10.31it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:03<00:04, 10.33it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.35it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.36it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.36it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:04<00:03, 10.37it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.32it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.34it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.36it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.37it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.38it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00,  9.69it/s]
[2025-04-12 17:54:10,908][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0812
[2025-04-12 17:54:11,172][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2001, Metrics: {'accuracy': 0.9365079365079365, 'f1': 0.9333333333333333}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.38it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.65it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.77it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 17:54:18,552][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0705
[2025-04-12 17:54:18,820][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1504, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.49it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.73it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.90it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 17:54:26,196][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0588
[2025-04-12 17:54:26,512][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1054, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.39it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.66it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 17:54:34,312][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0494
[2025-04-12 17:54:34,577][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1176, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:16,  4.36it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.63it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 17:54:41,967][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0417
[2025-04-12 17:54:42,238][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1547, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:16,  4.49it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.74it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.91it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 17:54:49,613][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0411
[2025-04-12 17:54:49,888][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1635, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
[2025-04-12 17:54:49,889][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▇███
wandb:          best_val_f1 ▁▇███
wandb:        best_val_loss █▇▃▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▅▂▁▁▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▇████████
wandb:               val_f1 ▁▇████████
wandb:             val_loss █▇▃▁▂▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.95238
wandb:          best_val_f1 0.94915
wandb:        best_val_loss 0.10538
wandb:                epoch 10
wandb:  final_test_accuracy 0.71429
wandb:        final_test_f1 0.60714
wandb: final_train_accuracy 0.99331
wandb:       final_train_f1 0.99327
wandb:   final_val_accuracy 0.95238
wandb:         final_val_f1 0.94915
wandb:        learning_rate 1e-05
wandb:           train_loss 0.04114
wandb:           train_time 80.55924
wandb:         val_accuracy 0.95238
wandb:               val_f1 0.95082
wandb:             val_loss 0.1635
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_175313-io46buyg
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_175313-io46buyg/logs
Cross-lingual experiment for question_type (fi → ar) completed successfully
Running cross-lingual complexity from fi to ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:55:10,957][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/fi_to_ar/complexity
experiment_name: cross_lingual_complexity_fi_to_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: fi
  eval_language: ar
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:55:10,958][__main__][INFO] - Normalized task: complexity
[2025-04-12 17:55:10,958][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 17:55:10,958][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:55:12,491][__main__][INFO] - Running cross-lingual experiment: fi -> ar
[2025-04-12 17:55:12,491][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 17:55:12,492][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:55:15,399][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:55:15,400][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:55:15,489][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:55:15,528][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:55:15,663][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 17:55:15,674][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:55:15,675][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 17:55:15,676][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:55:15,712][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:55:15,756][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:55:15,775][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 17:55:15,777][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:55:15,777][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 17:55:15,778][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:55:15,808][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:55:15,847][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:55:15,863][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 17:55:15,865][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:55:15,865][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 17:55:15,867][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 17:55:15,868][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:55:15,868][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:55:15,868][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:55:15,868][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:55:15,868][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:55:15,868][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-12 17:55:15,869][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 17:55:15,869][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-12 17:55:15,869][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:55:15,869][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:55:15,869][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:55:15,869][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:55:15,869][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:55:15,870][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-12 17:55:15,870][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 17:55:15,870][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-12 17:55:15,870][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:55:15,870][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:55:15,870][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:55:15,870][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:55:15,870][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:55:15,871][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-12 17:55:15,871][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 17:55:15,871][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-12 17:55:15,871][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 17:55:15,871][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:55:15,871][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:55:15,872][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
[2025-04-12 17:55:18,704][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:55:18,705][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:55:18,729][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:55:18,761][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:55:18,776][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 17:55:18,784][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:55:18,784][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 17:55:18,785][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:55:18,807][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:55:18,842][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:55:18,855][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 17:55:18,856][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:55:18,857][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 17:55:18,857][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:55:18,878][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:55:18,909][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:55:18,923][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 17:55:18,925][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:55:18,925][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 17:55:18,926][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 17:55:18,926][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:55:18,926][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:55:18,927][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:55:18,927][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:55:18,927][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:55:18,927][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-12 17:55:18,927][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 17:55:18,927][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-12 17:55:18,927][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:55:18,928][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:55:18,928][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:55:18,928][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:55:18,928][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:55:18,928][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-12 17:55:18,928][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 17:55:18,928][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-12 17:55:18,929][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:55:18,929][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:55:18,929][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:55:18,929][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:55:18,929][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:55:18,929][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-12 17:55:18,929][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 17:55:18,929][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-12 17:55:18,930][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 17:55:18,930][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:55:18,930][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:55:18,930][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:55:24,316][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:55:24,319][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 17:55:24,320][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:55:24,320][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:31,  1.23s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:41,  1.76it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:19,  3.67it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.28it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.59it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.60it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.88it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  7.86it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.59it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.11it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.48it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.75it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.94it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.08it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.18it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.25it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.29it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.73it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.84it/s]
[2025-04-12 17:55:35,013][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1149
[2025-04-12 17:55:35,246][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1860, Metrics: {'mse': 0.1862689107656479, 'rmse': 0.43158882140950766, 'r2': -1.8411738872528076}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.72it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.92it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.02it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 17:55:43,058][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0593
[2025-04-12 17:55:43,301][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1210, Metrics: {'mse': 0.12102789431810379, 'rmse': 0.3478906355711573, 'r2': -0.8460476398468018}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.46it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.71it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.89it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 17:55:51,317][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0391
[2025-04-12 17:55:51,575][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0560, Metrics: {'mse': 0.056453581899404526, 'rmse': 0.237599625208889, 'r2': 0.13890916109085083}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.30it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.59it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.81it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 17:55:59,360][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0270
[2025-04-12 17:55:59,620][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0526, Metrics: {'mse': 0.05263611674308777, 'rmse': 0.22942562355388244, 'r2': 0.19713729619979858}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.28it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.58it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.80it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 17:56:07,430][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0215
[2025-04-12 17:56:07,692][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0543, Metrics: {'mse': 0.054272498935461044, 'rmse': 0.2329645872991452, 'r2': 0.17217743396759033}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:17,  4.35it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.63it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:06,  9.78it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 17:56:15,074][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0181
[2025-04-12 17:56:15,339][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0702, Metrics: {'mse': 0.07022659480571747, 'rmse': 0.2650030090503077, 'r2': -0.07117164134979248}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.42it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.69it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.88it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 17:56:22,706][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0154
[2025-04-12 17:56:22,977][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0648, Metrics: {'mse': 0.06484231352806091, 'rmse': 0.2546415392823035, 'r2': 0.010955214500427246}
[2025-04-12 17:56:22,978][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▁▁
wandb:     best_val_mse █▅▁▁
wandb:      best_val_r2 ▁▄██
wandb:    best_val_rmse █▅▁▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▁▁▁▂▂
wandb:          val_mse █▅▁▁▁▂▂
wandb:           val_r2 ▁▄███▇▇
wandb:         val_rmse █▅▁▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05265
wandb:     best_val_mse 0.05264
wandb:      best_val_r2 0.19714
wandb:    best_val_rmse 0.22943
wandb:            epoch 7
wandb:   final_test_mse 0.08455
wandb:    final_test_r2 -0.45761
wandb:  final_test_rmse 0.29078
wandb:  final_train_mse 0.01609
wandb:   final_train_r2 0.20367
wandb: final_train_rmse 0.12686
wandb:    final_val_mse 0.05264
wandb:     final_val_r2 0.19714
wandb:   final_val_rmse 0.22943
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01542
wandb:       train_time 56.45279
wandb:         val_loss 0.06485
wandb:          val_mse 0.06484
wandb:           val_r2 0.01096
wandb:         val_rmse 0.25464
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_175511-frw64fr2
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_175511-frw64fr2/logs
Cross-lingual experiment for complexity (fi → ar) completed successfully
Running cross-lingual question_type from fi to en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:56:44,683][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/fi_to_en/question_type
experiment_name: cross_lingual_question_type_fi_to_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: fi
  eval_language: en
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:56:44,683][__main__][INFO] - Normalized task: question_type
[2025-04-12 17:56:44,683][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 17:56:44,683][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:56:46,664][__main__][INFO] - Running cross-lingual experiment: fi -> en
[2025-04-12 17:56:46,664][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 17:56:46,665][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:56:49,571][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:56:49,572][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:56:49,681][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:56:49,715][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:56:49,857][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 17:56:49,869][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:56:49,869][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 17:56:49,871][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:56:49,897][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:56:49,936][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:56:49,952][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 17:56:49,953][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:56:49,953][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 17:56:49,955][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:56:49,981][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:56:50,015][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:56:50,029][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 17:56:50,031][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:56:50,031][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 17:56:50,033][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 17:56:50,034][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:56:50,034][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:56:50,034][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:56:50,034][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:56:50,034][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 17:56:50,034][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-12 17:56:50,034][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 17:56:50,035][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:56:50,035][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:56:50,035][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:56:50,035][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:56:50,035][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:56:50,035][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-12 17:56:50,035][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-12 17:56:50,036][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 17:56:50,036][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:56:50,036][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:56:50,036][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:56:50,036][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:56:50,036][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:56:50,036][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:56:50,036][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:56:50,036][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 17:56:50,037][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:56:50,037][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 17:56:50,037][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:56:50,037][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:56:50,038][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
[2025-04-12 17:56:52,880][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:56:52,880][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:56:52,964][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:56:53,009][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:56:53,027][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:56:53,037][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:56:53,037][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:56:53,039][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:56:53,070][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:56:53,106][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:56:53,121][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:56:53,123][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:56:53,123][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:56:53,124][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:56:53,152][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:56:53,193][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:56:53,209][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:56:53,210][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:56:53,211][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:56:53,212][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:56:53,212][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:56:53,213][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:56:53,213][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:56:53,213][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:56:53,213][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-12 17:56:53,213][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 17:56:53,213][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:56:53,213][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 17:56:53,213][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:56:53,214][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:56:53,214][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:56:53,214][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:56:53,214][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 17:56:53,214][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 17:56:53,214][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:56:53,214][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:56:53,214][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 17:56:53,215][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 17:56:53,215][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 17:56:53,215][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 17:56:53,215][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 17:56:53,215][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 17:56:53,215][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:56:53,215][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 17:56:53,215][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:56:53,215][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:56:53,216][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:56:53,216][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:56:58,678][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:56:58,681][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 17:56:58,681][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:56:58,681][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<02:09,  1.75s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:57,  1.28it/s]Epoch 1/10:   5%|▌         | 4/75 [00:02<00:25,  2.83it/s]Epoch 1/10:   8%|▊         | 6/75 [00:02<00:15,  4.32it/s]Epoch 1/10:  11%|█         | 8/75 [00:02<00:11,  5.64it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:09,  6.77it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:10,  6.34it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:03<00:08,  7.41it/s]Epoch 1/10:  20%|██        | 15/75 [00:03<00:07,  8.23it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:03<00:06,  8.84it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:06,  9.28it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.60it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.83it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:04<00:05, 10.00it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:04<00:04, 10.11it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.19it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.25it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.29it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:05<00:03, 10.32it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:05<00:03, 10.34it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.35it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.37it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:08<00:00, 10.38it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.71it/s]Epoch 1/10: 100%|██████████| 75/75 [00:09<00:00,  8.32it/s]
[2025-04-12 17:57:09,921][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6911
[2025-04-12 17:57:10,155][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6877, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.65it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.85it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 17:57:17,991][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6242
[2025-04-12 17:57:18,234][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5929, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8771929824561403}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.29it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.58it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.79it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 17:57:26,268][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.3684
[2025-04-12 17:57:26,527][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2901, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.32it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.60it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.80it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.36it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.31it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.33it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.34it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.36it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.73it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.14it/s]
[2025-04-12 17:57:34,321][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1253
[2025-04-12 17:57:34,661][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1434, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9473684210526315}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.30it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.58it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.80it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.74it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.95it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.02it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.14it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.21it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.30it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.13it/s]
[2025-04-12 17:57:42,498][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0812
[2025-04-12 17:57:42,767][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2001, Metrics: {'accuracy': 0.9365079365079365, 'f1': 0.9333333333333333}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.39it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.66it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 17:57:50,155][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0705
[2025-04-12 17:57:50,431][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1504, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.55it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.78it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.93it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 17:57:57,802][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0588
[2025-04-12 17:57:58,059][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1054, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.38it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.65it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.77it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.14it/s]
[2025-04-12 17:58:05,868][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0494
[2025-04-12 17:58:06,140][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1176, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:16,  4.39it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.65it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.77it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 17:58:13,525][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0417
[2025-04-12 17:58:13,788][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1547, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:17,  4.33it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.61it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 17:58:21,170][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0411
[2025-04-12 17:58:21,438][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1635, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
[2025-04-12 17:58:21,439][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▇███
wandb:          best_val_f1 ▁▇███
wandb:        best_val_loss █▇▃▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▅▂▁▁▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▇████████
wandb:               val_f1 ▁▇████████
wandb:             val_loss █▇▃▁▂▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.95238
wandb:          best_val_f1 0.94915
wandb:        best_val_loss 0.10538
wandb:                epoch 10
wandb:  final_test_accuracy 0.78182
wandb:        final_test_f1 0.7551
wandb: final_train_accuracy 0.99331
wandb:       final_train_f1 0.99327
wandb:   final_val_accuracy 0.95238
wandb:         final_val_f1 0.94915
wandb:        learning_rate 1e-05
wandb:           train_loss 0.04114
wandb:           train_time 80.53641
wandb:         val_accuracy 0.95238
wandb:               val_f1 0.95082
wandb:             val_loss 0.1635
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_175644-dvdpzu6x
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_175644-dvdpzu6x/logs
Cross-lingual experiment for question_type (fi → en) completed successfully
Running cross-lingual complexity from fi to en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 17:58:44,952][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/fi_to_en/complexity
experiment_name: cross_lingual_complexity_fi_to_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: fi
  eval_language: en
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 17:58:44,952][__main__][INFO] - Normalized task: complexity
[2025-04-12 17:58:44,952][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 17:58:44,953][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 17:58:46,677][__main__][INFO] - Running cross-lingual experiment: fi -> en
[2025-04-12 17:58:46,677][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 17:58:46,678][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 17:58:49,563][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:58:49,564][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:58:49,652][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:58:49,683][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:58:49,790][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 17:58:49,801][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:58:49,802][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 17:58:49,803][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:58:49,825][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:58:49,860][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:58:49,875][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 17:58:49,876][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:58:49,876][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 17:58:49,878][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:58:49,901][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:58:49,939][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:58:49,954][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 17:58:49,956][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:58:49,956][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 17:58:49,958][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 17:58:49,958][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:58:49,959][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:58:49,959][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:58:49,959][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:58:49,959][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:58:49,959][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-12 17:58:49,959][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 17:58:49,959][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-12 17:58:49,960][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:58:49,960][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:58:49,960][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:58:49,960][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:58:49,960][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:58:49,960][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-12 17:58:49,960][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 17:58:49,961][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-12 17:58:49,961][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:58:49,961][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:58:49,961][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:58:49,961][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:58:49,961][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:58:49,961][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-12 17:58:49,962][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 17:58:49,962][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-12 17:58:49,962][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 17:58:49,962][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:58:49,962][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:58:49,963][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
[2025-04-12 17:58:52,793][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 17:58:52,794][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:58:52,820][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:58:52,858][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:58:52,874][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 17:58:52,883][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:58:52,884][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 17:58:52,885][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:58:52,910][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:58:52,947][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:58:52,963][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 17:58:52,964][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:58:52,964][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 17:58:52,965][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 17:58:52,994][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:58:53,036][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 17:58:53,052][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 17:58:53,053][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 17:58:53,054][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 17:58:53,055][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 17:58:53,056][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:58:53,056][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:58:53,056][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:58:53,056][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:58:53,056][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:58:53,056][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-12 17:58:53,057][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 17:58:53,057][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-12 17:58:53,057][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:58:53,057][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:58:53,057][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:58:53,057][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:58:53,057][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:58:53,057][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-12 17:58:53,058][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 17:58:53,058][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-12 17:58:53,058][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 17:58:53,058][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 17:58:53,058][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 17:58:53,058][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 17:58:53,058][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 17:58:53,059][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-12 17:58:53,059][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 17:58:53,059][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-12 17:58:53,059][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 17:58:53,059][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 17:58:53,059][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 17:58:53,060][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 17:58:58,579][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 17:58:58,582][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 17:58:58,582][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 17:58:58,582][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:58,  1.60s/it]Epoch 1/10:   3%|▎         | 2/75 [00:02<01:06,  1.11it/s]Epoch 1/10:   5%|▌         | 4/75 [00:02<00:28,  2.51it/s]Epoch 1/10:   8%|▊         | 6/75 [00:02<00:17,  3.92it/s]Epoch 1/10:  11%|█         | 8/75 [00:02<00:12,  5.23it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:10,  6.39it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:10,  6.07it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:03<00:08,  7.18it/s]Epoch 1/10:  20%|██        | 15/75 [00:03<00:07,  8.03it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:03<00:06,  8.68it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:06,  9.17it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.52it/s]Epoch 1/10:  31%|███       | 23/75 [00:04<00:05,  9.77it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:04<00:05,  9.95it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:04<00:04, 10.08it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.17it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.24it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:05<00:04, 10.28it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:05<00:03, 10.31it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:05<00:03, 10.34it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.34it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.35it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:06<00:03, 10.36it/s]Epoch 1/10:  60%|██████    | 45/75 [00:06<00:02, 10.37it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:06<00:02, 10.37it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.37it/s]Epoch 1/10:  71%|███████   | 53/75 [00:07<00:02, 10.37it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:08<00:00, 10.38it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.38it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.38it/s]Epoch 1/10: 100%|██████████| 75/75 [00:09<00:00, 10.70it/s]Epoch 1/10: 100%|██████████| 75/75 [00:09<00:00,  8.15it/s]
[2025-04-12 17:59:09,814][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1149
[2025-04-12 17:59:10,043][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1860, Metrics: {'mse': 0.1862689107656479, 'rmse': 0.43158882140950766, 'r2': -1.8411738872528076}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.75it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.93it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.03it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 17:59:17,855][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0593
[2025-04-12 17:59:18,095][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1210, Metrics: {'mse': 0.12102789431810379, 'rmse': 0.3478906355711573, 'r2': -0.8460476398468018}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.31it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.60it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.81it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.75it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.18it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 17:59:26,122][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0391
[2025-04-12 17:59:26,382][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0560, Metrics: {'mse': 0.056453581899404526, 'rmse': 0.237599625208889, 'r2': 0.13890916109085083}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.68it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.88it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.99it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.53it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 17:59:34,148][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0270
[2025-04-12 17:59:34,410][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0526, Metrics: {'mse': 0.05263611674308777, 'rmse': 0.22942562355388244, 'r2': 0.19713729619979858}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.24it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.54it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.77it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.39it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.74it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.94it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.08it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.17it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.24it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.36it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 17:59:42,233][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0215
[2025-04-12 17:59:42,499][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0543, Metrics: {'mse': 0.054272498935461044, 'rmse': 0.2329645872991452, 'r2': 0.17217743396759033}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.55it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.78it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.94it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 17:59:49,878][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0181
[2025-04-12 17:59:50,149][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0702, Metrics: {'mse': 0.07022659480571747, 'rmse': 0.2650030090503077, 'r2': -0.07117164134979248}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:17,  4.22it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.52it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.76it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.39it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:01<00:06,  9.74it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.95it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.09it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.18it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 17:59:57,539][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0154
[2025-04-12 17:59:57,799][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0648, Metrics: {'mse': 0.06484231352806091, 'rmse': 0.2546415392823035, 'r2': 0.010955214500427246}
[2025-04-12 17:59:57,800][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▁▁
wandb:     best_val_mse █▅▁▁
wandb:      best_val_r2 ▁▄██
wandb:    best_val_rmse █▅▁▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▁▁▁▂▂
wandb:          val_mse █▅▁▁▁▂▂
wandb:           val_r2 ▁▄███▇▇
wandb:         val_rmse █▅▁▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05265
wandb:     best_val_mse 0.05264
wandb:      best_val_r2 0.19714
wandb:    best_val_rmse 0.22943
wandb:            epoch 7
wandb:   final_test_mse 0.0652
wandb:    final_test_r2 -0.69187
wandb:  final_test_rmse 0.25535
wandb:  final_train_mse 0.01609
wandb:   final_train_r2 0.20367
wandb: final_train_rmse 0.12686
wandb:    final_val_mse 0.05264
wandb:     final_val_r2 0.19714
wandb:   final_val_rmse 0.22943
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01542
wandb:       train_time 57.18842
wandb:         val_loss 0.06485
wandb:          val_mse 0.06484
wandb:           val_r2 0.01096
wandb:         val_rmse 0.25464
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_175844-c1ymnedy
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_175844-c1ymnedy/logs
Cross-lingual experiment for complexity (fi → en) completed successfully
Running cross-lingual question_type from fi to id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:00:19,756][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/fi_to_id/question_type
experiment_name: cross_lingual_question_type_fi_to_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: fi
  eval_language: id
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:00:19,756][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:00:19,756][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:00:19,756][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:00:21,523][__main__][INFO] - Running cross-lingual experiment: fi -> id
[2025-04-12 18:00:21,523][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:00:21,523][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:00:24,447][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:00:24,448][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:00:24,527][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:00:24,558][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:00:24,663][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 18:00:24,673][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:00:24,674][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 18:00:24,675][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:00:24,700][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:00:24,737][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:00:24,752][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 18:00:24,753][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:00:24,753][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 18:00:24,755][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:00:24,779][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:00:24,814][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:00:24,829][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 18:00:24,831][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:00:24,831][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 18:00:24,832][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 18:00:24,833][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:00:24,833][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:00:24,833][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:00:24,833][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:00:24,833][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 18:00:24,833][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-12 18:00:24,834][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 18:00:24,834][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:00:24,834][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:00:24,834][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:00:24,834][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:00:24,834][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:00:24,834][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-12 18:00:24,834][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-12 18:00:24,835][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 18:00:24,835][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:00:24,835][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:00:24,835][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:00:24,835][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:00:24,835][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:00:24,835][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:00:24,835][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:00:24,836][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 18:00:24,836][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:00:24,836][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 18:00:24,836][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:00:24,836][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:00:24,836][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
[2025-04-12 18:00:27,663][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:00:27,664][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:00:27,685][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:00:27,718][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:00:27,753][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:00:27,760][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:00:27,761][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:00:27,762][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:00:27,782][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:00:27,814][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:00:27,828][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:00:27,830][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:00:27,830][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:00:27,831][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:00:27,851][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:00:27,883][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:00:27,898][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:00:27,900][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:00:27,900][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:00:27,901][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:00:27,902][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:00:27,902][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:00:27,902][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:00:27,902][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:00:27,903][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-12 18:00:27,903][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-12 18:00:27,903][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:00:27,903][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:00:27,903][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:00:27,903][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:00:27,903][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:00:27,903][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:00:27,904][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:00:27,904][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:00:27,904][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:00:27,904][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:00:27,904][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:00:27,904][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:00:27,904][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:00:27,904][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:00:27,905][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:00:27,905][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:00:27,905][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:00:27,905][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:00:27,905][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:00:27,905][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:00:27,905][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:00:27,905][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:00:33,019][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:00:33,021][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:00:33,022][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:00:33,022][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:40,  1.35s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:45,  1.62it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:20,  3.44it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.03it/s]Epoch 1/10:  11%|█         | 8/75 [00:02<00:10,  6.35it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.40it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.21it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.82it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.26it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.59it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.82it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.99it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.11it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.19it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.25it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.29it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  80%|████████  | 60/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00,  8.87it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:08<00:00,  9.28it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00,  9.59it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00,  9.82it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.70it/s]
[2025-04-12 18:00:43,987][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6911
[2025-04-12 18:00:44,214][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6877, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.69it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.89it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.00it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.32it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.34it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.36it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.73it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:00:52,037][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6242
[2025-04-12 18:00:52,276][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5929, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8771929824561403}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.41it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.67it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:01:00,285][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.3684
[2025-04-12 18:01:00,544][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2901, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.36it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.63it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.83it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.14it/s]
[2025-04-12 18:01:08,347][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1253
[2025-04-12 18:01:08,618][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1434, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9473684210526315}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.39it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.66it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 18:01:16,491][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0812
[2025-04-12 18:01:16,757][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2001, Metrics: {'accuracy': 0.9365079365079365, 'f1': 0.9333333333333333}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.82it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.97it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.57it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:01:24,123][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0705
[2025-04-12 18:01:24,380][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1504, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.40it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.66it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.33it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.35it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.36it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 18:01:31,767][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0588
[2025-04-12 18:01:32,260][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1054, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:15,  4.65it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.85it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.53it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:01:40,053][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0494
[2025-04-12 18:01:40,371][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1176, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:16,  4.48it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.73it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.90it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:01:47,751][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0417
[2025-04-12 18:01:48,066][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1547, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:16,  4.54it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.77it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.91it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:01:55,442][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0411
[2025-04-12 18:01:55,704][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1635, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
[2025-04-12 18:01:55,705][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▇███
wandb:          best_val_f1 ▁▇███
wandb:        best_val_loss █▇▃▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▅▂▁▁▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▇████████
wandb:               val_f1 ▁▇████████
wandb:             val_loss █▇▃▁▂▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.95238
wandb:          best_val_f1 0.94915
wandb:        best_val_loss 0.10538
wandb:                epoch 10
wandb:  final_test_accuracy 0.70909
wandb:        final_test_f1 0.60976
wandb: final_train_accuracy 0.99331
wandb:       final_train_f1 0.99327
wandb:   final_val_accuracy 0.95238
wandb:         final_val_f1 0.94915
wandb:        learning_rate 1e-05
wandb:           train_loss 0.04114
wandb:           train_time 80.33954
wandb:         val_accuracy 0.95238
wandb:               val_f1 0.95082
wandb:             val_loss 0.1635
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_180019-wglhno2a
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_180019-wglhno2a/logs
Cross-lingual experiment for question_type (fi → id) completed successfully
Running cross-lingual complexity from fi to id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:02:17,830][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/fi_to_id/complexity
experiment_name: cross_lingual_complexity_fi_to_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: fi
  eval_language: id
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:02:17,830][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:02:17,830][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:02:17,830][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:02:19,417][__main__][INFO] - Running cross-lingual experiment: fi -> id
[2025-04-12 18:02:19,417][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:02:19,418][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:02:22,285][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:02:22,285][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:02:22,346][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:02:22,379][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:02:22,478][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 18:02:22,489][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:02:22,489][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 18:02:22,490][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:02:22,512][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:02:22,548][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:02:22,563][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 18:02:22,564][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:02:22,564][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 18:02:22,565][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:02:22,594][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:02:22,632][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:02:22,648][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 18:02:22,649][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:02:22,650][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 18:02:22,651][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 18:02:22,651][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:02:22,651][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:02:22,651][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:02:22,651][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:02:22,652][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:02:22,652][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-12 18:02:22,652][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 18:02:22,652][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-12 18:02:22,652][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:02:22,652][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:02:22,652][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:02:22,653][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:02:22,653][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:02:22,653][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-12 18:02:22,653][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 18:02:22,653][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-12 18:02:22,653][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:02:22,653][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:02:22,654][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:02:22,654][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:02:22,654][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:02:22,654][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-12 18:02:22,654][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 18:02:22,654][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-12 18:02:22,654][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 18:02:22,654][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:02:22,655][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:02:22,655][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
[2025-04-12 18:02:25,448][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:02:25,449][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:02:25,472][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:02:25,508][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:02:25,523][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:02:25,531][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:02:25,531][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:02:25,533][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:02:25,556][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:02:25,591][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:02:25,606][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:02:25,607][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:02:25,608][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:02:25,609][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:02:25,630][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:02:25,664][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:02:25,679][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:02:25,681][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:02:25,681][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:02:25,682][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:02:25,683][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:02:25,683][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:02:25,684][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:02:25,684][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:02:25,684][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:02:25,684][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-12 18:02:25,684][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:02:25,684][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-12 18:02:25,685][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:02:25,685][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:02:25,685][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:02:25,685][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:02:25,685][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:02:25,685][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-12 18:02:25,685][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:02:25,686][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-12 18:02:25,686][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:02:25,686][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:02:25,686][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:02:25,686][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:02:25,686][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:02:25,686][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-12 18:02:25,686][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:02:25,687][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-12 18:02:25,687][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:02:25,687][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:02:25,687][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:02:25,687][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:02:30,908][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:02:30,911][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:02:30,911][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:02:30,911][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:35,  1.29s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:29,  2.47it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:17,  4.06it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.47it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  6.66it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.62it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.36it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.92it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.34it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.65it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.87it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.02it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.13it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.22it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.27it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.33it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00,  8.88it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00,  9.28it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00,  9.59it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00,  9.82it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.28it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.78it/s]
[2025-04-12 18:02:41,706][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1149
[2025-04-12 18:02:41,972][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1860, Metrics: {'mse': 0.1862689107656479, 'rmse': 0.43158882140950766, 'r2': -1.8411738872528076}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.70it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.90it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.01it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.55it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 18:02:49,794][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0593
[2025-04-12 18:02:50,038][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1210, Metrics: {'mse': 0.12102789431810379, 'rmse': 0.3478906355711573, 'r2': -0.8460476398468018}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.34it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.62it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.78it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 18:02:58,076][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0391
[2025-04-12 18:02:58,425][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0560, Metrics: {'mse': 0.056453581899404526, 'rmse': 0.237599625208889, 'r2': 0.13890916109085083}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.38it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.65it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.14it/s]
[2025-04-12 18:03:06,226][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0270
[2025-04-12 18:03:06,488][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0526, Metrics: {'mse': 0.05263611674308777, 'rmse': 0.22942562355388244, 'r2': 0.19713729619979858}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.37it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.65it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.77it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 18:03:14,298][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0215
[2025-04-12 18:03:14,562][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0543, Metrics: {'mse': 0.054272498935461044, 'rmse': 0.2329645872991452, 'r2': 0.17217743396759033}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.38it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.65it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:03:21,943][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0181
[2025-04-12 18:03:22,215][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0702, Metrics: {'mse': 0.07022659480571747, 'rmse': 0.2650030090503077, 'r2': -0.07117164134979248}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.51it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.76it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.92it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.33it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.35it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.36it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:03:29,591][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0154
[2025-04-12 18:03:29,859][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0648, Metrics: {'mse': 0.06484231352806091, 'rmse': 0.2546415392823035, 'r2': 0.010955214500427246}
[2025-04-12 18:03:29,860][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▁▁
wandb:     best_val_mse █▅▁▁
wandb:      best_val_r2 ▁▄██
wandb:    best_val_rmse █▅▁▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▁▁▁▂▂
wandb:          val_mse █▅▁▁▁▂▂
wandb:           val_r2 ▁▄███▇▇
wandb:         val_rmse █▅▁▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05265
wandb:     best_val_mse 0.05264
wandb:      best_val_r2 0.19714
wandb:    best_val_rmse 0.22943
wandb:            epoch 7
wandb:   final_test_mse 0.08625
wandb:    final_test_r2 -1.11536
wandb:  final_test_rmse 0.29369
wandb:  final_train_mse 0.01609
wandb:   final_train_r2 0.20367
wandb: final_train_rmse 0.12686
wandb:    final_val_mse 0.05264
wandb:     final_val_r2 0.19714
wandb:   final_val_rmse 0.22943
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01542
wandb:       train_time 56.70313
wandb:         val_loss 0.06485
wandb:          val_mse 0.06484
wandb:           val_r2 0.01096
wandb:         val_rmse 0.25464
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_180217-y4103yv3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_180217-y4103yv3/logs
Cross-lingual experiment for complexity (fi → id) completed successfully
Running cross-lingual question_type from fi to ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:03:52,215][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/fi_to_ja/question_type
experiment_name: cross_lingual_question_type_fi_to_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: fi
  eval_language: ja
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:03:52,215][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:03:52,215][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:03:52,215][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:03:53,962][__main__][INFO] - Running cross-lingual experiment: fi -> ja
[2025-04-12 18:03:53,963][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:03:53,963][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:03:56,856][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:03:56,856][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:03:56,937][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:03:56,972][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:03:57,077][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 18:03:57,088][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:03:57,088][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 18:03:57,089][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:03:57,116][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:03:57,154][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:03:57,170][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 18:03:57,171][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:03:57,172][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 18:03:57,173][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:03:57,197][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:03:57,234][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:03:57,250][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 18:03:57,252][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:03:57,252][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 18:03:57,253][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 18:03:57,254][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:03:57,254][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:03:57,254][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:03:57,254][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:03:57,254][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 18:03:57,254][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-12 18:03:57,254][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 18:03:57,255][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:03:57,255][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:03:57,255][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:03:57,255][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:03:57,255][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:03:57,255][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-12 18:03:57,255][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-12 18:03:57,255][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 18:03:57,256][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:03:57,256][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:03:57,256][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:03:57,256][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:03:57,256][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:03:57,256][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:03:57,256][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:03:57,256][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 18:03:57,256][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:03:57,257][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 18:03:57,257][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:03:57,257][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:03:57,257][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
[2025-04-12 18:04:00,117][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:04:00,118][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:04:00,142][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:04:00,172][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:04:00,212][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:04:00,221][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:04:00,222][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:04:00,223][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:04:00,241][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:04:00,267][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:04:00,279][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:04:00,280][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:04:00,280][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:04:00,281][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:04:00,299][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:04:00,323][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:04:00,334][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:04:00,336][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:04:00,336][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:04:00,337][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:04:00,337][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:04:00,337][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:04:00,337][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:04:00,337][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:04:00,337][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-12 18:04:00,338][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 18:04:00,338][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:04:00,338][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:04:00,338][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:04:00,338][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:04:00,338][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:04:00,338][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:04:00,338][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-12 18:04:00,339][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-12 18:04:00,339][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:04:00,339][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:04:00,339][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:04:00,339][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:04:00,339][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:04:00,339][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:04:00,339][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-12 18:04:00,340][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-12 18:04:00,340][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:04:00,340][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:04:00,340][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:04:00,340][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:04:00,340][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:04:00,340][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:04:05,253][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:04:05,256][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:04:05,256][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:04:05,256][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:27,  1.18s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:39,  1.83it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:18,  3.78it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.41it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.70it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.70it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.96it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  7.93it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.64it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.15it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.52it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.78it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.96it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.09it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.18it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.25it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.33it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.33it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.35it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.73it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.89it/s]
[2025-04-12 18:04:15,902][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6911
[2025-04-12 18:04:16,128][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6877, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.85it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.00it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.07it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-12 18:04:23,939][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6242
[2025-04-12 18:04:24,222][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5929, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8771929824561403}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.42it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.69it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.88it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:04:32,241][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.3684
[2025-04-12 18:04:32,756][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2901, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.58it/s]Epoch 4/10:   3%|▎         | 2/75 [00:00<00:23,  3.12it/s]Epoch 4/10:   5%|▌         | 4/75 [00:00<00:12,  5.56it/s]Epoch 4/10:   8%|▊         | 6/75 [00:00<00:09,  7.13it/s]Epoch 4/10:  11%|█         | 8/75 [00:01<00:08,  8.16it/s]Epoch 4/10:  13%|█▎        | 10/75 [00:01<00:07,  8.86it/s]Epoch 4/10:  16%|█▌        | 12/75 [00:01<00:06,  9.33it/s]Epoch 4/10:  19%|█▊        | 14/75 [00:01<00:06,  9.66it/s]Epoch 4/10:  21%|██▏       | 16/75 [00:01<00:05,  9.89it/s]Epoch 4/10:  24%|██▍       | 18/75 [00:02<00:05, 10.04it/s]Epoch 4/10:  27%|██▋       | 20/75 [00:02<00:05, 10.15it/s]Epoch 4/10:  29%|██▉       | 22/75 [00:02<00:05, 10.22it/s]Epoch 4/10:  32%|███▏      | 24/75 [00:02<00:04, 10.28it/s]Epoch 4/10:  35%|███▍      | 26/75 [00:02<00:04, 10.32it/s]Epoch 4/10:  37%|███▋      | 28/75 [00:03<00:04, 10.34it/s]Epoch 4/10:  40%|████      | 30/75 [00:03<00:04, 10.36it/s]Epoch 4/10:  43%|████▎     | 32/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  45%|████▌     | 34/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  48%|████▊     | 36/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  51%|█████     | 38/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  59%|█████▊    | 44/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  80%|████████  | 60/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  91%|█████████ | 68/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.40it/s]Epoch 4/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.41it/s]Epoch 4/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00,  9.80it/s]
[2025-04-12 18:04:40,814][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1253
[2025-04-12 18:04:41,077][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1434, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9473684210526315}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.19it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.50it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.76it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.39it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.75it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 18:04:48,889][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0812
[2025-04-12 18:04:49,151][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2001, Metrics: {'accuracy': 0.9365079365079365, 'f1': 0.9333333333333333}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.59it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.81it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.95it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 18:04:56,510][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0705
[2025-04-12 18:04:56,774][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1504, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.41it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.67it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.14it/s]
[2025-04-12 18:05:04,176][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0588
[2025-04-12 18:05:04,447][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1054, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.39it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.66it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:05:12,254][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0494
[2025-04-12 18:05:12,528][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1176, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:17,  4.27it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.56it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.80it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.33it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.35it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:05:19,904][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0417
[2025-04-12 18:05:20,187][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1547, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:17,  4.33it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.61it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:05:27,562][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0411
[2025-04-12 18:05:27,834][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1635, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
[2025-04-12 18:05:27,835][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▇███
wandb:          best_val_f1 ▁▇███
wandb:        best_val_loss █▇▃▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▅▂▁▁▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▇████████
wandb:               val_f1 ▁▇████████
wandb:             val_loss █▇▃▁▂▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.95238
wandb:          best_val_f1 0.94915
wandb:        best_val_loss 0.10538
wandb:                epoch 10
wandb:  final_test_accuracy 0.6413
wandb:        final_test_f1 0.67327
wandb: final_train_accuracy 0.99331
wandb:       final_train_f1 0.99327
wandb:   final_val_accuracy 0.95238
wandb:         final_val_f1 0.94915
wandb:        learning_rate 1e-05
wandb:           train_loss 0.04114
wandb:           train_time 80.3698
wandb:         val_accuracy 0.95238
wandb:               val_f1 0.95082
wandb:             val_loss 0.1635
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_180352-tu1odfgd
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_180352-tu1odfgd/logs
Cross-lingual experiment for question_type (fi → ja) completed successfully
Running cross-lingual complexity from fi to ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:05:48,501][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/fi_to_ja/complexity
experiment_name: cross_lingual_complexity_fi_to_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: fi
  eval_language: ja
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:05:48,502][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:05:48,502][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:05:48,502][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:05:49,844][__main__][INFO] - Running cross-lingual experiment: fi -> ja
[2025-04-12 18:05:49,845][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:05:49,845][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:05:52,731][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:05:52,732][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:05:52,805][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:05:52,844][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:05:52,964][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 18:05:52,974][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:05:52,975][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 18:05:52,977][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:05:53,004][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:05:53,048][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:05:53,064][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 18:05:53,065][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:05:53,066][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 18:05:53,067][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:05:53,090][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:05:53,123][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:05:53,137][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 18:05:53,139][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:05:53,139][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 18:05:53,140][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 18:05:53,140][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:05:53,141][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:05:53,141][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:05:53,141][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:05:53,141][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:05:53,141][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-12 18:05:53,141][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 18:05:53,142][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-12 18:05:53,142][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:05:53,142][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:05:53,142][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:05:53,142][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:05:53,142][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:05:53,142][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-12 18:05:53,142][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 18:05:53,143][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-12 18:05:53,143][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:05:53,143][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:05:53,143][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:05:53,143][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:05:53,143][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:05:53,143][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-12 18:05:53,144][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 18:05:53,144][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-12 18:05:53,144][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 18:05:53,144][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:05:53,144][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:05:53,144][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
[2025-04-12 18:05:56,028][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:05:56,028][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:05:56,063][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:05:56,104][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:05:56,120][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:05:56,130][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:05:56,130][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:05:56,131][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:05:56,158][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:05:56,197][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:05:56,213][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:05:56,214][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:05:56,214][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:05:56,215][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:05:56,239][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:05:56,273][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:05:56,287][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:05:56,289][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:05:56,289][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:05:56,290][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:05:56,291][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:05:56,291][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:05:56,291][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:05:56,291][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:05:56,291][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:05:56,291][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-12 18:05:56,291][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:05:56,292][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-04-12 18:05:56,292][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:05:56,292][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:05:56,292][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:05:56,292][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:05:56,292][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:05:56,292][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-12 18:05:56,292][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:05:56,293][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-12 18:05:56,293][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:05:56,293][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:05:56,293][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:05:56,293][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:05:56,293][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:05:56,293][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-12 18:05:56,294][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:05:56,294][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-12 18:05:56,294][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:05:56,294][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:05:56,294][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:05:56,294][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:06:01,341][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:06:01,344][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:06:01,344][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:06:01,344][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:23,  1.12s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:26,  2.74it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:15,  4.41it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:11,  5.83it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  6.99it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.91it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  7.77it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:07,  8.46it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  8.99it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.39it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.67it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.88it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.04it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.14it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.22it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.73it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.95it/s]
[2025-04-12 18:06:11,685][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1149
[2025-04-12 18:06:11,914][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1860, Metrics: {'mse': 0.1862689107656479, 'rmse': 0.43158882140950766, 'r2': -1.8411738872528076}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.84it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  8.00it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.08it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-12 18:06:19,714][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0593
[2025-04-12 18:06:19,962][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1210, Metrics: {'mse': 0.12102789431810379, 'rmse': 0.3478906355711573, 'r2': -0.8460476398468018}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.47it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.73it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.91it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:06:27,783][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0391
[2025-04-12 18:06:28,029][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0560, Metrics: {'mse': 0.056453581899404526, 'rmse': 0.237599625208889, 'r2': 0.13890916109085083}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.63it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.85it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.99it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 18:06:35,773][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0270
[2025-04-12 18:06:36,025][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0526, Metrics: {'mse': 0.05263611674308777, 'rmse': 0.22942562355388244, 'r2': 0.19713729619979858}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.30it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.59it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.81it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:06:43,787][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0215
[2025-04-12 18:06:44,051][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0543, Metrics: {'mse': 0.054272498935461044, 'rmse': 0.2329645872991452, 'r2': 0.17217743396759033}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.59it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.82it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:06:51,420][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0181
[2025-04-12 18:06:51,675][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0702, Metrics: {'mse': 0.07022659480571747, 'rmse': 0.2650030090503077, 'r2': -0.07117164134979248}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:15,  4.63it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.85it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.53it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:06:59,043][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0154
[2025-04-12 18:06:59,301][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0648, Metrics: {'mse': 0.06484231352806091, 'rmse': 0.2546415392823035, 'r2': 0.010955214500427246}
[2025-04-12 18:06:59,302][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▁▁
wandb:     best_val_mse █▅▁▁
wandb:      best_val_r2 ▁▄██
wandb:    best_val_rmse █▅▁▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▁▁▁▂▂
wandb:          val_mse █▅▁▁▁▂▂
wandb:           val_r2 ▁▄███▇▇
wandb:         val_rmse █▅▁▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05265
wandb:     best_val_mse 0.05264
wandb:      best_val_r2 0.19714
wandb:    best_val_rmse 0.22943
wandb:            epoch 7
wandb:   final_test_mse 0.12798
wandb:    final_test_r2 -1.45843
wandb:  final_test_rmse 0.35775
wandb:  final_train_mse 0.01609
wandb:   final_train_r2 0.20367
wandb: final_train_rmse 0.12686
wandb:    final_val_mse 0.05264
wandb:     final_val_r2 0.19714
wandb:   final_val_rmse 0.22943
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01542
wandb:       train_time 55.99419
wandb:         val_loss 0.06485
wandb:          val_mse 0.06484
wandb:           val_r2 0.01096
wandb:         val_rmse 0.25464
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_180548-kt6e34ps
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_180548-kt6e34ps/logs
Cross-lingual experiment for complexity (fi → ja) completed successfully
Running cross-lingual question_type from fi to ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:07:19,689][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/fi_to_ko/question_type
experiment_name: cross_lingual_question_type_fi_to_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: fi
  eval_language: ko
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:07:19,690][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:07:19,690][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:07:19,690][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:07:21,120][__main__][INFO] - Running cross-lingual experiment: fi -> ko
[2025-04-12 18:07:21,120][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:07:21,121][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:07:23,961][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:07:23,961][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:07:24,045][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:07:24,079][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:07:24,191][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 18:07:24,202][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:07:24,203][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 18:07:24,204][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:07:24,231][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:07:24,270][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:07:24,290][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 18:07:24,291][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:07:24,292][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 18:07:24,293][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:07:24,322][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:07:24,360][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:07:24,377][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 18:07:24,378][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:07:24,378][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 18:07:24,380][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 18:07:24,380][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:07:24,380][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:07:24,380][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:07:24,381][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:07:24,381][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 18:07:24,381][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-12 18:07:24,381][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 18:07:24,381][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:07:24,381][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:07:24,381][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:07:24,381][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:07:24,382][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:07:24,382][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-12 18:07:24,382][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-12 18:07:24,382][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 18:07:24,382][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:07:24,382][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:07:24,382][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:07:24,382][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:07:24,383][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:07:24,383][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:07:24,383][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:07:24,383][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 18:07:24,383][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:07:24,383][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 18:07:24,383][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:07:24,384][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:07:24,384][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
[2025-04-12 18:07:27,176][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:07:27,177][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:07:27,206][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:07:27,243][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:07:27,261][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 18:07:27,267][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:07:27,267][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 18:07:27,269][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:07:27,299][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:07:27,336][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:07:27,352][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 18:07:27,354][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:07:27,354][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 18:07:27,356][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:07:27,382][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:07:27,418][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:07:27,433][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 18:07:27,434][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:07:27,435][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 18:07:27,436][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 18:07:27,436][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:07:27,437][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:07:27,437][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:07:27,437][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:07:27,437][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-12 18:07:27,438][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-12 18:07:27,438][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 18:07:27,438][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:07:27,438][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:07:27,438][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:07:27,438][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:07:27,438][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:07:27,438][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:07:27,439][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:07:27,439][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 18:07:27,439][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:07:27,439][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:07:27,439][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:07:27,439][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:07:27,439][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:07:27,439][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:07:27,440][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:07:27,440][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 18:07:27,440][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:07:27,440][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 18:07:27,440][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:07:27,440][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:07:27,441][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:07:32,390][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:07:32,392][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:07:32,393][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:07:32,393][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:52,  1.52s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:50,  1.46it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:22,  3.16it/s]Epoch 1/10:   8%|▊         | 6/75 [00:02<00:14,  4.71it/s]Epoch 1/10:  11%|█         | 8/75 [00:02<00:11,  6.04it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:09,  7.13it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  7.99it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:07,  8.65it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.13it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:03<00:06,  9.49it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.75it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.94it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.07it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.16it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:04<00:04, 10.23it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  51%|█████     | 38/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  80%|████████  | 60/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:08<00:00,  8.84it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:08<00:00,  9.25it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00,  9.57it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00,  9.49it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.09it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.54it/s]
[2025-04-12 18:07:43,319][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6911
[2025-04-12 18:07:43,553][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6877, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.64it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.85it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:07:51,389][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6242
[2025-04-12 18:07:51,634][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5929, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8771929824561403}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.64it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.85it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:07:59,464][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.3684
[2025-04-12 18:07:59,712][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2901, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.25it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.54it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.78it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.39it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.75it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.33it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.35it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.36it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.36it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 18:08:07,514][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1253
[2025-04-12 18:08:07,769][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1434, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9473684210526315}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.50it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.74it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.91it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 18:08:15,537][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0812
[2025-04-12 18:08:15,799][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2001, Metrics: {'accuracy': 0.9365079365079365, 'f1': 0.9333333333333333}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.51it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.75it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.91it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.13it/s]
[2025-04-12 18:08:23,204][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0705
[2025-04-12 18:08:23,467][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1504, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:14,  5.05it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:08,  8.14it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.16it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.64it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.91it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 18:08:30,822][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0588
[2025-04-12 18:08:31,079][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1054, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.45it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.70it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.88it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.32it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.34it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.36it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.73it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 18:08:38,870][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0494
[2025-04-12 18:08:39,134][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1176, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:16,  4.48it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.73it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.90it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:08:46,513][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0417
[2025-04-12 18:08:46,764][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1547, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:15,  4.71it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.89it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  9.00it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:08:54,130][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0411
[2025-04-12 18:08:54,387][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1635, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
[2025-04-12 18:08:54,388][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▇███
wandb:          best_val_f1 ▁▇███
wandb:        best_val_loss █▇▃▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▅▂▁▁▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▇████████
wandb:               val_f1 ▁▇████████
wandb:             val_loss █▇▃▁▂▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.95238
wandb:          best_val_f1 0.94915
wandb:        best_val_loss 0.10538
wandb:                epoch 10
wandb:  final_test_accuracy 0.7
wandb:        final_test_f1 0.68571
wandb: final_train_accuracy 0.99331
wandb:       final_train_f1 0.99327
wandb:   final_val_accuracy 0.95238
wandb:         final_val_f1 0.94915
wandb:        learning_rate 1e-05
wandb:           train_loss 0.04114
wandb:           train_time 79.85688
wandb:         val_accuracy 0.95238
wandb:               val_f1 0.95082
wandb:             val_loss 0.1635
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_180719-730k68lj
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_180719-730k68lj/logs
Cross-lingual experiment for question_type (fi → ko) completed successfully
Running cross-lingual complexity from fi to ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:09:15,374][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/fi_to_ko/complexity
experiment_name: cross_lingual_complexity_fi_to_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: fi
  eval_language: ko
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:09:15,374][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:09:15,374][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:09:15,374][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:09:16,743][__main__][INFO] - Running cross-lingual experiment: fi -> ko
[2025-04-12 18:09:16,744][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:09:16,744][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:09:19,557][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:09:19,557][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:09:19,604][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:09:19,631][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:09:19,720][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 18:09:19,731][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:09:19,732][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 18:09:19,733][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:09:19,759][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:09:19,794][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:09:19,810][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 18:09:19,811][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:09:19,811][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 18:09:19,812][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:09:19,838][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:09:19,874][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:09:19,889][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 18:09:19,891][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:09:19,891][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 18:09:19,892][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 18:09:19,893][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:09:19,893][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:09:19,893][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:09:19,893][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:09:19,893][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:09:19,894][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-12 18:09:19,894][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 18:09:19,894][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-12 18:09:19,894][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:09:19,894][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:09:19,894][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:09:19,894][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:09:19,894][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:09:19,895][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-12 18:09:19,895][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 18:09:19,895][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-12 18:09:19,895][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:09:19,895][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:09:19,895][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:09:19,895][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:09:19,895][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:09:19,896][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-12 18:09:19,896][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 18:09:19,896][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-12 18:09:19,896][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 18:09:19,896][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:09:19,896][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:09:19,897][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
[2025-04-12 18:09:22,693][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:09:22,693][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:09:22,719][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:09:22,754][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:09:22,770][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 18:09:22,776][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:09:22,777][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 18:09:22,778][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:09:22,805][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:09:22,843][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:09:22,859][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 18:09:22,861][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:09:22,861][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 18:09:22,862][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:09:22,891][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:09:22,928][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:09:22,944][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 18:09:22,945][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:09:22,945][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 18:09:22,947][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 18:09:22,947][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:09:22,948][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:09:22,948][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:09:22,948][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:09:22,948][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:09:22,948][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-12 18:09:22,948][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 18:09:22,948][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-04-12 18:09:22,949][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:09:22,949][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:09:22,949][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:09:22,949][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:09:22,949][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:09:22,949][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-12 18:09:22,949][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 18:09:22,950][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-12 18:09:22,950][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:09:22,950][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:09:22,950][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:09:22,950][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:09:22,950][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:09:22,950][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-12 18:09:22,950][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 18:09:22,951][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-12 18:09:22,951][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 18:09:22,951][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:09:22,951][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:09:22,951][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:09:27,957][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:09:27,960][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:09:27,960][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:09:27,960][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:51,  1.51s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:49,  1.46it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:22,  3.17it/s]Epoch 1/10:   8%|▊         | 6/75 [00:02<00:14,  4.72it/s]Epoch 1/10:  11%|█         | 8/75 [00:02<00:11,  6.05it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:09,  7.14it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.00it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:07,  8.65it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.14it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:03<00:05,  9.50it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.76it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.95it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.08it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.17it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:04<00:04, 10.24it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  51%|█████     | 38/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  80%|████████  | 60/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:08<00:00,  8.89it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:08<00:00,  9.30it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00,  9.60it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00,  9.83it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.56it/s]
[2025-04-12 18:09:38,959][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1149
[2025-04-12 18:09:39,189][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1860, Metrics: {'mse': 0.1862689107656479, 'rmse': 0.43158882140950766, 'r2': -1.8411738872528076}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.83it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.98it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.06it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.57it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.87it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 18:09:47,037][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0593
[2025-04-12 18:09:47,275][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1210, Metrics: {'mse': 0.12102789431810379, 'rmse': 0.3478906355711573, 'r2': -0.8460476398468018}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.57it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.79it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.94it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:09:55,104][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0391
[2025-04-12 18:09:55,352][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0560, Metrics: {'mse': 0.056453581899404526, 'rmse': 0.237599625208889, 'r2': 0.13890916109085083}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.51it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.75it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.92it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:10:03,124][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0270
[2025-04-12 18:10:03,378][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0526, Metrics: {'mse': 0.05263611674308777, 'rmse': 0.22942562355388244, 'r2': 0.19713729619979858}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.36it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.64it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.83it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 18:10:11,154][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0215
[2025-04-12 18:10:11,403][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0543, Metrics: {'mse': 0.054272498935461044, 'rmse': 0.2329645872991452, 'r2': 0.17217743396759033}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.64it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.85it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:10:18,772][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0181
[2025-04-12 18:10:19,037][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0702, Metrics: {'mse': 0.07022659480571747, 'rmse': 0.2650030090503077, 'r2': -0.07117164134979248}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:15,  4.76it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.94it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.03it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:10:26,400][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0154
[2025-04-12 18:10:26,655][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0648, Metrics: {'mse': 0.06484231352806091, 'rmse': 0.2546415392823035, 'r2': 0.010955214500427246}
[2025-04-12 18:10:26,655][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▁▁
wandb:     best_val_mse █▅▁▁
wandb:      best_val_r2 ▁▄██
wandb:    best_val_rmse █▅▁▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▁▁▁▂▂
wandb:          val_mse █▅▁▁▁▂▂
wandb:           val_r2 ▁▄███▇▇
wandb:         val_rmse █▅▁▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05265
wandb:     best_val_mse 0.05264
wandb:      best_val_r2 0.19714
wandb:    best_val_rmse 0.22943
wandb:            epoch 7
wandb:   final_test_mse 0.07545
wandb:    final_test_r2 -1.34279
wandb:  final_test_rmse 0.27468
wandb:  final_train_mse 0.01609
wandb:   final_train_r2 0.20367
wandb: final_train_rmse 0.12686
wandb:    final_val_mse 0.05264
wandb:     final_val_r2 0.19714
wandb:   final_val_rmse 0.22943
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01542
wandb:       train_time 56.46157
wandb:         val_loss 0.06485
wandb:          val_mse 0.06484
wandb:           val_r2 0.01096
wandb:         val_rmse 0.25464
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_180915-cfqjf880
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_180915-cfqjf880/logs
Cross-lingual experiment for complexity (fi → ko) completed successfully
Running cross-lingual question_type from fi to ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:10:46,465][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/fi_to_ru/question_type
experiment_name: cross_lingual_question_type_fi_to_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: fi
  eval_language: ru
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:10:46,465][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:10:46,466][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:10:46,466][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:10:47,859][__main__][INFO] - Running cross-lingual experiment: fi -> ru
[2025-04-12 18:10:47,859][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:10:47,859][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:10:50,733][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:10:50,734][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:10:50,815][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:10:50,845][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:10:50,949][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 18:10:50,960][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:10:50,961][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 18:10:50,963][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:10:50,986][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:10:51,020][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:10:51,034][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 18:10:51,035][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:10:51,035][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 18:10:51,036][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:10:51,060][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:10:51,100][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:10:51,113][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 18:10:51,115][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:10:51,115][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 18:10:51,116][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 18:10:51,117][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:10:51,117][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:10:51,117][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:10:51,117][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:10:51,117][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 18:10:51,118][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-12 18:10:51,118][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 18:10:51,118][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:10:51,118][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:10:51,118][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:10:51,118][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:10:51,118][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:10:51,118][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-12 18:10:51,119][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-12 18:10:51,119][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 18:10:51,119][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:10:51,119][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:10:51,119][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:10:51,119][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:10:51,119][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:10:51,119][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:10:51,120][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:10:51,120][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 18:10:51,120][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:10:51,120][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 18:10:51,120][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:10:51,120][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:10:51,121][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
[2025-04-12 18:10:53,934][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:10:53,934][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:10:53,966][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:10:54,006][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:10:54,022][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 18:10:54,031][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:10:54,032][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 18:10:54,033][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:10:54,059][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:10:54,097][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:10:54,112][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 18:10:54,113][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:10:54,114][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 18:10:54,115][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:10:54,143][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:10:54,180][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:10:54,196][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 18:10:54,197][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:10:54,198][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 18:10:54,199][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 18:10:54,199][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:10:54,199][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:10:54,199][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:10:54,199][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:10:54,200][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 18:10:54,200][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-12 18:10:54,200][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 18:10:54,200][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:10:54,200][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:10:54,200][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:10:54,200][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:10:54,200][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:10:54,201][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:10:54,201][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:10:54,201][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 18:10:54,201][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:10:54,201][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:10:54,201][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:10:54,201][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:10:54,201][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:10:54,202][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:10:54,202][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:10:54,202][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 18:10:54,202][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:10:54,202][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 18:10:54,202][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:10:54,202][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:10:54,203][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:10:59,240][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:10:59,243][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:10:59,243][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:10:59,243][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<02:03,  1.67s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:54,  1.34it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:24,  2.94it/s]Epoch 1/10:   8%|▊         | 6/75 [00:02<00:15,  4.44it/s]Epoch 1/10:  11%|█         | 8/75 [00:02<00:11,  5.77it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:09,  6.90it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:10,  6.33it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:08,  7.40it/s]Epoch 1/10:  20%|██        | 15/75 [00:03<00:07,  8.22it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:03<00:06,  8.84it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:06,  9.29it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.61it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.84it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:04<00:05, 10.00it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:04<00:04, 10.12it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.20it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.26it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:05<00:03, 10.33it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:05<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  60%|██████    | 45/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.72it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.39it/s]
[2025-04-12 18:11:10,218][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6911
[2025-04-12 18:11:10,529][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6877, Metrics: {'accuracy': 0.5238095238095238, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.72it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.91it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.02it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 18:11:18,341][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6242
[2025-04-12 18:11:18,583][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5929, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8771929824561403}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.51it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.75it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.92it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:11:26,416][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.3684
[2025-04-12 18:11:26,673][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2901, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:14,  4.94it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.07it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.11it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.61it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 18:11:34,416][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1253
[2025-04-12 18:11:34,673][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.1434, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9473684210526315}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.46it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.71it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.88it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:11:42,435][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0812
[2025-04-12 18:11:42,692][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2001, Metrics: {'accuracy': 0.9365079365079365, 'f1': 0.9333333333333333}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.58it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.81it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:11:50,065][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0705
[2025-04-12 18:11:50,419][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1504, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:35,  2.10it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:14,  5.12it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:10,  6.93it/s]Epoch 7/10:   9%|▉         | 7/75 [00:01<00:08,  8.07it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:01<00:07,  8.81it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.31it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06,  9.65it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:06,  9.88it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:02<00:05, 10.04it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:02<00:05, 10.15it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.22it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.28it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.31it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.34it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:03<00:04, 10.36it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00,  9.84it/s]
[2025-04-12 18:11:58,040][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0588
[2025-04-12 18:11:58,525][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1054, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.54it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.78it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.94it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.76it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:12:06,311][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0494
[2025-04-12 18:12:06,586][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1176, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:16,  4.54it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.78it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.93it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:12:13,963][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0417
[2025-04-12 18:12:14,228][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1547, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9491525423728814}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:14,  5.00it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:08,  8.10it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  9.13it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.63it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.91it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 18:12:21,585][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0411
[2025-04-12 18:12:21,853][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1635, Metrics: {'accuracy': 0.9523809523809523, 'f1': 0.9508196721311475}
[2025-04-12 18:12:21,854][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▇███
wandb:          best_val_f1 ▁▇███
wandb:        best_val_loss █▇▃▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▇▅▂▁▁▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▇████████
wandb:               val_f1 ▁▇████████
wandb:             val_loss █▇▃▁▂▂▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.95238
wandb:          best_val_f1 0.94915
wandb:        best_val_loss 0.10538
wandb:                epoch 10
wandb:  final_test_accuracy 0.93636
wandb:        final_test_f1 0.93694
wandb: final_train_accuracy 0.99331
wandb:       final_train_f1 0.99327
wandb:   final_val_accuracy 0.95238
wandb:         final_val_f1 0.94915
wandb:        learning_rate 1e-05
wandb:           train_loss 0.04114
wandb:           train_time 80.57359
wandb:         val_accuracy 0.95238
wandb:               val_f1 0.95082
wandb:             val_loss 0.1635
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_181046-xayz2vt3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_181046-xayz2vt3/logs
Cross-lingual experiment for question_type (fi → ru) completed successfully
Running cross-lingual complexity from fi to ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:12:45,151][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/fi_to_ru/complexity
experiment_name: cross_lingual_complexity_fi_to_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: fi
  eval_language: ru
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:12:45,151][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:12:45,152][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:12:45,152][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:12:46,814][__main__][INFO] - Running cross-lingual experiment: fi -> ru
[2025-04-12 18:12:46,814][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:12:46,815][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:12:49,632][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:12:49,632][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:12:49,740][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:12:49,793][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:12:49,942][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 18:12:49,953][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:12:49,954][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 18:12:49,955][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:12:49,988][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:12:50,030][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:12:50,048][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 18:12:50,049][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:12:50,049][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 18:12:50,053][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:12:50,080][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:12:50,119][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:12:50,136][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 18:12:50,138][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:12:50,138][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 18:12:50,139][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 18:12:50,140][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:12:50,140][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:12:50,140][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:12:50,140][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:12:50,140][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:12:50,140][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-12 18:12:50,141][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 18:12:50,141][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-12 18:12:50,141][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:12:50,141][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:12:50,141][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:12:50,141][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:12:50,141][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:12:50,142][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-12 18:12:50,142][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 18:12:50,142][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-12 18:12:50,142][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:12:50,142][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:12:50,142][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:12:50,142][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:12:50,142][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:12:50,143][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-12 18:12:50,143][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 18:12:50,143][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-12 18:12:50,143][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 18:12:50,143][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:12:50,143][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:12:50,144][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
[2025-04-12 18:12:52,971][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:12:52,971][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:12:52,996][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:12:53,033][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:12:53,050][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 18:12:53,059][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:12:53,060][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 18:12:53,061][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:12:53,092][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:12:53,130][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:12:53,144][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 18:12:53,145][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:12:53,146][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 18:12:53,147][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:12:53,177][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:12:53,212][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:12:53,227][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 18:12:53,229][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:12:53,229][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 18:12:53,230][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 18:12:53,231][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:12:53,232][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:12:53,232][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:12:53,232][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:12:53,232][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:12:53,232][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-12 18:12:53,232][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 18:12:53,232][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-04-12 18:12:53,233][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:12:53,233][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:12:53,233][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:12:53,233][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:12:53,233][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:12:53,233][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-12 18:12:53,233][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 18:12:53,233][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-12 18:12:53,234][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:12:53,234][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:12:53,234][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:12:53,234][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:12:53,234][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:12:53,234][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-12 18:12:53,234][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 18:12:53,235][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-12 18:12:53,235][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 18:12:53,235][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:12:53,235][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:12:53,235][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:12:58,995][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:12:58,998][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:12:58,998][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:12:58,999][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<02:02,  1.65s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:54,  1.35it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:32,  2.23it/s]Epoch 1/10:   7%|▋         | 5/75 [00:02<00:17,  4.01it/s]Epoch 1/10:   9%|▉         | 7/75 [00:02<00:12,  5.53it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  6.76it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.72it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:08,  7.63it/s]Epoch 1/10:  20%|██        | 15/75 [00:03<00:07,  8.35it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:03<00:06,  8.90it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:06,  9.32it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.63it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.85it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:04<00:04, 10.01it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:04<00:04, 10.12it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.20it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.26it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:05<00:03, 10.33it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:05<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:06<00:02, 10.37it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:07<00:01, 10.33it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:07<00:01, 10.35it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.36it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:08<00:00, 10.38it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.72it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.35it/s]
[2025-04-12 18:13:10,133][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1149
[2025-04-12 18:13:10,363][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1860, Metrics: {'mse': 0.1862689107656479, 'rmse': 0.43158882140950766, 'r2': -1.8411738872528076}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:16,  4.59it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.81it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.95it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.74it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:13:18,195][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0593
[2025-04-12 18:13:18,441][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1210, Metrics: {'mse': 0.12102789431810379, 'rmse': 0.3478906355711573, 'r2': -0.8460476398468018}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.24it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.53it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.77it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.39it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.75it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 18:13:26,462][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0391
[2025-04-12 18:13:26,726][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0560, Metrics: {'mse': 0.056453581899404526, 'rmse': 0.237599625208889, 'r2': 0.13890916109085083}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.22it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.52it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.76it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.39it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.74it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 18:13:34,519][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0270
[2025-04-12 18:13:34,777][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0526, Metrics: {'mse': 0.05263611674308777, 'rmse': 0.22942562355388244, 'r2': 0.19713729619979858}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.67it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.87it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.99it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:13:42,582][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0215
[2025-04-12 18:13:42,848][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0543, Metrics: {'mse': 0.054272498935461044, 'rmse': 0.2329645872991452, 'r2': 0.17217743396759033}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.42it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.68it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 18:13:50,234][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0181
[2025-04-12 18:13:50,519][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0702, Metrics: {'mse': 0.07022659480571747, 'rmse': 0.2650030090503077, 'r2': -0.07117164134979248}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.42it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.69it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.75it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 18:13:57,904][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0154
[2025-04-12 18:13:58,166][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0648, Metrics: {'mse': 0.06484231352806091, 'rmse': 0.2546415392823035, 'r2': 0.010955214500427246}
[2025-04-12 18:13:58,166][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▁▁
wandb:     best_val_mse █▅▁▁
wandb:      best_val_r2 ▁▄██
wandb:    best_val_rmse █▅▁▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▁▁▁▂▂
wandb:          val_mse █▅▁▁▁▂▂
wandb:           val_r2 ▁▄███▇▇
wandb:         val_rmse █▅▁▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05265
wandb:     best_val_mse 0.05264
wandb:      best_val_r2 0.19714
wandb:    best_val_rmse 0.22943
wandb:            epoch 7
wandb:   final_test_mse 0.09236
wandb:    final_test_r2 -1.33613
wandb:  final_test_rmse 0.30391
wandb:  final_train_mse 0.01609
wandb:   final_train_r2 0.20367
wandb: final_train_rmse 0.12686
wandb:    final_val_mse 0.05264
wandb:     final_val_r2 0.19714
wandb:   final_val_rmse 0.22943
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01542
wandb:       train_time 57.01951
wandb:         val_loss 0.06485
wandb:          val_mse 0.06484
wandb:           val_r2 0.01096
wandb:         val_rmse 0.25464
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_181245-szbud38c
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_181245-szbud38c/logs
Cross-lingual experiment for complexity (fi → ru) completed successfully
Running cross-lingual question_type from id to ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:14:20,176][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/id_to_ar/question_type
experiment_name: cross_lingual_question_type_id_to_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: id
  eval_language: ar
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:14:20,176][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:14:20,176][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:14:20,176][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:14:21,609][__main__][INFO] - Running cross-lingual experiment: id -> ar
[2025-04-12 18:14:21,609][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:14:21,609][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:14:24,486][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:14:24,487][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:14:24,566][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:14:24,613][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:14:24,731][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:14:24,739][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:14:24,740][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:14:24,741][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:14:24,766][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:14:24,804][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:14:24,823][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:14:24,825][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:14:24,825][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:14:24,827][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:14:24,861][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:14:24,910][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:14:24,928][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:14:24,929][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:14:24,930][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:14:24,931][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:14:24,931][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:14:24,931][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:14:24,931][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:14:24,932][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:14:24,932][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-12 18:14:24,932][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-12 18:14:24,932][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:14:24,932][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:14:24,932][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:14:24,932][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:14:24,932][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:14:24,933][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:14:24,933][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:14:24,933][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:14:24,933][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:14:24,933][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:14:24,933][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:14:24,933][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:14:24,933][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:14:24,934][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:14:24,934][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:14:24,934][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:14:24,934][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:14:24,934][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:14:24,934][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:14:24,934][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:14:24,935][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:14:24,935][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
[2025-04-12 18:14:27,750][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:14:27,751][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:14:27,779][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:14:27,819][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:14:27,838][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 18:14:27,846][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:14:27,847][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 18:14:27,848][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:14:27,876][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:14:27,917][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:14:27,935][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 18:14:27,936][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:14:27,937][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 18:14:27,938][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:14:27,963][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:14:28,007][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:14:28,025][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 18:14:28,027][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:14:28,027][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 18:14:28,028][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 18:14:28,029][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:14:28,029][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:14:28,029][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:14:28,029][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:14:28,029][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-12 18:14:28,029][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-12 18:14:28,030][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 18:14:28,030][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:14:28,030][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:14:28,030][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:14:28,030][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:14:28,030][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:14:28,030][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-12 18:14:28,030][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-12 18:14:28,031][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 18:14:28,031][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:14:28,031][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:14:28,031][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:14:28,031][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:14:28,031][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:14:28,031][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-12 18:14:28,031][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-12 18:14:28,031][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 18:14:28,032][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:14:28,032][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 18:14:28,032][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:14:28,032][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:14:28,032][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:14:33,224][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:14:33,227][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:14:33,228][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:14:33,228][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:12,  1.24s/it]Epoch 1/10:   3%|▎         | 2/60 [00:01<00:33,  1.75it/s]Epoch 1/10:   7%|▋         | 4/60 [00:01<00:15,  3.64it/s]Epoch 1/10:  10%|█         | 6/60 [00:01<00:10,  5.26it/s]Epoch 1/10:  13%|█▎        | 8/60 [00:01<00:07,  6.57it/s]Epoch 1/10:  17%|█▋        | 10/60 [00:02<00:06,  7.59it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:02<00:07,  6.81it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:02<00:06,  7.81it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:02<00:05,  8.55it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:02<00:04,  9.08it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:03<00:04,  9.47it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:03<00:04,  9.74it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:03<00:03,  9.94it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:03<00:03, 10.07it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:03<00:03, 10.17it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:04<00:03, 10.24it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:04<00:02, 10.29it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:04<00:02, 10.32it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:04<00:02, 10.34it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:04<00:02, 10.36it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:05<00:02, 10.37it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:05<00:01, 10.40it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:06<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 60/60 [00:07<00:00,  8.51it/s]
[2025-04-12 18:14:42,751][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6869
[2025-04-12 18:14:42,995][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6911, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:12,  4.63it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:07,  7.85it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  8.98it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.45it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.79it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04,  9.99it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-12 18:14:49,369][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6436
[2025-04-12 18:14:49,630][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6485, Metrics: {'accuracy': 0.7083333333333334, 'f1': 0.5882352941176471}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:13,  4.31it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  7.60it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  8.82it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.42it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:01<00:05,  9.77it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04,  9.98it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:14:56,197][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5385
[2025-04-12 18:14:56,495][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4353, Metrics: {'accuracy': 0.8055555555555556, 'f1': 0.7741935483870968}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:13,  4.44it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  7.69it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  8.88it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.47it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:05,  9.80it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:15:02,821][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.2858
[2025-04-12 18:15:03,134][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.4068, Metrics: {'accuracy': 0.8194444444444444, 'f1': 0.7796610169491526}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:13,  4.28it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  7.58it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  8.81it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.42it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:01<00:05,  9.76it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04,  9.98it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-12 18:15:09,502][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1565
[2025-04-12 18:15:09,785][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.3975, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:13,  4.24it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:07,  7.54it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:06,  8.78it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.40it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:01<00:05,  9.76it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04,  9.97it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-12 18:15:16,154][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1012
[2025-04-12 18:15:16,480][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.4113, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:12,  4.66it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:07,  7.87it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:06,  8.99it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.54it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.85it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04, 10.03it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.15it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.23it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.16it/s]
[2025-04-12 18:15:22,390][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0787
[2025-04-12 18:15:22,695][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.3571, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:13,  4.38it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:07,  7.65it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:06,  8.85it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.45it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:05,  9.78it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04,  9.99it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.08it/s]
[2025-04-12 18:15:29,086][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0684
[2025-04-12 18:15:29,380][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.4242, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:13,  4.43it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:07,  7.69it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:06,  8.88it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.47it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:05,  9.80it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.13it/s]
[2025-04-12 18:15:35,305][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0517
[2025-04-12 18:15:35,605][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.5636, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:13,  4.33it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:07,  7.61it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:06,  8.82it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:05,  9.43it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:01<00:05,  9.77it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:04,  9.98it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-12 18:15:41,547][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0556
[2025-04-12 18:15:41,847][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.4819, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.8571428571428571}
[2025-04-12 18:15:41,847][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▅▇▇▇█
wandb:          best_val_f1 ▁▆▇▇██
wandb:        best_val_loss █▇▃▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▆▄▂▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▅▇▇▇▇████
wandb:               val_f1 ▁▆▇▇██████
wandb:             val_loss █▇▃▂▂▂▁▂▅▄
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.86111
wandb:          best_val_f1 0.84375
wandb:        best_val_loss 0.35707
wandb:                epoch 10
wandb:  final_test_accuracy 0.81818
wandb:        final_test_f1 0.74074
wandb: final_train_accuracy 0.98742
wandb:       final_train_f1 0.9869
wandb:   final_val_accuracy 0.86111
wandb:         final_val_f1 0.84375
wandb:        learning_rate 1e-05
wandb:           train_loss 0.05558
wandb:           train_time 66.15185
wandb:         val_accuracy 0.86111
wandb:               val_f1 0.85714
wandb:             val_loss 0.48189
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_181420-j1pn98lq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_181420-j1pn98lq/logs
Cross-lingual experiment for question_type (id → ar) completed successfully
Running cross-lingual complexity from id to ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:16:02,786][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/id_to_ar/complexity
experiment_name: cross_lingual_complexity_id_to_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: id
  eval_language: ar
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:16:02,787][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:16:02,787][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:16:02,787][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:16:04,059][__main__][INFO] - Running cross-lingual experiment: id -> ar
[2025-04-12 18:16:04,060][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:16:04,060][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:16:06,937][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:16:06,937][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:16:06,998][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:16:07,033][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:16:07,141][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:16:07,150][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:16:07,150][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:16:07,152][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:16:07,176][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:16:07,211][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:16:07,226][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:16:07,227][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:16:07,227][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:16:07,228][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:16:07,251][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:16:07,285][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:16:07,300][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:16:07,301][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:16:07,302][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:16:07,303][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:16:07,303][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:16:07,304][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:16:07,304][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:16:07,304][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:16:07,304][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:16:07,304][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-12 18:16:07,304][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:16:07,304][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-12 18:16:07,305][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:16:07,305][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:16:07,305][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:16:07,305][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:16:07,305][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:16:07,305][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-12 18:16:07,305][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:16:07,306][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-12 18:16:07,306][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:16:07,306][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:16:07,306][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:16:07,306][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:16:07,306][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:16:07,306][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-12 18:16:07,306][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:16:07,307][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-12 18:16:07,307][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:16:07,307][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:16:07,307][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:16:07,307][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
[2025-04-12 18:16:10,101][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:16:10,101][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:16:10,126][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:16:10,161][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:16:10,179][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 18:16:10,187][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:16:10,188][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 18:16:10,189][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:16:10,212][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:16:10,244][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:16:10,261][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 18:16:10,262][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:16:10,262][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 18:16:10,263][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:16:10,287][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:16:10,321][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:16:10,336][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 18:16:10,337][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:16:10,337][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 18:16:10,338][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 18:16:10,338][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:16:10,339][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:16:10,339][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:16:10,339][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:16:10,339][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:16:10,339][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-12 18:16:10,339][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 18:16:10,339][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-12 18:16:10,340][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:16:10,340][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:16:10,340][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:16:10,340][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:16:10,340][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:16:10,340][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-12 18:16:10,340][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 18:16:10,340][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-12 18:16:10,341][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:16:10,341][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:16:10,341][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:16:10,341][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:16:10,341][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:16:10,341][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-12 18:16:10,341][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 18:16:10,341][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-12 18:16:10,342][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 18:16:10,342][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:16:10,342][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:16:10,342][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:16:15,408][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:16:15,411][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:16:15,411][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:16:15,411][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:11,  1.21s/it]Epoch 1/10:   5%|▌         | 3/60 [00:01<00:21,  2.59it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:13,  4.22it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:09,  5.64it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:07,  6.82it/s]Epoch 1/10:  17%|█▋        | 10/60 [00:02<00:07,  6.29it/s]Epoch 1/10:  20%|██        | 12/60 [00:02<00:06,  7.39it/s]Epoch 1/10:  23%|██▎       | 14/60 [00:02<00:05,  8.22it/s]Epoch 1/10:  27%|██▋       | 16/60 [00:02<00:04,  8.84it/s]Epoch 1/10:  30%|███       | 18/60 [00:02<00:04,  9.29it/s]Epoch 1/10:  33%|███▎      | 20/60 [00:03<00:04,  9.62it/s]Epoch 1/10:  37%|███▋      | 22/60 [00:03<00:03,  9.85it/s]Epoch 1/10:  40%|████      | 24/60 [00:03<00:03, 10.01it/s]Epoch 1/10:  43%|████▎     | 26/60 [00:03<00:03, 10.13it/s]Epoch 1/10:  47%|████▋     | 28/60 [00:03<00:03, 10.21it/s]Epoch 1/10:  50%|█████     | 30/60 [00:04<00:02, 10.27it/s]Epoch 1/10:  53%|█████▎    | 32/60 [00:04<00:02, 10.31it/s]Epoch 1/10:  57%|█████▋    | 34/60 [00:04<00:02, 10.34it/s]Epoch 1/10:  60%|██████    | 36/60 [00:04<00:02, 10.36it/s]Epoch 1/10:  63%|██████▎   | 38/60 [00:04<00:02, 10.37it/s]Epoch 1/10:  67%|██████▋   | 40/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  70%|███████   | 42/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  73%|███████▎  | 44/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  77%|███████▋  | 46/60 [00:05<00:01, 10.40it/s]Epoch 1/10:  80%|████████  | 48/60 [00:05<00:01, 10.40it/s]Epoch 1/10:  83%|████████▎ | 50/60 [00:06<00:00, 10.41it/s]Epoch 1/10:  87%|████████▋ | 52/60 [00:06<00:00, 10.41it/s]Epoch 1/10:  90%|█████████ | 54/60 [00:06<00:00, 10.41it/s]Epoch 1/10:  93%|█████████▎| 56/60 [00:06<00:00, 10.41it/s]Epoch 1/10:  97%|█████████▋| 58/60 [00:06<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00, 10.79it/s]Epoch 1/10: 100%|██████████| 60/60 [00:07<00:00,  8.56it/s]
[2025-04-12 18:16:24,428][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1441
[2025-04-12 18:16:24,684][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0981, Metrics: {'mse': 0.0918080136179924, 'rmse': 0.3029983723025462, 'r2': -1.195882797241211}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:11,  5.06it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:06,  8.15it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  9.15it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.64it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.92it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.08it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.19it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.26it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.19it/s]
[2025-04-12 18:16:31,026][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0666
[2025-04-12 18:16:31,291][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1162, Metrics: {'mse': 0.11628685891628265, 'rmse': 0.34100859067812744, 'r2': -1.7813730239868164}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:11,  4.97it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  8.09it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  9.13it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.63it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:05,  9.91it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04, 10.08it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.19it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.25it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.36it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.18it/s]
[2025-04-12 18:16:37,185][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0550
[2025-04-12 18:16:37,448][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0323, Metrics: {'mse': 0.031377360224723816, 'rmse': 0.17713655812599446, 'r2': 0.24950993061065674}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:13,  4.41it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  7.68it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  8.87it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.46it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:05,  9.80it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:16:44,037][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0389
[2025-04-12 18:16:44,316][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0475, Metrics: {'mse': 0.049142591655254364, 'rmse': 0.22168128395345954, 'r2': -0.17540264129638672}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:13,  4.41it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  7.68it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  8.87it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.46it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:05,  9.79it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:16:50,250][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0319
[2025-04-12 18:16:50,569][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0584, Metrics: {'mse': 0.06045475974678993, 'rmse': 0.24587549643425213, 'r2': -0.4459693431854248}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:12,  4.81it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:07,  7.98it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:06,  9.07it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.59it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:05,  9.88it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04, 10.06it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.17it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.24it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.29it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.17it/s]
[2025-04-12 18:16:56,473][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0294
[2025-04-12 18:16:56,771][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0224, Metrics: {'mse': 0.02280096895992756, 'rmse': 0.1509998972182682, 'r2': 0.45464175939559937}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:12,  4.63it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:07,  7.84it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:06,  8.98it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.53it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.84it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04, 10.03it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.15it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.23it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.15it/s]
[2025-04-12 18:17:03,071][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0238
[2025-04-12 18:17:03,359][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0241, Metrics: {'mse': 0.02414759248495102, 'rmse': 0.15539495643344098, 'r2': 0.42243289947509766}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:25,  2.31it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:10,  5.44it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:07,  7.21it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:01<00:06,  8.29it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:01<00:05,  8.98it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:05,  9.43it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04,  9.74it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04,  9.94it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.08it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:02<00:04, 10.18it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.25it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.29it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.33it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.35it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:03<00:02, 10.37it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:05<00:01, 10.40it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:06<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 60/60 [00:06<00:00,  9.78it/s]
[2025-04-12 18:17:09,497][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0225
[2025-04-12 18:17:09,776][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0285, Metrics: {'mse': 0.027751648798584938, 'rmse': 0.1665882612868774, 'r2': 0.33623039722442627}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:13,  4.45it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:07,  7.71it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:06,  8.89it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.48it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:05,  9.80it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:17:15,708][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0205
[2025-04-12 18:17:16,004][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0220, Metrics: {'mse': 0.020459331572055817, 'rmse': 0.14303611981613532, 'r2': 0.5106495022773743}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:13,  4.35it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:07,  7.63it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:06,  8.84it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:05,  9.44it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:05,  9.78it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:04,  9.99it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.41it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:17:22,365][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0179
[2025-04-12 18:17:22,668][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0399, Metrics: {'mse': 0.04086657986044884, 'rmse': 0.20215484129856706, 'r2': 0.022544801235198975}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁▁
wandb:     best_val_mse █▂▁▁
wandb:      best_val_r2 ▁▇██
wandb:    best_val_rmse █▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▂▃▄▁▁▁▁▂
wandb:          val_mse ▆█▂▃▄▁▁▂▁▂
wandb:           val_r2 ▃▁▇▆▅██▇█▇
wandb:         val_rmse ▇█▂▄▅▁▁▂▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02202
wandb:     best_val_mse 0.02046
wandb:      best_val_r2 0.51065
wandb:    best_val_rmse 0.14304
wandb:            epoch 10
wandb:   final_test_mse 0.09734
wandb:    final_test_r2 -0.67815
wandb:  final_test_rmse 0.312
wandb:  final_train_mse 0.02796
wandb:   final_train_r2 0.22958
wandb: final_train_rmse 0.1672
wandb:    final_val_mse 0.02046
wandb:     final_val_r2 0.51065
wandb:   final_val_rmse 0.14304
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01793
wandb:       train_time 65.25485
wandb:         val_loss 0.03989
wandb:          val_mse 0.04087
wandb:           val_r2 0.02254
wandb:         val_rmse 0.20215
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_181602-lqhxcqd3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_181602-lqhxcqd3/logs
Cross-lingual experiment for complexity (id → ar) completed successfully
Running cross-lingual question_type from id to en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:17:44,098][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/id_to_en/question_type
experiment_name: cross_lingual_question_type_id_to_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: id
  eval_language: en
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:17:44,098][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:17:44,098][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:17:44,098][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:17:45,590][__main__][INFO] - Running cross-lingual experiment: id -> en
[2025-04-12 18:17:45,591][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:17:45,591][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:17:48,419][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:17:48,420][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:17:48,482][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:17:48,512][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:17:48,607][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:17:48,616][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:17:48,616][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:17:48,618][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:17:48,639][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:17:48,673][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:17:48,688][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:17:48,689][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:17:48,689][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:17:48,690][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:17:48,716][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:17:48,750][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:17:48,765][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:17:48,766][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:17:48,767][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:17:48,768][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:17:48,768][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:17:48,769][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:17:48,769][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:17:48,769][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:17:48,769][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-12 18:17:48,769][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-12 18:17:48,769][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:17:48,769][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:17:48,770][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:17:48,770][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:17:48,770][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:17:48,770][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:17:48,770][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:17:48,770][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:17:48,770][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:17:48,770][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:17:48,771][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:17:48,771][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:17:48,771][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:17:48,771][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:17:48,771][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:17:48,771][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:17:48,771][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:17:48,771][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:17:48,771][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:17:48,772][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:17:48,772][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:17:48,772][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
[2025-04-12 18:17:51,563][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:17:51,563][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:17:51,602][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:17:51,643][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:17:51,661][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 18:17:51,670][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:17:51,671][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 18:17:51,672][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:17:51,699][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:17:51,735][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:17:51,751][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 18:17:51,752][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:17:51,752][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 18:17:51,753][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:17:51,778][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:17:51,811][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:17:51,825][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 18:17:51,827][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:17:51,827][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 18:17:51,828][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 18:17:51,829][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:17:51,829][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:17:51,829][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:17:51,829][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:17:51,829][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-12 18:17:51,829][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 18:17:51,829][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 18:17:51,830][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:17:51,830][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:17:51,830][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:17:51,830][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:17:51,830][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:17:51,830][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:17:51,830][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:17:51,830][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 18:17:51,831][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:17:51,831][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:17:51,831][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:17:51,831][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:17:51,831][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:17:51,831][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:17:51,831][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:17:51,831][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 18:17:51,831][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:17:51,832][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 18:17:51,832][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:17:51,832][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:17:51,832][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:17:56,835][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:17:56,837][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:17:56,838][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:17:56,838][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:07,  1.15s/it]Epoch 1/10:   5%|▌         | 3/60 [00:01<00:21,  2.71it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:12,  4.37it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:09,  5.79it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:07,  6.95it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:02<00:07,  6.83it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:02<00:06,  7.70it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:02<00:05,  8.40it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:02<00:04,  8.94it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:02<00:04,  9.35it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:03<00:04,  9.64it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:03<00:03,  9.86it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:03<00:03, 10.02it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:03<00:03, 10.13it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:03<00:03, 10.21it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:04<00:02, 10.26it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:04<00:02, 10.30it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:04<00:02, 10.33it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:04<00:02, 10.35it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.36it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:05<00:01, 10.37it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:06<00:00, 10.38it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:06<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00,  8.63it/s]
[2025-04-12 18:18:05,813][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6869
[2025-04-12 18:18:06,086][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6911, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:12,  4.85it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:07,  8.00it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  9.07it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.56it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.85it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.04it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.15it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.38it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.38it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.38it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.15it/s]
[2025-04-12 18:18:12,459][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6436
[2025-04-12 18:18:12,739][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6485, Metrics: {'accuracy': 0.7083333333333334, 'f1': 0.5882352941176471}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:13,  4.30it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  7.58it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  8.81it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.41it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:01<00:05,  9.76it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04,  9.97it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.10it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.19it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.25it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.29it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.35it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.09it/s]
[2025-04-12 18:18:19,323][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5385
[2025-04-12 18:18:19,620][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4353, Metrics: {'accuracy': 0.8055555555555556, 'f1': 0.7741935483870968}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:15,  3.87it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  7.21it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  8.56it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.25it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:01<00:05,  9.64it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04,  9.89it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.05it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.16it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.23it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.27it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.31it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.33it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.36it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.05it/s]
[2025-04-12 18:18:25,993][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.2858
[2025-04-12 18:18:26,291][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.4068, Metrics: {'accuracy': 0.8194444444444444, 'f1': 0.7796610169491526}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:14,  4.19it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  7.50it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  8.73it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.36it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:01<00:05,  9.72it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04,  9.94it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.09it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.18it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.25it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.29it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.38it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.38it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.38it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.08it/s]
[2025-04-12 18:18:32,664][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1565
[2025-04-12 18:18:32,957][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.3975, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:22,  2.58it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:09,  5.81it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:07,  7.51it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:06,  8.51it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:01<00:05,  9.13it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:05,  9.53it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04,  9.79it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04,  9.98it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.10it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:02<00:04, 10.19it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.25it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.29it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.32it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.33it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:03<00:02, 10.35it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.36it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.37it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.37it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.38it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.38it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.38it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.38it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:05<00:01, 10.39it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 60/60 [00:06<00:00,  9.85it/s]
[2025-04-12 18:18:39,474][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1012
[2025-04-12 18:18:39,785][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.4113, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:12,  4.67it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:07,  7.87it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:06,  8.99it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.53it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.83it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04, 10.02it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.14it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.38it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:18:45,715][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0787
[2025-04-12 18:18:46,028][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.3571, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:13,  4.32it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:07,  7.60it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:06,  8.82it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.42it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:01<00:05,  9.76it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04,  9.97it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.10it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.19it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.11it/s]
[2025-04-12 18:18:52,389][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0684
[2025-04-12 18:18:52,703][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.4242, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:13,  4.29it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:07,  7.58it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:06,  8.80it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.41it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:01<00:05,  9.76it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04,  9.96it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.10it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.19it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.25it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.29it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.36it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.09it/s]
[2025-04-12 18:18:58,651][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0517
[2025-04-12 18:18:58,953][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.5636, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:13,  4.44it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:07,  7.69it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:06,  8.87it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:05,  9.46it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:05,  9.79it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:04,  9.99it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.35it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.36it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.38it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:19:04,884][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0556
[2025-04-12 18:19:05,165][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.4819, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.8571428571428571}
[2025-04-12 18:19:05,166][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▅▇▇▇█
wandb:          best_val_f1 ▁▆▇▇██
wandb:        best_val_loss █▇▃▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▆▄▂▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▅▇▇▇▇████
wandb:               val_f1 ▁▆▇▇██████
wandb:             val_loss █▇▃▂▂▂▁▂▅▄
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.86111
wandb:          best_val_f1 0.84375
wandb:        best_val_loss 0.35707
wandb:                epoch 10
wandb:  final_test_accuracy 0.70909
wandb:        final_test_f1 0.64444
wandb: final_train_accuracy 0.98742
wandb:       final_train_f1 0.9869
wandb:   final_val_accuracy 0.86111
wandb:         final_val_f1 0.84375
wandb:        learning_rate 1e-05
wandb:           train_loss 0.05558
wandb:           train_time 66.31157
wandb:         val_accuracy 0.86111
wandb:               val_f1 0.85714
wandb:             val_loss 0.48189
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_181744-a51u39sk
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_181744-a51u39sk/logs
Cross-lingual experiment for question_type (id → en) completed successfully
Running cross-lingual complexity from id to en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:19:24,763][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/id_to_en/complexity
experiment_name: cross_lingual_complexity_id_to_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: id
  eval_language: en
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:19:24,763][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:19:24,763][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:19:24,763][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:19:26,068][__main__][INFO] - Running cross-lingual experiment: id -> en
[2025-04-12 18:19:26,069][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:19:26,069][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:19:28,940][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:19:28,941][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:19:29,018][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:19:29,053][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:19:29,163][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:19:29,172][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:19:29,172][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:19:29,173][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:19:29,197][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:19:29,234][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:19:29,249][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:19:29,251][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:19:29,251][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:19:29,252][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:19:29,277][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:19:29,312][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:19:29,327][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:19:29,329][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:19:29,329][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:19:29,330][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:19:29,331][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:19:29,331][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:19:29,332][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:19:29,332][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:19:29,332][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:19:29,332][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-12 18:19:29,332][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:19:29,332][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-12 18:19:29,333][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:19:29,333][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:19:29,333][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:19:29,333][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:19:29,333][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:19:29,333][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-12 18:19:29,333][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:19:29,333][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-12 18:19:29,334][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:19:29,334][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:19:29,334][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:19:29,334][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:19:29,334][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:19:29,334][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-12 18:19:29,334][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:19:29,334][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-12 18:19:29,335][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:19:29,335][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:19:29,335][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:19:29,335][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
[2025-04-12 18:19:32,148][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:19:32,148][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:19:32,173][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:19:32,209][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:19:32,225][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 18:19:32,235][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:19:32,236][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 18:19:32,237][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:19:32,258][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:19:32,286][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:19:32,299][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 18:19:32,300][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:19:32,300][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 18:19:32,301][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:19:32,324][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:19:32,354][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:19:32,366][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 18:19:32,367][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:19:32,367][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 18:19:32,368][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 18:19:32,369][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:19:32,369][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:19:32,369][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:19:32,369][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:19:32,369][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:19:32,369][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-12 18:19:32,369][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 18:19:32,370][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-12 18:19:32,370][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:19:32,370][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:19:32,370][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:19:32,370][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:19:32,370][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:19:32,370][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-12 18:19:32,371][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 18:19:32,371][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-12 18:19:32,371][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:19:32,371][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:19:32,371][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:19:32,371][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:19:32,371][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:19:32,371][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-12 18:19:32,372][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 18:19:32,372][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-12 18:19:32,372][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 18:19:32,372][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:19:32,372][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:19:32,372][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:19:37,362][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:19:37,365][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:19:37,365][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:19:37,365][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:09,  1.18s/it]Epoch 1/10:   3%|▎         | 2/60 [00:01<00:31,  1.82it/s]Epoch 1/10:   7%|▋         | 4/60 [00:01<00:14,  3.77it/s]Epoch 1/10:  10%|█         | 6/60 [00:01<00:10,  5.40it/s]Epoch 1/10:  13%|█▎        | 8/60 [00:01<00:07,  6.70it/s]Epoch 1/10:  17%|█▋        | 10/60 [00:02<00:06,  7.70it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:02<00:07,  6.87it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:02<00:05,  7.86it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:02<00:05,  8.58it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:02<00:04,  9.11it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:03<00:04,  9.49it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:03<00:03,  9.76it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:03<00:03,  9.95it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:03<00:03, 10.09it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:03<00:03, 10.18it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:03<00:03, 10.25it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:04<00:02, 10.29it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:04<00:02, 10.33it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:04<00:02, 10.35it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:04<00:02, 10.36it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.37it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:05<00:01, 10.37it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:06<00:00, 10.34it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:06<00:00, 10.36it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:06<00:00, 10.37it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:06<00:00, 10.38it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:06<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 60/60 [00:07<00:00,  8.57it/s]
[2025-04-12 18:19:46,466][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1441
[2025-04-12 18:19:46,736][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0981, Metrics: {'mse': 0.0918080136179924, 'rmse': 0.3029983723025462, 'r2': -1.195882797241211}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:12,  4.66it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:07,  7.87it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  8.99it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.50it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.82it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.01it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.14it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.15it/s]
[2025-04-12 18:19:53,108][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0666
[2025-04-12 18:19:53,383][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1162, Metrics: {'mse': 0.11628685891628265, 'rmse': 0.34100859067812744, 'r2': -1.7813730239868164}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:11,  4.95it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  8.07it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  9.12it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.63it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:05,  9.90it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04, 10.07it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.18it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.25it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.18it/s]
[2025-04-12 18:19:59,278][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0550
[2025-04-12 18:19:59,560][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0323, Metrics: {'mse': 0.031377360224723816, 'rmse': 0.17713655812599446, 'r2': 0.24950993061065674}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:14,  3.94it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  7.29it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  8.61it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.27it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:01<00:05,  9.66it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04,  9.91it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.06it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.17it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.24it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.29it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.09it/s]
[2025-04-12 18:20:06,169][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0389
[2025-04-12 18:20:06,475][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0475, Metrics: {'mse': 0.049142591655254364, 'rmse': 0.22168128395345954, 'r2': -0.17540264129638672}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:12,  4.59it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  7.82it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  8.96it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.52it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:05,  9.83it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04, 10.02it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.14it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.33it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.35it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.36it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.37it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.38it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-12 18:20:12,395][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0319
[2025-04-12 18:20:12,686][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0584, Metrics: {'mse': 0.06045475974678993, 'rmse': 0.24587549643425213, 'r2': -0.4459693431854248}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:12,  4.54it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:07,  7.78it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:06,  8.94it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.50it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:05,  9.82it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04, 10.01it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.14it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-12 18:20:18,607][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0294
[2025-04-12 18:20:18,911][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0224, Metrics: {'mse': 0.02280096895992756, 'rmse': 0.1509998972182682, 'r2': 0.45464175939559937}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:13,  4.50it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:07,  7.75it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:06,  8.92it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.49it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.82it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04, 10.01it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.14it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.41it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.15it/s]
[2025-04-12 18:20:25,225][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0238
[2025-04-12 18:20:25,505][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0241, Metrics: {'mse': 0.02414759248495102, 'rmse': 0.15539495643344098, 'r2': 0.42243289947509766}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:13,  4.48it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:07,  7.72it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:06,  8.90it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.47it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:05,  9.80it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04, 10.01it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.14it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-12 18:20:31,427][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0225
[2025-04-12 18:20:31,726][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0285, Metrics: {'mse': 0.027751648798584938, 'rmse': 0.1665882612868774, 'r2': 0.33623039722442627}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:29,  2.02it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:11,  5.00it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:08,  6.82it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:01<00:06,  7.98it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:01<00:05,  8.75it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:05,  9.26it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04,  9.61it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04,  9.85it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:02<00:04, 10.02it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:02<00:04, 10.14it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.22it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.27it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.31it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.34it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:03<00:02, 10.36it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.39it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:05<00:01, 10.40it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:06<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 60/60 [00:06<00:00,  9.68it/s]
[2025-04-12 18:20:37,929][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0205
[2025-04-12 18:20:38,234][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0220, Metrics: {'mse': 0.020459331572055817, 'rmse': 0.14303611981613532, 'r2': 0.5106495022773743}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:13,  4.37it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:07,  7.64it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:06,  8.83it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:05,  9.43it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:05,  9.77it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:04,  9.98it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-12 18:20:44,596][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0179
[2025-04-12 18:20:44,900][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0399, Metrics: {'mse': 0.04086657986044884, 'rmse': 0.20215484129856706, 'r2': 0.022544801235198975}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁▁
wandb:     best_val_mse █▂▁▁
wandb:      best_val_r2 ▁▇██
wandb:    best_val_rmse █▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▂▃▄▁▁▁▁▂
wandb:          val_mse ▆█▂▃▄▁▁▂▁▂
wandb:           val_r2 ▃▁▇▆▅██▇█▇
wandb:         val_rmse ▇█▂▄▅▁▁▂▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02202
wandb:     best_val_mse 0.02046
wandb:      best_val_r2 0.51065
wandb:    best_val_rmse 0.14304
wandb:            epoch 10
wandb:   final_test_mse 0.04463
wandb:    final_test_r2 -0.15818
wandb:  final_test_rmse 0.21127
wandb:  final_train_mse 0.02796
wandb:   final_train_r2 0.22958
wandb: final_train_rmse 0.1672
wandb:    final_val_mse 0.02046
wandb:     final_val_r2 0.51065
wandb:   final_val_rmse 0.14304
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01793
wandb:       train_time 65.43616
wandb:         val_loss 0.03989
wandb:          val_mse 0.04087
wandb:           val_r2 0.02254
wandb:         val_rmse 0.20215
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_181924-ilb19856
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_181924-ilb19856/logs
Cross-lingual experiment for complexity (id → en) completed successfully
Running cross-lingual question_type from id to fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:21:04,654][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/id_to_fi/question_type
experiment_name: cross_lingual_question_type_id_to_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: id
  eval_language: fi
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:21:04,654][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:21:04,654][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:21:04,654][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:21:06,106][__main__][INFO] - Running cross-lingual experiment: id -> fi
[2025-04-12 18:21:06,107][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:21:06,107][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:21:08,979][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:21:08,979][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:21:09,057][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:21:09,095][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:21:09,211][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:21:09,220][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:21:09,221][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:21:09,222][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:21:09,248][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:21:09,288][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:21:09,304][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:21:09,305][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:21:09,306][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:21:09,307][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:21:09,333][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:21:09,379][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:21:09,398][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:21:09,400][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:21:09,401][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:21:09,402][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:21:09,403][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:21:09,403][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:21:09,403][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:21:09,403][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:21:09,403][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-12 18:21:09,404][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-12 18:21:09,404][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:21:09,404][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:21:09,404][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:21:09,404][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:21:09,404][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:21:09,404][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:21:09,404][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:21:09,405][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:21:09,405][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:21:09,405][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:21:09,405][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:21:09,405][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:21:09,405][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:21:09,405][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:21:09,405][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:21:09,406][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:21:09,406][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:21:09,406][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:21:09,406][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:21:09,406][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:21:09,406][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:21:09,407][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
[2025-04-12 18:21:12,253][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:21:12,253][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:21:12,280][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:21:12,317][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:21:12,335][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 18:21:12,344][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:21:12,345][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 18:21:12,348][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:21:12,372][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:21:12,417][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:21:12,436][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 18:21:12,437][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:21:12,437][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 18:21:12,438][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:21:12,462][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:21:12,500][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:21:12,520][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 18:21:12,522][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:21:12,522][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 18:21:12,524][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 18:21:12,524][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:21:12,524][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:21:12,524][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:21:12,524][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:21:12,525][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 18:21:12,525][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-12 18:21:12,525][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 18:21:12,525][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:21:12,525][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:21:12,525][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:21:12,525][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:21:12,525][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:21:12,526][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-12 18:21:12,526][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-12 18:21:12,526][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 18:21:12,526][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:21:12,526][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:21:12,526][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:21:12,526][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:21:12,526][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:21:12,526][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:21:12,527][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:21:12,527][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 18:21:12,527][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:21:12,527][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 18:21:12,527][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:21:12,527][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:21:12,527][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:21:17,671][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:21:17,674][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:21:17,674][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:21:17,674][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:09,  1.17s/it]Epoch 1/10:   3%|▎         | 2/60 [00:01<00:31,  1.84it/s]Epoch 1/10:   7%|▋         | 4/60 [00:01<00:14,  3.79it/s]Epoch 1/10:  10%|█         | 6/60 [00:01<00:09,  5.42it/s]Epoch 1/10:  13%|█▎        | 8/60 [00:01<00:07,  6.71it/s]Epoch 1/10:  17%|█▋        | 10/60 [00:02<00:06,  7.71it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:02<00:07,  6.86it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:02<00:05,  7.85it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:02<00:05,  8.58it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:02<00:04,  9.11it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:03<00:04,  9.48it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:03<00:03,  9.75it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:03<00:03,  9.94it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:03<00:03, 10.08it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:03<00:03, 10.18it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:03<00:03, 10.24it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:04<00:02, 10.29it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:04<00:02, 10.32it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:04<00:02, 10.35it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:04<00:02, 10.36it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.37it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:05<00:01, 10.40it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:06<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00,  8.59it/s]
[2025-04-12 18:21:26,634][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6869
[2025-04-12 18:21:26,902][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6911, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:12,  4.83it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:07,  7.99it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  9.07it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.58it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.87it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.05it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.17it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.24it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.29it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.15it/s]
[2025-04-12 18:21:33,273][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6436
[2025-04-12 18:21:33,852][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6485, Metrics: {'accuracy': 0.7083333333333334, 'f1': 0.5882352941176471}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:14,  4.15it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  7.46it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  8.73it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.37it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:01<00:05,  9.73it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04,  9.96it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.10it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-12 18:21:40,455][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5385
[2025-04-12 18:21:40,750][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4353, Metrics: {'accuracy': 0.8055555555555556, 'f1': 0.7741935483870968}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:13,  4.33it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  7.61it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  8.83it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.43it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:01<00:05,  9.77it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04,  9.98it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:21:47,086][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.2858
[2025-04-12 18:21:47,373][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.4068, Metrics: {'accuracy': 0.8194444444444444, 'f1': 0.7796610169491526}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:13,  4.54it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  7.78it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  8.93it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.50it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:05,  9.82it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04, 10.01it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.14it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.13it/s]
[2025-04-12 18:21:53,725][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1565
[2025-04-12 18:21:54,007][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.3975, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:13,  4.31it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:07,  7.59it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:06,  8.81it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.42it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:01<00:05,  9.77it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04,  9.98it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.37it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.38it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:22:00,351][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1012
[2025-04-12 18:22:00,652][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.4113, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:12,  4.79it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:07,  7.96it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:06,  9.05it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.58it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.87it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04, 10.05it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.16it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.24it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.29it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.38it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-12 18:22:06,569][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0787
[2025-04-12 18:22:06,877][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.3571, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:14,  4.19it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:07,  7.50it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:06,  8.75it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.38it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:01<00:05,  9.73it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04,  9.95it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.10it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.19it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.25it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.09it/s]
[2025-04-12 18:22:13,247][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0684
[2025-04-12 18:22:13,552][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.4242, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:13,  4.48it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:07,  7.73it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:06,  8.90it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.48it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:05,  9.80it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.04it/s]
[2025-04-12 18:22:19,531][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0517
[2025-04-12 18:22:19,831][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.5636, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:13,  4.29it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:07,  7.58it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:06,  8.80it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:05,  9.41it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:01<00:05,  9.76it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:04,  9.97it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 60/60 [00:05<00:00, 10.11it/s]
[2025-04-12 18:22:25,769][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0556
[2025-04-12 18:22:26,063][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.4819, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.8571428571428571}
[2025-04-12 18:22:26,064][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▅▇▇▇█
wandb:          best_val_f1 ▁▆▇▇██
wandb:        best_val_loss █▇▃▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▆▄▂▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▅▇▇▇▇████
wandb:               val_f1 ▁▆▇▇██████
wandb:             val_loss █▇▃▂▂▂▁▂▅▄
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.86111
wandb:          best_val_f1 0.84375
wandb:        best_val_loss 0.35707
wandb:                epoch 10
wandb:  final_test_accuracy 0.77273
wandb:        final_test_f1 0.77064
wandb: final_train_accuracy 0.98742
wandb:       final_train_f1 0.9869
wandb:   final_val_accuracy 0.86111
wandb:         final_val_f1 0.84375
wandb:        learning_rate 1e-05
wandb:           train_loss 0.05558
wandb:           train_time 66.41833
wandb:         val_accuracy 0.86111
wandb:               val_f1 0.85714
wandb:             val_loss 0.48189
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_182104-r6n25pyl
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_182104-r6n25pyl/logs
Cross-lingual experiment for question_type (id → fi) completed successfully
Running cross-lingual complexity from id to fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:22:46,334][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/id_to_fi/complexity
experiment_name: cross_lingual_complexity_id_to_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: id
  eval_language: fi
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:22:46,334][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:22:46,334][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:22:46,334][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:22:47,745][__main__][INFO] - Running cross-lingual experiment: id -> fi
[2025-04-12 18:22:47,746][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:22:47,746][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:22:50,844][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:22:50,844][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:22:50,903][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:22:50,937][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:22:51,051][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:22:51,061][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:22:51,061][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:22:51,063][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:22:51,085][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:22:51,120][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:22:51,136][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:22:51,137][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:22:51,137][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:22:51,138][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:22:51,162][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:22:51,195][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:22:51,210][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:22:51,212][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:22:51,212][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:22:51,213][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:22:51,213][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:22:51,213][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:22:51,214][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:22:51,214][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:22:51,214][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:22:51,214][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-12 18:22:51,214][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:22:51,214][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-12 18:22:51,215][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:22:51,215][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:22:51,215][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:22:51,215][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:22:51,215][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:22:51,215][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-12 18:22:51,215][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:22:51,215][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-12 18:22:51,216][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:22:51,216][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:22:51,216][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:22:51,216][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:22:51,216][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:22:51,216][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-12 18:22:51,216][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:22:51,217][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-12 18:22:51,217][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:22:51,217][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:22:51,217][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:22:51,218][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
[2025-04-12 18:22:53,907][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:22:53,908][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:22:53,931][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:22:53,964][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:22:53,979][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 18:22:53,989][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:22:53,989][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 18:22:53,990][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:22:54,012][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:22:54,047][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:22:54,063][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 18:22:54,064][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:22:54,065][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 18:22:54,066][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:22:54,092][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:22:54,131][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:22:54,146][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 18:22:54,148][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:22:54,148][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 18:22:54,150][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 18:22:54,150][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:22:54,150][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:22:54,150][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:22:54,151][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:22:54,151][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:22:54,151][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-12 18:22:54,151][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 18:22:54,151][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-12 18:22:54,151][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:22:54,151][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:22:54,151][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:22:54,152][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:22:54,152][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:22:54,152][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-12 18:22:54,152][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 18:22:54,152][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-12 18:22:54,152][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:22:54,152][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:22:54,153][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:22:54,153][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:22:54,153][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:22:54,153][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-12 18:22:54,153][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 18:22:54,153][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-12 18:22:54,153][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 18:22:54,153][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:22:54,154][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:22:54,154][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:22:59,241][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:22:59,244][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:22:59,244][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:22:59,244][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:37,  1.66s/it]Epoch 1/10:   3%|▎         | 2/60 [00:01<00:43,  1.34it/s]Epoch 1/10:   7%|▋         | 4/60 [00:01<00:18,  2.95it/s]Epoch 1/10:  10%|█         | 6/60 [00:02<00:12,  4.46it/s]Epoch 1/10:  13%|█▎        | 8/60 [00:02<00:08,  5.80it/s]Epoch 1/10:  17%|█▋        | 10/60 [00:02<00:07,  6.91it/s]Epoch 1/10:  20%|██        | 12/60 [00:02<00:06,  6.86it/s]Epoch 1/10:  23%|██▎       | 14/60 [00:03<00:05,  7.71it/s]Epoch 1/10:  27%|██▋       | 16/60 [00:03<00:05,  8.41it/s]Epoch 1/10:  30%|███       | 18/60 [00:03<00:04,  8.94it/s]Epoch 1/10:  33%|███▎      | 20/60 [00:03<00:04,  9.32it/s]Epoch 1/10:  37%|███▋      | 22/60 [00:03<00:03,  9.62it/s]Epoch 1/10:  40%|████      | 24/60 [00:03<00:03,  9.84it/s]Epoch 1/10:  43%|████▎     | 26/60 [00:04<00:03, 10.00it/s]Epoch 1/10:  47%|████▋     | 28/60 [00:04<00:03, 10.12it/s]Epoch 1/10:  50%|█████     | 30/60 [00:04<00:02, 10.20it/s]Epoch 1/10:  53%|█████▎    | 32/60 [00:04<00:02, 10.25it/s]Epoch 1/10:  57%|█████▋    | 34/60 [00:04<00:02, 10.30it/s]Epoch 1/10:  60%|██████    | 36/60 [00:05<00:02, 10.33it/s]Epoch 1/10:  63%|██████▎   | 38/60 [00:05<00:02, 10.35it/s]Epoch 1/10:  67%|██████▋   | 40/60 [00:05<00:01, 10.36it/s]Epoch 1/10:  70%|███████   | 42/60 [00:05<00:01, 10.36it/s]Epoch 1/10:  73%|███████▎  | 44/60 [00:05<00:01, 10.37it/s]Epoch 1/10:  77%|███████▋  | 46/60 [00:06<00:01, 10.38it/s]Epoch 1/10:  80%|████████  | 48/60 [00:06<00:01, 10.38it/s]Epoch 1/10:  83%|████████▎ | 50/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  87%|████████▋ | 52/60 [00:06<00:00, 10.38it/s]Epoch 1/10:  90%|█████████ | 54/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  93%|█████████▎| 56/60 [00:07<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 58/60 [00:07<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 60/60 [00:07<00:00, 10.77it/s]Epoch 1/10: 100%|██████████| 60/60 [00:07<00:00,  8.02it/s]
[2025-04-12 18:23:08,648][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1441
[2025-04-12 18:23:08,923][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0981, Metrics: {'mse': 0.0918080136179924, 'rmse': 0.3029983723025462, 'r2': -1.195882797241211}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:12,  4.64it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:07,  7.85it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  8.98it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.53it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.84it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.03it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.15it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.23it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-12 18:23:15,306][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0666
[2025-04-12 18:23:15,586][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1162, Metrics: {'mse': 0.11628685891628265, 'rmse': 0.34100859067812744, 'r2': -1.7813730239868164}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:11,  4.97it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  8.09it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  9.13it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.63it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:05,  9.90it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04, 10.07it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.18it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.24it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.29it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.18it/s]
[2025-04-12 18:23:21,483][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0550
[2025-04-12 18:23:21,754][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0323, Metrics: {'mse': 0.031377360224723816, 'rmse': 0.17713655812599446, 'r2': 0.24950993061065674}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:13,  4.31it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  7.59it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  8.81it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.42it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:01<00:05,  9.76it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04,  9.97it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.36it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.37it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.38it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:23:28,347][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0389
[2025-04-12 18:23:28,659][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0475, Metrics: {'mse': 0.049142591655254364, 'rmse': 0.22168128395345954, 'r2': -0.17540264129638672}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:13,  4.33it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  7.61it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  8.82it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.43it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:01<00:05,  9.77it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04,  9.97it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-12 18:23:34,602][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0319
[2025-04-12 18:23:34,886][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0584, Metrics: {'mse': 0.06045475974678993, 'rmse': 0.24587549643425213, 'r2': -0.4459693431854248}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:13,  4.47it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:07,  7.72it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:06,  8.90it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.47it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:05,  9.80it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-12 18:23:40,809][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0294
[2025-04-12 18:23:41,120][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0224, Metrics: {'mse': 0.02280096895992756, 'rmse': 0.1509998972182682, 'r2': 0.45464175939559937}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:13,  4.23it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:07,  7.53it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:06,  8.77it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.39it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:01<00:05,  9.74it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04,  9.95it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.10it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.19it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.25it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.09it/s]
[2025-04-12 18:23:47,472][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0238
[2025-04-12 18:23:47,784][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0241, Metrics: {'mse': 0.02414759248495102, 'rmse': 0.15539495643344098, 'r2': 0.42243289947509766}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:13,  4.32it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:07,  7.61it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:06,  8.82it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.42it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:01<00:05,  9.77it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04,  9.97it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.10it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.11it/s]
[2025-04-12 18:23:53,723][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0225
[2025-04-12 18:23:54,012][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0285, Metrics: {'mse': 0.027751648798584938, 'rmse': 0.1665882612868774, 'r2': 0.33623039722442627}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:13,  4.29it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:07,  7.58it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:06,  8.81it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.41it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:01<00:05,  9.76it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04,  9.97it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-12 18:23:59,955][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0205
[2025-04-12 18:24:00,253][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0220, Metrics: {'mse': 0.020459331572055817, 'rmse': 0.14303611981613532, 'r2': 0.5106495022773743}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:13,  4.31it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:07,  7.60it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:06,  8.82it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:05,  9.42it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:01<00:05,  9.76it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:04,  9.97it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-12 18:24:06,628][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0179
[2025-04-12 18:24:06,930][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0399, Metrics: {'mse': 0.04086657986044884, 'rmse': 0.20215484129856706, 'r2': 0.022544801235198975}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁▁
wandb:     best_val_mse █▂▁▁
wandb:      best_val_r2 ▁▇██
wandb:    best_val_rmse █▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▂▃▄▁▁▁▁▂
wandb:          val_mse ▆█▂▃▄▁▁▂▁▂
wandb:           val_r2 ▃▁▇▆▅██▇█▇
wandb:         val_rmse ▇█▂▄▅▁▁▂▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02202
wandb:     best_val_mse 0.02046
wandb:      best_val_r2 0.51065
wandb:    best_val_rmse 0.14304
wandb:            epoch 10
wandb:   final_test_mse 0.04413
wandb:    final_test_r2 -0.11769
wandb:  final_test_rmse 0.21008
wandb:  final_train_mse 0.02796
wandb:   final_train_r2 0.22958
wandb: final_train_rmse 0.1672
wandb:    final_val_mse 0.02046
wandb:     final_val_r2 0.51065
wandb:   final_val_rmse 0.14304
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01793
wandb:       train_time 65.76258
wandb:         val_loss 0.03989
wandb:          val_mse 0.04087
wandb:           val_r2 0.02254
wandb:         val_rmse 0.20215
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_182246-7e5vhh7g
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_182246-7e5vhh7g/logs
Cross-lingual experiment for complexity (id → fi) completed successfully
Running cross-lingual question_type from id to ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:24:26,813][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/id_to_ja/question_type
experiment_name: cross_lingual_question_type_id_to_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: id
  eval_language: ja
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:24:26,814][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:24:26,814][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:24:26,814][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:24:28,270][__main__][INFO] - Running cross-lingual experiment: id -> ja
[2025-04-12 18:24:28,270][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:24:28,271][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:24:31,118][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:24:31,118][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:24:31,225][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:24:31,258][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:24:31,360][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:24:31,369][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:24:31,369][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:24:31,371][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:24:31,396][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:24:31,429][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:24:31,444][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:24:31,446][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:24:31,446][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:24:31,447][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:24:31,469][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:24:31,505][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:24:31,519][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:24:31,521][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:24:31,521][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:24:31,522][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:24:31,523][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:24:31,523][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:24:31,523][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:24:31,524][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:24:31,524][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-12 18:24:31,524][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-12 18:24:31,524][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:24:31,524][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:24:31,524][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:24:31,524][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:24:31,525][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:24:31,525][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:24:31,525][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:24:31,525][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:24:31,525][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:24:31,525][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:24:31,525][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:24:31,525][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:24:31,526][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:24:31,526][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:24:31,526][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:24:31,526][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:24:31,526][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:24:31,526][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:24:31,526][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:24:31,526][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:24:31,527][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:24:31,527][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
[2025-04-12 18:24:34,357][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:24:34,358][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:24:34,384][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:24:34,421][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:24:34,437][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:24:34,447][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:24:34,447][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:24:34,449][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:24:34,473][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:24:34,507][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:24:34,522][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:24:34,523][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:24:34,523][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:24:34,524][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:24:34,550][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:24:34,585][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:24:34,599][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:24:34,601][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:24:34,601][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:24:34,602][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:24:34,602][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:24:34,603][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:24:34,603][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:24:34,603][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:24:34,603][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-12 18:24:34,603][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 18:24:34,603][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:24:34,603][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:24:34,604][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:24:34,604][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:24:34,604][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:24:34,604][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:24:34,604][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-12 18:24:34,604][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-12 18:24:34,604][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:24:34,604][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:24:34,604][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:24:34,605][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:24:34,605][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:24:34,605][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:24:34,605][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-12 18:24:34,605][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-12 18:24:34,605][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:24:34,605][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:24:34,605][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:24:34,605][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:24:34,606][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:24:34,606][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:24:39,691][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:24:39,694][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:24:39,694][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:24:39,694][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:17,  1.31s/it]Epoch 1/10:   5%|▌         | 3/60 [00:01<00:23,  2.44it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:13,  4.02it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:09,  5.43it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:02<00:07,  6.62it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:02<00:07,  6.67it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:02<00:06,  7.57it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:02<00:05,  8.30it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:02<00:04,  8.86it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:03<00:04,  9.29it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:03<00:04,  9.61it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:03<00:03,  9.84it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:03<00:03, 10.00it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:03<00:03, 10.11it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:04<00:03, 10.20it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:04<00:02, 10.25it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:04<00:02, 10.30it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:04<00:02, 10.33it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:04<00:02, 10.35it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:05<00:02, 10.36it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:06<00:01, 10.39it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:06<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 60/60 [00:07<00:00,  8.44it/s]
[2025-04-12 18:24:48,669][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6869
[2025-04-12 18:24:48,929][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6911, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:11,  5.17it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:06,  8.23it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:05,  9.21it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.68it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.94it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.10it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.20it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.26it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.31it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.34it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.19it/s]
[2025-04-12 18:24:55,286][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6436
[2025-04-12 18:24:55,558][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6485, Metrics: {'accuracy': 0.7083333333333334, 'f1': 0.5882352941176471}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:12,  4.68it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  7.88it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  9.00it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.54it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:05,  9.85it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04, 10.03it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.15it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.23it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.29it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-12 18:25:02,131][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5385
[2025-04-12 18:25:02,761][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4353, Metrics: {'accuracy': 0.8055555555555556, 'f1': 0.7741935483870968}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:13,  4.33it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  7.62it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  8.83it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.44it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:01<00:05,  9.78it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04,  9.99it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:25:09,099][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.2858
[2025-04-12 18:25:09,396][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.4068, Metrics: {'accuracy': 0.8194444444444444, 'f1': 0.7796610169491526}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:13,  4.30it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  7.59it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  8.81it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.41it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:01<00:05,  9.76it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04,  9.97it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-12 18:25:15,767][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1565
[2025-04-12 18:25:16,060][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.3975, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:13,  4.46it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:07,  7.72it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:06,  8.89it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.47it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:05,  9.80it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:25:22,405][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1012
[2025-04-12 18:25:22,690][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.4113, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:12,  4.55it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:07,  7.79it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:06,  8.94it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.50it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.81it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04, 10.01it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:25:28,624][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0787
[2025-04-12 18:25:28,913][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.3571, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:13,  4.25it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:07,  7.55it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:06,  8.78it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.40it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:01<00:05,  9.76it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04,  9.97it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:25:35,279][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0684
[2025-04-12 18:25:35,591][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.4242, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:13,  4.47it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:07,  7.72it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:06,  8.90it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.47it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:05,  9.80it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-12 18:25:41,511][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0517
[2025-04-12 18:25:41,820][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.5636, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:12,  4.58it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:07,  7.81it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:06,  8.95it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:05,  9.51it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:05,  9.83it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:04, 10.02it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04, 10.14it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 60/60 [00:05<00:00, 10.11it/s]
[2025-04-12 18:25:47,755][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0556
[2025-04-12 18:25:48,062][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.4819, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.8571428571428571}
[2025-04-12 18:25:48,062][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▅▇▇▇█
wandb:          best_val_f1 ▁▆▇▇██
wandb:        best_val_loss █▇▃▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▆▄▂▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▅▇▇▇▇████
wandb:               val_f1 ▁▆▇▇██████
wandb:             val_loss █▇▃▂▂▂▁▂▅▄
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.86111
wandb:          best_val_f1 0.84375
wandb:        best_val_loss 0.35707
wandb:                epoch 10
wandb:  final_test_accuracy 0.40217
wandb:        final_test_f1 0.12698
wandb: final_train_accuracy 0.98742
wandb:       final_train_f1 0.9869
wandb:   final_val_accuracy 0.86111
wandb:         final_val_f1 0.84375
wandb:        learning_rate 1e-05
wandb:           train_loss 0.05558
wandb:           train_time 66.5045
wandb:         val_accuracy 0.86111
wandb:               val_f1 0.85714
wandb:             val_loss 0.48189
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_182426-ah0wh6bd
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_182426-ah0wh6bd/logs
Cross-lingual experiment for question_type (id → ja) completed successfully
Running cross-lingual complexity from id to ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:26:07,823][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/id_to_ja/complexity
experiment_name: cross_lingual_complexity_id_to_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: id
  eval_language: ja
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:26:07,823][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:26:07,823][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:26:07,823][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:26:09,251][__main__][INFO] - Running cross-lingual experiment: id -> ja
[2025-04-12 18:26:09,251][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:26:09,251][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:26:12,127][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:26:12,127][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:26:12,183][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:26:12,214][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:26:12,308][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:26:12,317][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:26:12,318][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:26:12,319][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:26:12,342][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:26:12,376][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:26:12,391][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:26:12,393][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:26:12,393][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:26:12,394][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:26:12,420][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:26:12,463][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:26:12,478][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:26:12,480][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:26:12,480][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:26:12,482][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:26:12,482][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:26:12,482][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:26:12,482][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:26:12,482][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:26:12,483][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:26:12,483][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-12 18:26:12,483][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:26:12,483][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-12 18:26:12,483][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:26:12,483][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:26:12,483][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:26:12,484][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:26:12,484][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:26:12,484][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-12 18:26:12,484][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:26:12,484][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-12 18:26:12,484][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:26:12,484][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:26:12,484][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:26:12,485][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:26:12,485][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:26:12,485][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-12 18:26:12,485][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:26:12,485][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-12 18:26:12,485][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:26:12,485][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:26:12,486][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:26:12,486][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
[2025-04-12 18:26:15,355][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:26:15,355][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:26:15,384][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:26:15,420][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:26:15,437][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:26:15,446][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:26:15,447][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:26:15,448][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:26:15,476][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:26:15,513][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:26:15,530][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:26:15,531][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:26:15,531][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:26:15,532][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:26:15,557][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:26:15,591][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:26:15,604][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:26:15,606][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:26:15,606][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:26:15,607][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:26:15,607][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:26:15,607][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:26:15,607][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:26:15,608][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:26:15,608][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:26:15,608][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-12 18:26:15,608][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:26:15,608][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-04-12 18:26:15,608][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:26:15,609][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:26:15,609][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:26:15,609][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:26:15,609][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:26:15,609][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-12 18:26:15,609][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:26:15,609][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-12 18:26:15,609][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:26:15,610][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:26:15,610][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:26:15,610][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:26:15,610][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:26:15,610][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-12 18:26:15,610][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:26:15,610][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-12 18:26:15,610][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:26:15,611][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:26:15,611][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:26:15,611][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:26:20,659][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:26:20,662][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:26:20,662][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:26:20,662][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:06,  1.12s/it]Epoch 1/10:   3%|▎         | 2/60 [00:01<00:30,  1.92it/s]Epoch 1/10:   7%|▋         | 4/60 [00:01<00:14,  3.92it/s]Epoch 1/10:  10%|█         | 6/60 [00:01<00:09,  5.56it/s]Epoch 1/10:  13%|█▎        | 8/60 [00:01<00:07,  6.84it/s]Epoch 1/10:  17%|█▋        | 10/60 [00:01<00:06,  7.82it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:02<00:06,  7.03it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:02<00:05,  7.99it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:02<00:05,  8.69it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:02<00:04,  9.19it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:02<00:04,  9.54it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:03<00:03,  9.80it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:03<00:03,  9.98it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:03<00:03, 10.11it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:03<00:03, 10.19it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:03<00:03, 10.26it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:04<00:02, 10.30it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:04<00:02, 10.33it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:04<00:02, 10.35it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:04<00:02, 10.37it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:06<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00,  8.65it/s]
[2025-04-12 18:26:29,568][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1441
[2025-04-12 18:26:29,842][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0981, Metrics: {'mse': 0.0918080136179924, 'rmse': 0.3029983723025462, 'r2': -1.195882797241211}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:12,  4.60it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:07,  7.82it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  8.96it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.52it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.84it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.03it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.15it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.23it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.41it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.15it/s]
[2025-04-12 18:26:36,227][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0666
[2025-04-12 18:26:36,503][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1162, Metrics: {'mse': 0.11628685891628265, 'rmse': 0.34100859067812744, 'r2': -1.7813730239868164}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:12,  4.77it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  7.95it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  9.05it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.58it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:05,  9.87it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04, 10.05it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.17it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.24it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.29it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.16it/s]
[2025-04-12 18:26:42,412][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0550
[2025-04-12 18:26:42,681][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0323, Metrics: {'mse': 0.031377360224723816, 'rmse': 0.17713655812599446, 'r2': 0.24950993061065674}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:13,  4.34it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  7.63it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  8.84it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.44it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:01<00:05,  9.78it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04,  9.98it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.29it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.11it/s]
[2025-04-12 18:26:49,229][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0389
[2025-04-12 18:26:49,517][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0475, Metrics: {'mse': 0.049142591655254364, 'rmse': 0.22168128395345954, 'r2': -0.17540264129638672}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:13,  4.42it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  7.68it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  8.88it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.46it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:05,  9.80it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:04, 10.24it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.29it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.32it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.35it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.11it/s]
[2025-04-12 18:26:55,454][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0319
[2025-04-12 18:26:55,752][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0584, Metrics: {'mse': 0.06045475974678993, 'rmse': 0.24587549643425213, 'r2': -0.4459693431854248}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:12,  4.60it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:07,  7.82it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:06,  8.95it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.51it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:05,  9.83it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04, 10.02it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.14it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.23it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.13it/s]
[2025-04-12 18:27:01,675][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0294
[2025-04-12 18:27:01,951][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0224, Metrics: {'mse': 0.02280096895992756, 'rmse': 0.1509998972182682, 'r2': 0.45464175939559937}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:22,  2.63it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:09,  5.89it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:07,  7.57it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:06,  8.56it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:01<00:05,  9.17it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:05,  9.57it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04,  9.82it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.00it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.12it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:02<00:04, 10.21it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.27it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.30it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.33it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.35it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:03<00:02, 10.36it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 60/60 [00:06<00:00,  9.87it/s]
[2025-04-12 18:27:08,434][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0238
[2025-04-12 18:27:08,736][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0241, Metrics: {'mse': 0.02414759248495102, 'rmse': 0.15539495643344098, 'r2': 0.42243289947509766}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:13,  4.53it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:07,  7.77it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:06,  8.93it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.49it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:05,  9.81it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04, 10.01it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.14it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-12 18:27:14,654][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0225
[2025-04-12 18:27:14,949][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0285, Metrics: {'mse': 0.027751648798584938, 'rmse': 0.1665882612868774, 'r2': 0.33623039722442627}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:13,  4.37it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:07,  7.64it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:06,  8.84it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.44it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:05,  9.78it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04,  9.99it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:27:20,879][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0205
[2025-04-12 18:27:21,173][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0220, Metrics: {'mse': 0.020459331572055817, 'rmse': 0.14303611981613532, 'r2': 0.5106495022773743}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:13,  4.44it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:07,  7.70it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:06,  8.89it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:05,  9.47it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:05,  9.80it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 60/60 [00:05<00:00, 10.13it/s]
[2025-04-12 18:27:27,575][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0179
[2025-04-12 18:27:27,872][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0399, Metrics: {'mse': 0.04086657986044884, 'rmse': 0.20215484129856706, 'r2': 0.022544801235198975}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁▁
wandb:     best_val_mse █▂▁▁
wandb:      best_val_r2 ▁▇██
wandb:    best_val_rmse █▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▂▃▄▁▁▁▁▂
wandb:          val_mse ▆█▂▃▄▁▁▂▁▂
wandb:           val_r2 ▃▁▇▆▅██▇█▇
wandb:         val_rmse ▇█▂▄▅▁▁▂▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02202
wandb:     best_val_mse 0.02046
wandb:      best_val_r2 0.51065
wandb:    best_val_rmse 0.14304
wandb:            epoch 10
wandb:   final_test_mse 0.07865
wandb:    final_test_r2 -0.51072
wandb:  final_test_rmse 0.28044
wandb:  final_train_mse 0.02796
wandb:   final_train_r2 0.22958
wandb: final_train_rmse 0.1672
wandb:    final_val_mse 0.02046
wandb:     final_val_r2 0.51065
wandb:   final_val_rmse 0.14304
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01793
wandb:       train_time 65.23966
wandb:         val_loss 0.03989
wandb:          val_mse 0.04087
wandb:           val_r2 0.02254
wandb:         val_rmse 0.20215
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_182607-aqx3uc3s
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_182607-aqx3uc3s/logs
Cross-lingual experiment for complexity (id → ja) completed successfully
Running cross-lingual question_type from id to ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:27:48,035][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/id_to_ko/question_type
experiment_name: cross_lingual_question_type_id_to_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: id
  eval_language: ko
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:27:48,035][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:27:48,035][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:27:48,035][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:27:49,587][__main__][INFO] - Running cross-lingual experiment: id -> ko
[2025-04-12 18:27:49,587][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:27:49,588][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:27:52,703][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:27:52,704][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:27:52,773][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:27:52,808][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:27:52,915][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:27:52,924][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:27:52,925][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:27:52,926][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:27:52,950][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:27:52,983][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:27:52,996][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:27:52,997][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:27:52,998][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:27:52,998][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:27:53,023][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:27:53,056][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:27:53,072][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:27:53,074][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:27:53,074][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:27:53,075][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:27:53,075][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:27:53,075][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:27:53,075][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:27:53,076][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:27:53,076][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-12 18:27:53,076][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-12 18:27:53,076][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:27:53,076][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:27:53,076][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:27:53,076][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:27:53,077][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:27:53,077][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:27:53,077][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:27:53,077][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:27:53,077][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:27:53,077][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:27:53,077][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:27:53,077][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:27:53,077][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:27:53,078][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:27:53,078][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:27:53,078][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:27:53,078][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:27:53,078][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:27:53,078][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:27:53,078][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:27:53,079][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:27:53,079][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
[2025-04-12 18:27:55,780][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:27:55,781][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:27:55,806][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:27:55,847][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:27:55,865][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 18:27:55,871][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:27:55,872][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 18:27:55,873][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:27:55,900][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:27:55,938][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:27:55,954][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 18:27:55,956][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:27:55,956][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 18:27:55,958][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:27:55,984][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:27:56,022][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:27:56,038][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 18:27:56,040][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:27:56,040][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 18:27:56,041][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 18:27:56,042][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:27:56,042][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:27:56,042][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:27:56,042][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:27:56,042][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-12 18:27:56,042][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-12 18:27:56,042][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 18:27:56,042][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:27:56,043][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:27:56,043][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:27:56,043][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:27:56,043][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:27:56,043][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:27:56,043][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:27:56,043][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 18:27:56,043][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:27:56,044][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:27:56,044][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:27:56,044][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:27:56,044][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:27:56,044][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:27:56,044][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:27:56,044][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 18:27:56,044][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:27:56,044][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 18:27:56,044][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:27:56,045][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:27:56,045][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:28:01,100][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:28:01,103][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:28:01,103][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:28:01,103][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:16,  1.29s/it]Epoch 1/10:   5%|▌         | 3/60 [00:01<00:23,  2.45it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:13,  4.04it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:09,  5.45it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:02<00:07,  6.64it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:02<00:06,  7.60it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:02<00:05,  8.34it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:02<00:05,  8.91it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:02<00:04,  9.33it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:03<00:04,  9.64it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:03<00:03,  9.86it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:03<00:03, 10.02it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:03<00:03, 10.13it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:03<00:03, 10.20it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:03<00:03, 10.26it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:04<00:02, 10.29it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:04<00:02, 10.32it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:04<00:02, 10.34it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:04<00:02, 10.29it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.32it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:05<00:01, 10.34it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:05<00:01, 10.36it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:05<00:01, 10.37it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:06<00:00, 10.38it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:06<00:00, 10.38it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:06<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00,  8.57it/s]
[2025-04-12 18:28:10,272][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6869
[2025-04-12 18:28:10,566][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6911, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:12,  4.59it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:07,  7.81it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  8.95it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.51it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.82it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.02it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.14it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.02it/s]
[2025-04-12 18:28:17,156][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6436
[2025-04-12 18:28:17,423][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6485, Metrics: {'accuracy': 0.7083333333333334, 'f1': 0.5882352941176471}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:14,  4.19it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  7.50it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  8.75it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.38it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:01<00:05,  9.74it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04,  9.95it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.10it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.19it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.25it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.09it/s]
[2025-04-12 18:28:24,038][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5385
[2025-04-12 18:28:24,336][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4353, Metrics: {'accuracy': 0.8055555555555556, 'f1': 0.7741935483870968}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:13,  4.46it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  7.71it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  8.89it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.47it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:00<00:05,  9.80it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04,  9.99it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:28:30,662][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.2858
[2025-04-12 18:28:30,944][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.4068, Metrics: {'accuracy': 0.8194444444444444, 'f1': 0.7796610169491526}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:13,  4.38it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  7.65it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  8.85it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.45it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:05,  9.78it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04,  9.99it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.35it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.09it/s]
[2025-04-12 18:28:37,321][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1565
[2025-04-12 18:28:37,611][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.3975, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:13,  4.35it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:07,  7.63it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:06,  8.84it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.43it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:01<00:05,  9.77it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04,  9.98it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-12 18:28:43,972][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1012
[2025-04-12 18:28:44,256][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.4113, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:12,  4.56it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:07,  7.79it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:06,  8.94it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.50it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.81it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04, 10.01it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.38it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.38it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.13it/s]
[2025-04-12 18:28:50,184][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0787
[2025-04-12 18:28:50,514][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.3571, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:13,  4.53it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:07,  7.77it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:06,  8.92it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.49it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:05,  9.81it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04, 10.01it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.09it/s]
[2025-04-12 18:28:56,882][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0684
[2025-04-12 18:28:57,181][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.4242, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:13,  4.39it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:07,  7.66it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:06,  8.86it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.45it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:05,  9.78it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04,  9.99it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:29:03,115][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0517
[2025-04-12 18:29:03,426][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.5636, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:19,  2.97it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:09,  6.30it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:06,  7.90it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:06,  8.79it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:01<00:05,  9.33it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:05,  9.68it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04,  9.91it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.06it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.16it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:02<00:04, 10.23it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.28it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.32it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.34it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.36it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:03<00:02, 10.37it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 60/60 [00:06<00:00,  9.93it/s]
[2025-04-12 18:29:09,472][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0556
[2025-04-12 18:29:09,768][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.4819, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.8571428571428571}
[2025-04-12 18:29:09,769][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▅▇▇▇█
wandb:          best_val_f1 ▁▆▇▇██
wandb:        best_val_loss █▇▃▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▆▄▂▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▅▇▇▇▇████
wandb:               val_f1 ▁▆▇▇██████
wandb:             val_loss █▇▃▂▂▂▁▂▅▄
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.86111
wandb:          best_val_f1 0.84375
wandb:        best_val_loss 0.35707
wandb:                epoch 10
wandb:  final_test_accuracy 0.52727
wandb:        final_test_f1 0.23529
wandb: final_train_accuracy 0.98742
wandb:       final_train_f1 0.9869
wandb:   final_val_accuracy 0.86111
wandb:         final_val_f1 0.84375
wandb:        learning_rate 1e-05
wandb:           train_loss 0.05558
wandb:           train_time 66.498
wandb:         val_accuracy 0.86111
wandb:               val_f1 0.85714
wandb:             val_loss 0.48189
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_182748-ny4ohaaw
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_182748-ny4ohaaw/logs
Cross-lingual experiment for question_type (id → ko) completed successfully
Running cross-lingual complexity from id to ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:29:32,622][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/id_to_ko/complexity
experiment_name: cross_lingual_complexity_id_to_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: id
  eval_language: ko
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:29:32,622][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:29:32,622][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:29:32,623][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:29:34,252][__main__][INFO] - Running cross-lingual experiment: id -> ko
[2025-04-12 18:29:34,252][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:29:34,253][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:29:37,109][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:29:37,109][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:29:37,188][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:29:37,220][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:29:37,323][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:29:37,331][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:29:37,332][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:29:37,333][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:29:37,359][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:29:37,394][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:29:37,412][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:29:37,413][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:29:37,413][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:29:37,415][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:29:37,440][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:29:37,475][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:29:37,490][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:29:37,492][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:29:37,492][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:29:37,494][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:29:37,494][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:29:37,494][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:29:37,494][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:29:37,495][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:29:37,495][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:29:37,495][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-12 18:29:37,495][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:29:37,495][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-12 18:29:37,495][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:29:37,496][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:29:37,496][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:29:37,496][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:29:37,496][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:29:37,496][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-12 18:29:37,496][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:29:37,496][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-12 18:29:37,497][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:29:37,497][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:29:37,497][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:29:37,497][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:29:37,497][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:29:37,497][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-12 18:29:37,497][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:29:37,497][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-12 18:29:37,498][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:29:37,498][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:29:37,498][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:29:37,498][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
[2025-04-12 18:29:40,319][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:29:40,319][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:29:40,341][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:29:40,369][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:29:40,383][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 18:29:40,390][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:29:40,390][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 18:29:40,391][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:29:40,412][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:29:40,440][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:29:40,452][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 18:29:40,454][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:29:40,454][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 18:29:40,455][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:29:40,474][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:29:40,501][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:29:40,512][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 18:29:40,514][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:29:40,514][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 18:29:40,515][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 18:29:40,515][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:29:40,515][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:29:40,515][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:29:40,516][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:29:40,516][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:29:40,516][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-12 18:29:40,516][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 18:29:40,516][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-04-12 18:29:40,516][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:29:40,516][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:29:40,517][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:29:40,517][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:29:40,517][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:29:40,517][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-12 18:29:40,517][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 18:29:40,517][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-12 18:29:40,517][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:29:40,518][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:29:40,518][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:29:40,518][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:29:40,518][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:29:40,518][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-12 18:29:40,518][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 18:29:40,518][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-12 18:29:40,518][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 18:29:40,518][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:29:40,519][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:29:40,519][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:29:45,558][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:29:45,561][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:29:45,561][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:29:45,561][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:02,  1.05s/it]Epoch 1/10:   5%|▌         | 3/60 [00:01<00:19,  2.90it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:11,  4.61it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:08,  6.03it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:01<00:07,  7.16it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:02<00:06,  8.04it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:02<00:05,  8.69it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:02<00:04,  9.18it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:02<00:04,  9.53it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:02<00:04,  9.78it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:02<00:03,  9.96it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:03<00:03, 10.09it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:03<00:03, 10.18it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:03<00:03, 10.24it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:03<00:03, 10.29it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.32it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:04<00:02, 10.34it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:04<00:02, 10.36it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:04<00:02, 10.37it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:06<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 60/60 [00:06<00:00,  8.90it/s]
[2025-04-12 18:29:54,464][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1441
[2025-04-12 18:29:54,719][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0981, Metrics: {'mse': 0.0918080136179924, 'rmse': 0.3029983723025462, 'r2': -1.195882797241211}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:12,  4.68it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:07,  7.87it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  8.98it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.53it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.83it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.01it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.36it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.14it/s]
[2025-04-12 18:30:01,237][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0666
[2025-04-12 18:30:01,490][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1162, Metrics: {'mse': 0.11628685891628265, 'rmse': 0.34100859067812744, 'r2': -1.7813730239868164}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:11,  5.32it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:06,  8.31it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:05,  9.26it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.71it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:05,  9.95it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04, 10.10it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.19it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.25it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.38it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.38it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.38it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.17it/s]
[2025-04-12 18:30:07,390][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0550
[2025-04-12 18:30:07,670][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0323, Metrics: {'mse': 0.031377360224723816, 'rmse': 0.17713655812599446, 'r2': 0.24950993061065674}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:13,  4.31it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  7.59it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  8.81it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.41it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:01<00:05,  9.76it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04,  9.97it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.10it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.19it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.38it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-12 18:30:14,254][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0389
[2025-04-12 18:30:14,553][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0475, Metrics: {'mse': 0.049142591655254364, 'rmse': 0.22168128395345954, 'r2': -0.17540264129638672}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:13,  4.48it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  7.72it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  8.90it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.47it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:05,  9.80it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.38it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:30:20,484][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0319
[2025-04-12 18:30:20,783][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0584, Metrics: {'mse': 0.06045475974678993, 'rmse': 0.24587549643425213, 'r2': -0.4459693431854248}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:13,  4.45it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:07,  7.71it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:06,  8.89it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.47it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:05,  9.79it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.38it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:30:26,717][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0294
[2025-04-12 18:30:26,992][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0224, Metrics: {'mse': 0.02280096895992756, 'rmse': 0.1509998972182682, 'r2': 0.45464175939559937}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:13,  4.50it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:07,  7.74it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:06,  8.91it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.48it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.80it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.38it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.38it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-12 18:30:33,325][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0238
[2025-04-12 18:30:33,858][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0241, Metrics: {'mse': 0.02414759248495102, 'rmse': 0.15539495643344098, 'r2': 0.42243289947509766}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:13,  4.33it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:07,  7.61it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:06,  8.81it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.42it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:01<00:05,  9.76it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04,  9.97it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.19it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.25it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.29it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.35it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.36it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.38it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.38it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.11it/s]
[2025-04-12 18:30:39,795][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0225
[2025-04-12 18:30:40,081][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0285, Metrics: {'mse': 0.027751648798584938, 'rmse': 0.1665882612868774, 'r2': 0.33623039722442627}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:15,  3.92it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:07,  7.26it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:06,  8.59it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.27it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:01<00:05,  9.66it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04,  9.90it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.06it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.16it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.23it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.28it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.31it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.33it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.35it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.36it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 60/60 [00:06<00:00, 10.00it/s]
[2025-04-12 18:30:46,084][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0205
[2025-04-12 18:30:46,391][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0220, Metrics: {'mse': 0.020459331572055817, 'rmse': 0.14303611981613532, 'r2': 0.5106495022773743}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:13,  4.47it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:07,  7.72it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:06,  8.89it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:05,  9.45it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:00<00:05,  9.79it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:04,  9.99it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.38it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 60/60 [00:05<00:00, 10.11it/s]
[2025-04-12 18:30:52,753][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0179
[2025-04-12 18:30:53,048][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0399, Metrics: {'mse': 0.04086657986044884, 'rmse': 0.20215484129856706, 'r2': 0.022544801235198975}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁▁
wandb:     best_val_mse █▂▁▁
wandb:      best_val_r2 ▁▇██
wandb:    best_val_rmse █▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▂▃▄▁▁▁▁▂
wandb:          val_mse ▆█▂▃▄▁▁▂▁▂
wandb:           val_r2 ▃▁▇▆▅██▇█▇
wandb:         val_rmse ▇█▂▄▅▁▁▂▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02202
wandb:     best_val_mse 0.02046
wandb:      best_val_r2 0.51065
wandb:    best_val_rmse 0.14304
wandb:            epoch 10
wandb:   final_test_mse 0.04371
wandb:    final_test_r2 -0.35723
wandb:  final_test_rmse 0.20907
wandb:  final_train_mse 0.02796
wandb:   final_train_r2 0.22958
wandb: final_train_rmse 0.1672
wandb:    final_val_mse 0.02046
wandb:     final_val_r2 0.51065
wandb:   final_val_rmse 0.14304
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01793
wandb:       train_time 65.32954
wandb:         val_loss 0.03989
wandb:          val_mse 0.04087
wandb:           val_r2 0.02254
wandb:         val_rmse 0.20215
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_182932-omvzokc6
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_182932-omvzokc6/logs
Cross-lingual experiment for complexity (id → ko) completed successfully
Running cross-lingual question_type from id to ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:31:13,472][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/id_to_ru/question_type
experiment_name: cross_lingual_question_type_id_to_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: id
  eval_language: ru
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:31:13,472][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:31:13,472][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:31:13,472][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:31:14,932][__main__][INFO] - Running cross-lingual experiment: id -> ru
[2025-04-12 18:31:14,932][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:31:14,933][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:31:17,788][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:31:17,788][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:31:17,859][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:31:17,891][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:31:17,997][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:31:18,006][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:31:18,006][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:31:18,008][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:31:18,032][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:31:18,070][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:31:18,086][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:31:18,088][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:31:18,088][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:31:18,089][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:31:18,118][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:31:18,159][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:31:18,175][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:31:18,176][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:31:18,177][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:31:18,178][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:31:18,179][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:31:18,179][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:31:18,179][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:31:18,179][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:31:18,179][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-12 18:31:18,179][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-12 18:31:18,180][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:31:18,180][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:31:18,180][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:31:18,180][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:31:18,180][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:31:18,180][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:31:18,180][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:31:18,180][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:31:18,181][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:31:18,181][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:31:18,181][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:31:18,181][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:31:18,181][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:31:18,181][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:31:18,181][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:31:18,181][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:31:18,181][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:31:18,182][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:31:18,182][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:31:18,182][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:31:18,182][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:31:18,182][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
[2025-04-12 18:31:21,009][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:31:21,009][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:31:21,028][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:31:21,054][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:31:21,067][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 18:31:21,077][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:31:21,077][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 18:31:21,078][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:31:21,098][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:31:21,122][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:31:21,134][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 18:31:21,135][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:31:21,136][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 18:31:21,137][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:31:21,153][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:31:21,177][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:31:21,188][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 18:31:21,190][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:31:21,190][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 18:31:21,191][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 18:31:21,191][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:31:21,191][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:31:21,191][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:31:21,191][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:31:21,192][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 18:31:21,192][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-12 18:31:21,192][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 18:31:21,192][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:31:21,192][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:31:21,192][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:31:21,192][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:31:21,192][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:31:21,193][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:31:21,193][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:31:21,193][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 18:31:21,193][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:31:21,193][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:31:21,193][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:31:21,193][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:31:21,193][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:31:21,194][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:31:21,194][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:31:21,194][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 18:31:21,194][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:31:21,194][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 18:31:21,194][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:31:21,194][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:31:21,195][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:31:25,945][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:31:25,948][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:31:25,948][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:31:25,948][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:17,  1.31s/it]Epoch 1/10:   5%|▌         | 3/60 [00:01<00:23,  2.43it/s]Epoch 1/10:   8%|▊         | 5/60 [00:01<00:13,  4.02it/s]Epoch 1/10:  12%|█▏        | 7/60 [00:01<00:09,  5.43it/s]Epoch 1/10:  15%|█▌        | 9/60 [00:02<00:07,  6.62it/s]Epoch 1/10:  18%|█▊        | 11/60 [00:02<00:07,  6.61it/s]Epoch 1/10:  22%|██▏       | 13/60 [00:02<00:06,  7.52it/s]Epoch 1/10:  25%|██▌       | 15/60 [00:02<00:05,  8.25it/s]Epoch 1/10:  28%|██▊       | 17/60 [00:02<00:04,  8.83it/s]Epoch 1/10:  32%|███▏      | 19/60 [00:03<00:04,  9.27it/s]Epoch 1/10:  35%|███▌      | 21/60 [00:03<00:04,  9.59it/s]Epoch 1/10:  38%|███▊      | 23/60 [00:03<00:03,  9.82it/s]Epoch 1/10:  42%|████▏     | 25/60 [00:03<00:03,  9.99it/s]Epoch 1/10:  45%|████▌     | 27/60 [00:03<00:03, 10.12it/s]Epoch 1/10:  48%|████▊     | 29/60 [00:04<00:03, 10.20it/s]Epoch 1/10:  52%|█████▏    | 31/60 [00:04<00:02, 10.26it/s]Epoch 1/10:  55%|█████▌    | 33/60 [00:04<00:02, 10.30it/s]Epoch 1/10:  58%|█████▊    | 35/60 [00:04<00:02, 10.33it/s]Epoch 1/10:  62%|██████▏   | 37/60 [00:04<00:02, 10.35it/s]Epoch 1/10:  65%|██████▌   | 39/60 [00:05<00:02, 10.37it/s]Epoch 1/10:  68%|██████▊   | 41/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  72%|███████▏  | 43/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  75%|███████▌  | 45/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  78%|███████▊  | 47/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  82%|████████▏ | 49/60 [00:06<00:01, 10.39it/s]Epoch 1/10:  85%|████████▌ | 51/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  88%|████████▊ | 53/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  92%|█████████▏| 55/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▌| 57/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  98%|█████████▊| 59/60 [00:06<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 60/60 [00:07<00:00,  8.43it/s]
[2025-04-12 18:31:35,280][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6869
[2025-04-12 18:31:35,548][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6911, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:11,  5.12it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:06,  8.19it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:05,  9.20it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.67it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.93it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.09it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.19it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.25it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.30it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.33it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.39it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.19it/s]
[2025-04-12 18:31:41,900][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6436
[2025-04-12 18:31:42,162][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6485, Metrics: {'accuracy': 0.7083333333333334, 'f1': 0.5882352941176471}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:13,  4.37it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  7.65it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  8.85it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.45it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:05,  9.79it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04,  9.99it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.13it/s]
[2025-04-12 18:31:48,737][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5385
[2025-04-12 18:31:49,010][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4353, Metrics: {'accuracy': 0.8055555555555556, 'f1': 0.7741935483870968}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:13,  4.25it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:07,  7.55it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:06,  8.79it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:05,  9.40it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:01<00:05,  9.75it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:04,  9.97it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 60/60 [00:05<00:00, 10.07it/s]
[2025-04-12 18:31:55,388][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.2858
[2025-04-12 18:31:55,662][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.4068, Metrics: {'accuracy': 0.8194444444444444, 'f1': 0.7796610169491526}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:13,  4.41it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  7.68it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  8.87it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.46it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:05,  9.79it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.13it/s]
[2025-04-12 18:32:02,022][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1565
[2025-04-12 18:32:02,317][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.3975, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:12,  4.61it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:07,  7.83it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:06,  8.97it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.52it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:05,  9.84it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04, 10.03it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.15it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.23it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.35it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.38it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.15it/s]
[2025-04-12 18:32:08,648][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1012
[2025-04-12 18:32:08,943][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.4113, Metrics: {'accuracy': 0.8333333333333334, 'f1': 0.8}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:12,  4.60it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:07,  7.82it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:06,  8.96it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.52it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.83it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04, 10.02it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.15it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.23it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.13it/s]
[2025-04-12 18:32:14,869][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0787
[2025-04-12 18:32:15,146][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.3571, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:13,  4.43it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:07,  7.69it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:06,  8.88it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.46it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:05,  9.79it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.39it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-12 18:32:21,506][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0684
[2025-04-12 18:32:21,802][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.4242, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:13,  4.42it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:07,  7.68it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:06,  8.87it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.45it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:05,  9.79it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04,  9.99it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.21it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.28it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:32:27,732][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0517
[2025-04-12 18:32:28,027][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.5636, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.84375}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:26,  2.23it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:10,  5.32it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:07,  7.10it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:01<00:06,  8.20it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:01<00:05,  8.91it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:05,  9.38it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04,  9.70it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04,  9.91it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.06it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:02<00:04, 10.16it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.24it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.29it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.32it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.34it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:03<00:02, 10.36it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:05<00:01, 10.40it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.41it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:06<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 60/60 [00:06<00:00,  9.75it/s]
[2025-04-12 18:32:34,182][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0556
[2025-04-12 18:32:34,478][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.4819, Metrics: {'accuracy': 0.8611111111111112, 'f1': 0.8571428571428571}
[2025-04-12 18:32:34,479][src.training.lm_trainer][INFO] - Early stopping at epoch 10
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▅▇▇▇█
wandb:          best_val_f1 ▁▆▇▇██
wandb:        best_val_loss █▇▃▂▂▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▆▄▂▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▅▇▇▇▇████
wandb:               val_f1 ▁▆▇▇██████
wandb:             val_loss █▇▃▂▂▂▁▂▅▄
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.86111
wandb:          best_val_f1 0.84375
wandb:        best_val_loss 0.35707
wandb:                epoch 10
wandb:  final_test_accuracy 0.93636
wandb:        final_test_f1 0.93694
wandb: final_train_accuracy 0.98742
wandb:       final_train_f1 0.9869
wandb:   final_val_accuracy 0.86111
wandb:         final_val_f1 0.84375
wandb:        learning_rate 1e-05
wandb:           train_loss 0.05558
wandb:           train_time 66.31609
wandb:         val_accuracy 0.86111
wandb:               val_f1 0.85714
wandb:             val_loss 0.48189
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_183113-352hkaj3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_183113-352hkaj3/logs
Cross-lingual experiment for question_type (id → ru) completed successfully
Running cross-lingual complexity from id to ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:32:55,623][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/id_to_ru/complexity
experiment_name: cross_lingual_complexity_id_to_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: id
  eval_language: ru
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:32:55,623][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:32:55,624][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:32:55,624][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:32:57,422][__main__][INFO] - Running cross-lingual experiment: id -> ru
[2025-04-12 18:32:57,423][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:32:57,423][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:33:00,289][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:33:00,290][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:33:00,380][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:33:00,405][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:33:00,502][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:33:00,512][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:33:00,512][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:33:00,513][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:33:00,535][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:33:00,562][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:33:00,575][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:33:00,576][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:33:00,577][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:33:00,578][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:33:00,600][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:33:00,629][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:33:00,642][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:33:00,644][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:33:00,644][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:33:00,645][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:33:00,646][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:33:00,646][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:33:00,646][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:33:00,646][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:33:00,647][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:33:00,647][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-12 18:33:00,647][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:33:00,647][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-12 18:33:00,647][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:33:00,647][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:33:00,647][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:33:00,648][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:33:00,648][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:33:00,648][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-12 18:33:00,648][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:33:00,648][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-12 18:33:00,648][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:33:00,648][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:33:00,649][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:33:00,649][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:33:00,649][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:33:00,649][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-12 18:33:00,649][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:33:00,649][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-12 18:33:00,649][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:33:00,649][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:33:00,650][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:33:00,650][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
[2025-04-12 18:33:03,502][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:33:03,502][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:33:03,524][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:33:03,558][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:33:03,575][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 18:33:03,584][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:33:03,585][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 18:33:03,586][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:33:03,612][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:33:03,642][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:33:03,655][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 18:33:03,656][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:33:03,657][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 18:33:03,658][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:33:03,684][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:33:03,721][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:33:03,737][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 18:33:03,738][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:33:03,739][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 18:33:03,740][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 18:33:03,741][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:33:03,741][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:33:03,741][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:33:03,741][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:33:03,741][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:33:03,741][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-12 18:33:03,741][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 18:33:03,742][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-04-12 18:33:03,742][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:33:03,742][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:33:03,742][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:33:03,742][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:33:03,742][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:33:03,742][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-12 18:33:03,742][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 18:33:03,743][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-12 18:33:03,743][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:33:03,743][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:33:03,743][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:33:03,743][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:33:03,743][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:33:03,743][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-12 18:33:03,743][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 18:33:03,744][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-12 18:33:03,744][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 18:33:03,744][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:33:03,744][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:33:03,744][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:33:08,983][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:33:08,986][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:33:08,986][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:33:08,987][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/60 [00:01<01:18,  1.33s/it]Epoch 1/10:   3%|▎         | 2/60 [00:01<00:35,  1.64it/s]Epoch 1/10:   7%|▋         | 4/60 [00:01<00:16,  3.48it/s]Epoch 1/10:  10%|█         | 6/60 [00:01<00:10,  5.07it/s]Epoch 1/10:  13%|█▎        | 8/60 [00:02<00:08,  6.39it/s]Epoch 1/10:  17%|█▋        | 10/60 [00:02<00:06,  7.44it/s]Epoch 1/10:  20%|██        | 12/60 [00:02<00:06,  7.22it/s]Epoch 1/10:  23%|██▎       | 14/60 [00:02<00:05,  8.02it/s]Epoch 1/10:  27%|██▋       | 16/60 [00:02<00:05,  8.65it/s]Epoch 1/10:  30%|███       | 18/60 [00:03<00:04,  9.13it/s]Epoch 1/10:  33%|███▎      | 20/60 [00:03<00:04,  9.48it/s]Epoch 1/10:  37%|███▋      | 22/60 [00:03<00:03,  9.74it/s]Epoch 1/10:  40%|████      | 24/60 [00:03<00:03,  9.93it/s]Epoch 1/10:  43%|████▎     | 26/60 [00:03<00:03, 10.07it/s]Epoch 1/10:  47%|████▋     | 28/60 [00:04<00:03, 10.16it/s]Epoch 1/10:  50%|█████     | 30/60 [00:04<00:02, 10.23it/s]Epoch 1/10:  53%|█████▎    | 32/60 [00:04<00:02, 10.28it/s]Epoch 1/10:  57%|█████▋    | 34/60 [00:04<00:02, 10.31it/s]Epoch 1/10:  60%|██████    | 36/60 [00:04<00:02, 10.33it/s]Epoch 1/10:  63%|██████▎   | 38/60 [00:04<00:02, 10.35it/s]Epoch 1/10:  67%|██████▋   | 40/60 [00:05<00:01, 10.36it/s]Epoch 1/10:  70%|███████   | 42/60 [00:05<00:01, 10.37it/s]Epoch 1/10:  73%|███████▎  | 44/60 [00:05<00:01, 10.38it/s]Epoch 1/10:  77%|███████▋  | 46/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  80%|████████  | 48/60 [00:05<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 50/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  87%|████████▋ | 52/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  90%|█████████ | 54/60 [00:06<00:00, 10.39it/s]Epoch 1/10:  93%|█████████▎| 56/60 [00:06<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 58/60 [00:06<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 60/60 [00:07<00:00, 10.76it/s]Epoch 1/10: 100%|██████████| 60/60 [00:07<00:00,  8.40it/s]
[2025-04-12 18:33:18,506][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1441
[2025-04-12 18:33:18,766][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0981, Metrics: {'mse': 0.0918080136179924, 'rmse': 0.3029983723025462, 'r2': -1.195882797241211}
Epoch 2/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/60 [00:00<00:12,  4.67it/s]Epoch 2/10:   5%|▌         | 3/60 [00:00<00:07,  7.87it/s]Epoch 2/10:   8%|▊         | 5/60 [00:00<00:06,  8.98it/s]Epoch 2/10:  12%|█▏        | 7/60 [00:00<00:05,  9.53it/s]Epoch 2/10:  15%|█▌        | 9/60 [00:00<00:05,  9.84it/s]Epoch 2/10:  18%|█▊        | 11/60 [00:01<00:04, 10.02it/s]Epoch 2/10:  22%|██▏       | 13/60 [00:01<00:04, 10.14it/s]Epoch 2/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 2/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 2/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 2/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 2/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 2/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 2/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 2/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 2/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 2/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.37it/s]Epoch 2/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.38it/s]Epoch 2/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.39it/s]Epoch 2/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:33:25,153][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0666
[2025-04-12 18:33:25,433][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.1162, Metrics: {'mse': 0.11628685891628265, 'rmse': 0.34100859067812744, 'r2': -1.7813730239868164}
Epoch 3/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/60 [00:00<00:12,  4.82it/s]Epoch 3/10:   5%|▌         | 3/60 [00:00<00:07,  7.98it/s]Epoch 3/10:   8%|▊         | 5/60 [00:00<00:06,  9.06it/s]Epoch 3/10:  12%|█▏        | 7/60 [00:00<00:05,  9.58it/s]Epoch 3/10:  15%|█▌        | 9/60 [00:00<00:05,  9.87it/s]Epoch 3/10:  18%|█▊        | 11/60 [00:01<00:04, 10.05it/s]Epoch 3/10:  22%|██▏       | 13/60 [00:01<00:04, 10.16it/s]Epoch 3/10:  25%|██▌       | 15/60 [00:01<00:04, 10.24it/s]Epoch 3/10:  28%|██▊       | 17/60 [00:01<00:04, 10.28it/s]Epoch 3/10:  32%|███▏      | 19/60 [00:01<00:03, 10.32it/s]Epoch 3/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 3/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 3/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 3/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 3/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 3/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.38it/s]Epoch 3/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 3/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 3/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 60/60 [00:05<00:00, 10.16it/s]
[2025-04-12 18:33:31,343][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0550
[2025-04-12 18:33:31,621][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0323, Metrics: {'mse': 0.031377360224723816, 'rmse': 0.17713655812599446, 'r2': 0.24950993061065674}
Epoch 4/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/60 [00:00<00:24,  2.45it/s]Epoch 4/10:   5%|▌         | 3/60 [00:00<00:10,  5.63it/s]Epoch 4/10:   8%|▊         | 5/60 [00:00<00:07,  7.37it/s]Epoch 4/10:  12%|█▏        | 7/60 [00:00<00:06,  8.40it/s]Epoch 4/10:  15%|█▌        | 9/60 [00:01<00:05,  9.06it/s]Epoch 4/10:  18%|█▊        | 11/60 [00:01<00:05,  9.48it/s]Epoch 4/10:  22%|██▏       | 13/60 [00:01<00:04,  9.77it/s]Epoch 4/10:  25%|██▌       | 15/60 [00:01<00:04,  9.95it/s]Epoch 4/10:  28%|██▊       | 17/60 [00:01<00:04, 10.09it/s]Epoch 4/10:  32%|███▏      | 19/60 [00:02<00:04, 10.17it/s]Epoch 4/10:  35%|███▌      | 21/60 [00:02<00:03, 10.24it/s]Epoch 4/10:  38%|███▊      | 23/60 [00:02<00:03, 10.28it/s]Epoch 4/10:  42%|████▏     | 25/60 [00:02<00:03, 10.32it/s]Epoch 4/10:  45%|████▌     | 27/60 [00:02<00:03, 10.34it/s]Epoch 4/10:  48%|████▊     | 29/60 [00:03<00:02, 10.36it/s]Epoch 4/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.37it/s]Epoch 4/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.37it/s]Epoch 4/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 4/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 39/60 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 4/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.38it/s]Epoch 4/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 4/10:  82%|████████▏ | 49/60 [00:05<00:01, 10.39it/s]Epoch 4/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 4/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 60/60 [00:06<00:00,  9.83it/s]
[2025-04-12 18:33:38,393][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0389
[2025-04-12 18:33:38,689][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0475, Metrics: {'mse': 0.049142591655254364, 'rmse': 0.22168128395345954, 'r2': -0.17540264129638672}
Epoch 5/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/60 [00:00<00:13,  4.50it/s]Epoch 5/10:   5%|▌         | 3/60 [00:00<00:07,  7.74it/s]Epoch 5/10:   8%|▊         | 5/60 [00:00<00:06,  8.91it/s]Epoch 5/10:  12%|█▏        | 7/60 [00:00<00:05,  9.49it/s]Epoch 5/10:  15%|█▌        | 9/60 [00:00<00:05,  9.81it/s]Epoch 5/10:  18%|█▊        | 11/60 [00:01<00:04, 10.00it/s]Epoch 5/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 5/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 5/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 5/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 5/10:  35%|███▌      | 21/60 [00:02<00:03, 10.34it/s]Epoch 5/10:  38%|███▊      | 23/60 [00:02<00:03, 10.36it/s]Epoch 5/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 5/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 5/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 5/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 5/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 60/60 [00:05<00:00, 10.06it/s]
[2025-04-12 18:33:44,657][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0319
[2025-04-12 18:33:44,952][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0584, Metrics: {'mse': 0.06045475974678993, 'rmse': 0.24587549643425213, 'r2': -0.4459693431854248}
Epoch 6/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/60 [00:00<00:13,  4.53it/s]Epoch 6/10:   5%|▌         | 3/60 [00:00<00:07,  7.76it/s]Epoch 6/10:   8%|▊         | 5/60 [00:00<00:06,  8.92it/s]Epoch 6/10:  12%|█▏        | 7/60 [00:00<00:05,  9.49it/s]Epoch 6/10:  15%|█▌        | 9/60 [00:00<00:05,  9.81it/s]Epoch 6/10:  18%|█▊        | 11/60 [00:01<00:04, 10.01it/s]Epoch 6/10:  22%|██▏       | 13/60 [00:01<00:04, 10.13it/s]Epoch 6/10:  25%|██▌       | 15/60 [00:01<00:04, 10.22it/s]Epoch 6/10:  28%|██▊       | 17/60 [00:01<00:04, 10.27it/s]Epoch 6/10:  32%|███▏      | 19/60 [00:01<00:03, 10.31it/s]Epoch 6/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 6/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 6/10:  42%|████▏     | 25/60 [00:02<00:03, 10.37it/s]Epoch 6/10:  45%|████▌     | 27/60 [00:02<00:03, 10.38it/s]Epoch 6/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 6/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 6/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.38it/s]Epoch 6/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 6/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 6/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:33:50,881][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0294
[2025-04-12 18:33:51,169][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0224, Metrics: {'mse': 0.02280096895992756, 'rmse': 0.1509998972182682, 'r2': 0.45464175939559937}
Epoch 7/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/60 [00:00<00:13,  4.41it/s]Epoch 7/10:   5%|▌         | 3/60 [00:00<00:07,  7.68it/s]Epoch 7/10:   8%|▊         | 5/60 [00:00<00:06,  8.87it/s]Epoch 7/10:  12%|█▏        | 7/60 [00:00<00:05,  9.45it/s]Epoch 7/10:  15%|█▌        | 9/60 [00:00<00:05,  9.79it/s]Epoch 7/10:  18%|█▊        | 11/60 [00:01<00:04,  9.99it/s]Epoch 7/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 7/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 7/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 7/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 7/10:  35%|███▌      | 21/60 [00:02<00:03, 10.32it/s]Epoch 7/10:  38%|███▊      | 23/60 [00:02<00:03, 10.34it/s]Epoch 7/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 7/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 7/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 7/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 7/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 7/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 7/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 7/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 7/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.38it/s]Epoch 7/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 7/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 7/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 60/60 [00:05<00:00, 10.11it/s]
[2025-04-12 18:33:57,507][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0238
[2025-04-12 18:33:57,794][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0241, Metrics: {'mse': 0.02414759248495102, 'rmse': 0.15539495643344098, 'r2': 0.42243289947509766}
Epoch 8/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/60 [00:00<00:13,  4.41it/s]Epoch 8/10:   5%|▌         | 3/60 [00:00<00:07,  7.68it/s]Epoch 8/10:   8%|▊         | 5/60 [00:00<00:06,  8.87it/s]Epoch 8/10:  12%|█▏        | 7/60 [00:00<00:05,  9.46it/s]Epoch 8/10:  15%|█▌        | 9/60 [00:00<00:05,  9.79it/s]Epoch 8/10:  18%|█▊        | 11/60 [00:01<00:04,  9.99it/s]Epoch 8/10:  22%|██▏       | 13/60 [00:01<00:04, 10.12it/s]Epoch 8/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 8/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 8/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 8/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 8/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 8/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 8/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 8/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 8/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 8/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 8/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 8/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 60/60 [00:05<00:00, 10.10it/s]
[2025-04-12 18:34:03,735][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0225
[2025-04-12 18:34:04,022][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0285, Metrics: {'mse': 0.027751648798584938, 'rmse': 0.1665882612868774, 'r2': 0.33623039722442627}
Epoch 9/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/60 [00:00<00:13,  4.38it/s]Epoch 9/10:   5%|▌         | 3/60 [00:00<00:07,  7.66it/s]Epoch 9/10:   8%|▊         | 5/60 [00:00<00:06,  8.85it/s]Epoch 9/10:  12%|█▏        | 7/60 [00:00<00:05,  9.44it/s]Epoch 9/10:  15%|█▌        | 9/60 [00:00<00:05,  9.78it/s]Epoch 9/10:  18%|█▊        | 11/60 [00:01<00:04,  9.98it/s]Epoch 9/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 9/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 9/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 9/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 9/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 9/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 9/10:  42%|████▏     | 25/60 [00:02<00:03, 10.35it/s]Epoch 9/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 9/10:  48%|████▊     | 29/60 [00:02<00:02, 10.37it/s]Epoch 9/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.38it/s]Epoch 9/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.38it/s]Epoch 9/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.38it/s]Epoch 9/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.38it/s]Epoch 9/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.39it/s]Epoch 9/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.39it/s]Epoch 9/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 9/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 60/60 [00:05<00:00, 10.12it/s]
[2025-04-12 18:34:09,953][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0205
[2025-04-12 18:34:10,244][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0220, Metrics: {'mse': 0.020459331572055817, 'rmse': 0.14303611981613532, 'r2': 0.5106495022773743}
Epoch 10/10:   0%|          | 0/60 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/60 [00:00<00:13,  4.34it/s]Epoch 10/10:   5%|▌         | 3/60 [00:00<00:07,  7.62it/s]Epoch 10/10:   8%|▊         | 5/60 [00:00<00:06,  8.83it/s]Epoch 10/10:  12%|█▏        | 7/60 [00:00<00:05,  9.41it/s]Epoch 10/10:  15%|█▌        | 9/60 [00:01<00:05,  9.76it/s]Epoch 10/10:  18%|█▊        | 11/60 [00:01<00:04,  9.97it/s]Epoch 10/10:  22%|██▏       | 13/60 [00:01<00:04, 10.11it/s]Epoch 10/10:  25%|██▌       | 15/60 [00:01<00:04, 10.20it/s]Epoch 10/10:  28%|██▊       | 17/60 [00:01<00:04, 10.26it/s]Epoch 10/10:  32%|███▏      | 19/60 [00:01<00:03, 10.30it/s]Epoch 10/10:  35%|███▌      | 21/60 [00:02<00:03, 10.33it/s]Epoch 10/10:  38%|███▊      | 23/60 [00:02<00:03, 10.35it/s]Epoch 10/10:  42%|████▏     | 25/60 [00:02<00:03, 10.36it/s]Epoch 10/10:  45%|████▌     | 27/60 [00:02<00:03, 10.37it/s]Epoch 10/10:  48%|████▊     | 29/60 [00:02<00:02, 10.38it/s]Epoch 10/10:  52%|█████▏    | 31/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  55%|█████▌    | 33/60 [00:03<00:02, 10.38it/s]Epoch 10/10:  58%|█████▊    | 35/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  62%|██████▏   | 37/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 39/60 [00:03<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 41/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  72%|███████▏  | 43/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  75%|███████▌  | 45/60 [00:04<00:01, 10.39it/s]Epoch 10/10:  78%|███████▊  | 47/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  82%|████████▏ | 49/60 [00:04<00:01, 10.40it/s]Epoch 10/10:  85%|████████▌ | 51/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  88%|████████▊ | 53/60 [00:05<00:00, 10.39it/s]Epoch 10/10:  92%|█████████▏| 55/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  95%|█████████▌| 57/60 [00:05<00:00, 10.40it/s]Epoch 10/10:  98%|█████████▊| 59/60 [00:05<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 60/60 [00:05<00:00, 10.08it/s]
[2025-04-12 18:34:16,619][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0179
[2025-04-12 18:34:16,920][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0399, Metrics: {'mse': 0.04086657986044884, 'rmse': 0.20215484129856706, 'r2': 0.022544801235198975}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▁▁
wandb:     best_val_mse █▂▁▁
wandb:      best_val_r2 ▁▇██
wandb:    best_val_rmse █▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▃▂▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▂▃▄▁▁▁▁▂
wandb:          val_mse ▆█▂▃▄▁▁▂▁▂
wandb:           val_r2 ▃▁▇▆▅██▇█▇
wandb:         val_rmse ▇█▂▄▅▁▁▂▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02202
wandb:     best_val_mse 0.02046
wandb:      best_val_r2 0.51065
wandb:    best_val_rmse 0.14304
wandb:            epoch 10
wandb:   final_test_mse 0.07134
wandb:    final_test_r2 -0.80449
wandb:  final_test_rmse 0.2671
wandb:  final_train_mse 0.02796
wandb:   final_train_r2 0.22958
wandb: final_train_rmse 0.1672
wandb:    final_val_mse 0.02046
wandb:     final_val_r2 0.51065
wandb:   final_val_rmse 0.14304
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01793
wandb:       train_time 65.5589
wandb:         val_loss 0.03989
wandb:          val_mse 0.04087
wandb:           val_r2 0.02254
wandb:         val_rmse 0.20215
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_183255-k575gwkt
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_183255-k575gwkt/logs
Cross-lingual experiment for complexity (id → ru) completed successfully
Running cross-lingual question_type from ja to ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:34:38,088][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ja_to_ar/question_type
experiment_name: cross_lingual_question_type_ja_to_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ja
  eval_language: ar
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:34:38,088][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:34:38,089][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:34:38,089][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:34:39,706][__main__][INFO] - Running cross-lingual experiment: ja -> ar
[2025-04-12 18:34:39,707][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:34:39,707][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:34:42,556][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:34:42,557][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:34:42,663][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:34:42,691][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:34:42,787][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:34:42,798][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:34:42,798][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:34:42,799][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:34:42,820][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:34:42,848][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:34:42,860][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:34:42,861][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:34:42,862][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:34:42,863][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:34:42,881][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:34:42,908][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:34:42,920][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:34:42,922][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:34:42,922][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:34:42,923][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:34:42,924][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:34:42,924][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:34:42,924][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:34:42,924][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:34:42,924][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-12 18:34:42,925][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 18:34:42,925][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:34:42,925][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:34:42,925][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:34:42,925][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:34:42,925][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:34:42,925][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:34:42,925][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-12 18:34:42,926][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-12 18:34:42,926][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:34:42,926][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:34:42,926][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:34:42,926][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:34:42,926][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:34:42,926][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:34:42,926][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-12 18:34:42,927][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-12 18:34:42,927][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:34:42,927][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:34:42,927][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:34:42,927][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:34:42,927][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:34:42,928][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
[2025-04-12 18:34:45,741][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:34:45,741][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:34:45,766][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:34:45,799][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:34:45,816][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 18:34:45,824][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:34:45,825][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 18:34:45,826][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:34:45,853][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:34:45,886][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:34:45,900][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 18:34:45,901][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:34:45,901][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 18:34:45,902][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:34:45,930][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:34:45,966][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:34:45,982][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 18:34:45,983][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:34:45,983][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 18:34:45,985][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 18:34:45,985][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:34:45,985][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:34:45,985][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:34:45,986][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:34:45,986][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-12 18:34:45,986][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-12 18:34:45,986][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 18:34:45,986][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:34:45,986][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:34:45,986][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:34:45,986][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:34:45,987][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:34:45,987][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-12 18:34:45,987][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-12 18:34:45,987][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 18:34:45,987][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:34:45,987][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:34:45,987][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:34:45,987][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:34:45,988][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:34:45,988][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-12 18:34:45,988][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-12 18:34:45,988][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 18:34:45,988][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:34:45,988][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 18:34:45,988][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:34:45,988][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:34:45,989][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:34:50,979][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:34:50,982][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:34:50,982][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:34:50,982][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:36,  1.30s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:43,  1.67it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:20,  3.52it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.12it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.43it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.48it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.81it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  7.81it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:07,  8.54it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.08it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.46it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.74it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.94it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.07it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.17it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.23it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.93it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.77it/s]
[2025-04-12 18:35:01,836][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6804
[2025-04-12 18:35:02,207][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6727, Metrics: {'accuracy': 0.5869565217391305, 'f1': 0.3448275862068966}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:16,  4.62it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.83it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 18:35:10,022][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5002
[2025-04-12 18:35:10,237][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2261, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.43it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.69it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:35:18,274][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1331
[2025-04-12 18:35:18,508][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0395, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.47it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.73it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.90it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 18:35:26,250][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0604
[2025-04-12 18:35:26,483][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0276, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.20it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.50it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:08,  8.75it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.38it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.74it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.94it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.08it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.18it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.13it/s]
[2025-04-12 18:35:34,312][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0499
[2025-04-12 18:35:34,548][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0178, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.73it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.92it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.03it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.37it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 18:35:42,325][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0373
[2025-04-12 18:35:42,575][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0087, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:17,  4.35it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.63it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.83it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:01<00:06,  9.78it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:35:50,376][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0406
[2025-04-12 18:35:50,617][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0091, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.48it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.73it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.90it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:35:57,977][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0295
[2025-04-12 18:35:58,220][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0498, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:18,  3.90it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.25it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:08,  8.59it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.27it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:01<00:06,  9.67it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.91it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.07it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.17it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.24it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.14it/s]
[2025-04-12 18:36:05,622][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0240
[2025-04-12 18:36:05,869][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0058, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:17,  4.25it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.55it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.79it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:01<00:06,  9.75it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.33it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.35it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.36it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:36:13,675][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0274
[2025-04-12 18:36:13,922][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0061, Metrics: {'accuracy': 1.0, 'f1': 1.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██████
wandb:          best_val_f1 ▁██████
wandb:        best_val_loss █▃▁▁▁▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▆▂▁▁▁▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████████
wandb:               val_f1 ▁█████████
wandb:             val_loss █▃▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 1
wandb:          best_val_f1 1
wandb:        best_val_loss 0.0058
wandb:                epoch 10
wandb:  final_test_accuracy 0.5974
wandb:        final_test_f1 0.55072
wandb: final_train_accuracy 0.99748
wandb:       final_train_f1 0.99748
wandb:   final_val_accuracy 1
wandb:         final_val_f1 1
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02743
wandb:           train_time 80.63901
wandb:         val_accuracy 1
wandb:               val_f1 1
wandb:             val_loss 0.00609
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_183438-qs4r69i0
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_183438-qs4r69i0/logs
Cross-lingual experiment for question_type (ja → ar) completed successfully
Running cross-lingual complexity from ja to ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:36:34,784][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ja_to_ar/complexity
experiment_name: cross_lingual_complexity_ja_to_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ja
  eval_language: ar
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:36:34,784][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:36:34,784][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:36:34,784][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:36:36,369][__main__][INFO] - Running cross-lingual experiment: ja -> ar
[2025-04-12 18:36:36,369][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:36:36,369][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:36:39,189][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:36:39,189][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:36:39,276][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:36:39,307][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:36:39,405][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:36:39,416][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:36:39,416][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:36:39,418][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:36:39,441][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:36:39,473][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:36:39,487][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:36:39,488][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:36:39,488][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:36:39,489][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:36:39,512][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:36:39,548][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:36:39,564][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:36:39,566][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:36:39,566][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:36:39,567][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:36:39,568][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:36:39,568][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:36:39,568][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:36:39,568][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:36:39,568][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:36:39,568][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-12 18:36:39,569][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:36:39,569][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-04-12 18:36:39,569][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:36:39,569][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:36:39,569][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:36:39,569][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:36:39,569][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:36:39,569][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-12 18:36:39,570][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:36:39,570][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-12 18:36:39,570][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:36:39,570][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:36:39,570][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:36:39,570][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:36:39,570][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:36:39,571][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-12 18:36:39,571][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:36:39,571][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-12 18:36:39,571][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:36:39,571][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:36:39,571][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:36:39,572][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
[2025-04-12 18:36:42,365][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:36:42,365][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:36:42,390][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:36:42,419][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:36:42,432][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 18:36:42,440][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:36:42,441][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 18:36:42,442][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:36:42,463][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:36:42,496][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:36:42,509][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 18:36:42,510][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:36:42,510][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 18:36:42,511][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:36:42,536][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:36:42,570][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:36:42,584][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 18:36:42,586][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:36:42,586][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 18:36:42,587][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 18:36:42,587][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:36:42,587][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:36:42,587][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:36:42,587][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:36:42,588][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:36:42,588][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-12 18:36:42,588][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 18:36:42,588][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-12 18:36:42,588][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:36:42,588][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:36:42,588][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:36:42,589][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:36:42,589][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:36:42,589][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-12 18:36:42,589][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 18:36:42,589][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-12 18:36:42,589][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:36:42,589][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:36:42,590][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:36:42,590][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:36:42,590][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:36:42,590][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-12 18:36:42,590][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 18:36:42,590][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-12 18:36:42,590][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 18:36:42,590][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:36:42,591][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:36:42,591][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:36:47,345][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:36:47,348][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:36:47,348][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:36:47,348][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:29,  1.21s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:40,  1.78it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:19,  3.71it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.33it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.63it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.64it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.81it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  7.81it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:07,  8.54it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.08it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.46it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.73it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.93it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.06it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.16it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.23it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.94it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.86it/s]
[2025-04-12 18:36:57,959][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1499
[2025-04-12 18:36:58,155][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1009, Metrics: {'mse': 0.0998566597700119, 'rmse': 0.31600104393816786, 'r2': -0.6274563074111938}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:16,  4.62it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.83it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-12 18:37:05,964][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0531
[2025-04-12 18:37:06,166][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0418, Metrics: {'mse': 0.041176687926054, 'rmse': 0.2029203980038823, 'r2': 0.32890546321868896}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.67it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.87it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.99it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.53it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 18:37:14,185][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0348
[2025-04-12 18:37:14,414][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0293, Metrics: {'mse': 0.028884923085570335, 'rmse': 0.16995565034905527, 'r2': 0.5292356610298157}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.31it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.59it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.81it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:37:22,211][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0275
[2025-04-12 18:37:22,440][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0277, Metrics: {'mse': 0.027499238029122353, 'rmse': 0.16582894207321697, 'r2': 0.5518194437026978}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.30it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.59it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.81it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.11it/s]
[2025-04-12 18:37:30,284][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0227
[2025-04-12 18:37:30,527][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0152, Metrics: {'mse': 0.015124274417757988, 'rmse': 0.12298078881580646, 'r2': 0.7535057067871094}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.39it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.66it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.77it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:37:38,316][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0196
[2025-04-12 18:37:38,560][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0181, Metrics: {'mse': 0.018014637753367424, 'rmse': 0.1342186192499663, 'r2': 0.7063987851142883}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.53it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.76it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.92it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.33it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.34it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.36it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:37:45,922][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0170
[2025-04-12 18:37:46,166][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0219, Metrics: {'mse': 0.021557528525590897, 'rmse': 0.14682482257980392, 'r2': 0.648656964302063}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.56it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.78it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.93it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 18:37:53,523][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0169
[2025-04-12 18:37:53,768][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0204, Metrics: {'mse': 0.020431695505976677, 'rmse': 0.14293948197043627, 'r2': 0.6670057773590088}
[2025-04-12 18:37:53,768][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▂▂▁
wandb:     best_val_mse █▃▂▂▁
wandb:      best_val_r2 ▁▆▇▇█
wandb:    best_val_rmse █▄▃▃▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▂▂▁▁▂▁
wandb:          val_mse █▃▂▂▁▁▂▁
wandb:           val_r2 ▁▆▇▇██▇█
wandb:         val_rmse █▄▃▃▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01521
wandb:     best_val_mse 0.01512
wandb:      best_val_r2 0.75351
wandb:    best_val_rmse 0.12298
wandb:            epoch 8
wandb:   final_test_mse 0.13996
wandb:    final_test_r2 -1.41284
wandb:  final_test_rmse 0.37411
wandb:  final_train_mse 0.01217
wandb:   final_train_r2 0.69629
wandb: final_train_rmse 0.11031
wandb:    final_val_mse 0.01512
wandb:     final_val_r2 0.75351
wandb:   final_val_rmse 0.12298
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01691
wandb:       train_time 64.2782
wandb:         val_loss 0.02044
wandb:          val_mse 0.02043
wandb:           val_r2 0.66701
wandb:         val_rmse 0.14294
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_183634-6tnoy75o
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_183634-6tnoy75o/logs
Cross-lingual experiment for complexity (ja → ar) completed successfully
Running cross-lingual question_type from ja to en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:38:15,828][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ja_to_en/question_type
experiment_name: cross_lingual_question_type_ja_to_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ja
  eval_language: en
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:38:15,828][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:38:15,829][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:38:15,829][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:38:17,343][__main__][INFO] - Running cross-lingual experiment: ja -> en
[2025-04-12 18:38:17,343][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:38:17,344][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:38:20,222][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:38:20,222][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:38:20,399][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:38:20,428][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:38:20,516][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:38:20,529][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:38:20,530][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:38:20,531][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:38:20,553][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:38:20,581][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:38:20,594][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:38:20,596][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:38:20,596][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:38:20,597][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:38:20,619][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:38:20,645][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:38:20,657][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:38:20,658][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:38:20,659][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:38:20,660][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:38:20,661][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:38:20,661][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:38:20,661][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:38:20,661][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:38:20,661][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-12 18:38:20,661][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 18:38:20,661][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:38:20,662][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:38:20,662][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:38:20,662][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:38:20,662][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:38:20,662][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:38:20,662][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-12 18:38:20,662][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-12 18:38:20,662][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:38:20,662][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:38:20,663][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:38:20,663][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:38:20,663][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:38:20,663][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:38:20,663][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-12 18:38:20,663][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-12 18:38:20,663][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:38:20,664][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:38:20,664][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:38:20,664][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:38:20,664][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:38:20,665][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
[2025-04-12 18:38:23,490][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:38:23,490][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:38:23,518][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:38:23,555][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:38:23,572][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 18:38:23,581][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:38:23,581][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 18:38:23,583][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:38:23,605][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:38:23,640][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:38:23,656][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 18:38:23,657][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:38:23,657][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 18:38:23,659][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:38:23,681][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:38:23,715][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:38:23,730][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 18:38:23,731][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:38:23,731][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 18:38:23,733][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 18:38:23,733][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:38:23,734][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:38:23,734][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:38:23,734][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:38:23,734][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-12 18:38:23,734][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 18:38:23,734][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 18:38:23,734][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:38:23,734][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:38:23,735][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:38:23,735][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:38:23,735][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:38:23,735][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:38:23,735][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:38:23,735][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 18:38:23,735][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:38:23,735][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:38:23,736][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:38:23,736][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:38:23,736][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:38:23,736][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:38:23,736][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:38:23,736][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 18:38:23,736][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:38:23,736][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 18:38:23,736][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:38:23,737][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:38:23,737][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:38:28,941][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:38:28,944][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:38:28,944][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:38:28,944][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:57,  1.59s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:52,  1.40it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:23,  3.05it/s]Epoch 1/10:   8%|▊         | 6/75 [00:02<00:15,  4.58it/s]Epoch 1/10:  11%|█         | 8/75 [00:02<00:11,  5.91it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:09,  7.02it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.51it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:08,  7.55it/s]Epoch 1/10:  20%|██        | 15/75 [00:03<00:07,  8.35it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:03<00:06,  8.93it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.35it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.65it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.87it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:04<00:04, 10.03it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:04<00:04, 10.14it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.21it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.27it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.94it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.49it/s]
[2025-04-12 18:38:40,079][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6804
[2025-04-12 18:38:40,285][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6727, Metrics: {'accuracy': 0.5869565217391305, 'f1': 0.3448275862068966}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.81it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.97it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.06it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.58it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.87it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-12 18:38:48,070][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5002
[2025-04-12 18:38:48,282][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2261, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.63it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.84it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 18:38:56,311][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1331
[2025-04-12 18:38:56,543][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0395, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.41it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.68it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:39:04,304][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0604
[2025-04-12 18:39:04,546][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0276, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.31it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.59it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.80it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.07it/s]
[2025-04-12 18:39:12,427][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0499
[2025-04-12 18:39:12,656][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0178, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:17,  4.28it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.57it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.80it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:39:20,432][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0373
[2025-04-12 18:39:20,669][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0087, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.38it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.65it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:39:28,476][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0406
[2025-04-12 18:39:28,715][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0091, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.43it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.69it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 18:39:36,108][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0295
[2025-04-12 18:39:36,342][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0498, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:16,  4.42it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.69it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.88it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.12it/s]
[2025-04-12 18:39:43,757][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0240
[2025-04-12 18:39:44,003][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0058, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:16,  4.41it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.68it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 18:39:51,778][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0274
[2025-04-12 18:39:52,021][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0061, Metrics: {'accuracy': 1.0, 'f1': 1.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██████
wandb:          best_val_f1 ▁██████
wandb:        best_val_loss █▃▁▁▁▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▆▂▁▁▁▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████████
wandb:               val_f1 ▁█████████
wandb:             val_loss █▃▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 1
wandb:          best_val_f1 1
wandb:        best_val_loss 0.0058
wandb:                epoch 10
wandb:  final_test_accuracy 0.91818
wandb:        final_test_f1 0.92437
wandb: final_train_accuracy 0.99748
wandb:       final_train_f1 0.99748
wandb:   final_val_accuracy 1
wandb:         final_val_f1 1
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02743
wandb:           train_time 80.78356
wandb:         val_accuracy 1
wandb:               val_f1 1
wandb:             val_loss 0.00609
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_183815-wmh35h2m
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_183815-wmh35h2m/logs
Cross-lingual experiment for question_type (ja → en) completed successfully
Running cross-lingual complexity from ja to en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:40:13,028][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ja_to_en/complexity
experiment_name: cross_lingual_complexity_ja_to_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ja
  eval_language: en
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:40:13,028][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:40:13,028][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:40:13,028][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:40:14,487][__main__][INFO] - Running cross-lingual experiment: ja -> en
[2025-04-12 18:40:14,488][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:40:14,488][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:40:17,349][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:40:17,350][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:40:17,436][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:40:17,468][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:40:17,573][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:40:17,584][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:40:17,585][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:40:17,586][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:40:17,611][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:40:17,649][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:40:17,665][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:40:17,666][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:40:17,667][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:40:17,668][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:40:17,691][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:40:17,727][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:40:17,742][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:40:17,743][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:40:17,744][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:40:17,745][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:40:17,745][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:40:17,746][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:40:17,746][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:40:17,746][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:40:17,746][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:40:17,746][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-12 18:40:17,746][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:40:17,746][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-04-12 18:40:17,747][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:40:17,747][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:40:17,747][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:40:17,747][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:40:17,747][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:40:17,747][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-12 18:40:17,747][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:40:17,748][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-12 18:40:17,748][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:40:17,748][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:40:17,748][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:40:17,748][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:40:17,748][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:40:17,748][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-12 18:40:17,749][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:40:17,749][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-12 18:40:17,749][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:40:17,749][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:40:17,749][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:40:17,749][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
[2025-04-12 18:40:20,596][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:40:20,597][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:40:20,621][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:40:20,655][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:40:20,670][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 18:40:20,679][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:40:20,680][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 18:40:20,681][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:40:20,705][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:40:20,740][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:40:20,755][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 18:40:20,756][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:40:20,756][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 18:40:20,757][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:40:20,780][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:40:20,814][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:40:20,828][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 18:40:20,829][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:40:20,829][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 18:40:20,831][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 18:40:20,831][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:40:20,831][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:40:20,831][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:40:20,832][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:40:20,832][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:40:20,832][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-12 18:40:20,832][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 18:40:20,832][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-12 18:40:20,832][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:40:20,832][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:40:20,833][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:40:20,833][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:40:20,833][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:40:20,833][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-12 18:40:20,833][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 18:40:20,833][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-12 18:40:20,833][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:40:20,834][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:40:20,834][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:40:20,834][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:40:20,834][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:40:20,834][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-12 18:40:20,834][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 18:40:20,834][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-12 18:40:20,834][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 18:40:20,834][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:40:20,835][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:40:20,835][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:40:25,841][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:40:25,844][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:40:25,844][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:40:25,845][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:32,  1.25s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:28,  2.53it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:16,  4.14it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.56it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  6.74it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.74it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:08,  7.62it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:07,  8.34it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  8.89it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:06,  9.30it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.61it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.83it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:05,  9.99it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.11it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.19it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.25it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.29it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.32it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.35it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.37it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.37it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.37it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.93it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.84it/s]
[2025-04-12 18:40:36,517][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1499
[2025-04-12 18:40:36,715][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1009, Metrics: {'mse': 0.0998566597700119, 'rmse': 0.31600104393816786, 'r2': -0.6274563074111938}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.71it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.90it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.01it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.54it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.32it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.34it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.35it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.37it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.38it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.01it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 18:40:44,527][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0531
[2025-04-12 18:40:44,736][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0418, Metrics: {'mse': 0.041176687926054, 'rmse': 0.2029203980038823, 'r2': 0.32890546321868896}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.29it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.58it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.80it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:40:52,766][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0348
[2025-04-12 18:40:52,990][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0293, Metrics: {'mse': 0.028884923085570335, 'rmse': 0.16995565034905527, 'r2': 0.5292356610298157}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.38it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.64it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.77it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.14it/s]
[2025-04-12 18:41:00,778][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0275
[2025-04-12 18:41:01,010][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0277, Metrics: {'mse': 0.027499238029122353, 'rmse': 0.16582894207321697, 'r2': 0.5518194437026978}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.44it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.70it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.24it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.37it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:41:08,803][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0227
[2025-04-12 18:41:09,043][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0152, Metrics: {'mse': 0.015124274417757988, 'rmse': 0.12298078881580646, 'r2': 0.7535057067871094}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.39it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.66it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:41:16,822][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0196
[2025-04-12 18:41:17,057][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0181, Metrics: {'mse': 0.018014637753367424, 'rmse': 0.1342186192499663, 'r2': 0.7063987851142883}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.40it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.66it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.14it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.21it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.88it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 18:41:24,449][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0170
[2025-04-12 18:41:24,695][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0219, Metrics: {'mse': 0.021557528525590897, 'rmse': 0.14682482257980392, 'r2': 0.648656964302063}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.46it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.71it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.88it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 18:41:32,054][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0169
[2025-04-12 18:41:32,293][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0204, Metrics: {'mse': 0.020431695505976677, 'rmse': 0.14293948197043627, 'r2': 0.6670057773590088}
[2025-04-12 18:41:32,294][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▂▂▁
wandb:     best_val_mse █▃▂▂▁
wandb:      best_val_r2 ▁▆▇▇█
wandb:    best_val_rmse █▄▃▃▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▂▂▁▁▂▁
wandb:          val_mse █▃▂▂▁▁▂▁
wandb:           val_r2 ▁▆▇▇██▇█
wandb:         val_rmse █▄▃▃▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01521
wandb:     best_val_mse 0.01512
wandb:      best_val_r2 0.75351
wandb:    best_val_rmse 0.12298
wandb:            epoch 8
wandb:   final_test_mse 0.04621
wandb:    final_test_r2 -0.19915
wandb:  final_test_rmse 0.21497
wandb:  final_train_mse 0.01217
wandb:   final_train_r2 0.69629
wandb: final_train_rmse 0.11031
wandb:    final_val_mse 0.01512
wandb:     final_val_r2 0.75351
wandb:   final_val_rmse 0.12298
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01691
wandb:       train_time 64.25942
wandb:         val_loss 0.02044
wandb:          val_mse 0.02043
wandb:           val_r2 0.66701
wandb:         val_rmse 0.14294
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_184013-bj0fb65x
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_184013-bj0fb65x/logs
Cross-lingual experiment for complexity (ja → en) completed successfully
Running cross-lingual question_type from ja to fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:41:53,476][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ja_to_fi/question_type
experiment_name: cross_lingual_question_type_ja_to_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ja
  eval_language: fi
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:41:53,476][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:41:53,476][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:41:53,476][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:41:55,032][__main__][INFO] - Running cross-lingual experiment: ja -> fi
[2025-04-12 18:41:55,032][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:41:55,032][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:41:57,931][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:41:57,932][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:41:58,034][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:41:58,065][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:41:58,159][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:41:58,170][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:41:58,170][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:41:58,171][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:41:58,194][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:41:58,222][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:41:58,236][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:41:58,237][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:41:58,237][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:41:58,239][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:41:58,260][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:41:58,289][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:41:58,301][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:41:58,302][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:41:58,303][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:41:58,304][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:41:58,304][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:41:58,304][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:41:58,304][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:41:58,305][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:41:58,305][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-12 18:41:58,305][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 18:41:58,305][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:41:58,305][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:41:58,305][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:41:58,305][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:41:58,306][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:41:58,306][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:41:58,306][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-12 18:41:58,306][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-12 18:41:58,306][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:41:58,306][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:41:58,306][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:41:58,306][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:41:58,307][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:41:58,307][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:41:58,307][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-12 18:41:58,307][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-12 18:41:58,307][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:41:58,307][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:41:58,307][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:41:58,307][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:41:58,308][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:41:58,308][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
[2025-04-12 18:42:01,132][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:42:01,133][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:42:01,157][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:42:01,200][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:42:01,216][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 18:42:01,226][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:42:01,226][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 18:42:01,228][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:42:01,256][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:42:01,297][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:42:01,314][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 18:42:01,315][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:42:01,316][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 18:42:01,317][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:42:01,347][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:42:01,384][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:42:01,399][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 18:42:01,401][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:42:01,401][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 18:42:01,402][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 18:42:01,403][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:42:01,403][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:42:01,403][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:42:01,403][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:42:01,403][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 18:42:01,403][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-12 18:42:01,403][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 18:42:01,403][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:42:01,404][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:42:01,404][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:42:01,404][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:42:01,404][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:42:01,404][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-12 18:42:01,404][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-12 18:42:01,404][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 18:42:01,404][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:42:01,405][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:42:01,405][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:42:01,405][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:42:01,405][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:42:01,405][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:42:01,405][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:42:01,405][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 18:42:01,405][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:42:01,405][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 18:42:01,406][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:42:01,406][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:42:01,406][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:42:06,344][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:42:06,347][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:42:06,348][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:42:06,348][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:32,  1.26s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:42,  1.73it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:19,  3.61it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.22it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.53it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.55it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.66it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:08,  7.68it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:07,  8.44it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.00it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.40it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.69it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.90it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.04it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.14it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.22it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.26it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.29it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.32it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.94it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.81it/s]
[2025-04-12 18:42:17,099][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6804
[2025-04-12 18:42:17,306][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6727, Metrics: {'accuracy': 0.5869565217391305, 'f1': 0.3448275862068966}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.74it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.92it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.02it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-12 18:42:25,111][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5002
[2025-04-12 18:42:25,324][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2261, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.45it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.70it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.88it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.46it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.13it/s]
[2025-04-12 18:42:33,378][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1331
[2025-04-12 18:42:33,805][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0395, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.39it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.65it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.33it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.35it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.36it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.37it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:42:41,569][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0604
[2025-04-12 18:42:41,802][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0276, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.30it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.58it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.80it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.75it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.13it/s]
[2025-04-12 18:42:49,637][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0499
[2025-04-12 18:42:49,867][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0178, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:17,  4.32it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.61it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:42:57,658][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0373
[2025-04-12 18:42:57,901][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0087, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:17,  4.31it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.60it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:43:05,699][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0406
[2025-04-12 18:43:05,946][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0091, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.50it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.74it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.91it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.33it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.34it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.36it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.37it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:43:13,319][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0295
[2025-04-12 18:43:13,560][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0498, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:16,  4.56it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.78it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.94it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:43:20,928][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0240
[2025-04-12 18:43:21,169][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0058, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:16,  4.45it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.69it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:43:28,956][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0274
[2025-04-12 18:43:29,198][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0061, Metrics: {'accuracy': 1.0, 'f1': 1.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██████
wandb:          best_val_f1 ▁██████
wandb:        best_val_loss █▃▁▁▁▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▆▂▁▁▁▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████████
wandb:               val_f1 ▁█████████
wandb:             val_loss █▃▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 1
wandb:          best_val_f1 1
wandb:        best_val_loss 0.0058
wandb:                epoch 10
wandb:  final_test_accuracy 0.80909
wandb:        final_test_f1 0.83721
wandb: final_train_accuracy 0.99748
wandb:       final_train_f1 0.99748
wandb:   final_val_accuracy 1
wandb:         final_val_f1 1
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02743
wandb:           train_time 80.61751
wandb:         val_accuracy 1
wandb:               val_f1 1
wandb:             val_loss 0.00609
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_184153-4ng55rw6
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_184153-4ng55rw6/logs
Cross-lingual experiment for question_type (ja → fi) completed successfully
Running cross-lingual complexity from ja to fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:43:50,343][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ja_to_fi/complexity
experiment_name: cross_lingual_complexity_ja_to_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ja
  eval_language: fi
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:43:50,343][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:43:50,343][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:43:50,343][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:43:51,825][__main__][INFO] - Running cross-lingual experiment: ja -> fi
[2025-04-12 18:43:51,825][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:43:51,826][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:43:54,652][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:43:54,652][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:43:54,871][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:43:54,905][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:43:55,009][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:43:55,020][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:43:55,020][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:43:55,022][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:43:55,046][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:43:55,080][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:43:55,095][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:43:55,096][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:43:55,096][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:43:55,097][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:43:55,122][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:43:55,157][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:43:55,171][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:43:55,173][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:43:55,173][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:43:55,174][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:43:55,175][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:43:55,175][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:43:55,175][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:43:55,175][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:43:55,175][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:43:55,176][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-12 18:43:55,176][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:43:55,176][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-04-12 18:43:55,176][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:43:55,176][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:43:55,176][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:43:55,176][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:43:55,177][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:43:55,177][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-12 18:43:55,177][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:43:55,177][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-12 18:43:55,177][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:43:55,177][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:43:55,177][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:43:55,177][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:43:55,178][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:43:55,178][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-12 18:43:55,178][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:43:55,178][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-12 18:43:55,178][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:43:55,178][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:43:55,178][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:43:55,179][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
[2025-04-12 18:43:57,967][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:43:57,968][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:43:57,992][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:43:58,030][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:43:58,047][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 18:43:58,056][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:43:58,057][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 18:43:58,058][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:43:58,087][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:43:58,123][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:43:58,138][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 18:43:58,139][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:43:58,140][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 18:43:58,141][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:43:58,163][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:43:58,193][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:43:58,206][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 18:43:58,208][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:43:58,208][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 18:43:58,209][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 18:43:58,210][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:43:58,210][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:43:58,210][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:43:58,210][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:43:58,210][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:43:58,210][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-12 18:43:58,211][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 18:43:58,211][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-12 18:43:58,211][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:43:58,211][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:43:58,211][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:43:58,211][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:43:58,211][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:43:58,212][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-12 18:43:58,212][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 18:43:58,212][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-12 18:43:58,212][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:43:58,212][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:43:58,212][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:43:58,212][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:43:58,212][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:43:58,213][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-12 18:43:58,213][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 18:43:58,213][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-12 18:43:58,213][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 18:43:58,213][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:43:58,213][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:43:58,214][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:44:03,241][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:44:03,244][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:44:03,244][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:44:03,244][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:24,  1.14s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:38,  1.89it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:18,  3.86it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.49it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.78it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.77it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.90it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  7.88it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.60it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.12it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:02<00:05,  9.50it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.77it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.95it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.09it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.18it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.25it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.29it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.33it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.41it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.95it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.94it/s]
[2025-04-12 18:44:14,357][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1499
[2025-04-12 18:44:14,554][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1009, Metrics: {'mse': 0.0998566597700119, 'rmse': 0.31600104393816786, 'r2': -0.6274563074111938}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.85it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  8.00it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.07it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:44:22,378][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0531
[2025-04-12 18:44:22,588][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0418, Metrics: {'mse': 0.041176687926054, 'rmse': 0.2029203980038823, 'r2': 0.32890546321868896}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.25it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.54it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.79it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.41it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:44:30,618][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0348
[2025-04-12 18:44:30,865][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0293, Metrics: {'mse': 0.028884923085570335, 'rmse': 0.16995565034905527, 'r2': 0.5292356610298157}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.43it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.70it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.88it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 18:44:38,622][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0275
[2025-04-12 18:44:38,862][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0277, Metrics: {'mse': 0.027499238029122353, 'rmse': 0.16582894207321697, 'r2': 0.5518194437026978}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.27it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.57it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.80it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 18:44:46,684][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0227
[2025-04-12 18:44:46,916][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0152, Metrics: {'mse': 0.015124274417757988, 'rmse': 0.12298078881580646, 'r2': 0.7535057067871094}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.38it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.66it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 18:44:54,691][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0196
[2025-04-12 18:44:54,922][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0181, Metrics: {'mse': 0.018014637753367424, 'rmse': 0.1342186192499663, 'r2': 0.7063987851142883}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.49it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.74it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.91it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 18:45:02,270][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0170
[2025-04-12 18:45:02,719][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0219, Metrics: {'mse': 0.021557528525590897, 'rmse': 0.14682482257980392, 'r2': 0.648656964302063}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:15,  4.84it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  8.00it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  9.07it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.41it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-12 18:45:10,052][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0169
[2025-04-12 18:45:10,291][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0204, Metrics: {'mse': 0.020431695505976677, 'rmse': 0.14293948197043627, 'r2': 0.6670057773590088}
[2025-04-12 18:45:10,292][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▂▂▁
wandb:     best_val_mse █▃▂▂▁
wandb:      best_val_r2 ▁▆▇▇█
wandb:    best_val_rmse █▄▃▃▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▂▂▁▁▂▁
wandb:          val_mse █▃▂▂▁▁▂▁
wandb:           val_r2 ▁▆▇▇██▇█
wandb:         val_rmse █▄▃▃▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01521
wandb:     best_val_mse 0.01512
wandb:      best_val_r2 0.75351
wandb:    best_val_rmse 0.12298
wandb:            epoch 8
wandb:   final_test_mse 0.04828
wandb:    final_test_r2 -0.22257
wandb:  final_test_rmse 0.21972
wandb:  final_train_mse 0.01217
wandb:   final_train_r2 0.69629
wandb: final_train_rmse 0.11031
wandb:    final_val_mse 0.01512
wandb:     final_val_r2 0.75351
wandb:   final_val_rmse 0.12298
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01691
wandb:       train_time 64.32454
wandb:         val_loss 0.02044
wandb:          val_mse 0.02043
wandb:           val_r2 0.66701
wandb:         val_rmse 0.14294
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_184350-yq3je6xx
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_184350-yq3je6xx/logs
Cross-lingual experiment for complexity (ja → fi) completed successfully
Running cross-lingual question_type from ja to id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:45:31,376][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ja_to_id/question_type
experiment_name: cross_lingual_question_type_ja_to_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ja
  eval_language: id
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:45:31,376][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:45:31,376][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:45:31,376][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:45:32,942][__main__][INFO] - Running cross-lingual experiment: ja -> id
[2025-04-12 18:45:32,943][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:45:32,943][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:45:35,900][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:45:35,900][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:45:36,021][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:45:36,052][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:45:36,159][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:45:36,170][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:45:36,170][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:45:36,171][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:45:36,201][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:45:36,237][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:45:36,253][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:45:36,254][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:45:36,254][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:45:36,255][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:45:36,284][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:45:36,323][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:45:36,338][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:45:36,339][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:45:36,340][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:45:36,341][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:45:36,341][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:45:36,341][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:45:36,342][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:45:36,342][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:45:36,342][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-12 18:45:36,342][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 18:45:36,342][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:45:36,342][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:45:36,342][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:45:36,343][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:45:36,343][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:45:36,343][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:45:36,343][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-12 18:45:36,343][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-12 18:45:36,343][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:45:36,343][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:45:36,343][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:45:36,344][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:45:36,344][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:45:36,344][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:45:36,344][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-12 18:45:36,344][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-12 18:45:36,344][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:45:36,344][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:45:36,344][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:45:36,345][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:45:36,345][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:45:36,345][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
[2025-04-12 18:45:39,162][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:45:39,163][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:45:39,191][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:45:39,228][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:45:39,244][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:45:39,252][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:45:39,252][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:45:39,254][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:45:39,281][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:45:39,319][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:45:39,335][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:45:39,337][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:45:39,337][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:45:39,338][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:45:39,373][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:45:39,425][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:45:39,441][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:45:39,442][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:45:39,443][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:45:39,444][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:45:39,445][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:45:39,445][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:45:39,445][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:45:39,445][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:45:39,445][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-12 18:45:39,446][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-12 18:45:39,446][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:45:39,446][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:45:39,446][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:45:39,446][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:45:39,446][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:45:39,446][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:45:39,446][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:45:39,447][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:45:39,447][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:45:39,447][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:45:39,447][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:45:39,447][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:45:39,447][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:45:39,447][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:45:39,447][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:45:39,448][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:45:39,448][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:45:39,448][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:45:39,448][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:45:39,448][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:45:39,448][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:45:39,448][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:45:44,534][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:45:44,536][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:45:44,537][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:45:44,537][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:32,  1.25s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:42,  1.73it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:19,  3.63it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.24it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.55it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.57it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.35it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.93it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.35it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.65it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.86it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.02it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.13it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.20it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.26it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.29it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00,  8.73it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:08<00:00,  9.17it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00,  9.51it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00,  9.76it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.81it/s]
[2025-04-12 18:45:55,397][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6804
[2025-04-12 18:45:55,621][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6727, Metrics: {'accuracy': 0.5869565217391305, 'f1': 0.3448275862068966}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.92it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.05it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.10it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.61it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.24it/s]
[2025-04-12 18:46:03,405][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5002
[2025-04-12 18:46:03,803][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2261, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.32it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.59it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.81it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:46:11,833][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1331
[2025-04-12 18:46:12,070][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0395, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.34it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.62it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 18:46:19,841][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0604
[2025-04-12 18:46:20,082][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0276, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.16it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.47it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:08,  8.73it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.36it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.72it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.94it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.09it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.18it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.24it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.28it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:46:27,895][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0499
[2025-04-12 18:46:28,166][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0178, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.37it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.64it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:46:35,948][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0373
[2025-04-12 18:46:36,190][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0087, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:17,  4.26it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.55it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.79it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:01<00:06,  9.75it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.28it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 18:46:44,015][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0406
[2025-04-12 18:46:44,250][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0091, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.53it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.76it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.92it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:46:51,615][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0295
[2025-04-12 18:46:51,858][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0498, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:15,  4.67it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.86it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.99it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.53it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 18:46:59,209][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0240
[2025-04-12 18:46:59,457][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0058, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:16,  4.39it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.65it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 18:47:07,259][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0274
[2025-04-12 18:47:07,505][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0061, Metrics: {'accuracy': 1.0, 'f1': 1.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██████
wandb:          best_val_f1 ▁██████
wandb:        best_val_loss █▃▁▁▁▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▆▂▁▁▁▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████████
wandb:               val_f1 ▁█████████
wandb:             val_loss █▃▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 1
wandb:          best_val_f1 1
wandb:        best_val_loss 0.0058
wandb:                epoch 10
wandb:  final_test_accuracy 0.84545
wandb:        final_test_f1 0.83495
wandb: final_train_accuracy 0.99748
wandb:       final_train_f1 0.99748
wandb:   final_val_accuracy 1
wandb:         final_val_f1 1
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02743
wandb:           train_time 80.6253
wandb:         val_accuracy 1
wandb:               val_f1 1
wandb:             val_loss 0.00609
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_184531-4zanx4gs
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_184531-4zanx4gs/logs
Cross-lingual experiment for question_type (ja → id) completed successfully
Running cross-lingual complexity from ja to id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:47:28,955][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ja_to_id/complexity
experiment_name: cross_lingual_complexity_ja_to_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ja
  eval_language: id
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:47:28,955][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:47:28,956][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:47:28,956][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:47:30,696][__main__][INFO] - Running cross-lingual experiment: ja -> id
[2025-04-12 18:47:30,696][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:47:30,696][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:47:33,587][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:47:33,587][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:47:33,656][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:47:33,683][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:47:33,789][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:47:33,799][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:47:33,800][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:47:33,801][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:47:33,823][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:47:33,853][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:47:33,867][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:47:33,868][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:47:33,868][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:47:33,869][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:47:33,893][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:47:33,925][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:47:33,940][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:47:33,942][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:47:33,942][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:47:33,943][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:47:33,944][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:47:33,944][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:47:33,944][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:47:33,944][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:47:33,944][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:47:33,944][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-12 18:47:33,945][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:47:33,945][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-04-12 18:47:33,945][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:47:33,945][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:47:33,945][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:47:33,945][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:47:33,945][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:47:33,945][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-12 18:47:33,946][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:47:33,946][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-12 18:47:33,946][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:47:33,946][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:47:33,946][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:47:33,946][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:47:33,946][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:47:33,947][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-12 18:47:33,947][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:47:33,947][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-12 18:47:33,947][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:47:33,947][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:47:33,947][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:47:33,948][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
[2025-04-12 18:47:36,767][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:47:36,768][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:47:36,796][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:47:36,834][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:47:36,852][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 18:47:36,859][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:47:36,860][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 18:47:36,861][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:47:36,887][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:47:36,924][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:47:36,940][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 18:47:36,941][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:47:36,942][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 18:47:36,942][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:47:36,969][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:47:37,006][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:47:37,020][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 18:47:37,021][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:47:37,021][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 18:47:37,022][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 18:47:37,023][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:47:37,023][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:47:37,023][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:47:37,023][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:47:37,023][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:47:37,024][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-12 18:47:37,024][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 18:47:37,024][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-12 18:47:37,024][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:47:37,024][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:47:37,024][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:47:37,024][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:47:37,024][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:47:37,025][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-12 18:47:37,025][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 18:47:37,025][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-12 18:47:37,025][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:47:37,025][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:47:37,025][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:47:37,025][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:47:37,025][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:47:37,026][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-12 18:47:37,026][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 18:47:37,026][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-12 18:47:37,026][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 18:47:37,026][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:47:37,026][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:47:37,027][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:47:42,144][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:47:42,147][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:47:42,147][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:47:42,147][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:29,  1.22s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:41,  1.78it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:19,  3.70it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.32it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.62it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.64it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.40it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.97it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.38it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.68it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.89it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.05it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.15it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.23it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.28it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.34it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.39it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.39it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.33it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.35it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.37it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00,  8.74it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00,  9.18it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00,  9.52it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00,  9.77it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.85it/s]
[2025-04-12 18:47:52,781][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1499
[2025-04-12 18:47:52,983][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1009, Metrics: {'mse': 0.0998566597700119, 'rmse': 0.31600104393816786, 'r2': -0.6274563074111938}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.80it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.97it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.06it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.58it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.09it/s]
[2025-04-12 18:48:00,894][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0531
[2025-04-12 18:48:01,108][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0418, Metrics: {'mse': 0.041176687926054, 'rmse': 0.2029203980038823, 'r2': 0.32890546321868896}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:15,  4.64it/s]Epoch 3/10:   3%|▎         | 2/75 [00:00<00:17,  4.18it/s]Epoch 3/10:   5%|▌         | 4/75 [00:00<00:10,  6.69it/s]Epoch 3/10:   8%|▊         | 6/75 [00:00<00:08,  8.05it/s]Epoch 3/10:  11%|█         | 8/75 [00:01<00:07,  8.85it/s]Epoch 3/10:  13%|█▎        | 10/75 [00:01<00:06,  9.36it/s]Epoch 3/10:  16%|█▌        | 12/75 [00:01<00:06,  9.69it/s]Epoch 3/10:  19%|█▊        | 14/75 [00:01<00:06,  9.91it/s]Epoch 3/10:  21%|██▏       | 16/75 [00:01<00:05, 10.07it/s]Epoch 3/10:  24%|██▍       | 18/75 [00:02<00:05, 10.17it/s]Epoch 3/10:  27%|██▋       | 20/75 [00:02<00:05, 10.24it/s]Epoch 3/10:  29%|██▉       | 22/75 [00:02<00:05, 10.29it/s]Epoch 3/10:  32%|███▏      | 24/75 [00:02<00:04, 10.32it/s]Epoch 3/10:  35%|███▍      | 26/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  37%|███▋      | 28/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  40%|████      | 30/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  43%|████▎     | 32/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  45%|████▌     | 34/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  48%|████▊     | 36/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  51%|█████     | 38/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  59%|█████▊    | 44/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.41it/s]Epoch 3/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.41it/s]Epoch 3/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.41it/s]Epoch 3/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  80%|████████  | 60/75 [00:06<00:01, 10.41it/s]Epoch 3/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.41it/s]Epoch 3/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  91%|█████████ | 68/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.41it/s]Epoch 3/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.41it/s]Epoch 3/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.00it/s]
[2025-04-12 18:48:09,259][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0348
[2025-04-12 18:48:09,498][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0293, Metrics: {'mse': 0.028884923085570335, 'rmse': 0.16995565034905527, 'r2': 0.5292356610298157}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.36it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.64it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:48:17,264][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0275
[2025-04-12 18:48:17,506][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0277, Metrics: {'mse': 0.027499238029122353, 'rmse': 0.16582894207321697, 'r2': 0.5518194437026978}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.25it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.56it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.79it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:48:25,306][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0227
[2025-04-12 18:48:25,549][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0152, Metrics: {'mse': 0.015124274417757988, 'rmse': 0.12298078881580646, 'r2': 0.7535057067871094}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:17,  4.33it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.60it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:06,  9.78it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 18:48:33,349][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0196
[2025-04-12 18:48:33,782][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0181, Metrics: {'mse': 0.018014637753367424, 'rmse': 0.1342186192499663, 'r2': 0.7063987851142883}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.50it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.75it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.92it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.34it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.36it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.13it/s]
[2025-04-12 18:48:41,188][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0170
[2025-04-12 18:48:41,435][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0219, Metrics: {'mse': 0.021557528525590897, 'rmse': 0.14682482257980392, 'r2': 0.648656964302063}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.54it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.77it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.93it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 18:48:48,782][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0169
[2025-04-12 18:48:49,031][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0204, Metrics: {'mse': 0.020431695505976677, 'rmse': 0.14293948197043627, 'r2': 0.6670057773590088}
[2025-04-12 18:48:49,031][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▂▂▁
wandb:     best_val_mse █▃▂▂▁
wandb:      best_val_r2 ▁▆▇▇█
wandb:    best_val_rmse █▄▃▃▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▂▂▁▁▂▁
wandb:          val_mse █▃▂▂▁▁▂▁
wandb:           val_r2 ▁▆▇▇██▇█
wandb:         val_rmse █▄▃▃▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01521
wandb:     best_val_mse 0.01512
wandb:      best_val_r2 0.75351
wandb:    best_val_rmse 0.12298
wandb:            epoch 8
wandb:   final_test_mse 0.06347
wandb:    final_test_r2 -0.55652
wandb:  final_test_rmse 0.25192
wandb:  final_train_mse 0.01217
wandb:   final_train_r2 0.69629
wandb: final_train_rmse 0.11031
wandb:    final_val_mse 0.01512
wandb:     final_val_r2 0.75351
wandb:   final_val_rmse 0.12298
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01691
wandb:       train_time 64.72287
wandb:         val_loss 0.02044
wandb:          val_mse 0.02043
wandb:           val_r2 0.66701
wandb:         val_rmse 0.14294
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_184729-zwpip3op
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_184729-zwpip3op/logs
Cross-lingual experiment for complexity (ja → id) completed successfully
Running cross-lingual question_type from ja to ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:49:10,309][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ja_to_ko/question_type
experiment_name: cross_lingual_question_type_ja_to_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ja
  eval_language: ko
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:49:10,309][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:49:10,309][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:49:10,309][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:49:11,967][__main__][INFO] - Running cross-lingual experiment: ja -> ko
[2025-04-12 18:49:11,967][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:49:11,967][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:49:14,825][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:49:14,825][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:49:14,959][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:49:14,993][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:49:15,097][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:49:15,108][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:49:15,109][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:49:15,110][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:49:15,134][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:49:15,167][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:49:15,182][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:49:15,183][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:49:15,184][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:49:15,184][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:49:15,209][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:49:15,243][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:49:15,256][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:49:15,258][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:49:15,258][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:49:15,259][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:49:15,260][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:49:15,260][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:49:15,260][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:49:15,260][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:49:15,261][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-12 18:49:15,261][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 18:49:15,261][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:49:15,261][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:49:15,261][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:49:15,261][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:49:15,261][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:49:15,261][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:49:15,262][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-12 18:49:15,262][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-12 18:49:15,262][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:49:15,262][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:49:15,262][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:49:15,262][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:49:15,262][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:49:15,262][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:49:15,263][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-12 18:49:15,263][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-12 18:49:15,263][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:49:15,263][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:49:15,263][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:49:15,263][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:49:15,263][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:49:15,264][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
[2025-04-12 18:49:18,066][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:49:18,066][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:49:18,091][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:49:18,126][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:49:18,142][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 18:49:18,149][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:49:18,149][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 18:49:18,150][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:49:18,177][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:49:18,208][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:49:18,223][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 18:49:18,224][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:49:18,224][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 18:49:18,225][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:49:18,249][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:49:18,285][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:49:18,301][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 18:49:18,303][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:49:18,303][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 18:49:18,304][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 18:49:18,304][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:49:18,304][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:49:18,305][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:49:18,305][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:49:18,305][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-12 18:49:18,305][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-12 18:49:18,305][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 18:49:18,305][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:49:18,305][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:49:18,306][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:49:18,306][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:49:18,306][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:49:18,306][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:49:18,306][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:49:18,306][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 18:49:18,306][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:49:18,306][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:49:18,307][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:49:18,307][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:49:18,307][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:49:18,307][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:49:18,307][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:49:18,307][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 18:49:18,307][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:49:18,307][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 18:49:18,307][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:49:18,308][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:49:18,308][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:49:23,357][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:49:23,360][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:49:23,360][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:49:23,360][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:42,  1.39s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:31,  2.31it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:18,  3.85it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.25it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:10,  6.45it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.44it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.22it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.81it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.25it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.58it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.82it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.99it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.10it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.19it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.25it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.29it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00,  8.87it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:08<00:00,  9.28it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00,  9.59it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00,  9.82it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.48it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.68it/s]
[2025-04-12 18:49:34,250][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6804
[2025-04-12 18:49:34,452][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6727, Metrics: {'accuracy': 0.5869565217391305, 'f1': 0.3448275862068966}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.76it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.94it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.03it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-12 18:49:42,249][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5002
[2025-04-12 18:49:42,465][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2261, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.59it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.81it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.95it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:49:50,473][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1331
[2025-04-12 18:49:50,724][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0395, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.40it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.67it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:49:58,490][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0604
[2025-04-12 18:49:58,727][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0276, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.56it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.78it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.93it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.12it/s]
[2025-04-12 18:50:06,574][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0499
[2025-04-12 18:50:06,809][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0178, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:17,  4.32it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.61it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.04it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.15it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.23it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.28it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 18:50:14,607][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0373
[2025-04-12 18:50:14,854][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0087, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:17,  4.28it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.58it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.80it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 18:50:22,656][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0406
[2025-04-12 18:50:22,897][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0091, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.46it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.71it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.89it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:50:30,269][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0295
[2025-04-12 18:50:30,519][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0498, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:16,  4.48it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.72it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.89it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:50:37,882][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0240
[2025-04-12 18:50:38,129][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0058, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:17,  4.28it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.56it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.79it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:01<00:06,  9.75it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:50:45,937][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0274
[2025-04-12 18:50:46,187][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0061, Metrics: {'accuracy': 1.0, 'f1': 1.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██████
wandb:          best_val_f1 ▁██████
wandb:        best_val_loss █▃▁▁▁▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▆▂▁▁▁▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████████
wandb:               val_f1 ▁█████████
wandb:             val_loss █▃▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 1
wandb:          best_val_f1 1
wandb:        best_val_loss 0.0058
wandb:                epoch 10
wandb:  final_test_accuracy 0.87273
wandb:        final_test_f1 0.86792
wandb: final_train_accuracy 0.99748
wandb:       final_train_f1 0.99748
wandb:   final_val_accuracy 1
wandb:         final_val_f1 1
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02743
wandb:           train_time 80.58089
wandb:         val_accuracy 1
wandb:               val_f1 1
wandb:             val_loss 0.00609
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_184910-pgham2em
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_184910-pgham2em/logs
Cross-lingual experiment for question_type (ja → ko) completed successfully
Running cross-lingual complexity from ja to ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:51:06,593][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ja_to_ko/complexity
experiment_name: cross_lingual_complexity_ja_to_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ja
  eval_language: ko
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:51:06,593][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:51:06,594][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:51:06,594][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:51:08,161][__main__][INFO] - Running cross-lingual experiment: ja -> ko
[2025-04-12 18:51:08,161][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:51:08,162][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:51:11,059][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:51:11,060][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:51:11,178][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:51:11,211][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:51:11,317][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:51:11,328][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:51:11,328][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:51:11,329][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:51:11,354][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:51:11,389][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:51:11,406][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:51:11,408][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:51:11,408][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:51:11,409][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:51:11,434][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:51:11,471][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:51:11,486][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:51:11,487][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:51:11,488][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:51:11,489][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:51:11,489][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:51:11,489][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:51:11,490][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:51:11,490][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:51:11,490][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:51:11,490][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-12 18:51:11,490][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:51:11,490][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-04-12 18:51:11,491][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:51:11,491][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:51:11,491][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:51:11,491][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:51:11,491][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:51:11,491][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-12 18:51:11,491][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:51:11,491][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-12 18:51:11,492][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:51:11,492][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:51:11,492][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:51:11,492][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:51:11,492][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:51:11,492][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-12 18:51:11,492][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:51:11,493][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-12 18:51:11,493][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:51:11,493][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:51:11,493][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:51:11,493][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
[2025-04-12 18:51:14,340][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:51:14,341][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:51:14,367][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:51:14,406][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:51:14,423][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 18:51:14,429][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:51:14,430][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 18:51:14,431][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:51:14,456][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:51:14,493][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:51:14,508][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 18:51:14,509][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:51:14,509][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 18:51:14,511][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:51:14,536][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:51:14,572][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:51:14,587][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 18:51:14,588][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:51:14,588][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 18:51:14,589][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 18:51:14,590][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:51:14,590][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:51:14,590][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:51:14,590][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:51:14,590][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:51:14,590][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-12 18:51:14,590][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 18:51:14,591][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-04-12 18:51:14,591][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:51:14,591][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:51:14,591][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:51:14,591][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:51:14,591][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:51:14,591][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-12 18:51:14,592][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 18:51:14,592][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-12 18:51:14,592][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:51:14,592][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:51:14,592][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:51:14,592][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:51:14,592][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:51:14,593][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-12 18:51:14,593][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 18:51:14,593][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-12 18:51:14,593][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 18:51:14,593][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:51:14,593][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:51:14,593][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:51:19,830][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:51:19,833][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:51:19,833][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:51:19,833][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:48,  1.46s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:48,  1.51it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:21,  3.24it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:14,  4.80it/s]Epoch 1/10:  11%|█         | 8/75 [00:02<00:10,  6.13it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:09,  7.21it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.06it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:07,  8.70it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.18it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:03<00:05,  9.53it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.78it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.96it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.09it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.19it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:04<00:04, 10.25it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.29it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.30it/s]Epoch 1/10:  51%|█████     | 38/75 [00:05<00:03, 10.33it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.35it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  80%|████████  | 60/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01,  9.75it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01,  9.93it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.07it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:08<00:00,  8.78it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:08<00:00,  9.21it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00,  9.54it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00,  9.78it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.57it/s]
[2025-04-12 18:51:30,620][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1499
[2025-04-12 18:51:30,844][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1009, Metrics: {'mse': 0.0998566597700119, 'rmse': 0.31600104393816786, 'r2': -0.6274563074111938}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.78it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.95it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.04it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.57it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.87it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 18:51:38,705][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0531
[2025-04-12 18:51:38,925][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0418, Metrics: {'mse': 0.041176687926054, 'rmse': 0.2029203980038823, 'r2': 0.32890546321868896}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.32it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.60it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:51:46,942][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0348
[2025-04-12 18:51:47,174][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0293, Metrics: {'mse': 0.028884923085570335, 'rmse': 0.16995565034905527, 'r2': 0.5292356610298157}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.32it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.60it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:51:54,933][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0275
[2025-04-12 18:51:55,180][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0277, Metrics: {'mse': 0.027499238029122353, 'rmse': 0.16582894207321697, 'r2': 0.5518194437026978}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.28it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.58it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.79it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:52:02,982][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0227
[2025-04-12 18:52:03,230][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0152, Metrics: {'mse': 0.015124274417757988, 'rmse': 0.12298078881580646, 'r2': 0.7535057067871094}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:17,  4.32it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.61it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 18:52:11,036][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0196
[2025-04-12 18:52:11,276][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0181, Metrics: {'mse': 0.018014637753367424, 'rmse': 0.1342186192499663, 'r2': 0.7063987851142883}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:15,  4.80it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.97it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  9.05it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.58it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.87it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 18:52:18,628][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0170
[2025-04-12 18:52:18,872][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0219, Metrics: {'mse': 0.021557528525590897, 'rmse': 0.14682482257980392, 'r2': 0.648656964302063}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.43it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.69it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.88it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.34it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.36it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.04it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:52:26,239][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0169
[2025-04-12 18:52:26,487][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0204, Metrics: {'mse': 0.020431695505976677, 'rmse': 0.14293948197043627, 'r2': 0.6670057773590088}
[2025-04-12 18:52:26,488][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▂▂▁
wandb:     best_val_mse █▃▂▂▁
wandb:      best_val_r2 ▁▆▇▇█
wandb:    best_val_rmse █▄▃▃▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▂▂▁▁▂▁
wandb:          val_mse █▃▂▂▁▁▂▁
wandb:           val_r2 ▁▆▇▇██▇█
wandb:         val_rmse █▄▃▃▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01521
wandb:     best_val_mse 0.01512
wandb:      best_val_r2 0.75351
wandb:    best_val_rmse 0.12298
wandb:            epoch 8
wandb:   final_test_mse 0.04133
wandb:    final_test_r2 -0.2832
wandb:  final_test_rmse 0.20329
wandb:  final_train_mse 0.01217
wandb:   final_train_r2 0.69629
wandb: final_train_rmse 0.11031
wandb:    final_val_mse 0.01512
wandb:     final_val_r2 0.75351
wandb:   final_val_rmse 0.12298
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01691
wandb:       train_time 64.62035
wandb:         val_loss 0.02044
wandb:          val_mse 0.02043
wandb:           val_r2 0.66701
wandb:         val_rmse 0.14294
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_185106-wzwx9oog
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_185106-wzwx9oog/logs
Cross-lingual experiment for complexity (ja → ko) completed successfully
Running cross-lingual question_type from ja to ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:52:47,363][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ja_to_ru/question_type
experiment_name: cross_lingual_question_type_ja_to_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ja
  eval_language: ru
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:52:47,364][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:52:47,364][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:52:47,364][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:52:49,021][__main__][INFO] - Running cross-lingual experiment: ja -> ru
[2025-04-12 18:52:49,022][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:52:49,022][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:52:51,848][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:52:51,848][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:52:51,941][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:52:51,974][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:52:52,082][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:52:52,093][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:52:52,094][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:52:52,095][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:52:52,119][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:52:52,149][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:52:52,163][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:52:52,164][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:52:52,164][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:52:52,165][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:52:52,188][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:52:52,223][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:52:52,237][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:52:52,239][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:52:52,239][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:52:52,240][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:52:52,240][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:52:52,240][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:52:52,240][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:52:52,241][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:52:52,241][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-12 18:52:52,241][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 18:52:52,241][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:52:52,241][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:52:52,241][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:52:52,242][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:52:52,242][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:52:52,242][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:52:52,242][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-12 18:52:52,242][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-12 18:52:52,242][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:52:52,242][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:52:52,242][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:52:52,242][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:52:52,243][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:52:52,243][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:52:52,243][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-12 18:52:52,243][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-12 18:52:52,243][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:52:52,243][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:52:52,243][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:52:52,243][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:52:52,244][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:52:52,244][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
[2025-04-12 18:52:55,030][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:52:55,031][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:52:55,055][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:52:55,089][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:52:55,105][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 18:52:55,115][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:52:55,116][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 18:52:55,117][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:52:55,140][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:52:55,174][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:52:55,189][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 18:52:55,190][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:52:55,190][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 18:52:55,192][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:52:55,213][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:52:55,247][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:52:55,261][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 18:52:55,263][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:52:55,263][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 18:52:55,264][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 18:52:55,265][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:52:55,265][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:52:55,265][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:52:55,265][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:52:55,265][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 18:52:55,266][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-12 18:52:55,266][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 18:52:55,266][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:52:55,266][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:52:55,266][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:52:55,266][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:52:55,266][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:52:55,266][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:52:55,266][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:52:55,267][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 18:52:55,267][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:52:55,267][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:52:55,267][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:52:55,267][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:52:55,267][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:52:55,267][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:52:55,267][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:52:55,268][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 18:52:55,268][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:52:55,268][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 18:52:55,268][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:52:55,268][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:52:55,268][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:53:00,600][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:53:00,603][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:53:00,603][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:53:00,603][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:37,  1.31s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:29,  2.42it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:17,  4.00it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.41it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:10,  6.60it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.65it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:08,  7.55it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:07,  8.27it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  8.84it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:06,  9.27it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.58it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.81it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:05,  9.98it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.10it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.19it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.24it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.29it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.32it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.35it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.38it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.93it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.77it/s]
[2025-04-12 18:53:11,286][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6804
[2025-04-12 18:53:11,515][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6727, Metrics: {'accuracy': 0.5869565217391305, 'f1': 0.3448275862068966}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.72it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.91it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.01it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.55it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:53:19,359][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5002
[2025-04-12 18:53:19,570][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.2261, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.34it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.62it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.83it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.36it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.37it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.11it/s]
[2025-04-12 18:53:27,647][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.1331
[2025-04-12 18:53:27,886][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0395, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.28it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.57it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.79it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.74it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.95it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.18it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.31it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.33it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.35it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.36it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.37it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.13it/s]
[2025-04-12 18:53:35,691][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0604
[2025-04-12 18:53:35,933][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0276, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.22it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.51it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.76it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.38it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.74it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.95it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:53:43,730][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0499
[2025-04-12 18:53:43,971][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0178, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:17,  4.19it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.49it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:08,  8.74it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.37it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:06,  9.73it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.95it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.09it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.34it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.36it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.37it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 18:53:51,774][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0373
[2025-04-12 18:53:52,018][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0087, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.36it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.63it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.83it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:53:59,820][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0406
[2025-04-12 18:54:00,068][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0091, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:18,  4.06it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.38it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:08,  8.67it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.32it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:01<00:06,  9.69it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.92it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.07it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.17it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.24it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.28it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.31it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.33it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.34it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.36it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.13it/s]
[2025-04-12 18:54:07,474][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0295
[2025-04-12 18:54:07,710][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0498, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 9/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 9/10:   1%|▏         | 1/75 [00:00<00:16,  4.41it/s]Epoch 9/10:   4%|▍         | 3/75 [00:00<00:09,  7.66it/s]Epoch 9/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 9/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 9/10:  12%|█▏        | 9/75 [00:00<00:06,  9.77it/s]Epoch 9/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 9/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 9/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 9/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 9/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 9/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 9/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 9/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 9/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 9/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 9/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 9/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 9/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 9/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 9/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 9/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 9/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 9/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 9/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 9/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 9/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 9/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 9/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 9/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 9/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 9/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 9/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:54:15,075][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0240
[2025-04-12 18:54:15,317][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0058, Metrics: {'accuracy': 1.0, 'f1': 1.0}
Epoch 10/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 10/10:   1%|▏         | 1/75 [00:00<00:17,  4.29it/s]Epoch 10/10:   4%|▍         | 3/75 [00:00<00:09,  7.57it/s]Epoch 10/10:   7%|▋         | 5/75 [00:00<00:07,  8.79it/s]Epoch 10/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 10/10:  12%|█▏        | 9/75 [00:01<00:06,  9.74it/s]Epoch 10/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 10/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 10/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 10/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 10/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 10/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 10/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 10/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 10/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 10/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 10/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 10/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 10/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 10/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 10/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 10/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 10/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 10/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 10/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 10/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 10/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 10/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 10/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 10/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 10/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 10/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:54:23,113][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0274
[2025-04-12 18:54:23,359][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0061, Metrics: {'accuracy': 1.0, 'f1': 1.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██████
wandb:          best_val_f1 ▁██████
wandb:        best_val_loss █▃▁▁▁▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss █▆▂▁▁▁▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████████
wandb:               val_f1 ▁█████████
wandb:             val_loss █▃▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 1
wandb:          best_val_f1 1
wandb:        best_val_loss 0.0058
wandb:                epoch 10
wandb:  final_test_accuracy 0.89091
wandb:        final_test_f1 0.9
wandb: final_train_accuracy 0.99748
wandb:       final_train_f1 0.99748
wandb:   final_val_accuracy 1
wandb:         final_val_f1 1
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02743
wandb:           train_time 80.6309
wandb:         val_accuracy 1
wandb:               val_f1 1
wandb:             val_loss 0.00609
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_185247-smkz0geb
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_185247-smkz0geb/logs
Cross-lingual experiment for question_type (ja → ru) completed successfully
Running cross-lingual complexity from ja to ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:54:44,538][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ja_to_ru/complexity
experiment_name: cross_lingual_complexity_ja_to_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ja
  eval_language: ru
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:54:44,538][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:54:44,538][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:54:44,538][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:54:46,102][__main__][INFO] - Running cross-lingual experiment: ja -> ru
[2025-04-12 18:54:46,102][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:54:46,103][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:54:48,973][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:54:48,973][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:54:49,060][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:54:49,093][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:54:49,197][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 18:54:49,208][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:54:49,209][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 18:54:49,210][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:54:49,240][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:54:49,276][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:54:49,291][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 18:54:49,293][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:54:49,293][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 18:54:49,294][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:54:49,320][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:54:49,357][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:54:49,372][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 18:54:49,373][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:54:49,373][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 18:54:49,374][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 18:54:49,375][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:54:49,375][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:54:49,375][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:54:49,376][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:54:49,376][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:54:49,376][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-12 18:54:49,376][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 18:54:49,376][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-04-12 18:54:49,376][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:54:49,377][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:54:49,377][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:54:49,377][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:54:49,377][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:54:49,377][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-12 18:54:49,377][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 18:54:49,377][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-12 18:54:49,377][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:54:49,378][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:54:49,378][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:54:49,378][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:54:49,378][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:54:49,378][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-12 18:54:49,378][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 18:54:49,378][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-12 18:54:49,378][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 18:54:49,379][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:54:49,379][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:54:49,379][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
[2025-04-12 18:54:52,241][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:54:52,242][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:54:52,267][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:54:52,302][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:54:52,319][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 18:54:52,328][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:54:52,329][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 18:54:52,330][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:54:52,354][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:54:52,389][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:54:52,403][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 18:54:52,405][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:54:52,405][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 18:54:52,406][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:54:52,432][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:54:52,468][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:54:52,483][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 18:54:52,484][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:54:52,485][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 18:54:52,486][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 18:54:52,486][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:54:52,486][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:54:52,487][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:54:52,487][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:54:52,487][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:54:52,487][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-12 18:54:52,487][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 18:54:52,487][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-04-12 18:54:52,487][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:54:52,488][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:54:52,488][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:54:52,488][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:54:52,488][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:54:52,488][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-12 18:54:52,488][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 18:54:52,488][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-12 18:54:52,489][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:54:52,489][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:54:52,489][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:54:52,489][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:54:52,489][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:54:52,489][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-12 18:54:52,489][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 18:54:52,489][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-12 18:54:52,489][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 18:54:52,490][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:54:52,490][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:54:52,490][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:54:57,749][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:54:57,752][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:54:57,752][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:54:57,752][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:44,  1.41s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:46,  1.56it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:21,  3.35it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:14,  4.92it/s]Epoch 1/10:  11%|█         | 8/75 [00:02<00:10,  6.25it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.31it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.68it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:08,  7.70it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:07,  8.46it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:03<00:06,  9.01it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.42it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.70it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.91it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.05it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:04<00:04, 10.16it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.22it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.27it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.94it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.67it/s]
[2025-04-12 18:55:08,914][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1499
[2025-04-12 18:55:09,112][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.1009, Metrics: {'mse': 0.0998566597700119, 'rmse': 0.31600104393816786, 'r2': -0.6274563074111938}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.89it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.03it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.08it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.60it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.05it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 11.02it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-12 18:55:16,913][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0531
[2025-04-12 18:55:17,121][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0418, Metrics: {'mse': 0.041176687926054, 'rmse': 0.2029203980038823, 'r2': 0.32890546321868896}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.35it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.62it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.83it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:55:25,148][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0348
[2025-04-12 18:55:25,387][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0293, Metrics: {'mse': 0.028884923085570335, 'rmse': 0.16995565034905527, 'r2': 0.5292356610298157}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.36it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.64it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.77it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 18:55:33,160][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0275
[2025-04-12 18:55:33,402][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0277, Metrics: {'mse': 0.027499238029122353, 'rmse': 0.16582894207321697, 'r2': 0.5518194437026978}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.31it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.58it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.80it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:55:41,199][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0227
[2025-04-12 18:55:41,443][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0152, Metrics: {'mse': 0.015124274417757988, 'rmse': 0.12298078881580646, 'r2': 0.7535057067871094}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:17,  4.26it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.55it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.79it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:06,  9.75it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:55:49,246][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0196
[2025-04-12 18:55:49,496][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0181, Metrics: {'mse': 0.018014637753367424, 'rmse': 0.1342186192499663, 'r2': 0.7063987851142883}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.47it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.72it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.89it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 18:55:56,861][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0170
[2025-04-12 18:55:57,106][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0219, Metrics: {'mse': 0.021557528525590897, 'rmse': 0.14682482257980392, 'r2': 0.648656964302063}
Epoch 8/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 8/10:   1%|▏         | 1/75 [00:00<00:16,  4.41it/s]Epoch 8/10:   4%|▍         | 3/75 [00:00<00:09,  7.67it/s]Epoch 8/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 8/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 8/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 8/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 8/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 8/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 8/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 8/10:  25%|██▌       | 19/75 [00:01<00:05, 10.24it/s]Epoch 8/10:  28%|██▊       | 21/75 [00:02<00:05, 10.28it/s]Epoch 8/10:  31%|███       | 23/75 [00:02<00:05, 10.31it/s]Epoch 8/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 8/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 8/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 8/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 8/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 8/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 8/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 8/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 8/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.35it/s]Epoch 8/10:  60%|██████    | 45/75 [00:04<00:02, 10.36it/s]Epoch 8/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.37it/s]Epoch 8/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.37it/s]Epoch 8/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 8/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 8/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 8/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 8/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 8/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 8/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 8/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 11.03it/s]Epoch 8/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 18:56:04,484][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0169
[2025-04-12 18:56:04,717][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0204, Metrics: {'mse': 0.020431695505976677, 'rmse': 0.14293948197043627, 'r2': 0.6670057773590088}
[2025-04-12 18:56:04,718][src.training.lm_trainer][INFO] - Early stopping at epoch 8
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▂▂▁
wandb:     best_val_mse █▃▂▂▁
wandb:      best_val_r2 ▁▆▇▇█
wandb:    best_val_rmse █▄▃▃▁
wandb:            epoch ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▂▂▁▁▂▁
wandb:          val_mse █▃▂▂▁▁▂▁
wandb:           val_r2 ▁▆▇▇██▇█
wandb:         val_rmse █▄▃▃▁▁▂▂
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01521
wandb:     best_val_mse 0.01512
wandb:      best_val_r2 0.75351
wandb:    best_val_rmse 0.12298
wandb:            epoch 8
wandb:   final_test_mse 0.0948
wandb:    final_test_r2 -1.39799
wandb:  final_test_rmse 0.3079
wandb:  final_train_mse 0.01217
wandb:   final_train_r2 0.69629
wandb: final_train_rmse 0.11031
wandb:    final_val_mse 0.01512
wandb:     final_val_r2 0.75351
wandb:   final_val_rmse 0.12298
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01691
wandb:       train_time 64.45278
wandb:         val_loss 0.02044
wandb:          val_mse 0.02043
wandb:           val_r2 0.66701
wandb:         val_rmse 0.14294
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_185444-ohzdxhxd
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_185444-ohzdxhxd/logs
Cross-lingual experiment for complexity (ja → ru) completed successfully
Running cross-lingual question_type from ko to ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:56:25,245][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ko_to_ar/question_type
experiment_name: cross_lingual_question_type_ko_to_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ko
  eval_language: ar
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:56:25,245][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:56:25,245][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:56:25,245][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:56:26,869][__main__][INFO] - Running cross-lingual experiment: ko -> ar
[2025-04-12 18:56:26,870][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:56:26,870][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:56:29,736][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:56:29,736][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:56:29,798][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:56:29,825][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:56:29,920][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 18:56:29,927][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:56:29,928][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 18:56:29,929][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:56:29,953][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:56:29,987][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:56:30,000][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 18:56:30,002][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:56:30,002][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 18:56:30,003][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:56:30,024][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:56:30,057][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:56:30,071][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 18:56:30,072][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:56:30,073][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 18:56:30,074][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 18:56:30,075][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:56:30,075][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:56:30,075][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:56:30,075][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:56:30,075][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-12 18:56:30,075][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-12 18:56:30,075][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 18:56:30,075][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:56:30,076][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:56:30,076][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:56:30,076][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:56:30,076][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:56:30,076][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:56:30,076][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:56:30,076][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 18:56:30,077][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:56:30,077][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:56:30,077][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:56:30,077][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:56:30,077][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:56:30,077][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:56:30,077][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:56:30,077][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 18:56:30,078][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:56:30,078][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 18:56:30,078][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:56:30,078][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:56:30,078][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
[2025-04-12 18:56:32,966][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:56:32,966][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:56:32,988][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:56:33,022][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:56:33,037][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 18:56:33,045][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:56:33,046][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 18:56:33,047][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:56:33,068][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:56:33,099][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:56:33,115][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 18:56:33,116][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:56:33,116][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 18:56:33,117][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:56:33,141][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:56:33,174][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:56:33,189][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 18:56:33,190][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:56:33,191][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 18:56:33,192][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 18:56:33,192][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:56:33,192][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:56:33,192][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:56:33,193][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:56:33,193][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-12 18:56:33,193][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-12 18:56:33,193][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 18:56:33,193][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:56:33,193][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:56:33,193][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:56:33,194][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:56:33,194][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:56:33,194][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-12 18:56:33,194][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-12 18:56:33,194][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 18:56:33,194][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:56:33,194][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:56:33,194][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:56:33,195][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:56:33,195][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:56:33,195][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-12 18:56:33,195][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-12 18:56:33,195][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 18:56:33,195][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:56:33,195][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 18:56:33,195][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:56:33,196][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:56:33,196][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:56:38,121][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:56:38,124][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:56:38,124][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:56:38,124][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<00:50,  1.09s/it]Epoch 1/10:   4%|▍         | 2/47 [00:01<00:23,  1.95it/s]Epoch 1/10:   6%|▋         | 3/47 [00:01<00:14,  3.08it/s]Epoch 1/10:  11%|█         | 5/47 [00:01<00:08,  5.12it/s]Epoch 1/10:  15%|█▍        | 7/47 [00:01<00:06,  6.62it/s]Epoch 1/10:  19%|█▉        | 9/47 [00:01<00:04,  7.71it/s]Epoch 1/10:  21%|██▏       | 10/47 [00:02<00:05,  6.93it/s]Epoch 1/10:  26%|██▌       | 12/47 [00:02<00:04,  7.94it/s]Epoch 1/10:  30%|██▉       | 14/47 [00:02<00:03,  8.66it/s]Epoch 1/10:  34%|███▍      | 16/47 [00:02<00:03,  9.17it/s]Epoch 1/10:  38%|███▊      | 18/47 [00:02<00:03,  9.53it/s]Epoch 1/10:  43%|████▎     | 20/47 [00:03<00:02,  9.79it/s]Epoch 1/10:  47%|████▋     | 22/47 [00:03<00:02,  9.96it/s]Epoch 1/10:  51%|█████     | 24/47 [00:03<00:02, 10.09it/s]Epoch 1/10:  55%|█████▌    | 26/47 [00:03<00:02, 10.18it/s]Epoch 1/10:  60%|█████▉    | 28/47 [00:03<00:01, 10.24it/s]Epoch 1/10:  64%|██████▍   | 30/47 [00:04<00:01, 10.28it/s]Epoch 1/10:  68%|██████▊   | 32/47 [00:04<00:01, 10.31it/s]Epoch 1/10:  72%|███████▏  | 34/47 [00:04<00:01, 10.34it/s]Epoch 1/10:  77%|███████▋  | 36/47 [00:04<00:01, 10.35it/s]Epoch 1/10:  81%|████████  | 38/47 [00:04<00:00, 10.36it/s]Epoch 1/10:  85%|████████▌ | 40/47 [00:04<00:00, 10.37it/s]Epoch 1/10:  89%|████████▉ | 42/47 [00:05<00:00, 10.37it/s]Epoch 1/10:  94%|█████████▎| 44/47 [00:05<00:00, 10.38it/s]Epoch 1/10:  98%|█████████▊| 46/47 [00:05<00:00, 10.38it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.34it/s]
[2025-04-12 18:56:45,858][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6859
[2025-04-12 18:56:46,117][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6920, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:09,  4.78it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  7.94it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  9.02it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.55it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.85it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03, 10.03it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.15it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.14it/s]
[2025-04-12 18:56:51,213][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6712
[2025-04-12 18:56:51,480][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6836, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:10,  4.20it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.50it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.75it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.38it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:01<00:03,  9.74it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03,  9.95it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.09it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.18it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.36it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.08it/s]
[2025-04-12 18:56:56,804][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6063
[2025-04-12 18:56:57,108][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4413, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9253731343283582}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:10,  4.48it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  7.72it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  8.89it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.47it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:00<00:03,  9.80it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 18:57:02,154][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.3071
[2025-04-12 18:57:02,806][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2225, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:25,  1.81it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:09,  4.65it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:06,  6.50it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:01<00:05,  7.72it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:01<00:04,  8.55it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03,  9.11it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03,  9.50it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03,  9.77it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:02<00:03,  9.96it/s]Epoch 5/10:  40%|████      | 19/47 [00:02<00:02, 10.09it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.18it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.24it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.28it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:03<00:01, 10.31it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:03<00:01, 10.33it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.35it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.36it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.36it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:04<00:00, 10.37it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:04<00:00, 10.37it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.38it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00,  9.42it/s]
[2025-04-12 18:57:08,227][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1219
[2025-04-12 18:57:08,519][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2209, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:10,  4.35it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.63it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  8.83it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.43it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:01<00:03,  9.77it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03,  9.98it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 18:57:13,589][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0734
[2025-04-12 18:57:13,894][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1831, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.918918918918919}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:09,  4.65it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  7.86it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  8.98it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.53it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:00<00:03,  9.84it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.06it/s]
[2025-04-12 18:57:18,990][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0418
[2025-04-12 18:57:19,281][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1668, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:10,  4.30it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.59it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  8.80it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.41it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:01<00:03,  9.75it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03,  9.96it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.10it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.19it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.31it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.33it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.35it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.36it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.37it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.38it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.26it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.08it/s]
[2025-04-12 18:57:24,363][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0306
[2025-04-12 18:57:24,682][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1436, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:10,  4.19it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:05,  7.49it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:04,  8.75it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  9.37it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:01<00:03,  9.73it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03,  9.95it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03, 10.09it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03, 10.18it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 9/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 10.07it/s]
[2025-04-12 18:57:29,776][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0264
[2025-04-12 18:57:30,076][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1544, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:09,  4.74it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:05,  7.92it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:04,  9.02it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:00<00:04,  9.55it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:00<00:03,  9.85it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03, 10.03it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 10/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-12 18:57:34,725][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0245
[2025-04-12 18:57:35,030][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1645, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁██████
wandb:          best_val_f1 ▁▁██████
wandb:        best_val_loss ██▅▂▂▂▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▇▄▂▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁████████
wandb:               val_f1 ▁▁████████
wandb:             val_loss ██▅▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94444
wandb:        best_val_loss 0.1436
wandb:                epoch 10
wandb:  final_test_accuracy 0.62338
wandb:        final_test_f1 0.60274
wandb: final_train_accuracy 0.99729
wandb:       final_train_f1 0.99706
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94444
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02451
wandb:           train_time 54.81187
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94444
wandb:             val_loss 0.16452
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_185625-5twcg9bd
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_185625-5twcg9bd/logs
Cross-lingual experiment for question_type (ko → ar) completed successfully
Running cross-lingual complexity from ko to ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:57:55,824][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ko_to_ar/complexity
experiment_name: cross_lingual_complexity_ko_to_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ko
  eval_language: ar
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:57:55,825][__main__][INFO] - Normalized task: complexity
[2025-04-12 18:57:55,825][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 18:57:55,825][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:57:57,766][__main__][INFO] - Running cross-lingual experiment: ko -> ar
[2025-04-12 18:57:57,766][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 18:57:57,767][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:58:00,612][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:58:00,612][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:58:00,678][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:58:00,706][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:58:00,794][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 18:58:00,801][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:58:00,802][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 18:58:00,803][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:58:00,826][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:58:00,855][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:58:00,868][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 18:58:00,870][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:58:00,870][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 18:58:00,871][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:58:00,892][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:58:00,917][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:58:00,928][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 18:58:00,930][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:58:00,930][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 18:58:00,931][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 18:58:00,932][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:58:00,932][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:58:00,932][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:58:00,932][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:58:00,932][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:58:00,932][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-12 18:58:00,933][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 18:58:00,933][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-04-12 18:58:00,933][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:58:00,933][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:58:00,933][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:58:00,933][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:58:00,933][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:58:00,934][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-12 18:58:00,934][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 18:58:00,934][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-12 18:58:00,934][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:58:00,934][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:58:00,934][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:58:00,934][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:58:00,934][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:58:00,935][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-12 18:58:00,935][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 18:58:00,935][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-12 18:58:00,935][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 18:58:00,935][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:58:00,935][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:58:00,936][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
[2025-04-12 18:58:03,742][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:58:03,743][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:58:03,767][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:58:03,802][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:58:03,818][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 18:58:03,826][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:58:03,827][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 18:58:03,828][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:58:03,852][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:58:03,886][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:58:03,901][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 18:58:03,902][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:58:03,902][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 18:58:03,903][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:58:03,932][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:58:03,966][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:58:03,980][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 18:58:03,982][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:58:03,982][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 18:58:03,983][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 18:58:03,984][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:58:03,984][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:58:03,984][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:58:03,984][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:58:03,984][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:58:03,985][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-12 18:58:03,985][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 18:58:03,985][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-12 18:58:03,985][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:58:03,985][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:58:03,985][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:58:03,985][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:58:03,986][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:58:03,986][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-12 18:58:03,986][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 18:58:03,986][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-12 18:58:03,986][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 18:58:03,986][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 18:58:03,986][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 18:58:03,986][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 18:58:03,987][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 18:58:03,987][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-12 18:58:03,987][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 18:58:03,987][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-12 18:58:03,987][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 18:58:03,987][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:58:03,987][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:58:03,988][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:58:09,168][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:58:09,171][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 18:58:09,171][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:58:09,171][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<00:53,  1.17s/it]Epoch 1/10:   4%|▍         | 2/47 [00:01<00:24,  1.84it/s]Epoch 1/10:   9%|▊         | 4/47 [00:01<00:11,  3.78it/s]Epoch 1/10:  13%|█▎        | 6/47 [00:01<00:07,  5.41it/s]Epoch 1/10:  17%|█▋        | 8/47 [00:01<00:05,  6.70it/s]Epoch 1/10:  21%|██▏       | 10/47 [00:02<00:04,  7.70it/s]Epoch 1/10:  23%|██▎       | 11/47 [00:02<00:05,  6.88it/s]Epoch 1/10:  28%|██▊       | 13/47 [00:02<00:04,  7.86it/s]Epoch 1/10:  32%|███▏      | 15/47 [00:02<00:03,  8.59it/s]Epoch 1/10:  36%|███▌      | 17/47 [00:02<00:03,  9.11it/s]Epoch 1/10:  40%|████      | 19/47 [00:03<00:02,  9.49it/s]Epoch 1/10:  45%|████▍     | 21/47 [00:03<00:02,  9.75it/s]Epoch 1/10:  49%|████▉     | 23/47 [00:03<00:02,  9.94it/s]Epoch 1/10:  53%|█████▎    | 25/47 [00:03<00:02, 10.07it/s]Epoch 1/10:  57%|█████▋    | 27/47 [00:03<00:01, 10.17it/s]Epoch 1/10:  62%|██████▏   | 29/47 [00:03<00:01, 10.23it/s]Epoch 1/10:  66%|██████▌   | 31/47 [00:04<00:01, 10.28it/s]Epoch 1/10:  70%|███████   | 33/47 [00:04<00:01, 10.31it/s]Epoch 1/10:  74%|███████▍  | 35/47 [00:04<00:01, 10.34it/s]Epoch 1/10:  79%|███████▊  | 37/47 [00:04<00:00, 10.35it/s]Epoch 1/10:  83%|████████▎ | 39/47 [00:04<00:00, 10.36it/s]Epoch 1/10:  87%|████████▋ | 41/47 [00:05<00:00, 10.38it/s]Epoch 1/10:  91%|█████████▏| 43/47 [00:05<00:00, 10.38it/s]Epoch 1/10:  96%|█████████▌| 45/47 [00:05<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00, 11.09it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.23it/s]
[2025-04-12 18:58:17,390][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1679
[2025-04-12 18:58:17,655][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2253, Metrics: {'mse': 0.23345845937728882, 'rmse': 0.4831753919409481, 'r2': -3.955507755279541}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:09,  4.88it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  8.02it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  9.08it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.59it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.88it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03, 10.05it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.16it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.17it/s]
[2025-04-12 18:58:22,736][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0708
[2025-04-12 18:58:23,013][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0395, Metrics: {'mse': 0.04067068547010422, 'rmse': 0.2016697435663174, 'r2': 0.13670337200164795}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:10,  4.31it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.59it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.81it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.42it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:01<00:03,  9.76it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03,  9.97it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.08it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.18it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.24it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 18:58:28,318][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0324
[2025-04-12 18:58:28,608][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0373, Metrics: {'mse': 0.03878728672862053, 'rmse': 0.19694488246364902, 'r2': 0.17668139934539795}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:10,  4.41it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  7.68it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  8.87it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.45it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:00<00:03,  9.78it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.04it/s]
[2025-04-12 18:58:33,677][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0224
[2025-04-12 18:58:33,961][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0251, Metrics: {'mse': 0.02600804716348648, 'rmse': 0.1612701062301581, 'r2': 0.44793999195098877}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:10,  4.31it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  7.60it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  8.81it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.42it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:01<00:03,  9.76it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03,  9.97it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-12 18:58:39,042][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0156
[2025-04-12 18:58:39,352][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0336, Metrics: {'mse': 0.035184260457754135, 'rmse': 0.18757467968186467, 'r2': 0.2531610131263733}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:10,  4.42it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.68it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  8.87it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.46it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:00<00:03,  9.79it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-12 18:58:43,999][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0154
[2025-04-12 18:58:44,296][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0285, Metrics: {'mse': 0.02866884134709835, 'rmse': 0.1693187566310902, 'r2': 0.391460657119751}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:10,  4.44it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  7.69it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  8.86it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.45it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:00<00:03,  9.78it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-12 18:58:48,948][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0128
[2025-04-12 18:58:49,254][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0158, Metrics: {'mse': 0.016274383291602135, 'rmse': 0.1275710911280535, 'r2': 0.6545516848564148}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:10,  4.29it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.58it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  8.80it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.41it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:01<00:03,  9.76it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03,  9.97it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.19it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.07it/s]
[2025-04-12 18:58:54,343][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0139
[2025-04-12 18:58:54,638][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0184, Metrics: {'mse': 0.01938963308930397, 'rmse': 0.13924666275822903, 'r2': 0.5884258151054382}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:09,  4.77it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:05,  7.95it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:04,  9.03it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  9.56it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:00<00:03,  9.86it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03, 10.04it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03, 10.15it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 9/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-12 18:58:59,283][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0117
[2025-04-12 18:58:59,582][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0145, Metrics: {'mse': 0.014588968828320503, 'rmse': 0.12078480379716855, 'r2': 0.6903271675109863}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:10,  4.26it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:05,  7.56it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:04,  8.79it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:00<00:04,  9.40it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:01<00:03,  9.75it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03,  9.97it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03, 10.10it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03, 10.19it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 10/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 10.09it/s]
[2025-04-12 18:59:04,662][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0083
[2025-04-12 18:59:04,980][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0196, Metrics: {'mse': 0.01942511647939682, 'rmse': 0.13937401651454556, 'r2': 0.5876725912094116}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▂▁▁▁
wandb:     best_val_mse █▂▂▁▁▁
wandb:      best_val_r2 ▁▇▇███
wandb:    best_val_rmse █▃▂▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▂▂▁▂▁▁▁▁▁
wandb:          val_mse █▂▂▁▂▁▁▁▁▁
wandb:           val_r2 ▁▇▇█▇█████
wandb:         val_rmse █▃▂▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01449
wandb:     best_val_mse 0.01459
wandb:      best_val_r2 0.69033
wandb:    best_val_rmse 0.12078
wandb:            epoch 10
wandb:   final_test_mse 0.06881
wandb:    final_test_r2 -0.18624
wandb:  final_test_rmse 0.26231
wandb:  final_train_mse 0.01027
wandb:   final_train_r2 0.53885
wandb: final_train_rmse 0.10132
wandb:    final_val_mse 0.01459
wandb:     final_val_r2 0.69033
wandb:   final_val_rmse 0.12078
wandb:    learning_rate 1e-05
wandb:       train_loss 0.00828
wandb:       train_time 53.30727
wandb:         val_loss 0.01964
wandb:          val_mse 0.01943
wandb:           val_r2 0.58767
wandb:         val_rmse 0.13937
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_185755-7vogih2q
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_185755-7vogih2q/logs
Cross-lingual experiment for complexity (ko → ar) completed successfully
Running cross-lingual question_type from ko to en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 18:59:25,202][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ko_to_en/question_type
experiment_name: cross_lingual_question_type_ko_to_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ko
  eval_language: en
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 18:59:25,202][__main__][INFO] - Normalized task: question_type
[2025-04-12 18:59:25,202][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 18:59:25,202][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 18:59:26,888][__main__][INFO] - Running cross-lingual experiment: ko -> en
[2025-04-12 18:59:26,888][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 18:59:26,889][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 18:59:29,730][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:59:29,731][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:59:29,826][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:59:29,866][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:59:29,989][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 18:59:29,997][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:59:29,997][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 18:59:29,998][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:59:30,030][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:59:30,065][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:59:30,078][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 18:59:30,080][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:59:30,080][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 18:59:30,081][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:59:30,108][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:59:30,140][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:59:30,154][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 18:59:30,155][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:59:30,155][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 18:59:30,156][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 18:59:30,157][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:59:30,157][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:59:30,157][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:59:30,157][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:59:30,157][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-12 18:59:30,158][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-12 18:59:30,158][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 18:59:30,158][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:59:30,158][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:59:30,158][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:59:30,158][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:59:30,158][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:59:30,158][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:59:30,159][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:59:30,159][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 18:59:30,159][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:59:30,159][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:59:30,159][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:59:30,159][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:59:30,159][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:59:30,159][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:59:30,160][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:59:30,160][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 18:59:30,160][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:59:30,160][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 18:59:30,160][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:59:30,160][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:59:30,161][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
[2025-04-12 18:59:32,966][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 18:59:32,967][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:59:32,999][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:59:33,039][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:59:33,059][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 18:59:33,068][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:59:33,069][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 18:59:33,071][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:59:33,098][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:59:33,140][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:59:33,156][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 18:59:33,158][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:59:33,158][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 18:59:33,160][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 18:59:33,191][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:59:33,226][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 18:59:33,243][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 18:59:33,244][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 18:59:33,244][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 18:59:33,246][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 18:59:33,246][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:59:33,247][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:59:33,247][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:59:33,247][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:59:33,247][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-12 18:59:33,247][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 18:59:33,247][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 18:59:33,247][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 18:59:33,248][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:59:33,248][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:59:33,248][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:59:33,248][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:59:33,248][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 18:59:33,248][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 18:59:33,248][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 18:59:33,248][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:59:33,248][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 18:59:33,249][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 18:59:33,249][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 18:59:33,249][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 18:59:33,249][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 18:59:33,249][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 18:59:33,249][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 18:59:33,249][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 18:59:33,249][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 18:59:33,249][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 18:59:33,250][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 18:59:33,250][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 18:59:38,544][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 18:59:38,547][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 18:59:38,548][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 18:59:38,548][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<00:54,  1.18s/it]Epoch 1/10:   6%|▋         | 3/47 [00:01<00:16,  2.64it/s]Epoch 1/10:  11%|█         | 5/47 [00:01<00:09,  4.28it/s]Epoch 1/10:  15%|█▍        | 7/47 [00:01<00:07,  5.70it/s]Epoch 1/10:  19%|█▉        | 9/47 [00:01<00:05,  6.87it/s]Epoch 1/10:  23%|██▎       | 11/47 [00:02<00:05,  6.79it/s]Epoch 1/10:  28%|██▊       | 13/47 [00:02<00:04,  7.67it/s]Epoch 1/10:  32%|███▏      | 15/47 [00:02<00:03,  8.38it/s]Epoch 1/10:  36%|███▌      | 17/47 [00:02<00:03,  8.92it/s]Epoch 1/10:  40%|████      | 19/47 [00:03<00:03,  9.33it/s]Epoch 1/10:  45%|████▍     | 21/47 [00:03<00:02,  9.63it/s]Epoch 1/10:  49%|████▉     | 23/47 [00:03<00:02,  9.85it/s]Epoch 1/10:  53%|█████▎    | 25/47 [00:03<00:02, 10.01it/s]Epoch 1/10:  57%|█████▋    | 27/47 [00:03<00:01, 10.12it/s]Epoch 1/10:  62%|██████▏   | 29/47 [00:03<00:01, 10.20it/s]Epoch 1/10:  66%|██████▌   | 31/47 [00:04<00:01, 10.26it/s]Epoch 1/10:  70%|███████   | 33/47 [00:04<00:01, 10.29it/s]Epoch 1/10:  74%|███████▍  | 35/47 [00:04<00:01, 10.32it/s]Epoch 1/10:  79%|███████▊  | 37/47 [00:04<00:00, 10.34it/s]Epoch 1/10:  83%|████████▎ | 39/47 [00:04<00:00, 10.35it/s]Epoch 1/10:  87%|████████▋ | 41/47 [00:05<00:00, 10.37it/s]Epoch 1/10:  91%|█████████▏| 43/47 [00:05<00:00, 10.37it/s]Epoch 1/10:  96%|█████████▌| 45/47 [00:05<00:00, 10.38it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00, 11.07it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.22it/s]
[2025-04-12 18:59:46,471][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6859
[2025-04-12 18:59:46,736][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6920, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:09,  4.94it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  8.06it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  9.11it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.61it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.89it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03, 10.06it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.17it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.24it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.17it/s]
[2025-04-12 18:59:51,829][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6712
[2025-04-12 18:59:52,089][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6836, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:10,  4.28it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.57it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.80it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.40it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:01<00:03,  9.75it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03,  9.96it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.10it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.19it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.09it/s]
[2025-04-12 18:59:57,396][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6063
[2025-04-12 18:59:57,676][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4413, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9253731343283582}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:18,  2.43it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:07,  5.62it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:05,  7.35it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  8.39it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:01<00:04,  9.04it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03,  9.47it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03,  9.76it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03,  9.95it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.09it/s]Epoch 4/10:  40%|████      | 19/47 [00:02<00:02, 10.18it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.24it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.29it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.31it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.34it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:03<00:01, 10.35it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.36it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.37it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:04<00:00, 10.38it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00,  9.72it/s]
[2025-04-12 19:00:02,901][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.3071
[2025-04-12 19:00:03,202][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2225, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:10,  4.45it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  7.71it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  8.88it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.47it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:00<00:03,  9.79it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03, 10.00it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:00:08,282][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1219
[2025-04-12 19:00:08,579][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2209, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:10,  4.25it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.54it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  8.78it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.39it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:01<00:03,  9.74it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03,  9.96it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.09it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.18it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.31it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.33it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.37it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.06it/s]
[2025-04-12 19:00:13,659][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0734
[2025-04-12 19:00:13,942][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1831, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.918918918918919}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:09,  4.62it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  7.83it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  8.95it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.51it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:00<00:03,  9.82it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03, 10.01it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.26it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:00:19,017][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0418
[2025-04-12 19:00:19,324][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1668, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:10,  4.25it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.54it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  8.78it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.39it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:01<00:03,  9.75it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03,  9.96it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.10it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.19it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.07it/s]
[2025-04-12 19:00:24,419][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0306
[2025-04-12 19:00:24,732][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1436, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:10,  4.33it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:05,  7.61it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:04,  8.82it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  9.42it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:01<00:03,  9.76it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03,  9.97it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 9/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 10.09it/s]
[2025-04-12 19:00:29,811][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0264
[2025-04-12 19:00:30,108][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1544, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:10,  4.53it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:05,  7.76it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:04,  8.91it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:00<00:04,  9.48it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:00<00:03,  9.80it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03, 10.00it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 10/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-12 19:00:34,760][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0245
[2025-04-12 19:00:35,081][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1645, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁██████
wandb:          best_val_f1 ▁▁██████
wandb:        best_val_loss ██▅▂▂▂▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▇▄▂▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁████████
wandb:               val_f1 ▁▁████████
wandb:             val_loss ██▅▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94444
wandb:        best_val_loss 0.1436
wandb:                epoch 10
wandb:  final_test_accuracy 0.97273
wandb:        final_test_f1 0.97345
wandb: final_train_accuracy 0.99729
wandb:       final_train_f1 0.99706
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94444
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02451
wandb:           train_time 54.334
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94444
wandb:             val_loss 0.16452
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_185925-m1i0mrh2
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_185925-m1i0mrh2/logs
Cross-lingual experiment for question_type (ko → en) completed successfully
Running cross-lingual complexity from ko to en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:00:56,148][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ko_to_en/complexity
experiment_name: cross_lingual_complexity_ko_to_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ko
  eval_language: en
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:00:56,148][__main__][INFO] - Normalized task: complexity
[2025-04-12 19:00:56,148][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 19:00:56,149][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:00:57,642][__main__][INFO] - Running cross-lingual experiment: ko -> en
[2025-04-12 19:00:57,642][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 19:00:57,643][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:01:00,540][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:01:00,541][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:01:00,620][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:01:00,653][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:01:00,756][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 19:01:00,763][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:01:00,764][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 19:01:00,765][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:01:00,788][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:01:00,818][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:01:00,830][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 19:01:00,832][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:01:00,832][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 19:01:00,833][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:01:00,853][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:01:00,882][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:01:00,895][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 19:01:00,897][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:01:00,897][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 19:01:00,899][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 19:01:00,899][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:01:00,899][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:01:00,899][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:01:00,899][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:01:00,900][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:01:00,900][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-12 19:01:00,900][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 19:01:00,900][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-04-12 19:01:00,900][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:01:00,900][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:01:00,900][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:01:00,901][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:01:00,901][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:01:00,901][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-12 19:01:00,901][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 19:01:00,901][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-12 19:01:00,901][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:01:00,901][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:01:00,901][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:01:00,902][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:01:00,902][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:01:00,902][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-12 19:01:00,902][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 19:01:00,902][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-12 19:01:00,902][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 19:01:00,902][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:01:00,903][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:01:00,903][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
[2025-04-12 19:01:03,735][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:01:03,735][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:01:03,767][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:01:03,811][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:01:03,828][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 19:01:03,838][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:01:03,838][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 19:01:03,840][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:01:03,868][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:01:03,903][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:01:03,919][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 19:01:03,920][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:01:03,921][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 19:01:03,922][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:01:03,950][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:01:03,989][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:01:04,005][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 19:01:04,006][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:01:04,007][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 19:01:04,008][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 19:01:04,009][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:01:04,009][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:01:04,009][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:01:04,009][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:01:04,009][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:01:04,010][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-12 19:01:04,010][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 19:01:04,010][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-12 19:01:04,010][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:01:04,010][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:01:04,010][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:01:04,010][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:01:04,010][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:01:04,011][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-12 19:01:04,011][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 19:01:04,011][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-12 19:01:04,011][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:01:04,011][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:01:04,011][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:01:04,011][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:01:04,012][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:01:04,012][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-12 19:01:04,012][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 19:01:04,012][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-12 19:01:04,012][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 19:01:04,012][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:01:04,012][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:01:04,013][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:01:09,203][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:01:09,206][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 19:01:09,206][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:01:09,206][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<00:59,  1.29s/it]Epoch 1/10:   4%|▍         | 2/47 [00:01<00:26,  1.68it/s]Epoch 1/10:   9%|▊         | 4/47 [00:01<00:12,  3.54it/s]Epoch 1/10:  13%|█▎        | 6/47 [00:01<00:07,  5.14it/s]Epoch 1/10:  17%|█▋        | 8/47 [00:01<00:06,  6.46it/s]Epoch 1/10:  21%|██▏       | 10/47 [00:02<00:04,  7.50it/s]Epoch 1/10:  23%|██▎       | 11/47 [00:02<00:05,  6.72it/s]Epoch 1/10:  28%|██▊       | 13/47 [00:02<00:04,  7.73it/s]Epoch 1/10:  32%|███▏      | 15/47 [00:02<00:03,  8.49it/s]Epoch 1/10:  36%|███▌      | 17/47 [00:02<00:03,  9.04it/s]Epoch 1/10:  40%|████      | 19/47 [00:03<00:02,  9.43it/s]Epoch 1/10:  45%|████▍     | 21/47 [00:03<00:02,  9.72it/s]Epoch 1/10:  49%|████▉     | 23/47 [00:03<00:02,  9.92it/s]Epoch 1/10:  53%|█████▎    | 25/47 [00:03<00:02, 10.06it/s]Epoch 1/10:  57%|█████▋    | 27/47 [00:03<00:01, 10.16it/s]Epoch 1/10:  62%|██████▏   | 29/47 [00:04<00:01, 10.23it/s]Epoch 1/10:  66%|██████▌   | 31/47 [00:04<00:01, 10.28it/s]Epoch 1/10:  70%|███████   | 33/47 [00:04<00:01, 10.32it/s]Epoch 1/10:  74%|███████▍  | 35/47 [00:04<00:01, 10.34it/s]Epoch 1/10:  79%|███████▊  | 37/47 [00:04<00:00, 10.36it/s]Epoch 1/10:  83%|████████▎ | 39/47 [00:05<00:00, 10.37it/s]Epoch 1/10:  87%|████████▋ | 41/47 [00:05<00:00, 10.38it/s]Epoch 1/10:  91%|█████████▏| 43/47 [00:05<00:00, 10.39it/s]Epoch 1/10:  96%|█████████▌| 45/47 [00:05<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00, 11.09it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.05it/s]
[2025-04-12 19:01:17,217][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1679
[2025-04-12 19:01:17,465][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2253, Metrics: {'mse': 0.23345845937728882, 'rmse': 0.4831753919409481, 'r2': -3.955507755279541}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:08,  5.14it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  8.21it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  9.20it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.68it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.94it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03, 10.10it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.20it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.26it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.31it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.34it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.36it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.21it/s]
[2025-04-12 19:01:22,537][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0708
[2025-04-12 19:01:22,801][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0395, Metrics: {'mse': 0.04067068547010422, 'rmse': 0.2016697435663174, 'r2': 0.13670337200164795}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:10,  4.36it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.64it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.84it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.44it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:00<00:03,  9.78it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03,  9.98it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:01:28,118][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0324
[2025-04-12 19:01:28,551][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0373, Metrics: {'mse': 0.03878728672862053, 'rmse': 0.19694488246364902, 'r2': 0.17668139934539795}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:10,  4.35it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  7.63it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  8.83it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.43it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:01<00:03,  9.78it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:01:33,603][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0224
[2025-04-12 19:01:33,903][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0251, Metrics: {'mse': 0.02600804716348648, 'rmse': 0.1612701062301581, 'r2': 0.44793999195098877}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:10,  4.25it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  7.55it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  8.79it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.40it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:01<00:03,  9.76it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03,  9.97it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.06it/s]
[2025-04-12 19:01:39,005][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0156
[2025-04-12 19:01:39,300][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0336, Metrics: {'mse': 0.035184260457754135, 'rmse': 0.18757467968186467, 'r2': 0.2531610131263733}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:10,  4.35it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.63it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  8.84it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.44it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:01<00:03,  9.78it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-12 19:01:43,953][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0154
[2025-04-12 19:01:44,237][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0285, Metrics: {'mse': 0.02866884134709835, 'rmse': 0.1693187566310902, 'r2': 0.391460657119751}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:10,  4.37it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  7.65it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  8.85it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.45it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:00<00:03,  9.78it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-12 19:01:48,883][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0128
[2025-04-12 19:01:49,182][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0158, Metrics: {'mse': 0.016274383291602135, 'rmse': 0.1275710911280535, 'r2': 0.6545516848564148}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:10,  4.33it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.62it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  8.83it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.43it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:01<00:03,  9.77it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.09it/s]
[2025-04-12 19:01:54,262][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0139
[2025-04-12 19:01:54,549][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0184, Metrics: {'mse': 0.01938963308930397, 'rmse': 0.13924666275822903, 'r2': 0.5884258151054382}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:10,  4.47it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:05,  7.73it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:04,  8.90it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  9.48it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:00<00:03,  9.81it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03, 10.01it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 9/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 10.06it/s]
[2025-04-12 19:01:59,221][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0117
[2025-04-12 19:01:59,532][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0145, Metrics: {'mse': 0.014588968828320503, 'rmse': 0.12078480379716855, 'r2': 0.6903271675109863}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:10,  4.28it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:05,  7.57it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:04,  8.79it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:00<00:04,  9.40it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:01<00:03,  9.75it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03,  9.97it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 10/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:02:04,612][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0083
[2025-04-12 19:02:04,913][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0196, Metrics: {'mse': 0.01942511647939682, 'rmse': 0.13937401651454556, 'r2': 0.5876725912094116}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▂▁▁▁
wandb:     best_val_mse █▂▂▁▁▁
wandb:      best_val_r2 ▁▇▇███
wandb:    best_val_rmse █▃▂▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▂▂▁▂▁▁▁▁▁
wandb:          val_mse █▂▂▁▂▁▁▁▁▁
wandb:           val_r2 ▁▇▇█▇█████
wandb:         val_rmse █▃▂▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01449
wandb:     best_val_mse 0.01459
wandb:      best_val_r2 0.69033
wandb:    best_val_rmse 0.12078
wandb:            epoch 10
wandb:   final_test_mse 0.03548
wandb:    final_test_r2 0.07934
wandb:  final_test_rmse 0.18836
wandb:  final_train_mse 0.01027
wandb:   final_train_r2 0.53885
wandb: final_train_rmse 0.10132
wandb:    final_val_mse 0.01459
wandb:     final_val_r2 0.69033
wandb:   final_val_rmse 0.12078
wandb:    learning_rate 1e-05
wandb:       train_loss 0.00828
wandb:       train_time 53.53382
wandb:         val_loss 0.01964
wandb:          val_mse 0.01943
wandb:           val_r2 0.58767
wandb:         val_rmse 0.13937
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_190056-rg0zoqjz
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_190056-rg0zoqjz/logs
Cross-lingual experiment for complexity (ko → en) completed successfully
Running cross-lingual question_type from ko to fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:02:25,470][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ko_to_fi/question_type
experiment_name: cross_lingual_question_type_ko_to_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ko
  eval_language: fi
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:02:25,470][__main__][INFO] - Normalized task: question_type
[2025-04-12 19:02:25,470][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 19:02:25,471][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:02:26,934][__main__][INFO] - Running cross-lingual experiment: ko -> fi
[2025-04-12 19:02:26,934][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 19:02:26,934][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:02:29,791][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:02:29,792][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:02:29,884][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:02:29,925][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:02:30,040][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 19:02:30,047][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:02:30,048][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 19:02:30,049][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:02:30,081][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:02:30,117][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:02:30,130][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 19:02:30,132][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:02:30,132][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 19:02:30,133][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:02:30,161][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:02:30,199][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:02:30,214][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 19:02:30,216][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:02:30,216][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 19:02:30,217][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 19:02:30,218][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:02:30,218][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:02:30,218][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:02:30,218][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:02:30,218][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-12 19:02:30,218][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-12 19:02:30,218][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 19:02:30,219][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:02:30,219][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:02:30,219][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:02:30,219][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:02:30,219][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:02:30,219][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 19:02:30,219][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 19:02:30,219][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 19:02:30,220][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:02:30,220][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:02:30,220][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:02:30,220][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:02:30,220][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:02:30,220][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:02:30,220][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:02:30,220][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 19:02:30,221][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:02:30,221][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 19:02:30,221][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:02:30,221][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:02:30,221][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
[2025-04-12 19:02:33,057][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:02:33,058][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:02:33,088][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:02:33,130][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:02:33,148][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 19:02:33,158][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:02:33,159][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 19:02:33,160][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:02:33,184][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:02:33,216][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:02:33,231][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 19:02:33,232][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:02:33,232][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 19:02:33,233][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:02:33,257][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:02:33,294][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:02:33,310][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 19:02:33,311][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:02:33,312][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 19:02:33,313][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 19:02:33,313][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:02:33,313][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:02:33,313][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:02:33,313][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:02:33,314][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 19:02:33,314][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-12 19:02:33,314][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 19:02:33,314][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:02:33,314][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:02:33,314][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:02:33,314][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:02:33,314][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:02:33,315][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-12 19:02:33,315][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-12 19:02:33,315][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 19:02:33,315][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:02:33,315][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:02:33,315][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:02:33,315][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:02:33,315][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:02:33,316][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:02:33,316][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:02:33,316][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 19:02:33,316][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:02:33,316][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 19:02:33,316][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:02:33,316][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:02:33,317][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:02:38,520][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:02:38,522][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 19:02:38,523][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:02:38,523][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<00:56,  1.24s/it]Epoch 1/10:   4%|▍         | 2/47 [00:01<00:25,  1.76it/s]Epoch 1/10:   9%|▊         | 4/47 [00:01<00:11,  3.67it/s]Epoch 1/10:  13%|█▎        | 6/47 [00:01<00:07,  5.28it/s]Epoch 1/10:  17%|█▋        | 8/47 [00:01<00:05,  6.59it/s]Epoch 1/10:  21%|██▏       | 10/47 [00:02<00:04,  7.61it/s]Epoch 1/10:  23%|██▎       | 11/47 [00:02<00:05,  6.90it/s]Epoch 1/10:  28%|██▊       | 13/47 [00:02<00:04,  7.88it/s]Epoch 1/10:  32%|███▏      | 15/47 [00:02<00:03,  8.60it/s]Epoch 1/10:  36%|███▌      | 17/47 [00:02<00:03,  9.12it/s]Epoch 1/10:  40%|████      | 19/47 [00:03<00:02,  9.50it/s]Epoch 1/10:  45%|████▍     | 21/47 [00:03<00:02,  9.76it/s]Epoch 1/10:  49%|████▉     | 23/47 [00:03<00:02,  9.95it/s]Epoch 1/10:  53%|█████▎    | 25/47 [00:03<00:02, 10.09it/s]Epoch 1/10:  57%|█████▋    | 27/47 [00:03<00:01, 10.18it/s]Epoch 1/10:  62%|██████▏   | 29/47 [00:04<00:01, 10.25it/s]Epoch 1/10:  66%|██████▌   | 31/47 [00:04<00:01, 10.29it/s]Epoch 1/10:  70%|███████   | 33/47 [00:04<00:01, 10.33it/s]Epoch 1/10:  74%|███████▍  | 35/47 [00:04<00:01, 10.35it/s]Epoch 1/10:  79%|███████▊  | 37/47 [00:04<00:00, 10.37it/s]Epoch 1/10:  83%|████████▎ | 39/47 [00:04<00:00, 10.38it/s]Epoch 1/10:  87%|████████▋ | 41/47 [00:05<00:00, 10.39it/s]Epoch 1/10:  91%|█████████▏| 43/47 [00:05<00:00, 10.39it/s]Epoch 1/10:  96%|█████████▌| 45/47 [00:05<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00, 11.09it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.15it/s]
[2025-04-12 19:02:46,380][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6859
[2025-04-12 19:02:46,626][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6920, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:09,  4.64it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  7.85it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  8.97it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.52it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.83it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.15it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-12 19:02:51,729][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6712
[2025-04-12 19:02:51,986][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6836, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:10,  4.23it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.53it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.77it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.39it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:01<00:03,  9.75it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03,  9.97it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.09it/s]
[2025-04-12 19:02:57,290][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6063
[2025-04-12 19:02:57,588][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4413, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9253731343283582}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:10,  4.37it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  7.65it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  8.85it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.44it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:00<00:03,  9.78it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-12 19:03:02,634][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.3071
[2025-04-12 19:03:02,932][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2225, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:18,  2.48it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:07,  5.69it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:05,  7.41it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  8.44it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:01<00:04,  9.09it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03,  9.51it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03,  9.79it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03,  9.98it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.11it/s]Epoch 5/10:  40%|████      | 19/47 [00:02<00:02, 10.20it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.26it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.30it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.33it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.35it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:03<00:01, 10.37it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:04<00:00, 10.39it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00,  9.73it/s]
[2025-04-12 19:03:08,189][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1219
[2025-04-12 19:03:08,485][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2209, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:10,  4.29it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.58it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  8.81it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.42it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:01<00:03,  9.77it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03,  9.98it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.09it/s]
[2025-04-12 19:03:13,567][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0734
[2025-04-12 19:03:13,877][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1831, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.918918918918919}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:10,  4.27it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  7.57it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  8.80it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.41it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:01<00:03,  9.76it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03,  9.97it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:03:18,952][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0418
[2025-04-12 19:03:19,247][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1668, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:10,  4.27it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.56it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  8.79it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.40it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:01<00:03,  9.75it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03,  9.97it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.08it/s]
[2025-04-12 19:03:24,328][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0306
[2025-04-12 19:03:24,619][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1436, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:10,  4.30it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:05,  7.59it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:04,  8.81it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  9.42it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:01<00:03,  9.77it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03,  9.98it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 9/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:03:29,697][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0264
[2025-04-12 19:03:29,996][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1544, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:10,  4.50it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:05,  7.75it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:04,  8.91it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:00<00:04,  9.48it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:00<00:03,  9.81it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03, 10.01it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 10/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 10.13it/s]
[2025-04-12 19:03:34,639][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0245
[2025-04-12 19:03:34,941][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1645, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁██████
wandb:          best_val_f1 ▁▁██████
wandb:        best_val_loss ██▅▂▂▂▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▇▄▂▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁████████
wandb:               val_f1 ▁▁████████
wandb:             val_loss ██▅▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94444
wandb:        best_val_loss 0.1436
wandb:                epoch 10
wandb:  final_test_accuracy 0.86364
wandb:        final_test_f1 0.87603
wandb: final_train_accuracy 0.99729
wandb:       final_train_f1 0.99706
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94444
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02451
wandb:           train_time 54.32855
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94444
wandb:             val_loss 0.16452
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_190225-la9v2jeq
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_190225-la9v2jeq/logs
Cross-lingual experiment for question_type (ko → fi) completed successfully
Running cross-lingual complexity from ko to fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:03:56,501][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ko_to_fi/complexity
experiment_name: cross_lingual_complexity_ko_to_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ko
  eval_language: fi
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:03:56,502][__main__][INFO] - Normalized task: complexity
[2025-04-12 19:03:56,502][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 19:03:56,502][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:03:58,108][__main__][INFO] - Running cross-lingual experiment: ko -> fi
[2025-04-12 19:03:58,108][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 19:03:58,109][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:04:01,035][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:04:01,035][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:04:01,120][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:04:01,158][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:04:01,279][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 19:04:01,286][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:04:01,286][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 19:04:01,288][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:04:01,321][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:04:01,365][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:04:01,383][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 19:04:01,384][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:04:01,385][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 19:04:01,386][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:04:01,417][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:04:01,462][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:04:01,477][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 19:04:01,478][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:04:01,479][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 19:04:01,480][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 19:04:01,480][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:04:01,481][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:04:01,481][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:04:01,481][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:04:01,481][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:04:01,481][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-12 19:04:01,481][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 19:04:01,482][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-04-12 19:04:01,482][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:04:01,482][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:04:01,482][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:04:01,482][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:04:01,482][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:04:01,482][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-12 19:04:01,483][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 19:04:01,483][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-12 19:04:01,483][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:04:01,483][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:04:01,483][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:04:01,483][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:04:01,483][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:04:01,483][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-12 19:04:01,484][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 19:04:01,484][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-12 19:04:01,484][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 19:04:01,484][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:04:01,484][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:04:01,484][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
[2025-04-12 19:04:04,303][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:04:04,303][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:04:04,332][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:04:04,374][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:04:04,393][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 19:04:04,403][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:04:04,404][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 19:04:04,405][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:04:04,438][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:04:04,482][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:04:04,499][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 19:04:04,500][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:04:04,500][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 19:04:04,502][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:04:04,533][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:04:04,575][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:04:04,590][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 19:04:04,592][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:04:04,592][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 19:04:04,594][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 19:04:04,594][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:04:04,595][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:04:04,595][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:04:04,595][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:04:04,595][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:04:04,595][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-12 19:04:04,595][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 19:04:04,595][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-12 19:04:04,596][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:04:04,596][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:04:04,596][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:04:04,596][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:04:04,596][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:04:04,596][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-12 19:04:04,596][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 19:04:04,596][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-12 19:04:04,597][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:04:04,597][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:04:04,597][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:04:04,597][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:04:04,597][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:04:04,597][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-12 19:04:04,597][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 19:04:04,597][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-12 19:04:04,598][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 19:04:04,598][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:04:04,598][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:04:04,598][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:04:10,058][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:04:10,061][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 19:04:10,061][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:04:10,061][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<01:01,  1.33s/it]Epoch 1/10:   4%|▍         | 2/47 [00:01<00:27,  1.65it/s]Epoch 1/10:   9%|▊         | 4/47 [00:01<00:12,  3.47it/s]Epoch 1/10:  13%|█▎        | 6/47 [00:01<00:08,  5.06it/s]Epoch 1/10:  17%|█▋        | 8/47 [00:02<00:06,  6.38it/s]Epoch 1/10:  21%|██▏       | 10/47 [00:02<00:04,  7.43it/s]Epoch 1/10:  23%|██▎       | 11/47 [00:02<00:05,  6.75it/s]Epoch 1/10:  28%|██▊       | 13/47 [00:02<00:04,  7.76it/s]Epoch 1/10:  32%|███▏      | 15/47 [00:02<00:03,  8.51it/s]Epoch 1/10:  36%|███▌      | 17/47 [00:02<00:03,  9.05it/s]Epoch 1/10:  40%|████      | 19/47 [00:03<00:02,  9.44it/s]Epoch 1/10:  45%|████▍     | 21/47 [00:03<00:02,  9.73it/s]Epoch 1/10:  49%|████▉     | 23/47 [00:03<00:02,  9.92it/s]Epoch 1/10:  53%|█████▎    | 25/47 [00:03<00:02, 10.06it/s]Epoch 1/10:  57%|█████▋    | 27/47 [00:03<00:01, 10.16it/s]Epoch 1/10:  62%|██████▏   | 29/47 [00:04<00:01, 10.23it/s]Epoch 1/10:  66%|██████▌   | 31/47 [00:04<00:01, 10.28it/s]Epoch 1/10:  70%|███████   | 33/47 [00:04<00:01, 10.32it/s]Epoch 1/10:  74%|███████▍  | 35/47 [00:04<00:01, 10.34it/s]Epoch 1/10:  79%|███████▊  | 37/47 [00:04<00:00, 10.36it/s]Epoch 1/10:  83%|████████▎ | 39/47 [00:05<00:00, 10.37it/s]Epoch 1/10:  87%|████████▋ | 41/47 [00:05<00:00, 10.38it/s]Epoch 1/10:  91%|█████████▏| 43/47 [00:05<00:00, 10.39it/s]Epoch 1/10:  96%|█████████▌| 45/47 [00:05<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00, 11.08it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.02it/s]
[2025-04-12 19:04:18,639][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1679
[2025-04-12 19:04:18,901][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2253, Metrics: {'mse': 0.23345845937728882, 'rmse': 0.4831753919409481, 'r2': -3.955507755279541}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:09,  4.78it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  7.95it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  9.05it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.57it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.87it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03, 10.05it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.16it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.24it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.29it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.33it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.15it/s]
[2025-04-12 19:04:23,999][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0708
[2025-04-12 19:04:24,279][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0395, Metrics: {'mse': 0.04067068547010422, 'rmse': 0.2016697435663174, 'r2': 0.13670337200164795}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:10,  4.24it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.54it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.78it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.40it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:01<00:03,  9.76it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03,  9.98it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:04:29,604][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0324
[2025-04-12 19:04:29,907][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0373, Metrics: {'mse': 0.03878728672862053, 'rmse': 0.19694488246364902, 'r2': 0.17668139934539795}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:13,  3.53it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:06,  6.91it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:05,  8.35it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.11it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:01<00:03,  9.55it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03,  9.83it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.01it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.13it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.22it/s]Epoch 4/10:  40%|████      | 19/47 [00:02<00:02, 10.28it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.31it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.00it/s]
[2025-04-12 19:04:35,007][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0224
[2025-04-12 19:04:35,297][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0251, Metrics: {'mse': 0.02600804716348648, 'rmse': 0.1612701062301581, 'r2': 0.44793999195098877}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:10,  4.39it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  7.66it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  8.86it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.45it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:00<00:03,  9.79it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.07it/s]
[2025-04-12 19:04:40,395][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0156
[2025-04-12 19:04:40,675][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0336, Metrics: {'mse': 0.035184260457754135, 'rmse': 0.18757467968186467, 'r2': 0.2531610131263733}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:10,  4.49it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.75it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  8.92it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.49it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:00<00:03,  9.82it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.24it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-12 19:04:45,328][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0154
[2025-04-12 19:04:45,618][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0285, Metrics: {'mse': 0.02866884134709835, 'rmse': 0.1693187566310902, 'r2': 0.391460657119751}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:10,  4.57it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  7.79it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  8.94it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.51it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:00<00:03,  9.83it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.37it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.14it/s]
[2025-04-12 19:04:50,258][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0128
[2025-04-12 19:04:50,546][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0158, Metrics: {'mse': 0.016274383291602135, 'rmse': 0.1275710911280535, 'r2': 0.6545516848564148}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:09,  4.60it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.83it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  8.97it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.52it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:00<00:03,  9.82it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.14it/s]
[2025-04-12 19:04:55,587][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0139
[2025-04-12 19:04:55,878][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0184, Metrics: {'mse': 0.01938963308930397, 'rmse': 0.13924666275822903, 'r2': 0.5884258151054382}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:10,  4.47it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:05,  7.72it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:04,  8.88it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  9.47it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:00<00:03,  9.80it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03, 10.00it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 9/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-12 19:05:00,530][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0117
[2025-04-12 19:05:02,811][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0145, Metrics: {'mse': 0.014588968828320503, 'rmse': 0.12078480379716855, 'r2': 0.6903271675109863}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:24,  1.90it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:09,  4.80it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:06,  6.63it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:01<00:05,  7.84it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:01<00:04,  8.64it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03,  9.19it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03,  9.56it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03,  9.81it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:02<00:03,  9.99it/s]Epoch 10/10:  40%|████      | 19/47 [00:02<00:02, 10.12it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.20it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.26it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.30it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:03<00:01, 10.33it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:03<00:01, 10.35it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:04<00:00, 10.39it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00,  9.50it/s]
[2025-04-12 19:05:08,186][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0083
[2025-04-12 19:05:08,473][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0196, Metrics: {'mse': 0.01942511647939682, 'rmse': 0.13937401651454556, 'r2': 0.5876725912094116}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▂▁▁▁
wandb:     best_val_mse █▂▂▁▁▁
wandb:      best_val_r2 ▁▇▇███
wandb:    best_val_rmse █▃▂▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▂▂▁▂▁▁▁▁▁
wandb:          val_mse █▂▂▁▂▁▁▁▁▁
wandb:           val_r2 ▁▇▇█▇█████
wandb:         val_rmse █▃▂▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01449
wandb:     best_val_mse 0.01459
wandb:      best_val_r2 0.69033
wandb:    best_val_rmse 0.12078
wandb:            epoch 10
wandb:   final_test_mse 0.03167
wandb:    final_test_r2 0.1979
wandb:  final_test_rmse 0.17797
wandb:  final_train_mse 0.01027
wandb:   final_train_r2 0.53885
wandb: final_train_rmse 0.10132
wandb:    final_val_mse 0.01459
wandb:     final_val_r2 0.69033
wandb:   final_val_rmse 0.12078
wandb:    learning_rate 1e-05
wandb:       train_loss 0.00828
wandb:       train_time 55.69912
wandb:         val_loss 0.01964
wandb:          val_mse 0.01943
wandb:           val_r2 0.58767
wandb:         val_rmse 0.13937
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_190356-rh7gi9sa
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_190356-rh7gi9sa/logs
Cross-lingual experiment for complexity (ko → fi) completed successfully
Running cross-lingual question_type from ko to id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:05:30,393][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ko_to_id/question_type
experiment_name: cross_lingual_question_type_ko_to_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ko
  eval_language: id
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:05:30,393][__main__][INFO] - Normalized task: question_type
[2025-04-12 19:05:30,393][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 19:05:30,393][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:05:31,844][__main__][INFO] - Running cross-lingual experiment: ko -> id
[2025-04-12 19:05:31,845][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 19:05:31,845][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:05:34,672][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:05:34,672][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:05:34,741][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:05:34,776][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:05:34,887][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 19:05:34,894][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:05:34,895][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 19:05:34,896][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:05:34,921][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:05:34,970][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:05:34,986][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 19:05:34,988][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:05:34,988][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 19:05:34,989][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:05:35,019][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:05:35,069][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:05:35,089][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 19:05:35,091][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:05:35,091][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 19:05:35,092][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 19:05:35,094][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:05:35,094][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:05:35,094][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:05:35,094][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:05:35,094][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-12 19:05:35,094][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-12 19:05:35,094][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 19:05:35,095][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:05:35,095][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:05:35,095][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:05:35,095][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:05:35,095][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:05:35,095][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 19:05:35,095][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 19:05:35,095][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 19:05:35,096][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:05:35,096][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:05:35,096][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:05:35,096][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:05:35,096][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:05:35,096][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:05:35,096][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:05:35,096][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 19:05:35,097][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:05:35,097][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 19:05:35,097][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:05:35,097][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:05:35,098][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
[2025-04-12 19:05:37,911][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:05:37,911][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:05:37,950][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:05:37,998][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:05:38,018][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 19:05:38,026][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:05:38,026][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 19:05:38,028][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:05:38,064][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:05:38,110][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:05:38,127][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 19:05:38,129][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:05:38,129][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 19:05:38,131][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:05:38,168][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:05:38,213][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:05:38,230][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 19:05:38,231][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:05:38,232][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 19:05:38,233][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 19:05:38,233][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:05:38,234][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:05:38,234][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:05:38,234][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:05:38,234][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-12 19:05:38,234][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-12 19:05:38,234][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 19:05:38,234][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:05:38,235][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:05:38,235][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:05:38,235][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:05:38,235][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:05:38,235][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 19:05:38,235][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 19:05:38,235][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 19:05:38,235][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:05:38,236][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:05:38,236][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:05:38,236][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:05:38,236][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:05:38,236][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:05:38,236][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:05:38,236][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 19:05:38,236][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:05:38,236][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 19:05:38,237][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:05:38,237][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:05:38,237][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:05:43,582][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:05:43,584][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 19:05:43,585][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:05:43,585][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<00:55,  1.20s/it]Epoch 1/10:   4%|▍         | 2/47 [00:01<00:25,  1.80it/s]Epoch 1/10:   9%|▊         | 4/47 [00:01<00:11,  3.73it/s]Epoch 1/10:  13%|█▎        | 6/47 [00:01<00:07,  5.35it/s]Epoch 1/10:  17%|█▋        | 8/47 [00:01<00:05,  6.65it/s]Epoch 1/10:  21%|██▏       | 10/47 [00:02<00:04,  7.66it/s]Epoch 1/10:  26%|██▌       | 12/47 [00:02<00:04,  8.41it/s]Epoch 1/10:  30%|██▉       | 14/47 [00:02<00:03,  8.98it/s]Epoch 1/10:  34%|███▍      | 16/47 [00:02<00:03,  9.38it/s]Epoch 1/10:  38%|███▊      | 18/47 [00:02<00:02,  9.68it/s]Epoch 1/10:  43%|████▎     | 20/47 [00:03<00:02,  9.89it/s]Epoch 1/10:  47%|████▋     | 22/47 [00:03<00:02, 10.04it/s]Epoch 1/10:  51%|█████     | 24/47 [00:03<00:02, 10.14it/s]Epoch 1/10:  55%|█████▌    | 26/47 [00:03<00:02, 10.22it/s]Epoch 1/10:  60%|█████▉    | 28/47 [00:03<00:01, 10.27it/s]Epoch 1/10:  64%|██████▍   | 30/47 [00:04<00:01, 10.30it/s]Epoch 1/10:  68%|██████▊   | 32/47 [00:04<00:01, 10.33it/s]Epoch 1/10:  72%|███████▏  | 34/47 [00:04<00:01, 10.35it/s]Epoch 1/10:  77%|███████▋  | 36/47 [00:04<00:01, 10.36it/s]Epoch 1/10:  81%|████████  | 38/47 [00:04<00:00, 10.37it/s]Epoch 1/10:  85%|████████▌ | 40/47 [00:04<00:00, 10.37it/s]Epoch 1/10:  89%|████████▉ | 42/47 [00:05<00:00, 10.38it/s]Epoch 1/10:  94%|█████████▎| 44/47 [00:05<00:00, 10.39it/s]Epoch 1/10:  98%|█████████▊| 46/47 [00:05<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.34it/s]
[2025-04-12 19:05:51,343][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6859
[2025-04-12 19:05:51,765][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6920, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:10,  4.42it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  7.67it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  8.86it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.45it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.78it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03,  9.98it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:05:57,034][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6712
[2025-04-12 19:05:57,303][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6836, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:10,  4.26it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.56it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.79it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.40it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:01<00:03,  9.74it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03,  9.95it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.09it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.18it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.07it/s]
[2025-04-12 19:06:02,632][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6063
[2025-04-12 19:06:02,922][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4413, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9253731343283582}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:09,  4.69it/s]Epoch 4/10:   4%|▍         | 2/47 [00:00<00:11,  4.09it/s]Epoch 4/10:   9%|▊         | 4/47 [00:00<00:06,  6.60it/s]Epoch 4/10:  13%|█▎        | 6/47 [00:00<00:05,  7.97it/s]Epoch 4/10:  17%|█▋        | 8/47 [00:01<00:04,  8.80it/s]Epoch 4/10:  21%|██▏       | 10/47 [00:01<00:03,  9.32it/s]Epoch 4/10:  26%|██▌       | 12/47 [00:01<00:03,  9.66it/s]Epoch 4/10:  30%|██▉       | 14/47 [00:01<00:03,  9.89it/s]Epoch 4/10:  34%|███▍      | 16/47 [00:01<00:03, 10.04it/s]Epoch 4/10:  38%|███▊      | 18/47 [00:02<00:02, 10.15it/s]Epoch 4/10:  43%|████▎     | 20/47 [00:02<00:02, 10.22it/s]Epoch 4/10:  47%|████▋     | 22/47 [00:02<00:02, 10.27it/s]Epoch 4/10:  51%|█████     | 24/47 [00:02<00:02, 10.30it/s]Epoch 4/10:  55%|█████▌    | 26/47 [00:02<00:02, 10.33it/s]Epoch 4/10:  60%|█████▉    | 28/47 [00:02<00:01, 10.35it/s]Epoch 4/10:  64%|██████▍   | 30/47 [00:03<00:01, 10.36it/s]Epoch 4/10:  68%|██████▊   | 32/47 [00:03<00:01, 10.37it/s]Epoch 4/10:  72%|███████▏  | 34/47 [00:03<00:01, 10.37it/s]Epoch 4/10:  77%|███████▋  | 36/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  81%|████████  | 38/47 [00:03<00:00, 10.38it/s]Epoch 4/10:  85%|████████▌ | 40/47 [00:04<00:00, 10.38it/s]Epoch 4/10:  89%|████████▉ | 42/47 [00:04<00:00, 10.39it/s]Epoch 4/10:  94%|█████████▎| 44/47 [00:04<00:00, 10.39it/s]Epoch 4/10:  98%|█████████▊| 46/47 [00:04<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00,  9.78it/s]
[2025-04-12 19:06:08,120][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.3071
[2025-04-12 19:06:08,402][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2225, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:09,  4.62it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  7.83it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  8.96it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.52it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:00<00:03,  9.83it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-12 19:06:13,472][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1219
[2025-04-12 19:06:13,774][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2209, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:10,  4.41it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.67it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  8.86it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.45it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:00<00:03,  9.78it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03,  9.98it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.09it/s]
[2025-04-12 19:06:18,851][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0734
[2025-04-12 19:06:19,161][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1831, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.918918918918919}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:10,  4.22it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  7.52it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  8.76it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.38it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:01<00:03,  9.74it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03,  9.96it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.10it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.19it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.24it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.31it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.06it/s]
[2025-04-12 19:06:24,260][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0418
[2025-04-12 19:06:24,563][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1668, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:09,  4.64it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.85it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  8.97it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.52it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:00<00:03,  9.83it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03, 10.01it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-12 19:06:29,641][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0306
[2025-04-12 19:06:29,936][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1436, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:10,  4.26it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:05,  7.55it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:04,  8.78it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  9.40it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:01<00:03,  9.75it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03,  9.96it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03, 10.10it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03, 10.19it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 9/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 10.08it/s]
[2025-04-12 19:06:35,037][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0264
[2025-04-12 19:06:35,342][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1544, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:10,  4.26it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:05,  7.55it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:04,  8.78it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:00<00:04,  9.40it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:01<00:03,  9.75it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03,  9.96it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03, 10.10it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03, 10.19it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 10/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 10.06it/s]
[2025-04-12 19:06:40,015][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0245
[2025-04-12 19:06:40,383][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1645, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁██████
wandb:          best_val_f1 ▁▁██████
wandb:        best_val_loss ██▅▂▂▂▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▇▄▂▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁████████
wandb:               val_f1 ▁▁████████
wandb:             val_loss ██▅▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94444
wandb:        best_val_loss 0.1436
wandb:                epoch 10
wandb:  final_test_accuracy 0.82727
wandb:        final_test_f1 0.81188
wandb: final_train_accuracy 0.99729
wandb:       final_train_f1 0.99706
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94444
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02451
wandb:           train_time 54.67871
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94444
wandb:             val_loss 0.16452
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_190530-3lqxmb9e
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_190530-3lqxmb9e/logs
Cross-lingual experiment for question_type (ko → id) completed successfully
Running cross-lingual complexity from ko to id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:07:01,022][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ko_to_id/complexity
experiment_name: cross_lingual_complexity_ko_to_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ko
  eval_language: id
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:07:01,022][__main__][INFO] - Normalized task: complexity
[2025-04-12 19:07:01,022][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 19:07:01,022][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:07:02,408][__main__][INFO] - Running cross-lingual experiment: ko -> id
[2025-04-12 19:07:02,408][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 19:07:02,408][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:07:05,272][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:07:05,272][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:07:05,364][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:07:05,405][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:07:05,552][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 19:07:05,560][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:07:05,560][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 19:07:05,562][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:07:05,599][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:07:05,647][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:07:05,666][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 19:07:05,667][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:07:05,668][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 19:07:05,669][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:07:05,702][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:07:05,745][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:07:05,762][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 19:07:05,763][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:07:05,763][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 19:07:05,765][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 19:07:05,766][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:07:05,766][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:07:05,766][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:07:05,766][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:07:05,766][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:07:05,767][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-12 19:07:05,767][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 19:07:05,767][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-04-12 19:07:05,767][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:07:05,767][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:07:05,767][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:07:05,767][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:07:05,767][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:07:05,768][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-12 19:07:05,768][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 19:07:05,768][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-12 19:07:05,768][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:07:05,768][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:07:05,768][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:07:05,768][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:07:05,768][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:07:05,769][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-12 19:07:05,769][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 19:07:05,769][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-12 19:07:05,769][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 19:07:05,769][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:07:05,769][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:07:05,770][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
[2025-04-12 19:07:08,560][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:07:08,561][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:07:08,593][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:07:08,632][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:07:08,650][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 19:07:08,657][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:07:08,658][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 19:07:08,659][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:07:08,688][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:07:08,728][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:07:08,744][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 19:07:08,745][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:07:08,746][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 19:07:08,747][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:07:08,776][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:07:08,816][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:07:08,831][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 19:07:08,833][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:07:08,833][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 19:07:08,834][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 19:07:08,834][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:07:08,835][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:07:08,835][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:07:08,835][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:07:08,835][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:07:08,835][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-12 19:07:08,835][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 19:07:08,835][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-12 19:07:08,836][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:07:08,836][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:07:08,836][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:07:08,836][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:07:08,836][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:07:08,836][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-12 19:07:08,836][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 19:07:08,836][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-12 19:07:08,837][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:07:08,837][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:07:08,837][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:07:08,837][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:07:08,837][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:07:08,838][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-12 19:07:08,838][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 19:07:08,838][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-12 19:07:08,838][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 19:07:08,838][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:07:08,838][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:07:08,838][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:07:14,354][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:07:14,357][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 19:07:14,357][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:07:14,357][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<01:03,  1.39s/it]Epoch 1/10:   4%|▍         | 2/47 [00:01<00:28,  1.58it/s]Epoch 1/10:   6%|▋         | 3/47 [00:01<00:17,  2.55it/s]Epoch 1/10:  11%|█         | 5/47 [00:01<00:09,  4.45it/s]Epoch 1/10:  15%|█▍        | 7/47 [00:01<00:06,  5.98it/s]Epoch 1/10:  19%|█▉        | 9/47 [00:02<00:05,  7.16it/s]Epoch 1/10:  23%|██▎       | 11/47 [00:02<00:04,  8.06it/s]Epoch 1/10:  28%|██▊       | 13/47 [00:02<00:03,  8.72it/s]Epoch 1/10:  32%|███▏      | 15/47 [00:02<00:03,  9.20it/s]Epoch 1/10:  36%|███▌      | 17/47 [00:02<00:03,  9.55it/s]Epoch 1/10:  40%|████      | 19/47 [00:03<00:02,  9.80it/s]Epoch 1/10:  45%|████▍     | 21/47 [00:03<00:02,  9.98it/s]Epoch 1/10:  49%|████▉     | 23/47 [00:03<00:02, 10.10it/s]Epoch 1/10:  53%|█████▎    | 25/47 [00:03<00:02, 10.19it/s]Epoch 1/10:  57%|█████▋    | 27/47 [00:03<00:01, 10.25it/s]Epoch 1/10:  62%|██████▏   | 29/47 [00:04<00:01, 10.29it/s]Epoch 1/10:  66%|██████▌   | 31/47 [00:04<00:01, 10.32it/s]Epoch 1/10:  70%|███████   | 33/47 [00:04<00:01, 10.34it/s]Epoch 1/10:  74%|███████▍  | 35/47 [00:04<00:01, 10.36it/s]Epoch 1/10:  79%|███████▊  | 37/47 [00:04<00:00, 10.37it/s]Epoch 1/10:  83%|████████▎ | 39/47 [00:05<00:00, 10.37it/s]Epoch 1/10:  87%|████████▋ | 41/47 [00:05<00:00, 10.38it/s]Epoch 1/10:  91%|█████████▏| 43/47 [00:05<00:00, 10.39it/s]Epoch 1/10:  96%|█████████▌| 45/47 [00:05<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00, 11.09it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.06it/s]
[2025-04-12 19:07:22,418][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1679
[2025-04-12 19:07:22,692][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2253, Metrics: {'mse': 0.23345845937728882, 'rmse': 0.4831753919409481, 'r2': -3.955507755279541}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:10,  4.45it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  7.70it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  8.88it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.46it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.79it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-12 19:07:27,956][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0708
[2025-04-12 19:07:28,235][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0395, Metrics: {'mse': 0.04067068547010422, 'rmse': 0.2016697435663174, 'r2': 0.13670337200164795}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:09,  4.61it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.83it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.97it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.52it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:00<00:03,  9.83it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.13it/s]
[2025-04-12 19:07:33,542][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0324
[2025-04-12 19:07:33,873][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0373, Metrics: {'mse': 0.03878728672862053, 'rmse': 0.19694488246364902, 'r2': 0.17668139934539795}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:10,  4.27it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  7.56it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  8.79it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.41it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:01<00:03,  9.75it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03,  9.97it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.10it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:07:38,930][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0224
[2025-04-12 19:07:39,215][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0251, Metrics: {'mse': 0.02600804716348648, 'rmse': 0.1612701062301581, 'r2': 0.44793999195098877}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:10,  4.31it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  7.59it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  8.81it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.41it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:01<00:03,  9.76it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03,  9.97it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.07it/s]
[2025-04-12 19:07:44,313][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0156
[2025-04-12 19:07:44,605][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0336, Metrics: {'mse': 0.035184260457754135, 'rmse': 0.18757467968186467, 'r2': 0.2531610131263733}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:10,  4.32it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.60it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  8.82it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.42it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:01<00:03,  9.77it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03,  9.98it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.37it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.07it/s]
[2025-04-12 19:07:49,276][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0154
[2025-04-12 19:07:49,567][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0285, Metrics: {'mse': 0.02866884134709835, 'rmse': 0.1693187566310902, 'r2': 0.391460657119751}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:10,  4.33it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  7.61it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  8.82it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.42it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:01<00:03,  9.76it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03,  9.98it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.27it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-12 19:07:54,220][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0128
[2025-04-12 19:07:54,536][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0158, Metrics: {'mse': 0.016274383291602135, 'rmse': 0.1275710911280535, 'r2': 0.6545516848564148}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:09,  4.65it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.85it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  8.98it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.52it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:00<00:03,  9.83it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-12 19:07:59,603][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0139
[2025-04-12 19:07:59,891][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0184, Metrics: {'mse': 0.01938963308930397, 'rmse': 0.13924666275822903, 'r2': 0.5884258151054382}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:10,  4.43it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:05,  7.68it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:04,  8.87it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  9.46it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:00<00:03,  9.79it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 9/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:08:04,548][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0117
[2025-04-12 19:08:04,847][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0145, Metrics: {'mse': 0.014588968828320503, 'rmse': 0.12078480379716855, 'r2': 0.6903271675109863}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:10,  4.30it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:05,  7.58it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:04,  8.78it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:00<00:04,  9.39it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:01<00:03,  9.75it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03,  9.96it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03, 10.10it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03, 10.19it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 10/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.37it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 10.09it/s]
[2025-04-12 19:08:09,939][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0083
[2025-04-12 19:08:10,241][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0196, Metrics: {'mse': 0.01942511647939682, 'rmse': 0.13937401651454556, 'r2': 0.5876725912094116}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▂▁▁▁
wandb:     best_val_mse █▂▂▁▁▁
wandb:      best_val_r2 ▁▇▇███
wandb:    best_val_rmse █▃▂▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▂▂▁▂▁▁▁▁▁
wandb:          val_mse █▂▂▁▂▁▁▁▁▁
wandb:           val_r2 ▁▇▇█▇█████
wandb:         val_rmse █▃▂▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01449
wandb:     best_val_mse 0.01459
wandb:      best_val_r2 0.69033
wandb:    best_val_rmse 0.12078
wandb:            epoch 10
wandb:   final_test_mse 0.02827
wandb:    final_test_r2 0.30661
wandb:  final_test_rmse 0.16814
wandb:  final_train_mse 0.01027
wandb:   final_train_r2 0.53885
wandb: final_train_rmse 0.10132
wandb:    final_val_mse 0.01459
wandb:     final_val_r2 0.69033
wandb:   final_val_rmse 0.12078
wandb:    learning_rate 1e-05
wandb:       train_loss 0.00828
wandb:       train_time 53.66029
wandb:         val_loss 0.01964
wandb:          val_mse 0.01943
wandb:           val_r2 0.58767
wandb:         val_rmse 0.13937
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_190701-9s9g37zc
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_190701-9s9g37zc/logs
Cross-lingual experiment for complexity (ko → id) completed successfully
Running cross-lingual question_type from ko to ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:08:30,608][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ko_to_ja/question_type
experiment_name: cross_lingual_question_type_ko_to_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ko
  eval_language: ja
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:08:30,608][__main__][INFO] - Normalized task: question_type
[2025-04-12 19:08:30,608][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 19:08:30,608][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:08:32,051][__main__][INFO] - Running cross-lingual experiment: ko -> ja
[2025-04-12 19:08:32,051][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 19:08:32,051][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:08:34,896][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:08:34,897][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:08:34,985][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:08:35,024][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:08:35,144][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 19:08:35,152][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:08:35,152][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 19:08:35,154][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:08:35,182][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:08:35,220][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:08:35,238][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 19:08:35,239][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:08:35,239][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 19:08:35,241][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:08:35,266][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:08:35,298][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:08:35,313][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 19:08:35,315][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:08:35,315][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 19:08:35,316][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 19:08:35,317][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:08:35,317][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:08:35,317][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:08:35,317][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:08:35,318][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-12 19:08:35,318][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-12 19:08:35,318][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 19:08:35,318][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:08:35,318][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:08:35,318][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:08:35,318][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:08:35,318][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:08:35,319][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 19:08:35,319][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 19:08:35,319][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 19:08:35,319][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:08:35,319][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:08:35,319][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:08:35,319][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:08:35,319][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:08:35,320][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:08:35,320][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:08:35,320][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 19:08:35,320][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:08:35,320][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 19:08:35,320][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:08:35,320][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:08:35,321][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
[2025-04-12 19:08:38,120][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:08:38,121][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:08:38,152][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:08:38,193][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:08:38,211][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 19:08:38,220][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:08:38,221][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 19:08:38,223][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:08:38,253][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:08:38,299][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:08:38,317][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 19:08:38,318][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:08:38,318][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 19:08:38,320][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:08:38,348][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:08:38,389][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:08:38,407][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 19:08:38,408][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:08:38,409][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 19:08:38,410][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 19:08:38,410][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:08:38,411][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:08:38,411][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:08:38,411][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:08:38,411][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-12 19:08:38,411][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 19:08:38,411][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 19:08:38,411][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:08:38,411][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:08:38,412][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:08:38,412][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:08:38,412][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:08:38,412][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-12 19:08:38,412][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-12 19:08:38,412][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 19:08:38,412][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:08:38,412][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:08:38,413][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:08:38,413][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:08:38,413][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:08:38,413][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-12 19:08:38,413][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-12 19:08:38,413][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 19:08:38,413][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:08:38,413][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 19:08:38,413][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:08:38,414][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:08:38,414][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:08:43,413][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:08:43,416][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 19:08:43,416][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:08:43,416][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<01:01,  1.33s/it]Epoch 1/10:   4%|▍         | 2/47 [00:01<00:27,  1.64it/s]Epoch 1/10:   9%|▊         | 4/47 [00:01<00:12,  3.47it/s]Epoch 1/10:  13%|█▎        | 6/47 [00:01<00:08,  5.07it/s]Epoch 1/10:  17%|█▋        | 8/47 [00:02<00:06,  6.39it/s]Epoch 1/10:  21%|██▏       | 10/47 [00:02<00:04,  7.43it/s]Epoch 1/10:  23%|██▎       | 11/47 [00:02<00:05,  6.66it/s]Epoch 1/10:  28%|██▊       | 13/47 [00:02<00:04,  7.68it/s]Epoch 1/10:  32%|███▏      | 15/47 [00:02<00:03,  8.45it/s]Epoch 1/10:  36%|███▌      | 17/47 [00:02<00:03,  9.01it/s]Epoch 1/10:  40%|████      | 19/47 [00:03<00:02,  9.41it/s]Epoch 1/10:  45%|████▍     | 21/47 [00:03<00:02,  9.70it/s]Epoch 1/10:  49%|████▉     | 23/47 [00:03<00:02,  9.91it/s]Epoch 1/10:  53%|█████▎    | 25/47 [00:03<00:02, 10.05it/s]Epoch 1/10:  57%|█████▋    | 27/47 [00:03<00:01, 10.16it/s]Epoch 1/10:  62%|██████▏   | 29/47 [00:04<00:01, 10.23it/s]Epoch 1/10:  66%|██████▌   | 31/47 [00:04<00:01, 10.28it/s]Epoch 1/10:  70%|███████   | 33/47 [00:04<00:01, 10.31it/s]Epoch 1/10:  74%|███████▍  | 35/47 [00:04<00:01, 10.34it/s]Epoch 1/10:  79%|███████▊  | 37/47 [00:04<00:00, 10.35it/s]Epoch 1/10:  83%|████████▎ | 39/47 [00:05<00:00, 10.37it/s]Epoch 1/10:  87%|████████▋ | 41/47 [00:05<00:00, 10.38it/s]Epoch 1/10:  91%|█████████▏| 43/47 [00:05<00:00, 10.39it/s]Epoch 1/10:  96%|█████████▌| 45/47 [00:05<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00, 11.09it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.00it/s]
[2025-04-12 19:08:51,493][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6859
[2025-04-12 19:08:51,751][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6920, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:09,  4.76it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  7.94it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  9.04it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.57it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.86it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03, 10.05it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.16it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.24it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.29it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.31it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.33it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.35it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.16it/s]
[2025-04-12 19:08:56,839][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6712
[2025-04-12 19:08:57,110][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6836, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:10,  4.38it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.64it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.84it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.44it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:00<00:03,  9.78it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:09:02,416][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6063
[2025-04-12 19:09:02,774][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4413, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9253731343283582}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:10,  4.41it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  7.68it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  8.87it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.46it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:00<00:03,  9.79it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03, 10.00it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-12 19:09:07,822][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.3071
[2025-04-12 19:09:08,118][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2225, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:10,  4.47it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  7.73it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  8.90it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.47it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:00<00:03,  9.80it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03,  9.98it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:09:13,202][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1219
[2025-04-12 19:09:13,492][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2209, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:10,  4.39it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.66it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  8.86it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.45it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:00<00:03,  9.79it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-12 19:09:18,542][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0734
[2025-04-12 19:09:18,827][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1831, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.918918918918919}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:09,  4.61it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  7.83it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  8.96it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.52it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:00<00:03,  9.83it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-12 19:09:23,905][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0418
[2025-04-12 19:09:24,200][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1668, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:10,  4.38it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.65it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  8.85it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.44it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:00<00:03,  9.78it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-12 19:09:29,261][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0306
[2025-04-12 19:09:29,558][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1436, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:09,  4.60it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:05,  7.81it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:04,  8.95it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  9.51it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:00<00:03,  9.83it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 9/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 10.14it/s]
[2025-04-12 19:09:34,658][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0264
[2025-04-12 19:09:34,937][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1544, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:10,  4.46it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:05,  7.72it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:04,  8.88it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:00<00:04,  9.46it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:00<00:03,  9.80it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03, 10.00it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 10/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 10.09it/s]
[2025-04-12 19:09:39,597][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0245
[2025-04-12 19:09:39,900][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1645, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁██████
wandb:          best_val_f1 ▁▁██████
wandb:        best_val_loss ██▅▂▂▂▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▇▄▂▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁████████
wandb:               val_f1 ▁▁████████
wandb:             val_loss ██▅▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94444
wandb:        best_val_loss 0.1436
wandb:                epoch 10
wandb:  final_test_accuracy 0.91304
wandb:        final_test_f1 0.9322
wandb: final_train_accuracy 0.99729
wandb:       final_train_f1 0.99706
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94444
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02451
wandb:           train_time 54.28813
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94444
wandb:             val_loss 0.16452
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_190830-n1dgzzzu
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_190830-n1dgzzzu/logs
Cross-lingual experiment for question_type (ko → ja) completed successfully
Running cross-lingual complexity from ko to ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:10:01,209][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ko_to_ja/complexity
experiment_name: cross_lingual_complexity_ko_to_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ko
  eval_language: ja
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:10:01,209][__main__][INFO] - Normalized task: complexity
[2025-04-12 19:10:01,209][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 19:10:01,209][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:10:02,584][__main__][INFO] - Running cross-lingual experiment: ko -> ja
[2025-04-12 19:10:02,584][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 19:10:02,584][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:10:05,505][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:10:05,506][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:10:05,601][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:10:05,645][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:10:05,774][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 19:10:05,781][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:10:05,782][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 19:10:05,783][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:10:05,813][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:10:05,858][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:10:05,875][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 19:10:05,877][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:10:05,877][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 19:10:05,879][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:10:05,911][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:10:05,949][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:10:05,964][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 19:10:05,966][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:10:05,966][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 19:10:05,968][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 19:10:05,968][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:10:05,968][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:10:05,968][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:10:05,968][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:10:05,969][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:10:05,969][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-12 19:10:05,969][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 19:10:05,969][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-04-12 19:10:05,969][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:10:05,969][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:10:05,970][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:10:05,970][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:10:05,970][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:10:05,970][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-12 19:10:05,970][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 19:10:05,970][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-12 19:10:05,970][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:10:05,971][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:10:05,971][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:10:05,971][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:10:05,971][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:10:05,971][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-12 19:10:05,971][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 19:10:05,971][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-12 19:10:05,971][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 19:10:05,971][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:10:05,972][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:10:05,972][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
[2025-04-12 19:10:08,850][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:10:08,851][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:10:08,880][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:10:08,918][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:10:08,935][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 19:10:08,945][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:10:08,945][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 19:10:08,947][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:10:08,979][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:10:09,021][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:10:09,036][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 19:10:09,037][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:10:09,037][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 19:10:09,039][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:10:09,063][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:10:09,097][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:10:09,112][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 19:10:09,113][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:10:09,113][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 19:10:09,114][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 19:10:09,115][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:10:09,115][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:10:09,115][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:10:09,115][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:10:09,115][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:10:09,115][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-12 19:10:09,115][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 19:10:09,116][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-04-12 19:10:09,116][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:10:09,116][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:10:09,116][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:10:09,116][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:10:09,116][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:10:09,116][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-12 19:10:09,116][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 19:10:09,117][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-12 19:10:09,117][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:10:09,117][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:10:09,117][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:10:09,117][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:10:09,117][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:10:09,117][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-12 19:10:09,117][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 19:10:09,118][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-12 19:10:09,118][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 19:10:09,118][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:10:09,118][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:10:09,118][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:10:14,249][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:10:14,252][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 19:10:14,253][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:10:14,253][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<01:06,  1.44s/it]Epoch 1/10:   6%|▋         | 3/47 [00:01<00:19,  2.25it/s]Epoch 1/10:  11%|█         | 5/47 [00:01<00:11,  3.77it/s]Epoch 1/10:  15%|█▍        | 7/47 [00:02<00:07,  5.16it/s]Epoch 1/10:  19%|█▉        | 9/47 [00:02<00:05,  6.37it/s]Epoch 1/10:  23%|██▎       | 11/47 [00:02<00:05,  6.45it/s]Epoch 1/10:  28%|██▊       | 13/47 [00:02<00:04,  7.38it/s]Epoch 1/10:  32%|███▏      | 15/47 [00:02<00:03,  8.14it/s]Epoch 1/10:  36%|███▌      | 17/47 [00:03<00:03,  8.74it/s]Epoch 1/10:  40%|████      | 19/47 [00:03<00:03,  9.20it/s]Epoch 1/10:  45%|████▍     | 21/47 [00:03<00:02,  9.54it/s]Epoch 1/10:  49%|████▉     | 23/47 [00:03<00:02,  9.79it/s]Epoch 1/10:  53%|█████▎    | 25/47 [00:03<00:02,  9.97it/s]Epoch 1/10:  57%|█████▋    | 27/47 [00:04<00:01, 10.09it/s]Epoch 1/10:  62%|██████▏   | 29/47 [00:04<00:01, 10.18it/s]Epoch 1/10:  66%|██████▌   | 31/47 [00:04<00:01, 10.25it/s]Epoch 1/10:  70%|███████   | 33/47 [00:04<00:01, 10.29it/s]Epoch 1/10:  74%|███████▍  | 35/47 [00:04<00:01, 10.33it/s]Epoch 1/10:  79%|███████▊  | 37/47 [00:05<00:00, 10.35it/s]Epoch 1/10:  83%|████████▎ | 39/47 [00:05<00:00, 10.36it/s]Epoch 1/10:  87%|████████▋ | 41/47 [00:05<00:00, 10.38it/s]Epoch 1/10:  91%|█████████▏| 43/47 [00:05<00:00, 10.39it/s]Epoch 1/10:  96%|█████████▌| 45/47 [00:05<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00, 11.09it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  7.87it/s]
[2025-04-12 19:10:22,473][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1679
[2025-04-12 19:10:22,743][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2253, Metrics: {'mse': 0.23345845937728882, 'rmse': 0.4831753919409481, 'r2': -3.955507755279541}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:09,  4.65it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  7.87it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  8.99it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.54it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.85it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03, 10.04it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.16it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.29it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.33it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.37it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.39it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.41it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.41it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.16it/s]
[2025-04-12 19:10:27,838][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0708
[2025-04-12 19:10:28,114][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0395, Metrics: {'mse': 0.04067068547010422, 'rmse': 0.2016697435663174, 'r2': 0.13670337200164795}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:10,  4.33it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.61it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.82it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.43it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:01<00:03,  9.77it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03,  9.98it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:10:33,429][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0324
[2025-04-12 19:10:33,814][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0373, Metrics: {'mse': 0.03878728672862053, 'rmse': 0.19694488246364902, 'r2': 0.17668139934539795}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:10,  4.22it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  7.53it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  8.76it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.39it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:01<00:03,  9.74it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03,  9.96it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.10it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.30it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.08it/s]
[2025-04-12 19:10:38,889][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0224
[2025-04-12 19:10:39,192][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0251, Metrics: {'mse': 0.02600804716348648, 'rmse': 0.1612701062301581, 'r2': 0.44793999195098877}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:10,  4.31it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  7.60it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  8.82it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.43it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:01<00:03,  9.77it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03,  9.98it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-12 19:10:44,274][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0156
[2025-04-12 19:10:44,562][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0336, Metrics: {'mse': 0.035184260457754135, 'rmse': 0.18757467968186467, 'r2': 0.2531610131263733}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:10,  4.50it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.75it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  8.92it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.49it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:00<00:03,  9.82it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-12 19:10:49,209][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0154
[2025-04-12 19:10:49,517][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0285, Metrics: {'mse': 0.02866884134709835, 'rmse': 0.1693187566310902, 'r2': 0.391460657119751}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:09,  4.62it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  7.84it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  8.98it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.52it/s]Epoch 7/10:  17%|█▋        | 8/47 [00:00<00:04,  9.62it/s]Epoch 7/10:  21%|██▏       | 10/47 [00:01<00:03,  9.92it/s]Epoch 7/10:  26%|██▌       | 12/47 [00:01<00:03, 10.09it/s]Epoch 7/10:  30%|██▉       | 14/47 [00:01<00:03, 10.19it/s]Epoch 7/10:  34%|███▍      | 16/47 [00:01<00:03, 10.26it/s]Epoch 7/10:  38%|███▊      | 18/47 [00:01<00:02, 10.31it/s]Epoch 7/10:  43%|████▎     | 20/47 [00:02<00:02, 10.33it/s]Epoch 7/10:  47%|████▋     | 22/47 [00:02<00:02, 10.36it/s]Epoch 7/10:  51%|█████     | 24/47 [00:02<00:02, 10.37it/s]Epoch 7/10:  55%|█████▌    | 26/47 [00:02<00:02, 10.38it/s]Epoch 7/10:  60%|█████▉    | 28/47 [00:02<00:01, 10.39it/s]Epoch 7/10:  64%|██████▍   | 30/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  68%|██████▊   | 32/47 [00:03<00:01, 10.40it/s]Epoch 7/10:  72%|███████▏  | 34/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  77%|███████▋  | 36/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  81%|████████  | 38/47 [00:03<00:00, 10.40it/s]Epoch 7/10:  85%|████████▌ | 40/47 [00:03<00:00, 10.40it/s]Epoch 7/10:  89%|████████▉ | 42/47 [00:04<00:00, 10.41it/s]Epoch 7/10:  94%|█████████▎| 44/47 [00:04<00:00, 10.41it/s]Epoch 7/10:  98%|█████████▊| 46/47 [00:04<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.14it/s]
[2025-04-12 19:10:54,156][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0128
[2025-04-12 19:10:54,450][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0158, Metrics: {'mse': 0.016274383291602135, 'rmse': 0.1275710911280535, 'r2': 0.6545516848564148}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:10,  4.45it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.71it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  8.89it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.47it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:00<00:03,  9.80it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03, 10.00it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.41it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.09it/s]
[2025-04-12 19:10:59,524][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0139
[2025-04-12 19:10:59,821][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0184, Metrics: {'mse': 0.01938963308930397, 'rmse': 0.13924666275822903, 'r2': 0.5884258151054382}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:10,  4.45it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:05,  7.71it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:04,  8.89it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  9.48it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:00<00:03,  9.81it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03, 10.01it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 9/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.38it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.39it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.39it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.40it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.40it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-12 19:11:04,472][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0117
[2025-04-12 19:11:04,759][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0145, Metrics: {'mse': 0.014588968828320503, 'rmse': 0.12078480379716855, 'r2': 0.6903271675109863}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:10,  4.26it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:05,  7.55it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:04,  8.79it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:00<00:04,  9.40it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:01<00:03,  9.75it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03,  9.97it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03, 10.11it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 10/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 10.08it/s]
[2025-04-12 19:11:09,858][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0083
[2025-04-12 19:11:10,160][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0196, Metrics: {'mse': 0.01942511647939682, 'rmse': 0.13937401651454556, 'r2': 0.5876725912094116}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▂▁▁▁
wandb:     best_val_mse █▂▂▁▁▁
wandb:      best_val_r2 ▁▇▇███
wandb:    best_val_rmse █▃▂▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▂▂▁▂▁▁▁▁▁
wandb:          val_mse █▂▂▁▂▁▁▁▁▁
wandb:           val_r2 ▁▇▇█▇█████
wandb:         val_rmse █▃▂▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01449
wandb:     best_val_mse 0.01459
wandb:      best_val_r2 0.69033
wandb:    best_val_rmse 0.12078
wandb:            epoch 10
wandb:   final_test_mse 0.02416
wandb:    final_test_r2 0.53584
wandb:  final_test_rmse 0.15545
wandb:  final_train_mse 0.01027
wandb:   final_train_r2 0.53885
wandb: final_train_rmse 0.10132
wandb:    final_val_mse 0.01459
wandb:     final_val_r2 0.69033
wandb:   final_val_rmse 0.12078
wandb:    learning_rate 1e-05
wandb:       train_loss 0.00828
wandb:       train_time 53.66523
wandb:         val_loss 0.01964
wandb:          val_mse 0.01943
wandb:           val_r2 0.58767
wandb:         val_rmse 0.13937
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_191001-opwzwb0v
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_191001-opwzwb0v/logs
Cross-lingual experiment for complexity (ko → ja) completed successfully
Running cross-lingual question_type from ko to ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:11:30,633][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ko_to_ru/question_type
experiment_name: cross_lingual_question_type_ko_to_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ko
  eval_language: ru
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:11:30,633][__main__][INFO] - Normalized task: question_type
[2025-04-12 19:11:30,633][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 19:11:30,633][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:11:32,073][__main__][INFO] - Running cross-lingual experiment: ko -> ru
[2025-04-12 19:11:32,073][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 19:11:32,074][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:11:35,016][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:11:35,016][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:11:35,094][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:11:35,130][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:11:35,256][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 19:11:35,264][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:11:35,264][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 19:11:35,266][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:11:35,296][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:11:35,336][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:11:35,352][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 19:11:35,353][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:11:35,353][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 19:11:35,355][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:11:35,381][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:11:35,422][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:11:35,440][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 19:11:35,442][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:11:35,442][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 19:11:35,443][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 19:11:35,443][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:11:35,443][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:11:35,444][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:11:35,444][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:11:35,444][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-12 19:11:35,444][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-12 19:11:35,444][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 19:11:35,444][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:11:35,444][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:11:35,445][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:11:35,445][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:11:35,445][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:11:35,445][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 19:11:35,445][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 19:11:35,445][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 19:11:35,445][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:11:35,445][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:11:35,446][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:11:35,446][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:11:35,446][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:11:35,446][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:11:35,446][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:11:35,446][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 19:11:35,446][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:11:35,446][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 19:11:35,446][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:11:35,447][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:11:35,447][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
[2025-04-12 19:11:38,316][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:11:38,317][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:11:38,347][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:11:38,389][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:11:38,409][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 19:11:38,419][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:11:38,419][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 19:11:38,421][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:11:38,454][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:11:38,496][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:11:38,511][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 19:11:38,513][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:11:38,513][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 19:11:38,514][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:11:38,538][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:11:38,574][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:11:38,590][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 19:11:38,591][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:11:38,592][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 19:11:38,593][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 19:11:38,593][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:11:38,593][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:11:38,593][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:11:38,593][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:11:38,594][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 19:11:38,594][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-12 19:11:38,594][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 19:11:38,594][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:11:38,594][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:11:38,594][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:11:38,594][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:11:38,594][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:11:38,595][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 19:11:38,595][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 19:11:38,595][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 19:11:38,595][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:11:38,595][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:11:38,595][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:11:38,595][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:11:38,595][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:11:38,596][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:11:38,596][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:11:38,596][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 19:11:38,596][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:11:38,596][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 19:11:38,596][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:11:38,596][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:11:38,597][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:11:43,655][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:11:43,658][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 19:11:43,658][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:11:43,658][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<00:56,  1.23s/it]Epoch 1/10:   6%|▋         | 3/47 [00:01<00:17,  2.56it/s]Epoch 1/10:  11%|█         | 5/47 [00:01<00:10,  4.18it/s]Epoch 1/10:  15%|█▍        | 7/47 [00:01<00:07,  5.60it/s]Epoch 1/10:  19%|█▉        | 9/47 [00:01<00:05,  6.78it/s]Epoch 1/10:  21%|██▏       | 10/47 [00:02<00:05,  6.24it/s]Epoch 1/10:  26%|██▌       | 12/47 [00:02<00:04,  7.34it/s]Epoch 1/10:  30%|██▉       | 14/47 [00:02<00:04,  8.18it/s]Epoch 1/10:  34%|███▍      | 16/47 [00:02<00:03,  8.81it/s]Epoch 1/10:  38%|███▊      | 18/47 [00:02<00:03,  9.27it/s]Epoch 1/10:  43%|████▎     | 20/47 [00:03<00:02,  9.60it/s]Epoch 1/10:  47%|████▋     | 22/47 [00:03<00:02,  9.83it/s]Epoch 1/10:  51%|█████     | 24/47 [00:03<00:02, 10.00it/s]Epoch 1/10:  55%|█████▌    | 26/47 [00:03<00:02, 10.12it/s]Epoch 1/10:  60%|█████▉    | 28/47 [00:03<00:01, 10.20it/s]Epoch 1/10:  64%|██████▍   | 30/47 [00:04<00:01, 10.26it/s]Epoch 1/10:  68%|██████▊   | 32/47 [00:04<00:01, 10.30it/s]Epoch 1/10:  72%|███████▏  | 34/47 [00:04<00:01, 10.33it/s]Epoch 1/10:  77%|███████▋  | 36/47 [00:04<00:01, 10.35it/s]Epoch 1/10:  81%|████████  | 38/47 [00:04<00:00, 10.36it/s]Epoch 1/10:  85%|████████▌ | 40/47 [00:05<00:00, 10.37it/s]Epoch 1/10:  89%|████████▉ | 42/47 [00:05<00:00, 10.38it/s]Epoch 1/10:  94%|█████████▎| 44/47 [00:05<00:00, 10.39it/s]Epoch 1/10:  98%|█████████▊| 46/47 [00:05<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.15it/s]
[2025-04-12 19:11:51,412][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6859
[2025-04-12 19:11:51,677][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6920, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:09,  4.64it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  7.86it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  8.98it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.53it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.84it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03, 10.03it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.15it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.15it/s]
[2025-04-12 19:11:56,776][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6712
[2025-04-12 19:11:57,061][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.6836, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:10,  4.46it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.71it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.89it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.47it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:00<00:03,  9.80it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03, 10.00it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:12:02,364][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.6063
[2025-04-12 19:12:02,771][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.4413, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9253731343283582}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:10,  4.45it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:05,  7.71it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:04,  8.89it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:00<00:04,  9.47it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:00<00:03,  9.80it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03, 10.00it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03, 10.13it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:01<00:02, 10.27it/s]Epoch 4/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:12:07,830][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.3071
[2025-04-12 19:12:08,128][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.2225, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:10,  4.41it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  7.67it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  8.87it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.46it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:00<00:03,  9.79it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:12:13,205][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1219
[2025-04-12 19:12:13,503][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2209, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:09,  4.68it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.88it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  9.00it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.54it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:00<00:03,  9.84it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03, 10.03it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.15it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.23it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.15it/s]
[2025-04-12 19:12:18,557][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0734
[2025-04-12 19:12:18,857][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.1831, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.918918918918919}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:10,  4.36it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  7.64it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  8.84it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.44it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:00<00:03,  9.78it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03,  9.98it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.40it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.40it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.40it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.40it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:12:23,942][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0418
[2025-04-12 19:12:24,239][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.1668, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:10,  4.21it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.51it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  8.76it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.38it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:01<00:03,  9.74it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03,  9.96it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.10it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.09it/s]
[2025-04-12 19:12:29,331][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0306
[2025-04-12 19:12:29,640][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.1436, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:11,  4.14it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:05,  7.46it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:04,  8.73it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  9.37it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:01<00:03,  9.72it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03,  9.95it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03, 10.09it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03, 10.18it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 9/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 10.08it/s]
[2025-04-12 19:12:34,741][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0264
[2025-04-12 19:12:35,046][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.1544, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:10,  4.55it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:05,  7.79it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:04,  8.94it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:00<00:04,  9.50it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:00<00:03,  9.82it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03, 10.02it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03, 10.14it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03, 10.22it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:01<00:02, 10.28it/s]Epoch 10/10:  40%|████      | 19/47 [00:01<00:02, 10.31it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.34it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 10.14it/s]
[2025-04-12 19:12:39,685][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0245
[2025-04-12 19:12:39,974][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.1645, Metrics: {'accuracy': 0.9444444444444444, 'f1': 0.9444444444444444}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁▁██████
wandb:          best_val_f1 ▁▁██████
wandb:        best_val_loss ██▅▂▂▂▁▁
wandb:                epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:           train_loss ██▇▄▂▂▁▁▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁▁████████
wandb:               val_f1 ▁▁████████
wandb:             val_loss ██▅▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.94444
wandb:          best_val_f1 0.94444
wandb:        best_val_loss 0.1436
wandb:                epoch 10
wandb:  final_test_accuracy 0.95455
wandb:        final_test_f1 0.95495
wandb: final_train_accuracy 0.99729
wandb:       final_train_f1 0.99706
wandb:   final_val_accuracy 0.94444
wandb:         final_val_f1 0.94444
wandb:        learning_rate 1e-05
wandb:           train_loss 0.02451
wandb:           train_time 54.33309
wandb:         val_accuracy 0.94444
wandb:               val_f1 0.94444
wandb:             val_loss 0.16452
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_191130-3uh8bbjs
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_191130-3uh8bbjs/logs
Cross-lingual experiment for question_type (ko → ru) completed successfully
Running cross-lingual complexity from ko to ru
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:12:59,735][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ko_to_ru/complexity
experiment_name: cross_lingual_complexity_ko_to_ru
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ko
  eval_language: ru
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:12:59,736][__main__][INFO] - Normalized task: complexity
[2025-04-12 19:12:59,736][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 19:12:59,736][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:13:01,254][__main__][INFO] - Running cross-lingual experiment: ko -> ru
[2025-04-12 19:13:01,255][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 19:13:01,255][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:13:04,147][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:13:04,147][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:13:04,214][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:13:04,249][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:13:04,357][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 19:13:04,365][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:13:04,365][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 19:13:04,367][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:13:04,393][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:13:04,429][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:13:04,443][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 19:13:04,445][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:13:04,445][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 19:13:04,446][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:13:04,471][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:13:04,509][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:13:04,527][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 19:13:04,528][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:13:04,529][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 19:13:04,530][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 19:13:04,531][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:13:04,531][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:13:04,531][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:13:04,531][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:13:04,531][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:13:04,532][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-12 19:13:04,532][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 19:13:04,532][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-04-12 19:13:04,532][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:13:04,532][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:13:04,532][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:13:04,532][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:13:04,533][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:13:04,533][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-12 19:13:04,533][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 19:13:04,533][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-12 19:13:04,533][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:13:04,533][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:13:04,533][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:13:04,533][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:13:04,534][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:13:04,534][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-12 19:13:04,534][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 19:13:04,534][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-12 19:13:04,534][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 19:13:04,534][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:13:04,535][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:13:04,535][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
[2025-04-12 19:13:07,343][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:13:07,343][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:13:07,377][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:13:07,421][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:13:07,439][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 19:13:07,449][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:13:07,449][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 19:13:07,451][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:13:07,479][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:13:07,516][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:13:07,533][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 19:13:07,534][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:13:07,534][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 19:13:07,536][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:13:07,566][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:13:07,605][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:13:07,621][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 19:13:07,622][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:13:07,622][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 19:13:07,624][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 19:13:07,624][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:13:07,624][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:13:07,624][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:13:07,625][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:13:07,625][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:13:07,625][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-12 19:13:07,625][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 19:13:07,625][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-04-12 19:13:07,625][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:13:07,626][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:13:07,626][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:13:07,626][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:13:07,626][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:13:07,626][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-12 19:13:07,626][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 19:13:07,626][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-12 19:13:07,627][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:13:07,627][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:13:07,627][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:13:07,627][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:13:07,627][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:13:07,627][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-12 19:13:07,627][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 19:13:07,627][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-12 19:13:07,627][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 19:13:07,628][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:13:07,628][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:13:07,628][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:13:12,767][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:13:12,770][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 19:13:12,770][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:13:12,770][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 1/10:   2%|▏         | 1/47 [00:01<00:56,  1.23s/it]Epoch 1/10:   4%|▍         | 2/47 [00:01<00:25,  1.76it/s]Epoch 1/10:   9%|▊         | 4/47 [00:01<00:11,  3.67it/s]Epoch 1/10:  13%|█▎        | 6/47 [00:01<00:07,  5.28it/s]Epoch 1/10:  17%|█▋        | 8/47 [00:01<00:05,  6.59it/s]Epoch 1/10:  21%|██▏       | 10/47 [00:02<00:04,  7.60it/s]Epoch 1/10:  23%|██▎       | 11/47 [00:02<00:05,  6.84it/s]Epoch 1/10:  28%|██▊       | 13/47 [00:02<00:04,  7.83it/s]Epoch 1/10:  32%|███▏      | 15/47 [00:02<00:03,  8.56it/s]Epoch 1/10:  36%|███▌      | 17/47 [00:02<00:03,  9.09it/s]Epoch 1/10:  40%|████      | 19/47 [00:03<00:02,  9.47it/s]Epoch 1/10:  45%|████▍     | 21/47 [00:03<00:02,  9.74it/s]Epoch 1/10:  49%|████▉     | 23/47 [00:03<00:02,  9.94it/s]Epoch 1/10:  53%|█████▎    | 25/47 [00:03<00:02, 10.07it/s]Epoch 1/10:  57%|█████▋    | 27/47 [00:03<00:01, 10.11it/s]Epoch 1/10:  62%|██████▏   | 29/47 [00:04<00:01, 10.19it/s]Epoch 1/10:  66%|██████▌   | 31/47 [00:04<00:01, 10.25it/s]Epoch 1/10:  70%|███████   | 33/47 [00:04<00:01, 10.29it/s]Epoch 1/10:  74%|███████▍  | 35/47 [00:04<00:01, 10.32it/s]Epoch 1/10:  79%|███████▊  | 37/47 [00:04<00:00, 10.35it/s]Epoch 1/10:  83%|████████▎ | 39/47 [00:05<00:00, 10.36it/s]Epoch 1/10:  87%|████████▋ | 41/47 [00:05<00:00, 10.37it/s]Epoch 1/10:  91%|█████████▏| 43/47 [00:05<00:00, 10.38it/s]Epoch 1/10:  96%|█████████▌| 45/47 [00:05<00:00, 10.39it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00, 11.08it/s]Epoch 1/10: 100%|██████████| 47/47 [00:05<00:00,  8.12it/s]
[2025-04-12 19:13:20,624][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1679
[2025-04-12 19:13:20,875][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.2253, Metrics: {'mse': 0.23345845937728882, 'rmse': 0.4831753919409481, 'r2': -3.955507755279541}
Epoch 2/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 2/10:   2%|▏         | 1/47 [00:00<00:08,  5.21it/s]Epoch 2/10:   6%|▋         | 3/47 [00:00<00:05,  8.25it/s]Epoch 2/10:  11%|█         | 5/47 [00:00<00:04,  9.22it/s]Epoch 2/10:  15%|█▍        | 7/47 [00:00<00:04,  9.68it/s]Epoch 2/10:  19%|█▉        | 9/47 [00:00<00:03,  9.94it/s]Epoch 2/10:  23%|██▎       | 11/47 [00:01<00:03, 10.09it/s]Epoch 2/10:  28%|██▊       | 13/47 [00:01<00:03, 10.19it/s]Epoch 2/10:  32%|███▏      | 15/47 [00:01<00:03, 10.25it/s]Epoch 2/10:  36%|███▌      | 17/47 [00:01<00:02, 10.30it/s]Epoch 2/10:  40%|████      | 19/47 [00:01<00:02, 10.32it/s]Epoch 2/10:  45%|████▍     | 21/47 [00:02<00:02, 10.35it/s]Epoch 2/10:  49%|████▉     | 23/47 [00:02<00:02, 10.36it/s]Epoch 2/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.37it/s]Epoch 2/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.38it/s]Epoch 2/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 2/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 2/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 2/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.39it/s]Epoch 2/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 2/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.40it/s]Epoch 2/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 11.29it/s]Epoch 2/10: 100%|██████████| 47/47 [00:04<00:00, 10.18it/s]
[2025-04-12 19:13:25,954][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0708
[2025-04-12 19:13:26,218][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0395, Metrics: {'mse': 0.04067068547010422, 'rmse': 0.2016697435663174, 'r2': 0.13670337200164795}
Epoch 3/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 3/10:   2%|▏         | 1/47 [00:00<00:10,  4.27it/s]Epoch 3/10:   6%|▋         | 3/47 [00:00<00:05,  7.56it/s]Epoch 3/10:  11%|█         | 5/47 [00:00<00:04,  8.79it/s]Epoch 3/10:  15%|█▍        | 7/47 [00:00<00:04,  9.40it/s]Epoch 3/10:  19%|█▉        | 9/47 [00:01<00:03,  9.75it/s]Epoch 3/10:  23%|██▎       | 11/47 [00:01<00:03,  9.96it/s]Epoch 3/10:  28%|██▊       | 13/47 [00:01<00:03, 10.10it/s]Epoch 3/10:  32%|███▏      | 15/47 [00:01<00:03, 10.19it/s]Epoch 3/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 3/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 3/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 3/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 3/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 3/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 3/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.38it/s]Epoch 3/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 3/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 3/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 3/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 3/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 3/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 3/10: 100%|██████████| 47/47 [00:04<00:00, 10.07it/s]
[2025-04-12 19:13:31,553][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0324
[2025-04-12 19:13:31,846][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0373, Metrics: {'mse': 0.03878728672862053, 'rmse': 0.19694488246364902, 'r2': 0.17668139934539795}
Epoch 4/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 4/10:   2%|▏         | 1/47 [00:00<00:22,  2.00it/s]Epoch 4/10:   6%|▋         | 3/47 [00:00<00:08,  4.98it/s]Epoch 4/10:  11%|█         | 5/47 [00:00<00:06,  6.80it/s]Epoch 4/10:  15%|█▍        | 7/47 [00:01<00:05,  7.96it/s]Epoch 4/10:  19%|█▉        | 9/47 [00:01<00:04,  8.73it/s]Epoch 4/10:  23%|██▎       | 11/47 [00:01<00:03,  9.25it/s]Epoch 4/10:  28%|██▊       | 13/47 [00:01<00:03,  9.60it/s]Epoch 4/10:  32%|███▏      | 15/47 [00:01<00:03,  9.83it/s]Epoch 4/10:  36%|███▌      | 17/47 [00:02<00:02, 10.00it/s]Epoch 4/10:  40%|████      | 19/47 [00:02<00:02, 10.12it/s]Epoch 4/10:  45%|████▍     | 21/47 [00:02<00:02, 10.20it/s]Epoch 4/10:  49%|████▉     | 23/47 [00:02<00:02, 10.26it/s]Epoch 4/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.30it/s]Epoch 4/10:  57%|█████▋    | 27/47 [00:03<00:01, 10.32it/s]Epoch 4/10:  62%|██████▏   | 29/47 [00:03<00:01, 10.34it/s]Epoch 4/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.35it/s]Epoch 4/10:  70%|███████   | 33/47 [00:03<00:01, 10.36it/s]Epoch 4/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.37it/s]Epoch 4/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.37it/s]Epoch 4/10:  83%|████████▎ | 39/47 [00:04<00:00, 10.38it/s]Epoch 4/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 4/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 4/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 4/10: 100%|██████████| 47/47 [00:04<00:00,  9.54it/s]
[2025-04-12 19:13:37,167][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0224
[2025-04-12 19:13:37,457][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0251, Metrics: {'mse': 0.02600804716348648, 'rmse': 0.1612701062301581, 'r2': 0.44793999195098877}
Epoch 5/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 5/10:   2%|▏         | 1/47 [00:00<00:10,  4.39it/s]Epoch 5/10:   6%|▋         | 3/47 [00:00<00:05,  7.66it/s]Epoch 5/10:  11%|█         | 5/47 [00:00<00:04,  8.84it/s]Epoch 5/10:  15%|█▍        | 7/47 [00:00<00:04,  9.43it/s]Epoch 5/10:  19%|█▉        | 9/47 [00:00<00:03,  9.77it/s]Epoch 5/10:  23%|██▎       | 11/47 [00:01<00:03,  9.97it/s]Epoch 5/10:  28%|██▊       | 13/47 [00:01<00:03, 10.10it/s]Epoch 5/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 5/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 5/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 5/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 5/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 5/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 5/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 5/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 5/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 5/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 5/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 5/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 5/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 5/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 5/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 5/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-12 19:13:42,535][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0156
[2025-04-12 19:13:42,827][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0336, Metrics: {'mse': 0.035184260457754135, 'rmse': 0.18757467968186467, 'r2': 0.2531610131263733}
Epoch 6/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 6/10:   2%|▏         | 1/47 [00:00<00:10,  4.45it/s]Epoch 6/10:   6%|▋         | 3/47 [00:00<00:05,  7.71it/s]Epoch 6/10:  11%|█         | 5/47 [00:00<00:04,  8.88it/s]Epoch 6/10:  15%|█▍        | 7/47 [00:00<00:04,  9.46it/s]Epoch 6/10:  19%|█▉        | 9/47 [00:00<00:03,  9.79it/s]Epoch 6/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 6/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 6/10:  32%|███▏      | 15/47 [00:01<00:03, 10.14it/s]Epoch 6/10:  36%|███▌      | 17/47 [00:01<00:02, 10.22it/s]Epoch 6/10:  40%|████      | 19/47 [00:01<00:02, 10.27it/s]Epoch 6/10:  45%|████▍     | 21/47 [00:02<00:02, 10.31it/s]Epoch 6/10:  49%|████▉     | 23/47 [00:02<00:02, 10.33it/s]Epoch 6/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 6/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.36it/s]Epoch 6/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 6/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.37it/s]Epoch 6/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 6/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 6/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 6/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 6/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 6/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 6/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 6/10: 100%|██████████| 47/47 [00:04<00:00, 10.10it/s]
[2025-04-12 19:13:47,481][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0154
[2025-04-12 19:13:47,767][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0285, Metrics: {'mse': 0.02866884134709835, 'rmse': 0.1693187566310902, 'r2': 0.391460657119751}
Epoch 7/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 7/10:   2%|▏         | 1/47 [00:00<00:10,  4.40it/s]Epoch 7/10:   6%|▋         | 3/47 [00:00<00:05,  7.66it/s]Epoch 7/10:  11%|█         | 5/47 [00:00<00:04,  8.86it/s]Epoch 7/10:  15%|█▍        | 7/47 [00:00<00:04,  9.45it/s]Epoch 7/10:  19%|█▉        | 9/47 [00:00<00:03,  9.78it/s]Epoch 7/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 7/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 7/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 7/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 7/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 7/10:  45%|████▍     | 21/47 [00:02<00:02, 10.33it/s]Epoch 7/10:  49%|████▉     | 23/47 [00:02<00:02, 10.35it/s]Epoch 7/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 7/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 7/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 7/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 7/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 7/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 7/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 7/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 7/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 7/10: 100%|██████████| 47/47 [00:04<00:00, 10.08it/s]
[2025-04-12 19:13:52,432][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0128
[2025-04-12 19:13:52,725][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0158, Metrics: {'mse': 0.016274383291602135, 'rmse': 0.1275710911280535, 'r2': 0.6545516848564148}
Epoch 8/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 8/10:   2%|▏         | 1/47 [00:00<00:10,  4.45it/s]Epoch 8/10:   6%|▋         | 3/47 [00:00<00:05,  7.70it/s]Epoch 8/10:  11%|█         | 5/47 [00:00<00:04,  8.88it/s]Epoch 8/10:  15%|█▍        | 7/47 [00:00<00:04,  9.46it/s]Epoch 8/10:  19%|█▉        | 9/47 [00:00<00:03,  9.79it/s]Epoch 8/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 8/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 8/10:  32%|███▏      | 15/47 [00:01<00:03, 10.21it/s]Epoch 8/10:  36%|███▌      | 17/47 [00:01<00:02, 10.26it/s]Epoch 8/10:  40%|████      | 19/47 [00:01<00:02, 10.30it/s]Epoch 8/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 8/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 8/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.36it/s]Epoch 8/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 8/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 8/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 8/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.39it/s]Epoch 8/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 8/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 8/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 8/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 8/10: 100%|██████████| 47/47 [00:04<00:00, 10.12it/s]
[2025-04-12 19:13:57,789][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.0139
[2025-04-12 19:13:58,092][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.0184, Metrics: {'mse': 0.01938963308930397, 'rmse': 0.13924666275822903, 'r2': 0.5884258151054382}
Epoch 9/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 9/10:   2%|▏         | 1/47 [00:00<00:18,  2.44it/s]Epoch 9/10:   6%|▋         | 3/47 [00:00<00:07,  5.62it/s]Epoch 9/10:  11%|█         | 5/47 [00:00<00:05,  7.35it/s]Epoch 9/10:  15%|█▍        | 7/47 [00:00<00:04,  8.39it/s]Epoch 9/10:  19%|█▉        | 9/47 [00:01<00:04,  9.04it/s]Epoch 9/10:  23%|██▎       | 11/47 [00:01<00:03,  9.47it/s]Epoch 9/10:  28%|██▊       | 13/47 [00:01<00:03,  9.76it/s]Epoch 9/10:  32%|███▏      | 15/47 [00:01<00:03,  9.95it/s]Epoch 9/10:  36%|███▌      | 17/47 [00:01<00:02, 10.09it/s]Epoch 9/10:  40%|████      | 19/47 [00:02<00:02, 10.18it/s]Epoch 9/10:  45%|████▍     | 21/47 [00:02<00:02, 10.24it/s]Epoch 9/10:  49%|████▉     | 23/47 [00:02<00:02, 10.29it/s]Epoch 9/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.32it/s]Epoch 9/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.34it/s]Epoch 9/10:  62%|██████▏   | 29/47 [00:03<00:01, 10.35it/s]Epoch 9/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.36it/s]Epoch 9/10:  70%|███████   | 33/47 [00:03<00:01, 10.37it/s]Epoch 9/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.37it/s]Epoch 9/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 9/10:  83%|████████▎ | 39/47 [00:04<00:00, 10.38it/s]Epoch 9/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.38it/s]Epoch 9/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 9/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 9/10: 100%|██████████| 47/47 [00:04<00:00,  9.70it/s]
[2025-04-12 19:14:02,938][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.0117
[2025-04-12 19:14:03,223][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.0145, Metrics: {'mse': 0.014588968828320503, 'rmse': 0.12078480379716855, 'r2': 0.6903271675109863}
Epoch 10/10:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 10/10:   2%|▏         | 1/47 [00:00<00:10,  4.41it/s]Epoch 10/10:   6%|▋         | 3/47 [00:00<00:05,  7.68it/s]Epoch 10/10:  11%|█         | 5/47 [00:00<00:04,  8.86it/s]Epoch 10/10:  15%|█▍        | 7/47 [00:00<00:04,  9.45it/s]Epoch 10/10:  19%|█▉        | 9/47 [00:00<00:03,  9.79it/s]Epoch 10/10:  23%|██▎       | 11/47 [00:01<00:03,  9.99it/s]Epoch 10/10:  28%|██▊       | 13/47 [00:01<00:03, 10.12it/s]Epoch 10/10:  32%|███▏      | 15/47 [00:01<00:03, 10.20it/s]Epoch 10/10:  36%|███▌      | 17/47 [00:01<00:02, 10.25it/s]Epoch 10/10:  40%|████      | 19/47 [00:01<00:02, 10.29it/s]Epoch 10/10:  45%|████▍     | 21/47 [00:02<00:02, 10.32it/s]Epoch 10/10:  49%|████▉     | 23/47 [00:02<00:02, 10.34it/s]Epoch 10/10:  53%|█████▎    | 25/47 [00:02<00:02, 10.35it/s]Epoch 10/10:  57%|█████▋    | 27/47 [00:02<00:01, 10.37it/s]Epoch 10/10:  62%|██████▏   | 29/47 [00:02<00:01, 10.37it/s]Epoch 10/10:  66%|██████▌   | 31/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  70%|███████   | 33/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  74%|███████▍  | 35/47 [00:03<00:01, 10.38it/s]Epoch 10/10:  79%|███████▊  | 37/47 [00:03<00:00, 10.38it/s]Epoch 10/10:  83%|████████▎ | 39/47 [00:03<00:00, 10.38it/s]Epoch 10/10:  87%|████████▋ | 41/47 [00:04<00:00, 10.39it/s]Epoch 10/10:  91%|█████████▏| 43/47 [00:04<00:00, 10.39it/s]Epoch 10/10:  96%|█████████▌| 45/47 [00:04<00:00, 10.39it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 11.28it/s]Epoch 10/10: 100%|██████████| 47/47 [00:04<00:00, 10.11it/s]
[2025-04-12 19:14:08,296][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.0083
[2025-04-12 19:14:08,600][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.0196, Metrics: {'mse': 0.01942511647939682, 'rmse': 0.13937401651454556, 'r2': 0.5876725912094116}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▂▂▁▁▁
wandb:     best_val_mse █▂▂▁▁▁
wandb:      best_val_r2 ▁▇▇███
wandb:    best_val_rmse █▃▂▂▁▁
wandb:            epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁▁▁▁
wandb:       train_loss █▄▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▂▂▁▂▁▁▁▁▁
wandb:          val_mse █▂▂▁▂▁▁▁▁▁
wandb:           val_r2 ▁▇▇█▇█████
wandb:         val_rmse █▃▂▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.01449
wandb:     best_val_mse 0.01459
wandb:      best_val_r2 0.69033
wandb:    best_val_rmse 0.12078
wandb:            epoch 10
wandb:   final_test_mse 0.07402
wandb:    final_test_r2 -0.87238
wandb:  final_test_rmse 0.27207
wandb:  final_train_mse 0.01027
wandb:   final_train_r2 0.53885
wandb: final_train_rmse 0.10132
wandb:    final_val_mse 0.01459
wandb:     final_val_r2 0.69033
wandb:   final_val_rmse 0.12078
wandb:    learning_rate 1e-05
wandb:       train_loss 0.00828
wandb:       train_time 53.76468
wandb:         val_loss 0.01964
wandb:          val_mse 0.01943
wandb:           val_r2 0.58767
wandb:         val_rmse 0.13937
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_191259-qt0ajywm
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_191259-qt0ajywm/logs
Cross-lingual experiment for complexity (ko → ru) completed successfully
Running cross-lingual question_type from ru to ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:14:29,942][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ru_to_ar/question_type
experiment_name: cross_lingual_question_type_ru_to_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ru
  eval_language: ar
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:14:29,942][__main__][INFO] - Normalized task: question_type
[2025-04-12 19:14:29,942][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 19:14:29,942][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:14:31,482][__main__][INFO] - Running cross-lingual experiment: ru -> ar
[2025-04-12 19:14:31,483][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 19:14:31,483][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:14:34,356][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:14:34,356][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:14:34,425][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:14:34,461][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:14:34,570][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 19:14:34,581][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:14:34,582][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 19:14:34,583][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:14:34,611][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:14:34,647][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:14:34,663][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 19:14:34,664][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:14:34,664][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 19:14:34,665][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:14:34,690][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:14:34,726][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:14:34,741][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 19:14:34,742][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:14:34,743][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 19:14:34,744][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 19:14:34,745][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:14:34,745][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:14:34,745][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:14:34,745][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:14:34,745][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 19:14:34,745][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-12 19:14:34,745][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 19:14:34,746][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:14:34,746][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:14:34,746][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:14:34,746][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:14:34,746][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:14:34,746][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 19:14:34,746][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 19:14:34,747][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 19:14:34,747][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:14:34,747][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:14:34,747][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:14:34,747][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:14:34,747][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:14:34,747][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:14:34,747][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:14:34,747][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 19:14:34,748][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:14:34,748][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 19:14:34,748][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:14:34,748][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:14:34,748][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
[2025-04-12 19:14:37,580][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:14:37,581][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:14:37,607][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:14:37,645][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:14:37,663][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 19:14:37,671][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:14:37,671][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 19:14:37,673][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:14:37,701][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:14:37,746][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:14:37,762][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 19:14:37,763][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:14:37,763][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 19:14:37,764][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:14:37,795][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:14:37,845][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:14:37,863][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 19:14:37,864][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:14:37,864][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 19:14:37,865][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 19:14:37,866][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:14:37,866][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:14:37,866][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:14:37,866][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:14:37,867][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-04-12 19:14:37,867][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-04-12 19:14:37,867][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 19:14:37,867][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:14:37,867][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:14:37,867][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:14:37,867][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:14:37,867][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:14:37,868][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-04-12 19:14:37,868][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-04-12 19:14:37,868][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 19:14:37,868][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:14:37,868][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:14:37,868][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:14:37,868][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:14:37,868][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:14:37,869][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-04-12 19:14:37,869][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-04-12 19:14:37,869][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 19:14:37,869][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:14:37,869][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 19:14:37,869][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:14:37,869][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:14:37,870][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:14:43,565][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:14:43,568][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 19:14:43,568][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:14:43,568][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:22,  1.12s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:38,  1.92it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:18,  3.92it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:12,  5.56it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:09,  6.84it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:09,  6.80it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:08,  7.71it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:07,  8.43it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  8.97it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:06,  9.38it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.67it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.88it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.04it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.14it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.22it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.27it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.96it/s]
[2025-04-12 19:14:54,423][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6880
[2025-04-12 19:14:54,686][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6861, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.75it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.93it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.02it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 19:15:02,516][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5772
[2025-04-12 19:15:02,794][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5686, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8918918918918919}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:27,  2.69it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:12,  5.96it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:09,  7.63it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  8.60it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:07,  9.20it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.59it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06,  9.84it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.01it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.13it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:02<00:05, 10.21it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.27it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.31it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00,  9.98it/s]
[2025-04-12 19:15:10,955][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.3079
[2025-04-12 19:15:11,239][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2806, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8947368421052632}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.35it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.63it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 19:15:19,005][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1866
[2025-04-12 19:15:19,295][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.3082, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.9041095890410958}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.49it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.74it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.91it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.49it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 19:15:26,663][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1752
[2025-04-12 19:15:26,948][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2940, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.36it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.64it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 19:15:34,322][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1391
[2025-04-12 19:15:34,623][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.3761, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666}
[2025-04-12 19:15:34,624][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▆▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▇▃▂▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████
wandb:               val_f1 ▁█████
wandb:             val_loss █▆▁▁▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.88889
wandb:          best_val_f1 0.89474
wandb:        best_val_loss 0.28061
wandb:                epoch 6
wandb:  final_test_accuracy 0.50649
wandb:        final_test_f1 0.48649
wandb: final_train_accuracy 0.97404
wandb:       final_train_f1 0.97397
wandb:   final_val_accuracy 0.88889
wandb:         final_val_f1 0.89474
wandb:        learning_rate 1e-05
wandb:           train_loss 0.13908
wandb:           train_time 48.57139
wandb:         val_accuracy 0.91667
wandb:               val_f1 0.91667
wandb:             val_loss 0.37611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_191430-lwndmbl4
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_191430-lwndmbl4/logs
Cross-lingual experiment for question_type (ru → ar) completed successfully
Running cross-lingual complexity from ru to ar
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:15:56,608][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ru_to_ar/complexity
experiment_name: cross_lingual_complexity_ru_to_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ru
  eval_language: ar
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:15:56,608][__main__][INFO] - Normalized task: complexity
[2025-04-12 19:15:56,608][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 19:15:56,608][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:15:58,068][__main__][INFO] - Running cross-lingual experiment: ru -> ar
[2025-04-12 19:15:58,069][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 19:15:58,069][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:16:00,934][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:16:00,934][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:16:01,000][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:16:01,030][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:16:01,124][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 19:16:01,135][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:16:01,136][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 19:16:01,137][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:16:01,157][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:16:01,186][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:16:01,198][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 19:16:01,200][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:16:01,200][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 19:16:01,201][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:16:01,220][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:16:01,246][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:16:01,258][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 19:16:01,260][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:16:01,260][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 19:16:01,261][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 19:16:01,261][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:16:01,262][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:16:01,262][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:16:01,262][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:16:01,262][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:16:01,262][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-12 19:16:01,262][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 19:16:01,262][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-04-12 19:16:01,263][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:16:01,263][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:16:01,263][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:16:01,263][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:16:01,263][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:16:01,263][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-12 19:16:01,263][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 19:16:01,264][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-12 19:16:01,264][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:16:01,264][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:16:01,264][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:16:01,264][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:16:01,264][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:16:01,264][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-12 19:16:01,264][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 19:16:01,265][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-12 19:16:01,265][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 19:16:01,265][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:16:01,265][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:16:01,265][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
[2025-04-12 19:16:04,060][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:16:04,060][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:16:04,090][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:16:04,132][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:16:04,151][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-04-12 19:16:04,159][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:16:04,160][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-04-12 19:16:04,162][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:16:04,199][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:16:04,249][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:16:04,266][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-04-12 19:16:04,268][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:16:04,268][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-04-12 19:16:04,269][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:16:04,303][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:16:04,347][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:16:04,362][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-04-12 19:16:04,363][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:16:04,364][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-04-12 19:16:04,365][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-04-12 19:16:04,365][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:16:04,365][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:16:04,365][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:16:04,365][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:16:04,366][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:16:04,366][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-04-12 19:16:04,366][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-04-12 19:16:04,366][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-04-12 19:16:04,366][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:16:04,366][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:16:04,366][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:16:04,367][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:16:04,367][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:16:04,367][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-04-12 19:16:04,367][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-04-12 19:16:04,367][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-04-12 19:16:04,367][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:16:04,367][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:16:04,367][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:16:04,368][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:16:04,368][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:16:04,368][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-04-12 19:16:04,368][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-04-12 19:16:04,368][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-04-12 19:16:04,368][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-04-12 19:16:04,368][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:16:04,368][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:16:04,369][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:16:09,427][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:16:09,430][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 19:16:09,430][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:16:09,430][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:28,  1.20s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:27,  2.61it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:16,  4.25it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.66it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:09,  6.83it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:10,  6.29it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:08,  7.38it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:07,  8.21it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  8.83it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:06,  9.28it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.60it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.84it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.00it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.12it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.20it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.26it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.32it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.88it/s]
[2025-04-12 19:16:19,994][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1265
[2025-04-12 19:16:20,281][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0745, Metrics: {'mse': 0.0731661468744278, 'rmse': 0.27049241555804815, 'r2': -0.5728176832199097}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.02it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.12it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.14it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.63it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.22it/s]
[2025-04-12 19:16:28,082][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0386
[2025-04-12 19:16:28,586][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0477, Metrics: {'mse': 0.045184120535850525, 'rmse': 0.21256556761585477, 'r2': 0.028698623180389404}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.21it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.51it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.76it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.39it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.74it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.08it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.18it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.24it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 19:16:36,625][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0273
[2025-04-12 19:16:36,905][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0754, Metrics: {'mse': 0.07845417410135269, 'rmse': 0.2800967227608218, 'r2': -0.6864919662475586}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.39it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.66it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 19:16:44,291][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0175
[2025-04-12 19:16:44,591][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0243, Metrics: {'mse': 0.02160707488656044, 'rmse': 0.14699345184925905, 'r2': 0.5355230569839478}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.26it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.55it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.78it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.75it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.37it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.31it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.33it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.35it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.36it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.37it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.14it/s]
[2025-04-12 19:16:52,379][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0176
[2025-04-12 19:16:52,683][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0275, Metrics: {'mse': 0.02430662140250206, 'rmse': 0.15590580939305007, 'r2': 0.47749215364456177}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.64it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.84it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 19:17:00,045][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0130
[2025-04-12 19:17:00,350][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0362, Metrics: {'mse': 0.03263107314705849, 'rmse': 0.1806407294799777, 'r2': 0.2985454201698303}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:17,  4.29it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.57it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.80it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:01<00:06,  9.75it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 19:17:07,742][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0106
[2025-04-12 19:17:08,044][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0558, Metrics: {'mse': 0.054593343287706375, 'rmse': 0.23365218442742275, 'r2': -0.1735670566558838}
[2025-04-12 19:17:08,045][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▁
wandb:     best_val_mse █▄▁
wandb:      best_val_r2 ▁▅█
wandb:    best_val_rmse █▅▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄█▁▁▃▅
wandb:          val_mse ▇▄█▁▁▂▅
wandb:           val_r2 ▂▅▁██▇▄
wandb:         val_rmse ▇▄█▁▁▃▆
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02434
wandb:     best_val_mse 0.02161
wandb:      best_val_r2 0.53552
wandb:    best_val_rmse 0.14699
wandb:            epoch 7
wandb:   final_test_mse 0.06771
wandb:    final_test_r2 -0.16725
wandb:  final_test_rmse 0.26021
wandb:  final_train_mse 0.01532
wandb:   final_train_r2 0.23148
wandb: final_train_rmse 0.12378
wandb:    final_val_mse 0.02161
wandb:     final_val_r2 0.53552
wandb:   final_val_rmse 0.14699
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01064
wandb:       train_time 56.4988
wandb:         val_loss 0.05576
wandb:          val_mse 0.05459
wandb:           val_r2 -0.17357
wandb:         val_rmse 0.23365
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_191556-rtz5xtao
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_191556-rtz5xtao/logs
Cross-lingual experiment for complexity (ru → ar) completed successfully
Running cross-lingual question_type from ru to en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:17:28,750][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ru_to_en/question_type
experiment_name: cross_lingual_question_type_ru_to_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ru
  eval_language: en
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:17:28,750][__main__][INFO] - Normalized task: question_type
[2025-04-12 19:17:28,750][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 19:17:28,750][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:17:30,320][__main__][INFO] - Running cross-lingual experiment: ru -> en
[2025-04-12 19:17:30,320][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 19:17:30,321][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:17:33,184][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:17:33,184][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:17:33,261][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:17:33,293][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:17:33,381][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 19:17:33,392][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:17:33,392][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 19:17:33,393][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:17:33,417][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:17:33,444][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:17:33,457][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 19:17:33,458][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:17:33,458][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 19:17:33,460][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:17:33,485][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:17:33,518][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:17:33,532][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 19:17:33,533][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:17:33,534][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 19:17:33,535][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 19:17:33,535][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:17:33,535][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:17:33,535][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:17:33,536][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:17:33,536][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 19:17:33,536][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-12 19:17:33,536][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 19:17:33,536][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:17:33,536][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:17:33,536][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:17:33,537][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:17:33,537][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:17:33,537][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 19:17:33,537][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 19:17:33,537][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 19:17:33,537][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:17:33,537][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:17:33,538][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:17:33,538][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:17:33,538][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:17:33,538][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:17:33,538][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:17:33,538][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 19:17:33,538][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:17:33,538][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 19:17:33,538][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:17:33,539][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:17:33,539][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
[2025-04-12 19:17:36,372][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:17:36,373][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:17:36,401][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:17:36,438][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:17:36,455][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 19:17:36,465][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:17:36,466][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 19:17:36,467][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:17:36,493][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:17:36,530][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:17:36,547][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 19:17:36,549][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:17:36,549][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 19:17:36,550][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:17:36,579][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:17:36,620][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:17:36,637][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 19:17:36,638][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:17:36,638][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 19:17:36,639][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 19:17:36,640][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:17:36,640][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:17:36,641][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:17:36,641][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:17:36,641][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-12 19:17:36,641][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 19:17:36,641][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 19:17:36,641][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:17:36,641][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:17:36,642][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:17:36,642][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:17:36,642][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:17:36,642][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 19:17:36,642][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 19:17:36,642][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 19:17:36,642][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:17:36,642][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:17:36,643][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:17:36,643][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:17:36,643][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:17:36,643][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:17:36,643][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:17:36,643][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 19:17:36,643][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:17:36,643][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 19:17:36,643][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:17:36,644][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:17:36,644][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:17:41,643][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:17:41,646][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 19:17:41,646][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:17:41,646][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:39,  1.35s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:45,  1.62it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:27,  2.62it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:15,  4.55it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:11,  6.07it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  7.25it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  7.07it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  7.92it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.59it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:03<00:06,  9.09it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.46it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.73it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.92it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.06it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.16it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.23it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:08<00:00, 10.39it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.77it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.72it/s]
[2025-04-12 19:17:52,375][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6880
[2025-04-12 19:17:52,626][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6861, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.65it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.86it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.98it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.53it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.84it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 19:18:00,470][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5772
[2025-04-12 19:18:00,754][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5686, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8918918918918919}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.35it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.63it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.78it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 19:18:08,727][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.3079
[2025-04-12 19:18:09,011][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2806, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8947368421052632}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.37it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.65it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.77it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.33it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.35it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.36it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.36it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.37it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 19:18:16,790][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1866
[2025-04-12 19:18:17,075][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.3082, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.9041095890410958}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.84it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:08,  8.00it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.08it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 19:18:24,445][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1752
[2025-04-12 19:18:24,742][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2940, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:15,  4.83it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.99it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  9.07it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.59it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.88it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.21it/s]
[2025-04-12 19:18:32,093][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1391
[2025-04-12 19:18:32,377][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.3761, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666}
[2025-04-12 19:18:32,378][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▆▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▇▃▂▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████
wandb:               val_f1 ▁█████
wandb:             val_loss █▆▁▁▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.88889
wandb:          best_val_f1 0.89474
wandb:        best_val_loss 0.28061
wandb:                epoch 6
wandb:  final_test_accuracy 0.92727
wandb:        final_test_f1 0.9322
wandb: final_train_accuracy 0.97404
wandb:       final_train_f1 0.97397
wandb:   final_val_accuracy 0.88889
wandb:         final_val_f1 0.89474
wandb:        learning_rate 1e-05
wandb:           train_loss 0.13908
wandb:           train_time 48.60906
wandb:         val_accuracy 0.91667
wandb:               val_f1 0.91667
wandb:             val_loss 0.37611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_191728-dry0uk98
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_191728-dry0uk98/logs
Cross-lingual experiment for question_type (ru → en) completed successfully
Running cross-lingual complexity from ru to en
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:18:58,572][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ru_to_en/complexity
experiment_name: cross_lingual_complexity_ru_to_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ru
  eval_language: en
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:18:58,573][__main__][INFO] - Normalized task: complexity
[2025-04-12 19:18:58,573][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 19:18:58,573][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:19:00,321][__main__][INFO] - Running cross-lingual experiment: ru -> en
[2025-04-12 19:19:00,321][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 19:19:00,321][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:19:03,191][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:19:03,191][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:19:03,270][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:19:03,308][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:19:03,409][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 19:19:03,420][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:19:03,421][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 19:19:03,421][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:19:03,445][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:19:03,477][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:19:03,490][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 19:19:03,492][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:19:03,492][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 19:19:03,493][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:19:03,514][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:19:03,543][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:19:03,556][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 19:19:03,557][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:19:03,558][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 19:19:03,559][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 19:19:03,559][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:19:03,559][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:19:03,559][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:19:03,560][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:19:03,560][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:19:03,560][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-12 19:19:03,560][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 19:19:03,560][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-04-12 19:19:03,560][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:19:03,561][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:19:03,561][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:19:03,561][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:19:03,561][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:19:03,561][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-12 19:19:03,561][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 19:19:03,561][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-12 19:19:03,562][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:19:03,562][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:19:03,562][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:19:03,562][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:19:03,562][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:19:03,562][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-12 19:19:03,562][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 19:19:03,562][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-12 19:19:03,562][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 19:19:03,563][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:19:03,563][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:19:03,563][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
[2025-04-12 19:19:06,436][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:19:06,436][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:19:06,462][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:19:06,498][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:19:06,515][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-12 19:19:06,524][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:19:06,525][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-12 19:19:06,526][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:19:06,553][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:19:06,590][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:19:06,605][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-12 19:19:06,606][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:19:06,607][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-12 19:19:06,608][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:19:06,632][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:19:06,668][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:19:06,684][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-12 19:19:06,686][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:19:06,686][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-12 19:19:06,687][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-12 19:19:06,688][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:19:06,688][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:19:06,688][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:19:06,688][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:19:06,688][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:19:06,689][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-12 19:19:06,689][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-12 19:19:06,689][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-12 19:19:06,689][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:19:06,689][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:19:06,689][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:19:06,689][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:19:06,689][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:19:06,690][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-12 19:19:06,690][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-12 19:19:06,690][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-12 19:19:06,690][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:19:06,690][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:19:06,690][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:19:06,690][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:19:06,690][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:19:06,691][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-12 19:19:06,691][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-12 19:19:06,691][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-12 19:19:06,691][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-12 19:19:06,691][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:19:06,691][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:19:06,692][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:19:12,575][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:19:12,578][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 19:19:12,579][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:19:12,579][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:36,  1.31s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:29,  2.44it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:17,  4.02it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.43it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  6.62it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:10,  6.21it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:08,  7.31it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:07,  8.16it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  8.79it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:03<00:06,  9.26it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.59it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05,  9.82it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.00it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.12it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:04<00:04, 10.20it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.26it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  80%|████████  | 60/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00, 10.41it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.78it/s]
[2025-04-12 19:19:23,414][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1265
[2025-04-12 19:19:23,679][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0745, Metrics: {'mse': 0.0731661468744278, 'rmse': 0.27049241555804815, 'r2': -0.5728176832199097}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:16,  4.60it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.82it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.41it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 19:19:31,499][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0386
[2025-04-12 19:19:31,780][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0477, Metrics: {'mse': 0.045184120535850525, 'rmse': 0.21256556761585477, 'r2': 0.028698623180389404}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.60it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.83it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 19:19:39,792][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0273
[2025-04-12 19:19:40,079][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0754, Metrics: {'mse': 0.07845417410135269, 'rmse': 0.2800967227608218, 'r2': -0.6864919662475586}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:18,  4.11it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.43it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:08,  8.69it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.34it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.71it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.94it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.09it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 19:19:47,469][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0175
[2025-04-12 19:19:47,803][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0243, Metrics: {'mse': 0.02160707488656044, 'rmse': 0.14699345184925905, 'r2': 0.5355230569839478}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.37it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.65it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 19:19:55,577][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0176
[2025-04-12 19:19:55,880][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0275, Metrics: {'mse': 0.02430662140250206, 'rmse': 0.15590580939305007, 'r2': 0.47749215364456177}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.39it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.66it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.13it/s]
[2025-04-12 19:20:03,283][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0130
[2025-04-12 19:20:03,855][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0362, Metrics: {'mse': 0.03263107314705849, 'rmse': 0.1806407294799777, 'r2': 0.2985454201698303}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:17,  4.31it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.60it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.81it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.41it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 19:20:11,230][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0106
[2025-04-12 19:20:11,533][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0558, Metrics: {'mse': 0.054593343287706375, 'rmse': 0.23365218442742275, 'r2': -0.1735670566558838}
[2025-04-12 19:20:11,533][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▁
wandb:     best_val_mse █▄▁
wandb:      best_val_r2 ▁▅█
wandb:    best_val_rmse █▅▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄█▁▁▃▅
wandb:          val_mse ▇▄█▁▁▂▅
wandb:           val_r2 ▂▅▁██▇▄
wandb:         val_rmse ▇▄█▁▁▃▆
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02434
wandb:     best_val_mse 0.02161
wandb:      best_val_r2 0.53552
wandb:    best_val_rmse 0.14699
wandb:            epoch 7
wandb:   final_test_mse 0.0205
wandb:    final_test_r2 0.46794
wandb:  final_test_rmse 0.1432
wandb:  final_train_mse 0.01532
wandb:   final_train_r2 0.23148
wandb: final_train_rmse 0.12378
wandb:    final_val_mse 0.02161
wandb:     final_val_r2 0.53552
wandb:   final_val_rmse 0.14699
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01064
wandb:       train_time 56.66817
wandb:         val_loss 0.05576
wandb:          val_mse 0.05459
wandb:           val_r2 -0.17357
wandb:         val_rmse 0.23365
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_191858-c72rjo17
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_191858-c72rjo17/logs
Cross-lingual experiment for complexity (ru → en) completed successfully
Running cross-lingual question_type from ru to fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:20:34,365][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ru_to_fi/question_type
experiment_name: cross_lingual_question_type_ru_to_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ru
  eval_language: fi
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:20:34,365][__main__][INFO] - Normalized task: question_type
[2025-04-12 19:20:34,365][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 19:20:34,365][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:20:36,055][__main__][INFO] - Running cross-lingual experiment: ru -> fi
[2025-04-12 19:20:36,055][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 19:20:36,056][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:20:38,963][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:20:38,963][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:20:39,061][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:20:39,099][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:20:39,222][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 19:20:39,233][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:20:39,233][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 19:20:39,235][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:20:39,272][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:20:39,317][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:20:39,335][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 19:20:39,337][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:20:39,337][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 19:20:39,338][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:20:39,370][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:20:39,413][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:20:39,430][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 19:20:39,432][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:20:39,432][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 19:20:39,434][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 19:20:39,434][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:20:39,434][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:20:39,434][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:20:39,435][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:20:39,435][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 19:20:39,435][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-12 19:20:39,435][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 19:20:39,435][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:20:39,435][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:20:39,435][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:20:39,436][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:20:39,436][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:20:39,436][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 19:20:39,436][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 19:20:39,436][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 19:20:39,436][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:20:39,436][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:20:39,436][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:20:39,437][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:20:39,437][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:20:39,437][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:20:39,437][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:20:39,437][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 19:20:39,437][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:20:39,437][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 19:20:39,437][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:20:39,438][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:20:39,438][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'question_type', submetric: 'None'
[2025-04-12 19:20:42,266][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:20:42,266][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:20:42,360][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:20:42,395][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:20:42,410][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 19:20:42,420][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:20:42,420][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 19:20:42,422][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:20:42,443][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:20:42,470][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:20:42,482][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 19:20:42,483][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:20:42,483][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 19:20:42,484][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:20:42,503][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:20:42,531][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:20:42,544][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 19:20:42,546][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:20:42,546][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 19:20:42,547][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 19:20:42,547][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:20:42,547][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:20:42,547][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:20:42,548][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:20:42,548][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 19:20:42,548][src.data.datasets][INFO] -   Label 1: 598 examples (50.0%)
[2025-04-12 19:20:42,548][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 19:20:42,548][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:20:42,548][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:20:42,548][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:20:42,549][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:20:42,549][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:20:42,549][src.data.datasets][INFO] -   Label 0: 33 examples (52.4%)
[2025-04-12 19:20:42,549][src.data.datasets][INFO] -   Label 1: 30 examples (47.6%)
[2025-04-12 19:20:42,549][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 19:20:42,549][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:20:42,549][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:20:42,549][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:20:42,550][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:20:42,550][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:20:42,550][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:20:42,550][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:20:42,550][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 19:20:42,550][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:20:42,550][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 19:20:42,550][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:20:42,551][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:20:42,551][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:20:48,187][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:20:48,189][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 19:20:48,190][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:20:48,190][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:37,  1.32s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:44,  1.66it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:20,  3.50it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.10it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.42it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.46it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.77it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  7.77it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:07,  8.52it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.06it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.45it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.73it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.93it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.05it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.16it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.23it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.77it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.74it/s]
[2025-04-12 19:20:59,434][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6880
[2025-04-12 19:20:59,715][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6861, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:16,  4.48it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.73it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.90it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 19:21:07,559][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5772
[2025-04-12 19:21:07,841][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5686, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8918918918918919}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.25it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.56it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.79it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 19:21:15,854][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.3079
[2025-04-12 19:21:16,149][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2806, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8947368421052632}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.70it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.90it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.02it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 19:21:23,897][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1866
[2025-04-12 19:21:24,194][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.3082, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.9041095890410958}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.24it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.54it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.78it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.75it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 19:21:31,569][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1752
[2025-04-12 19:21:31,872][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2940, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:25,  2.91it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:11,  6.23it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:08,  7.84it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  8.75it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:07,  9.31it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.66it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.89it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.05it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.16it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:02<00:05, 10.24it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.29it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:03<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.01it/s]
[2025-04-12 19:21:39,367][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1391
[2025-04-12 19:21:39,656][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.3761, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666}
[2025-04-12 19:21:39,656][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▆▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▇▃▂▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████
wandb:               val_f1 ▁█████
wandb:             val_loss █▆▁▁▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.88889
wandb:          best_val_f1 0.89474
wandb:        best_val_loss 0.28061
wandb:                epoch 6
wandb:  final_test_accuracy 0.78182
wandb:        final_test_f1 0.8125
wandb: final_train_accuracy 0.97404
wandb:       final_train_f1 0.97397
wandb:   final_val_accuracy 0.88889
wandb:         final_val_f1 0.89474
wandb:        learning_rate 1e-05
wandb:           train_loss 0.13908
wandb:           train_time 48.80114
wandb:         val_accuracy 0.91667
wandb:               val_f1 0.91667
wandb:             val_loss 0.37611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_192034-jc5k7b3l
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_192034-jc5k7b3l/logs
Cross-lingual experiment for question_type (ru → fi) completed successfully
Running cross-lingual complexity from ru to fi
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:22:04,180][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ru_to_fi/complexity
experiment_name: cross_lingual_complexity_ru_to_fi
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ru
  eval_language: fi
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:22:04,181][__main__][INFO] - Normalized task: complexity
[2025-04-12 19:22:04,181][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 19:22:04,181][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:22:06,200][__main__][INFO] - Running cross-lingual experiment: ru -> fi
[2025-04-12 19:22:06,200][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 19:22:06,201][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:22:09,049][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:22:09,049][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:22:09,147][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:22:09,185][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:22:09,299][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 19:22:09,310][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:22:09,311][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 19:22:09,312][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:22:09,341][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:22:09,380][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:22:09,396][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 19:22:09,398][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:22:09,398][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 19:22:09,399][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:22:09,429][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:22:09,467][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:22:09,482][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 19:22:09,484][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:22:09,484][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 19:22:09,485][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 19:22:09,486][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:22:09,486][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:22:09,486][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:22:09,487][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:22:09,487][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:22:09,487][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-12 19:22:09,487][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 19:22:09,487][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-04-12 19:22:09,487][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:22:09,488][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:22:09,488][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:22:09,488][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:22:09,488][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:22:09,488][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-12 19:22:09,488][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 19:22:09,488][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-12 19:22:09,488][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:22:09,489][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:22:09,489][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:22:09,489][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:22:09,489][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:22:09,489][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-12 19:22:09,489][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 19:22:09,489][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-12 19:22:09,489][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 19:22:09,490][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:22:09,490][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:22:09,490][src.data.datasets][INFO] - Creating dataloaders for language: 'fi', task: 'complexity', submetric: 'None'
[2025-04-12 19:22:12,277][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:22:12,277][src.data.datasets][INFO] - Loading 'base' dataset for fi language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:22:12,302][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:22:12,342][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:22:12,366][src.data.datasets][INFO] - Filtered from 7460 to 1195 examples for language 'fi'
[2025-04-12 19:22:12,375][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:22:12,376][src.data.datasets][INFO] - Loaded 1195 examples for fi (train)
[2025-04-12 19:22:12,377][src.data.datasets][INFO] - Loading 'base' dataset for fi language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:22:12,408][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:22:12,451][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:22:12,470][src.data.datasets][INFO] - Filtered from 441 to 63 examples for language 'fi'
[2025-04-12 19:22:12,471][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:22:12,471][src.data.datasets][INFO] - Loaded 63 examples for fi (validation)
[2025-04-12 19:22:12,472][src.data.datasets][INFO] - Loading 'base' dataset for fi language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:22:12,508][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:22:12,550][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:22:12,566][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'fi'
[2025-04-12 19:22:12,568][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:22:12,568][src.data.datasets][INFO] - Loaded 110 examples for fi (test)
[2025-04-12 19:22:12,569][src.data.datasets][INFO] - Loaded datasets: train=1195, val=63, test=110 examples
[2025-04-12 19:22:12,570][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:22:12,570][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:22:12,570][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:22:12,570][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:22:12,571][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:22:12,571][src.data.datasets][INFO] -   Mean: 0.3374, Std: 0.1422
[2025-04-12 19:22:12,571][src.data.datasets][INFO] - Sample text: Onko Tampereen rantatunneli Suomen pisin maantietu...
[2025-04-12 19:22:12,571][src.data.datasets][INFO] - Sample label: 0.36075112223625183
[2025-04-12 19:22:12,571][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:22:12,571][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:22:12,571][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:22:12,572][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:22:12,572][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:22:12,572][src.data.datasets][INFO] -   Mean: 0.4768, Std: 0.2560
[2025-04-12 19:22:12,572][src.data.datasets][INFO] - Sample text: Entä viestivätkö naisen silmät miehelle, että ”usk...
[2025-04-12 19:22:12,572][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-12 19:22:12,572][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:22:12,572][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:22:12,573][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:22:12,573][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:22:12,573][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:22:12,573][src.data.datasets][INFO] -   Mean: 0.3572, Std: 0.1987
[2025-04-12 19:22:12,573][src.data.datasets][INFO] - Sample text: Kenen toimesta tämä on tehty?...
[2025-04-12 19:22:12,573][src.data.datasets][INFO] - Sample label: 0.2568965554237366
[2025-04-12 19:22:12,573][src.data.datasets][INFO] - Created datasets: train=1195, val=63, test=110
[2025-04-12 19:22:12,573][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:22:12,574][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:22:12,574][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:22:18,583][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:22:18,586][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 19:22:18,586][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:22:18,586][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:34,  1.27s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:42,  1.71it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:19,  3.59it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.20it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.51it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.54it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.76it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  7.77it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:07,  8.51it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.06it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.45it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.73it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.93it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.07it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.17it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.24it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.41it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.78it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.78it/s]
[2025-04-12 19:22:29,755][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1265
[2025-04-12 19:22:30,001][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0745, Metrics: {'mse': 0.0731661468744278, 'rmse': 0.27049241555804815, 'r2': -0.5728176832199097}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.63it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.83it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.97it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 19:22:37,817][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0386
[2025-04-12 19:22:38,072][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0477, Metrics: {'mse': 0.045184120535850525, 'rmse': 0.21256556761585477, 'r2': 0.028698623180389404}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.59it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.81it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 19:22:46,092][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0273
[2025-04-12 19:22:46,394][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0754, Metrics: {'mse': 0.07845417410135269, 'rmse': 0.2800967227608218, 'r2': -0.6864919662475586}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.52it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.77it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.93it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.50it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.82it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.18it/s]
[2025-04-12 19:22:53,762][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0175
[2025-04-12 19:22:54,046][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0243, Metrics: {'mse': 0.02160707488656044, 'rmse': 0.14699345184925905, 'r2': 0.5355230569839478}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.34it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.62it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.83it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.78it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.41it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 19:23:01,819][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0176
[2025-04-12 19:23:02,312][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0275, Metrics: {'mse': 0.02430662140250206, 'rmse': 0.15590580939305007, 'r2': 0.47749215364456177}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:31,  2.33it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:13,  5.47it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:09,  7.23it/s]Epoch 6/10:   9%|▉         | 7/75 [00:01<00:08,  8.30it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:07,  8.99it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.44it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.74it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:06,  9.94it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.09it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:02<00:05, 10.18it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.25it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.29it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.32it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:03<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00,  9.91it/s]
[2025-04-12 19:23:09,884][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0130
[2025-04-12 19:23:10,440][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0362, Metrics: {'mse': 0.03263107314705849, 'rmse': 0.1806407294799777, 'r2': 0.2985454201698303}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.38it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.65it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 19:23:17,815][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0106
[2025-04-12 19:23:18,116][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0558, Metrics: {'mse': 0.054593343287706375, 'rmse': 0.23365218442742275, 'r2': -0.1735670566558838}
[2025-04-12 19:23:18,117][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▁
wandb:     best_val_mse █▄▁
wandb:      best_val_r2 ▁▅█
wandb:    best_val_rmse █▅▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄█▁▁▃▅
wandb:          val_mse ▇▄█▁▁▂▅
wandb:           val_r2 ▂▅▁██▇▄
wandb:         val_rmse ▇▄█▁▁▃▆
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02434
wandb:     best_val_mse 0.02161
wandb:      best_val_r2 0.53552
wandb:    best_val_rmse 0.14699
wandb:            epoch 7
wandb:   final_test_mse 0.03128
wandb:    final_test_r2 0.20788
wandb:  final_test_rmse 0.17686
wandb:  final_train_mse 0.01532
wandb:   final_train_r2 0.23148
wandb: final_train_rmse 0.12378
wandb:    final_val_mse 0.02161
wandb:     final_val_r2 0.53552
wandb:   final_val_rmse 0.14699
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01064
wandb:       train_time 56.91045
wandb:         val_loss 0.05576
wandb:          val_mse 0.05459
wandb:           val_r2 -0.17357
wandb:         val_rmse 0.23365
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_192204-9w1i7l8q
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_192204-9w1i7l8q/logs
Cross-lingual experiment for complexity (ru → fi) completed successfully
Running cross-lingual question_type from ru to id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:23:40,792][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ru_to_id/question_type
experiment_name: cross_lingual_question_type_ru_to_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ru
  eval_language: id
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:23:40,792][__main__][INFO] - Normalized task: question_type
[2025-04-12 19:23:40,793][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 19:23:40,793][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:23:42,173][__main__][INFO] - Running cross-lingual experiment: ru -> id
[2025-04-12 19:23:42,173][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 19:23:42,173][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:23:45,098][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:23:45,098][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:23:45,176][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:23:45,210][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:23:45,325][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 19:23:45,336][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:23:45,337][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 19:23:45,338][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:23:45,365][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:23:45,402][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:23:45,417][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 19:23:45,419][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:23:45,419][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 19:23:45,420][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:23:45,445][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:23:45,484][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:23:45,500][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 19:23:45,502][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:23:45,502][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 19:23:45,503][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 19:23:45,504][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:23:45,504][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:23:45,504][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:23:45,504][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:23:45,505][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 19:23:45,505][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-12 19:23:45,505][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 19:23:45,505][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:23:45,505][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:23:45,505][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:23:45,505][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:23:45,506][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:23:45,506][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 19:23:45,506][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 19:23:45,506][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 19:23:45,506][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:23:45,506][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:23:45,506][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:23:45,506][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:23:45,507][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:23:45,507][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:23:45,507][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:23:45,507][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 19:23:45,507][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:23:45,507][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 19:23:45,507][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:23:45,508][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:23:45,508][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'question_type', submetric: 'None'
[2025-04-12 19:23:48,385][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:23:48,385][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:23:48,415][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:23:48,453][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:23:48,473][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 19:23:48,480][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:23:48,481][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 19:23:48,482][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:23:48,508][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:23:48,546][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:23:48,563][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 19:23:48,564][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:23:48,564][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 19:23:48,566][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:23:48,596][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:23:48,638][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:23:48,654][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 19:23:48,655][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:23:48,656][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 19:23:48,657][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 19:23:48,657][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:23:48,657][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:23:48,657][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:23:48,657][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:23:48,658][src.data.datasets][INFO] -   Label 0: 497 examples (52.1%)
[2025-04-12 19:23:48,658][src.data.datasets][INFO] -   Label 1: 457 examples (47.9%)
[2025-04-12 19:23:48,658][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 19:23:48,658][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:23:48,658][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:23:48,658][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:23:48,658][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:23:48,658][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:23:48,659][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 19:23:48,659][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 19:23:48,659][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 19:23:48,659][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:23:48,659][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:23:48,659][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:23:48,659][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:23:48,659][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:23:48,660][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:23:48,660][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:23:48,660][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 19:23:48,660][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:23:48,660][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 19:23:48,660][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:23:48,660][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:23:48,661][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:23:54,011][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:23:54,015][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 19:23:54,015][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:23:54,015][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:36,  1.30s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:29,  2.44it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:17,  4.02it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.43it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  6.62it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.59it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.34it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.90it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.32it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.63it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.85it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.01it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.12it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.20it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.26it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.33it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.38it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00,  8.92it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00,  9.31it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00,  9.62it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00,  9.84it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.34it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.77it/s]
[2025-04-12 19:24:04,726][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6880
[2025-04-12 19:24:04,972][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6861, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:15,  4.76it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.93it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.03it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.85it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 19:24:12,788][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5772
[2025-04-12 19:24:13,060][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5686, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8918918918918919}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.21it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.52it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.76it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.39it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.74it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 19:24:21,106][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.3079
[2025-04-12 19:24:21,402][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2806, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8947368421052632}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.25it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.54it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.78it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.75it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.14it/s]
[2025-04-12 19:24:29,196][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1866
[2025-04-12 19:24:29,499][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.3082, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.9041095890410958}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.35it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.63it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.83it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 19:24:36,888][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1752
[2025-04-12 19:24:37,176][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2940, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.41it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.67it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 19:24:44,561][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1391
[2025-04-12 19:24:44,864][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.3761, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666}
[2025-04-12 19:24:44,865][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▆▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▇▃▂▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████
wandb:               val_f1 ▁█████
wandb:             val_loss █▆▁▁▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.88889
wandb:          best_val_f1 0.89474
wandb:        best_val_loss 0.28061
wandb:                epoch 6
wandb:  final_test_accuracy 0.84545
wandb:        final_test_f1 0.85714
wandb: final_train_accuracy 0.97404
wandb:       final_train_f1 0.97397
wandb:   final_val_accuracy 0.88889
wandb:         final_val_f1 0.89474
wandb:        learning_rate 1e-05
wandb:           train_loss 0.13908
wandb:           train_time 48.69701
wandb:         val_accuracy 0.91667
wandb:               val_f1 0.91667
wandb:             val_loss 0.37611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_192340-1jlg30v6
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_192340-1jlg30v6/logs
Cross-lingual experiment for question_type (ru → id) completed successfully
Running cross-lingual complexity from ru to id
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:25:05,811][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ru_to_id/complexity
experiment_name: cross_lingual_complexity_ru_to_id
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ru
  eval_language: id
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:25:05,811][__main__][INFO] - Normalized task: complexity
[2025-04-12 19:25:05,811][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 19:25:05,812][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:25:07,258][__main__][INFO] - Running cross-lingual experiment: ru -> id
[2025-04-12 19:25:07,258][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 19:25:07,259][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:25:10,162][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:25:10,163][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:25:10,249][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:25:10,290][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:25:10,419][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 19:25:10,430][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:25:10,431][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 19:25:10,432][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:25:10,465][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:25:10,509][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:25:10,527][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 19:25:10,528][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:25:10,528][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 19:25:10,530][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:25:10,560][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:25:10,603][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:25:10,621][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 19:25:10,622][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:25:10,623][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 19:25:10,624][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 19:25:10,625][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:25:10,625][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:25:10,625][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:25:10,625][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:25:10,625][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:25:10,626][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-12 19:25:10,626][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 19:25:10,626][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-04-12 19:25:10,626][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:25:10,626][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:25:10,626][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:25:10,626][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:25:10,627][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:25:10,627][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-12 19:25:10,627][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 19:25:10,627][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-12 19:25:10,627][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:25:10,627][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:25:10,627][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:25:10,627][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:25:10,628][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:25:10,628][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-12 19:25:10,628][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 19:25:10,628][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-12 19:25:10,628][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 19:25:10,628][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:25:10,628][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:25:10,629][src.data.datasets][INFO] - Creating dataloaders for language: 'id', task: 'complexity', submetric: 'None'
[2025-04-12 19:25:13,488][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:25:13,488][src.data.datasets][INFO] - Loading 'base' dataset for id language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:25:13,515][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:25:13,550][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:25:13,566][src.data.datasets][INFO] - Filtered from 7460 to 954 examples for language 'id'
[2025-04-12 19:25:13,574][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:25:13,574][src.data.datasets][INFO] - Loaded 954 examples for id (train)
[2025-04-12 19:25:13,576][src.data.datasets][INFO] - Loading 'base' dataset for id language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:25:13,601][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:25:13,637][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:25:13,653][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'id'
[2025-04-12 19:25:13,654][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:25:13,655][src.data.datasets][INFO] - Loaded 72 examples for id (validation)
[2025-04-12 19:25:13,656][src.data.datasets][INFO] - Loading 'base' dataset for id language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:25:13,681][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:25:13,720][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:25:13,736][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'id'
[2025-04-12 19:25:13,738][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:25:13,738][src.data.datasets][INFO] - Loaded 110 examples for id (test)
[2025-04-12 19:25:13,739][src.data.datasets][INFO] - Loaded datasets: train=954, val=72, test=110 examples
[2025-04-12 19:25:13,740][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:25:13,740][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:25:13,740][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:25:13,740][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:25:13,740][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:25:13,740][src.data.datasets][INFO] -   Mean: 0.3795, Std: 0.1905
[2025-04-12 19:25:13,741][src.data.datasets][INFO] - Sample text: Apakah Gunung Tandikat termasuk gunung api aktif ?...
[2025-04-12 19:25:13,741][src.data.datasets][INFO] - Sample label: 0.6247802972793579
[2025-04-12 19:25:13,741][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:25:13,741][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:25:13,741][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:25:13,741][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:25:13,741][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:25:13,742][src.data.datasets][INFO] -   Mean: 0.4959, Std: 0.2045
[2025-04-12 19:25:13,742][src.data.datasets][INFO] - Sample text: Gimana toh ini?...
[2025-04-12 19:25:13,742][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-12 19:25:13,742][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:25:13,742][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:25:13,742][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:25:13,742][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:25:13,742][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:25:13,743][src.data.datasets][INFO] -   Mean: 0.3831, Std: 0.2019
[2025-04-12 19:25:13,743][src.data.datasets][INFO] - Sample text: Mampukah Bunga mel epaskan diri dari cengkeraman H...
[2025-04-12 19:25:13,743][src.data.datasets][INFO] - Sample label: 0.5277201533317566
[2025-04-12 19:25:13,743][src.data.datasets][INFO] - Created datasets: train=954, val=72, test=110
[2025-04-12 19:25:13,743][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:25:13,743][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:25:13,743][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:25:18,943][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:25:18,946][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 19:25:18,946][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:25:18,946][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:34,  1.28s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:42,  1.70it/s]Epoch 1/10:   5%|▌         | 4/75 [00:01<00:19,  3.58it/s]Epoch 1/10:   8%|▊         | 6/75 [00:01<00:13,  5.18it/s]Epoch 1/10:  11%|█         | 8/75 [00:01<00:10,  6.49it/s]Epoch 1/10:  13%|█▎        | 10/75 [00:02<00:08,  7.53it/s]Epoch 1/10:  16%|█▌        | 12/75 [00:02<00:07,  8.31it/s]Epoch 1/10:  19%|█▊        | 14/75 [00:02<00:06,  8.89it/s]Epoch 1/10:  21%|██▏       | 16/75 [00:02<00:06,  9.32it/s]Epoch 1/10:  24%|██▍       | 18/75 [00:02<00:05,  9.63it/s]Epoch 1/10:  27%|██▋       | 20/75 [00:03<00:05,  9.86it/s]Epoch 1/10:  29%|██▉       | 22/75 [00:03<00:05, 10.02it/s]Epoch 1/10:  32%|███▏      | 24/75 [00:03<00:05, 10.13it/s]Epoch 1/10:  35%|███▍      | 26/75 [00:03<00:04, 10.21it/s]Epoch 1/10:  37%|███▋      | 28/75 [00:03<00:04, 10.26it/s]Epoch 1/10:  40%|████      | 30/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  43%|████▎     | 32/75 [00:04<00:04, 10.33it/s]Epoch 1/10:  45%|████▌     | 34/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  48%|████▊     | 36/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  51%|█████     | 38/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  53%|█████▎    | 40/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  56%|█████▌    | 42/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  59%|█████▊    | 44/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  61%|██████▏   | 46/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  64%|██████▍   | 48/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  67%|██████▋   | 50/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  69%|██████▉   | 52/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  72%|███████▏  | 54/75 [00:06<00:02, 10.38it/s]Epoch 1/10:  75%|███████▍  | 56/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  77%|███████▋  | 58/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  80%|████████  | 60/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  83%|████████▎ | 62/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  85%|████████▌ | 64/75 [00:07<00:01, 10.39it/s]Epoch 1/10:  88%|████████▊ | 66/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  91%|█████████ | 68/75 [00:07<00:00,  8.92it/s]Epoch 1/10:  93%|█████████▎| 70/75 [00:08<00:00,  9.31it/s]Epoch 1/10:  96%|█████████▌| 72/75 [00:08<00:00,  9.62it/s]Epoch 1/10:  99%|█████████▊| 74/75 [00:08<00:00,  9.84it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.80it/s]
[2025-04-12 19:25:29,481][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1265
[2025-04-12 19:25:29,726][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0745, Metrics: {'mse': 0.0731661468744278, 'rmse': 0.27049241555804815, 'r2': -0.5728176832199097}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:16,  4.43it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:09,  7.69it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  8.87it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.45it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.79it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 19:25:37,557][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0386
[2025-04-12 19:25:37,828][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0477, Metrics: {'mse': 0.045184120535850525, 'rmse': 0.21256556761585477, 'r2': 0.028698623180389404}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.14it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.46it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:08,  8.72it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.36it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.73it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.95it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.09it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 19:25:45,884][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0273
[2025-04-12 19:25:46,191][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0754, Metrics: {'mse': 0.07845417410135269, 'rmse': 0.2800967227608218, 'r2': -0.6864919662475586}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.59it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.82it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.96it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 19:25:53,554][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0175
[2025-04-12 19:25:53,849][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0243, Metrics: {'mse': 0.02160707488656044, 'rmse': 0.14699345184925905, 'r2': 0.5355230569839478}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.33it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.61it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 19:26:01,631][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0176
[2025-04-12 19:26:01,934][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0275, Metrics: {'mse': 0.02430662140250206, 'rmse': 0.15590580939305007, 'r2': 0.47749215364456177}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:21,  3.41it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:10,  6.78it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:08,  8.26it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.04it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:06,  9.51it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.80it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06,  9.99it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.11it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.20it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:02<00:05, 10.26it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.30it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.35it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.38it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.07it/s]
[2025-04-12 19:26:09,386][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0130
[2025-04-12 19:26:09,693][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0362, Metrics: {'mse': 0.03263107314705849, 'rmse': 0.1806407294799777, 'r2': 0.2985454201698303}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.40it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.66it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.86it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 19:26:17,074][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0106
[2025-04-12 19:26:17,373][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0558, Metrics: {'mse': 0.054593343287706375, 'rmse': 0.23365218442742275, 'r2': -0.1735670566558838}
[2025-04-12 19:26:17,374][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▁
wandb:     best_val_mse █▄▁
wandb:      best_val_r2 ▁▅█
wandb:    best_val_rmse █▅▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄█▁▁▃▅
wandb:          val_mse ▇▄█▁▁▂▅
wandb:           val_r2 ▂▅▁██▇▄
wandb:         val_rmse ▇▄█▁▁▃▆
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02434
wandb:     best_val_mse 0.02161
wandb:      best_val_r2 0.53552
wandb:    best_val_rmse 0.14699
wandb:            epoch 7
wandb:   final_test_mse 0.03328
wandb:    final_test_r2 0.18392
wandb:  final_test_rmse 0.18241
wandb:  final_train_mse 0.01532
wandb:   final_train_r2 0.23148
wandb: final_train_rmse 0.12378
wandb:    final_val_mse 0.02161
wandb:     final_val_r2 0.53552
wandb:   final_val_rmse 0.14699
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01064
wandb:       train_time 56.42152
wandb:         val_loss 0.05576
wandb:          val_mse 0.05459
wandb:           val_r2 -0.17357
wandb:         val_rmse 0.23365
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_192505-v2e3zbd3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_192505-v2e3zbd3/logs
Cross-lingual experiment for complexity (ru → id) completed successfully
Running cross-lingual question_type from ru to ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:26:38,613][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ru_to_ja/question_type
experiment_name: cross_lingual_question_type_ru_to_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ru
  eval_language: ja
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:26:38,613][__main__][INFO] - Normalized task: question_type
[2025-04-12 19:26:38,614][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 19:26:38,614][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:26:40,126][__main__][INFO] - Running cross-lingual experiment: ru -> ja
[2025-04-12 19:26:40,127][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 19:26:40,127][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:26:43,241][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:26:43,242][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:26:43,312][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:26:43,349][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:26:43,467][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 19:26:43,480][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:26:43,480][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 19:26:43,482][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:26:43,507][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:26:43,548][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:26:43,565][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 19:26:43,566][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:26:43,567][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 19:26:43,568][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:26:43,595][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:26:43,634][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:26:43,650][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 19:26:43,652][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:26:43,652][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 19:26:43,653][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 19:26:43,653][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:26:43,654][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:26:43,654][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:26:43,654][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:26:43,654][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 19:26:43,654][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-12 19:26:43,654][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 19:26:43,654][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:26:43,655][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:26:43,655][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:26:43,655][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:26:43,655][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:26:43,655][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 19:26:43,655][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 19:26:43,655][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 19:26:43,655][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:26:43,656][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:26:43,656][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:26:43,656][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:26:43,656][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:26:43,656][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:26:43,656][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:26:43,656][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 19:26:43,656][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:26:43,657][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 19:26:43,657][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:26:43,657][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:26:43,657][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'question_type', submetric: 'None'
[2025-04-12 19:26:46,361][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:26:46,361][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:26:46,390][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:26:46,427][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:26:46,444][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 19:26:46,454][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:26:46,455][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 19:26:46,456][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:26:46,483][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:26:46,523][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:26:46,542][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 19:26:46,543][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:26:46,543][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 19:26:46,544][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:26:46,575][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:26:46,618][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:26:46,635][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 19:26:46,636][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:26:46,636][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 19:26:46,638][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 19:26:46,639][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:26:46,639][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:26:46,639][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:26:46,639][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:26:46,639][src.data.datasets][INFO] -   Label 0: 595 examples (50.0%)
[2025-04-12 19:26:46,639][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-12 19:26:46,639][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 19:26:46,639][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:26:46,640][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:26:46,640][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:26:46,640][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:26:46,640][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:26:46,640][src.data.datasets][INFO] -   Label 0: 22 examples (47.8%)
[2025-04-12 19:26:46,640][src.data.datasets][INFO] -   Label 1: 24 examples (52.2%)
[2025-04-12 19:26:46,640][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 19:26:46,640][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:26:46,641][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:26:46,641][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:26:46,641][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:26:46,641][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:26:46,641][src.data.datasets][INFO] -   Label 0: 37 examples (40.2%)
[2025-04-12 19:26:46,641][src.data.datasets][INFO] -   Label 1: 55 examples (59.8%)
[2025-04-12 19:26:46,641][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 19:26:46,641][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:26:46,641][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 19:26:46,642][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:26:46,642][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:26:46,642][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:26:52,006][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:26:52,008][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 19:26:52,009][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:26:52,009][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:27,  1.18s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:40,  1.82it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:24,  2.90it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:14,  4.90it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:10,  6.42it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:01<00:08,  7.54it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.25it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.07it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  8.71it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.18it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.53it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.78it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.96it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.09it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.17it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:03<00:04, 10.24it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.28it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.32it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.36it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.40it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.40it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.95it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.76it/s]
[2025-04-12 19:27:02,724][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6880
[2025-04-12 19:27:02,974][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6861, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:24,  3.03it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:11,  6.36it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:08,  7.95it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  8.83it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:01<00:07,  9.36it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06,  9.70it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06,  9.92it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.07it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.17it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:02<00:05, 10.24it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.28it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.32it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.34it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:07<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.03it/s]
[2025-04-12 19:27:10,908][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5772
[2025-04-12 19:27:11,185][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5686, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8918918918918919}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:16,  4.36it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.64it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 19:27:19,232][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.3079
[2025-04-12 19:27:19,534][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2806, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8947368421052632}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.23it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.53it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.78it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.39it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.75it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.96it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 19:27:27,313][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1866
[2025-04-12 19:27:27,622][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.3082, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.9041095890410958}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.47it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.73it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.90it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.47it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.80it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.00it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.13it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 19:27:34,986][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1752
[2025-04-12 19:27:35,277][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2940, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:17,  4.34it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.62it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.83it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:06,  9.78it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 19:27:42,652][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1391
[2025-04-12 19:27:42,946][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.3761, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666}
[2025-04-12 19:27:42,947][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▆▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▇▃▂▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████
wandb:               val_f1 ▁█████
wandb:             val_loss █▆▁▁▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.88889
wandb:          best_val_f1 0.89474
wandb:        best_val_loss 0.28061
wandb:                epoch 6
wandb:  final_test_accuracy 0.73913
wandb:        final_test_f1 0.81818
wandb: final_train_accuracy 0.97404
wandb:       final_train_f1 0.97397
wandb:   final_val_accuracy 0.88889
wandb:         final_val_f1 0.89474
wandb:        learning_rate 1e-05
wandb:           train_loss 0.13908
wandb:           train_time 48.78377
wandb:         val_accuracy 0.91667
wandb:               val_f1 0.91667
wandb:             val_loss 0.37611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_192638-fvpbxj9c
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_192638-fvpbxj9c/logs
Cross-lingual experiment for question_type (ru → ja) completed successfully
Running cross-lingual complexity from ru to ja
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:28:04,132][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ru_to_ja/complexity
experiment_name: cross_lingual_complexity_ru_to_ja
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ru
  eval_language: ja
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:28:04,132][__main__][INFO] - Normalized task: complexity
[2025-04-12 19:28:04,132][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 19:28:04,132][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:28:05,499][__main__][INFO] - Running cross-lingual experiment: ru -> ja
[2025-04-12 19:28:05,499][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 19:28:05,499][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:28:08,364][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:28:08,365][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:28:08,431][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:28:08,462][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:28:08,563][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 19:28:08,574][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:28:08,575][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 19:28:08,576][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:28:08,599][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:28:08,637][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:28:08,652][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 19:28:08,654][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:28:08,654][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 19:28:08,655][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:28:08,680][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:28:08,715][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:28:08,729][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 19:28:08,731][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:28:08,731][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 19:28:08,732][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 19:28:08,733][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:28:08,733][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:28:08,733][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:28:08,733][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:28:08,734][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:28:08,734][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-12 19:28:08,734][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 19:28:08,734][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-04-12 19:28:08,734][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:28:08,734][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:28:08,735][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:28:08,735][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:28:08,735][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:28:08,735][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-12 19:28:08,735][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 19:28:08,735][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-12 19:28:08,735][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:28:08,736][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:28:08,736][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:28:08,736][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:28:08,736][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:28:08,736][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-12 19:28:08,736][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 19:28:08,736][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-12 19:28:08,736][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 19:28:08,736][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:28:08,737][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:28:08,737][src.data.datasets][INFO] - Creating dataloaders for language: 'ja', task: 'complexity', submetric: 'None'
[2025-04-12 19:28:11,548][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:28:11,548][src.data.datasets][INFO] - Loading 'base' dataset for ja language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:28:11,584][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:28:11,632][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:28:11,650][src.data.datasets][INFO] - Filtered from 7460 to 1191 examples for language 'ja'
[2025-04-12 19:28:11,659][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:28:11,660][src.data.datasets][INFO] - Loaded 1191 examples for ja (train)
[2025-04-12 19:28:11,662][src.data.datasets][INFO] - Loading 'base' dataset for ja language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:28:11,700][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:28:11,746][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:28:11,765][src.data.datasets][INFO] - Filtered from 441 to 46 examples for language 'ja'
[2025-04-12 19:28:11,766][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:28:11,766][src.data.datasets][INFO] - Loaded 46 examples for ja (validation)
[2025-04-12 19:28:11,768][src.data.datasets][INFO] - Loading 'base' dataset for ja language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:28:11,805][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:28:11,854][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:28:11,873][src.data.datasets][INFO] - Filtered from 719 to 92 examples for language 'ja'
[2025-04-12 19:28:11,875][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:28:11,875][src.data.datasets][INFO] - Loaded 92 examples for ja (test)
[2025-04-12 19:28:11,877][src.data.datasets][INFO] - Loaded datasets: train=1191, val=46, test=92 examples
[2025-04-12 19:28:11,877][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:28:11,877][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:28:11,878][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:28:11,878][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:28:11,878][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:28:11,878][src.data.datasets][INFO] -   Mean: 0.3996, Std: 0.2002
[2025-04-12 19:28:11,878][src.data.datasets][INFO] - Sample text: 温井ダム建設時に地域住民から反対はあった？...
[2025-04-12 19:28:11,878][src.data.datasets][INFO] - Sample label: 0.49930843710899353
[2025-04-12 19:28:11,879][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:28:11,879][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:28:11,879][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:28:11,879][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:28:11,879][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:28:11,879][src.data.datasets][INFO] -   Mean: 0.4592, Std: 0.2477
[2025-04-12 19:28:11,879][src.data.datasets][INFO] - Sample text: これからの日本ラグビー史にどれだけの栄光を刻むのか。...
[2025-04-12 19:28:11,879][src.data.datasets][INFO] - Sample label: 0.5879725217819214
[2025-04-12 19:28:11,880][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:28:11,880][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:28:11,880][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:28:11,880][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:28:11,880][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:28:11,880][src.data.datasets][INFO] -   Mean: 0.4902, Std: 0.2282
[2025-04-12 19:28:11,880][src.data.datasets][INFO] - Sample text: 玉置氏は信者ではないのか?...
[2025-04-12 19:28:11,880][src.data.datasets][INFO] - Sample label: 0.17927710711956024
[2025-04-12 19:28:11,880][src.data.datasets][INFO] - Created datasets: train=1191, val=46, test=92
[2025-04-12 19:28:11,881][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:28:11,881][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:28:11,881][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:28:17,173][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:28:17,176][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 19:28:17,176][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:28:17,177][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:43,  1.40s/it]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:31,  2.30it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:18,  3.83it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:12,  5.23it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:10,  6.44it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:09,  6.51it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:08,  7.44it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:07,  8.17it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:03<00:06,  8.77it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:06,  9.22it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.56it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05,  9.80it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:05,  9.97it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:04<00:04, 10.09it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.19it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.25it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.33it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.35it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.38it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.41it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:07<00:01, 10.41it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:08<00:00, 10.41it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00, 10.41it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00, 10.41it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.79it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.67it/s]
[2025-04-12 19:28:28,017][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1265
[2025-04-12 19:28:28,279][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0745, Metrics: {'mse': 0.0731661468744278, 'rmse': 0.27049241555804815, 'r2': -0.5728176832199097}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.08it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.16it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.17it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.66it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.92it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.09it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.19it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.26it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.36it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-12 19:28:36,083][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0386
[2025-04-12 19:28:36,353][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0477, Metrics: {'mse': 0.045184120535850525, 'rmse': 0.21256556761585477, 'r2': 0.028698623180389404}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.22it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.52it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.77it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.40it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.75it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 19:28:44,387][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0273
[2025-04-12 19:28:44,687][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0754, Metrics: {'mse': 0.07845417410135269, 'rmse': 0.2800967227608218, 'r2': -0.6864919662475586}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:15,  4.87it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:08,  8.02it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  9.09it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.60it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.89it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.06it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.30it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 19:28:52,072][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0175
[2025-04-12 19:28:52,368][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0243, Metrics: {'mse': 0.02160707488656044, 'rmse': 0.14699345184925905, 'r2': 0.5355230569839478}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:15,  4.73it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.92it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  9.02it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.56it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.86it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06, 10.04it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.16it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.20it/s]
[2025-04-12 19:29:00,132][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0176
[2025-04-12 19:29:00,432][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0275, Metrics: {'mse': 0.02430662140250206, 'rmse': 0.15590580939305007, 'r2': 0.47749215364456177}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:17,  4.34it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.63it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.41it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.41it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 19:29:07,797][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0130
[2025-04-12 19:29:08,079][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0362, Metrics: {'mse': 0.03263107314705849, 'rmse': 0.1806407294799777, 'r2': 0.2985454201698303}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.35it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.63it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.84it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.78it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06,  9.99it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.41it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.17it/s]
[2025-04-12 19:29:15,457][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0106
[2025-04-12 19:29:15,750][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0558, Metrics: {'mse': 0.054593343287706375, 'rmse': 0.23365218442742275, 'r2': -0.1735670566558838}
[2025-04-12 19:29:15,751][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▁
wandb:     best_val_mse █▄▁
wandb:      best_val_r2 ▁▅█
wandb:    best_val_rmse █▅▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄█▁▁▃▅
wandb:          val_mse ▇▄█▁▁▂▅
wandb:           val_r2 ▂▅▁██▇▄
wandb:         val_rmse ▇▄█▁▁▃▆
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02434
wandb:     best_val_mse 0.02161
wandb:      best_val_r2 0.53552
wandb:    best_val_rmse 0.14699
wandb:            epoch 7
wandb:   final_test_mse 0.05547
wandb:    final_test_r2 -0.06554
wandb:  final_test_rmse 0.23552
wandb:  final_train_mse 0.01532
wandb:   final_train_r2 0.23148
wandb: final_train_rmse 0.12378
wandb:    final_val_mse 0.02161
wandb:     final_val_r2 0.53552
wandb:   final_val_rmse 0.14699
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01064
wandb:       train_time 56.38966
wandb:         val_loss 0.05576
wandb:          val_mse 0.05459
wandb:           val_r2 -0.17357
wandb:         val_rmse 0.23365
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_192804-q2wafwdw
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_192804-q2wafwdw/logs
Cross-lingual experiment for complexity (ru → ja) completed successfully
Running cross-lingual question_type from ru to ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:29:37,490][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ru_to_ko/question_type
experiment_name: cross_lingual_question_type_ru_to_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ru
  eval_language: ko
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: question_type
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:29:37,490][__main__][INFO] - Normalized task: question_type
[2025-04-12 19:29:37,490][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-12 19:29:37,490][__main__][INFO] - Determined Task Type: classification
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:29:38,990][__main__][INFO] - Running cross-lingual experiment: ru -> ko
[2025-04-12 19:29:38,991][__main__][INFO] - Task: question_type, Task Type: classification
[2025-04-12 19:29:38,991][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:29:41,879][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:29:41,880][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:29:41,965][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:29:42,004][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:29:42,109][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 19:29:42,120][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:29:42,121][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 19:29:42,122][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:29:42,149][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:29:42,183][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:29:42,195][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 19:29:42,197][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:29:42,197][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 19:29:42,198][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:29:42,222][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:29:42,253][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:29:42,266][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 19:29:42,301][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:29:42,302][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 19:29:42,303][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 19:29:42,303][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:29:42,303][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:29:42,303][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:29:42,303][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:29:42,304][src.data.datasets][INFO] -   Label 0: 597 examples (50.0%)
[2025-04-12 19:29:42,304][src.data.datasets][INFO] -   Label 1: 597 examples (50.0%)
[2025-04-12 19:29:42,304][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 19:29:42,304][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:29:42,304][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:29:42,304][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:29:42,304][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:29:42,305][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:29:42,305][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 19:29:42,305][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 19:29:42,305][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 19:29:42,305][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:29:42,305][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:29:42,305][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:29:42,305][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:29:42,308][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:29:42,309][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:29:42,309][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:29:42,309][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 19:29:42,309][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:29:42,309][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 19:29:42,309][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:29:42,310][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:29:42,310][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'question_type', submetric: 'None'
[2025-04-12 19:29:45,177][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:29:45,178][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:29:45,211][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:29:45,252][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:29:45,270][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 19:29:45,276][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:29:45,276][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 19:29:45,278][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:29:45,307][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:29:45,344][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:29:45,360][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 19:29:45,362][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:29:45,362][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 19:29:45,363][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:29:45,389][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:29:45,423][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:29:45,436][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 19:29:45,437][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:29:45,438][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 19:29:45,439][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 19:29:45,439][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:29:45,439][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:29:45,439][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:29:45,440][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:29:45,440][src.data.datasets][INFO] -   Label 0: 398 examples (53.9%)
[2025-04-12 19:29:45,440][src.data.datasets][INFO] -   Label 1: 341 examples (46.1%)
[2025-04-12 19:29:45,440][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 19:29:45,440][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:29:45,440][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:29:45,440][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:29:45,440][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:29:45,441][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:29:45,441][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-12 19:29:45,441][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-12 19:29:45,441][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 19:29:45,441][src.data.datasets][INFO] - Sample label: 0
[2025-04-12 19:29:45,441][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-12 19:29:45,441][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-12 19:29:45,441][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-12 19:29:45,442][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-12 19:29:45,442][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-12 19:29:45,442][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-12 19:29:45,442][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 19:29:45,442][src.data.datasets][INFO] - Sample label: 1
[2025-04-12 19:29:45,442][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 19:29:45,442][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:29:45,442][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:29:45,443][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:29:50,627][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:29:50,630][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-12 19:29:50,630][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:29:50,630][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:52,  1.53s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:50,  1.45it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:30,  2.38it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:16,  4.22it/s]Epoch 1/10:   9%|▉         | 7/75 [00:02<00:11,  5.75it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  6.96it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:08,  7.89it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.59it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  9.10it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:03<00:06,  9.47it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.67it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05,  9.88it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.03it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.13it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:04<00:04, 10.21it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.26it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.30it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.33it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.34it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:05<00:03, 10.36it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.37it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.38it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.39it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.38it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.39it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00,  8.96it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:08<00:00,  9.34it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00,  9.63it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00,  9.85it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.35it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.53it/s]
[2025-04-12 19:30:01,588][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.6880
[2025-04-12 19:30:01,839][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.6861, Metrics: {'accuracy': 0.5, 'f1': 0.0}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  5.11it/s]Epoch 2/10:   3%|▎         | 2/75 [00:00<00:14,  5.02it/s]Epoch 2/10:   5%|▌         | 4/75 [00:00<00:09,  7.42it/s]Epoch 2/10:   8%|▊         | 6/75 [00:00<00:08,  8.58it/s]Epoch 2/10:  11%|█         | 8/75 [00:00<00:07,  9.23it/s]Epoch 2/10:  13%|█▎        | 10/75 [00:01<00:06,  9.62it/s]Epoch 2/10:  16%|█▌        | 12/75 [00:01<00:06,  9.87it/s]Epoch 2/10:  19%|█▊        | 14/75 [00:01<00:06, 10.04it/s]Epoch 2/10:  21%|██▏       | 16/75 [00:01<00:05, 10.15it/s]Epoch 2/10:  24%|██▍       | 18/75 [00:01<00:05, 10.22it/s]Epoch 2/10:  27%|██▋       | 20/75 [00:02<00:05, 10.28it/s]Epoch 2/10:  29%|██▉       | 22/75 [00:02<00:05, 10.31it/s]Epoch 2/10:  32%|███▏      | 24/75 [00:02<00:04, 10.34it/s]Epoch 2/10:  35%|███▍      | 26/75 [00:02<00:04, 10.35it/s]Epoch 2/10:  37%|███▋      | 28/75 [00:02<00:04, 10.36it/s]Epoch 2/10:  40%|████      | 30/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  43%|████▎     | 32/75 [00:03<00:04, 10.37it/s]Epoch 2/10:  45%|████▌     | 34/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  48%|████▊     | 36/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  51%|█████     | 38/75 [00:03<00:03, 10.38it/s]Epoch 2/10:  53%|█████▎    | 40/75 [00:04<00:03, 10.38it/s]Epoch 2/10:  56%|█████▌    | 42/75 [00:04<00:03, 10.37it/s]Epoch 2/10:  59%|█████▊    | 44/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  61%|██████▏   | 46/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  64%|██████▍   | 48/75 [00:04<00:02, 10.38it/s]Epoch 2/10:  67%|██████▋   | 50/75 [00:05<00:02, 10.37it/s]Epoch 2/10:  69%|██████▉   | 52/75 [00:05<00:02, 10.37it/s]Epoch 2/10:  72%|███████▏  | 54/75 [00:05<00:02, 10.38it/s]Epoch 2/10:  75%|███████▍  | 56/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  77%|███████▋  | 58/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  80%|████████  | 60/75 [00:05<00:01, 10.38it/s]Epoch 2/10:  83%|████████▎ | 62/75 [00:06<00:01, 10.38it/s]Epoch 2/10:  85%|████████▌ | 64/75 [00:06<00:01, 10.38it/s]Epoch 2/10:  88%|████████▊ | 66/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  91%|█████████ | 68/75 [00:06<00:00, 10.38it/s]Epoch 2/10:  93%|█████████▎| 70/75 [00:06<00:00, 10.39it/s]Epoch 2/10:  96%|█████████▌| 72/75 [00:07<00:00, 10.39it/s]Epoch 2/10:  99%|█████████▊| 74/75 [00:07<00:00, 10.39it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.06it/s]
[2025-04-12 19:30:09,758][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.5772
[2025-04-12 19:30:10,038][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5686, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8918918918918919}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.32it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.60it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.35it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.36it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 19:30:18,070][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.3079
[2025-04-12 19:30:18,369][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.2806, Metrics: {'accuracy': 0.8888888888888888, 'f1': 0.8947368421052632}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:17,  4.18it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.49it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:08,  8.74it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.37it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:01<00:06,  9.73it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06,  9.95it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.09it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.37it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.39it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.14it/s]
[2025-04-12 19:30:26,162][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.1866
[2025-04-12 19:30:26,468][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.3082, Metrics: {'accuracy': 0.9027777777777778, 'f1': 0.9041095890410958}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:16,  4.38it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.65it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.85it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.44it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:00<00:06,  9.77it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.20it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.26it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.33it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.35it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.38it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.38it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.37it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.37it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.37it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.38it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.38it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.37it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.38it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.39it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 19:30:33,858][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.1752
[2025-04-12 19:30:34,169][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.2940, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:17,  4.29it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.58it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.80it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.41it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:01<00:06,  9.76it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06,  9.97it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.10it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.19it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.25it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.30it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.37it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.38it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.38it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.39it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.38it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.39it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.38it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.39it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.39it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.39it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.40it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.80it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 19:30:41,561][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.1391
[2025-04-12 19:30:41,873][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.3761, Metrics: {'accuracy': 0.9166666666666666, 'f1': 0.9166666666666666}
[2025-04-12 19:30:41,873][src.training.lm_trainer][INFO] - Early stopping at epoch 6
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁██
wandb:          best_val_f1 ▁██
wandb:        best_val_loss █▆▁
wandb:                epoch ▁▁▂▂▄▄▅▅▇▇██
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁▁▁▁▁▁
wandb:           train_loss █▇▃▂▁▁
wandb:           train_time ▁
wandb:         val_accuracy ▁█████
wandb:               val_f1 ▁█████
wandb:             val_loss █▆▁▁▁▃
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.88889
wandb:          best_val_f1 0.89474
wandb:        best_val_loss 0.28061
wandb:                epoch 6
wandb:  final_test_accuracy 0.58182
wandb:        final_test_f1 0.69737
wandb: final_train_accuracy 0.97404
wandb:       final_train_f1 0.97397
wandb:   final_val_accuracy 0.88889
wandb:         final_val_f1 0.89474
wandb:        learning_rate 1e-05
wandb:           train_loss 0.13908
wandb:           train_time 49.07823
wandb:         val_accuracy 0.91667
wandb:               val_f1 0.91667
wandb:             val_loss 0.37611
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_192937-9n1u75bk
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_192937-9n1u75bk/logs
Cross-lingual experiment for question_type (ru → ko) completed successfully
Running cross-lingual complexity from ru to ko
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-12 19:31:02,961][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/cross_lingual_output/ru_to_ko/complexity
experiment_name: cross_lingual_complexity_ru_to_ko
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
  train_language: ru
  eval_language: ko
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.3
  freeze_model: false
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 1.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_probe_cross_lingual
  tasks: complexity
  cross_lingual: true
  train_language: en
  eval_language: fi
  task_type: auto
  use_controls: false

[2025-04-12 19:31:02,962][__main__][INFO] - Normalized task: complexity
[2025-04-12 19:31:02,962][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-12 19:31:02,962][__main__][INFO] - Determined Task Type: regression
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-12 19:31:04,701][__main__][INFO] - Running cross-lingual experiment: ru -> ko
[2025-04-12 19:31:04,702][__main__][INFO] - Task: complexity, Task Type: regression
[2025-04-12 19:31:04,702][src.data.datasets][INFO] - Creating dataloaders for language: 'ru', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-12 19:31:07,555][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:31:07,556][src.data.datasets][INFO] - Loading 'base' dataset for ru language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:31:07,619][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:31:07,651][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:31:07,759][src.data.datasets][INFO] - Filtered from 7460 to 1194 examples for language 'ru'
[2025-04-12 19:31:07,770][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:31:07,771][src.data.datasets][INFO] - Loaded 1194 examples for ru (train)
[2025-04-12 19:31:07,772][src.data.datasets][INFO] - Loading 'base' dataset for ru language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:31:07,796][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:31:07,836][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:31:07,851][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ru'
[2025-04-12 19:31:07,853][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:31:07,853][src.data.datasets][INFO] - Loaded 72 examples for ru (validation)
[2025-04-12 19:31:07,854][src.data.datasets][INFO] - Loading 'base' dataset for ru language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:31:07,881][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:31:07,915][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:31:07,929][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ru'
[2025-04-12 19:31:07,930][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:31:07,931][src.data.datasets][INFO] - Loaded 110 examples for ru (test)
[2025-04-12 19:31:07,932][src.data.datasets][INFO] - Loaded datasets: train=1194, val=72, test=110 examples
[2025-04-12 19:31:07,932][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:31:07,933][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:31:07,933][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:31:07,933][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:31:07,933][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:31:07,933][src.data.datasets][INFO] -   Mean: 0.3953, Std: 0.1412
[2025-04-12 19:31:07,933][src.data.datasets][INFO] - Sample text: В каком фильме снимался Дзюн Фукуяма?...
[2025-04-12 19:31:07,934][src.data.datasets][INFO] - Sample label: 0.2535911500453949
[2025-04-12 19:31:07,934][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:31:07,934][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:31:07,934][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:31:07,934][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:31:07,934][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:31:07,934][src.data.datasets][INFO] -   Mean: 0.5093, Std: 0.2157
[2025-04-12 19:31:07,935][src.data.datasets][INFO] - Sample text: Нету ли проблем с активацией или эксплуатацией?...
[2025-04-12 19:31:07,935][src.data.datasets][INFO] - Sample label: 0.4788985252380371
[2025-04-12 19:31:07,935][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:31:07,935][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:31:07,935][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:31:07,935][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:31:07,935][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:31:07,936][src.data.datasets][INFO] -   Mean: 0.5252, Std: 0.1988
[2025-04-12 19:31:07,936][src.data.datasets][INFO] - Sample text: Можно ли лечить пищевую аллергию?...
[2025-04-12 19:31:07,936][src.data.datasets][INFO] - Sample label: 0.6023502945899963
[2025-04-12 19:31:07,936][src.data.datasets][INFO] - Created datasets: train=1194, val=72, test=110
[2025-04-12 19:31:07,936][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:31:07,936][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:31:07,937][src.data.datasets][INFO] - Creating dataloaders for language: 'ko', task: 'complexity', submetric: 'None'
[2025-04-12 19:31:10,752][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-12 19:31:10,752][src.data.datasets][INFO] - Loading 'base' dataset for ko language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:31:10,782][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:31:10,820][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:31:10,836][src.data.datasets][INFO] - Filtered from 7460 to 739 examples for language 'ko'
[2025-04-12 19:31:10,842][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:31:10,843][src.data.datasets][INFO] - Loaded 739 examples for ko (train)
[2025-04-12 19:31:10,843][src.data.datasets][INFO] - Loading 'base' dataset for ko language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:31:10,867][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:31:10,903][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:31:10,920][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'ko'
[2025-04-12 19:31:10,922][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:31:10,922][src.data.datasets][INFO] - Loaded 72 examples for ko (validation)
[2025-04-12 19:31:10,923][src.data.datasets][INFO] - Loading 'base' dataset for ko language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-12 19:31:10,948][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:31:10,984][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-12 19:31:11,000][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'ko'
[2025-04-12 19:31:11,001][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-12 19:31:11,002][src.data.datasets][INFO] - Loaded 110 examples for ko (test)
[2025-04-12 19:31:11,003][src.data.datasets][INFO] - Loaded datasets: train=739, val=72, test=110 examples
[2025-04-12 19:31:11,003][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:31:11,003][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:31:11,003][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:31:11,003][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:31:11,004][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:31:11,004][src.data.datasets][INFO] -   Mean: 0.3773, Std: 0.1492
[2025-04-12 19:31:11,004][src.data.datasets][INFO] - Sample text: 6.25전쟁 당시 남한 편에서 싸운 나라는 몇 개국인가?...
[2025-04-12 19:31:11,004][src.data.datasets][INFO] - Sample label: 0.5104557871818542
[2025-04-12 19:31:11,004][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:31:11,004][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:31:11,004][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:31:11,005][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:31:11,005][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:31:11,005][src.data.datasets][INFO] -   Mean: 0.4695, Std: 0.2171
[2025-04-12 19:31:11,005][src.data.datasets][INFO] - Sample text: 그러면 우리가 과학기술을 발전시킬 수 있는 구체적인 방법은 무엇인가?...
[2025-04-12 19:31:11,005][src.data.datasets][INFO] - Sample label: 0.5001630187034607
[2025-04-12 19:31:11,005][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-12 19:31:11,005][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-12 19:31:11,006][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-12 19:31:11,006][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-12 19:31:11,006][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-12 19:31:11,006][src.data.datasets][INFO] -   Mean: 0.4444, Std: 0.1795
[2025-04-12 19:31:11,006][src.data.datasets][INFO] - Sample text: 정치 경제 사회의 국가적 추이를 이보다 더 화끈하게 변화시킨 사건이 뭔가?...
[2025-04-12 19:31:11,006][src.data.datasets][INFO] - Sample label: 0.6488407850265503
[2025-04-12 19:31:11,006][src.data.datasets][INFO] - Created datasets: train=739, val=72, test=110
[2025-04-12 19:31:11,006][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-12 19:31:11,007][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-12 19:31:11,007][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-12 19:31:16,219][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-12 19:31:16,222][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-12 19:31:16,222][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-12 19:31:16,222][__main__][INFO] - Successfully created model for cross-lingual experiment
Epoch 1/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 1/10:   1%|▏         | 1/75 [00:01<01:38,  1.33s/it]Epoch 1/10:   3%|▎         | 2/75 [00:01<00:44,  1.64it/s]Epoch 1/10:   4%|▍         | 3/75 [00:01<00:27,  2.66it/s]Epoch 1/10:   7%|▋         | 5/75 [00:01<00:15,  4.59it/s]Epoch 1/10:   9%|▉         | 7/75 [00:01<00:11,  6.12it/s]Epoch 1/10:  12%|█▏        | 9/75 [00:02<00:09,  7.29it/s]Epoch 1/10:  15%|█▍        | 11/75 [00:02<00:07,  8.16it/s]Epoch 1/10:  17%|█▋        | 13/75 [00:02<00:07,  8.80it/s]Epoch 1/10:  20%|██        | 15/75 [00:02<00:06,  9.26it/s]Epoch 1/10:  23%|██▎       | 17/75 [00:02<00:06,  9.60it/s]Epoch 1/10:  25%|██▌       | 19/75 [00:03<00:05,  9.83it/s]Epoch 1/10:  28%|██▊       | 21/75 [00:03<00:05, 10.00it/s]Epoch 1/10:  31%|███       | 23/75 [00:03<00:05, 10.12it/s]Epoch 1/10:  33%|███▎      | 25/75 [00:03<00:04, 10.21it/s]Epoch 1/10:  36%|███▌      | 27/75 [00:03<00:04, 10.26it/s]Epoch 1/10:  39%|███▊      | 29/75 [00:04<00:04, 10.31it/s]Epoch 1/10:  41%|████▏     | 31/75 [00:04<00:04, 10.33it/s]Epoch 1/10:  44%|████▍     | 33/75 [00:04<00:04, 10.35it/s]Epoch 1/10:  47%|████▋     | 35/75 [00:04<00:03, 10.37it/s]Epoch 1/10:  49%|████▉     | 37/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  52%|█████▏    | 39/75 [00:04<00:03, 10.38it/s]Epoch 1/10:  55%|█████▍    | 41/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  57%|█████▋    | 43/75 [00:05<00:03, 10.39it/s]Epoch 1/10:  60%|██████    | 45/75 [00:05<00:02, 10.39it/s]Epoch 1/10:  63%|██████▎   | 47/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  65%|██████▌   | 49/75 [00:05<00:02, 10.40it/s]Epoch 1/10:  68%|██████▊   | 51/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  71%|███████   | 53/75 [00:06<00:02, 10.40it/s]Epoch 1/10:  73%|███████▎  | 55/75 [00:06<00:01, 10.39it/s]Epoch 1/10:  76%|███████▌  | 57/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  79%|███████▊  | 59/75 [00:06<00:01, 10.40it/s]Epoch 1/10:  81%|████████▏ | 61/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  84%|████████▍ | 63/75 [00:07<00:01, 10.40it/s]Epoch 1/10:  87%|████████▋ | 65/75 [00:07<00:00, 10.40it/s]Epoch 1/10:  89%|████████▉ | 67/75 [00:07<00:00,  8.86it/s]Epoch 1/10:  92%|█████████▏| 69/75 [00:07<00:00,  9.27it/s]Epoch 1/10:  95%|█████████▍| 71/75 [00:08<00:00,  9.58it/s]Epoch 1/10:  97%|█████████▋| 73/75 [00:08<00:00,  9.82it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00, 10.33it/s]Epoch 1/10: 100%|██████████| 75/75 [00:08<00:00,  8.73it/s]
[2025-04-12 19:31:27,066][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.1265
[2025-04-12 19:31:27,336][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0745, Metrics: {'mse': 0.0731661468744278, 'rmse': 0.27049241555804815, 'r2': -0.5728176832199097}
Epoch 2/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 2/10:   1%|▏         | 1/75 [00:00<00:14,  4.96it/s]Epoch 2/10:   4%|▍         | 3/75 [00:00<00:08,  8.09it/s]Epoch 2/10:   7%|▋         | 5/75 [00:00<00:07,  9.12it/s]Epoch 2/10:   9%|▉         | 7/75 [00:00<00:07,  9.62it/s]Epoch 2/10:  12%|█▏        | 9/75 [00:00<00:06,  9.90it/s]Epoch 2/10:  15%|█▍        | 11/75 [00:01<00:06, 10.07it/s]Epoch 2/10:  17%|█▋        | 13/75 [00:01<00:06, 10.17it/s]Epoch 2/10:  20%|██        | 15/75 [00:01<00:05, 10.24it/s]Epoch 2/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 2/10:  25%|██▌       | 19/75 [00:01<00:05, 10.32it/s]Epoch 2/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 2/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 2/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 2/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 2/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 2/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 2/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 2/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 2/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 2/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 2/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 2/10:  71%|███████   | 53/75 [00:05<00:02, 10.41it/s]Epoch 2/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.41it/s]Epoch 2/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 2/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 2/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 2/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 2/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 2/10: 100%|██████████| 75/75 [00:07<00:00, 10.23it/s]
[2025-04-12 19:31:35,143][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.0386
[2025-04-12 19:31:35,424][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.0477, Metrics: {'mse': 0.045184120535850525, 'rmse': 0.21256556761585477, 'r2': 0.028698623180389404}
Epoch 3/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 3/10:   1%|▏         | 1/75 [00:00<00:17,  4.31it/s]Epoch 3/10:   4%|▍         | 3/75 [00:00<00:09,  7.60it/s]Epoch 3/10:   7%|▋         | 5/75 [00:00<00:07,  8.82it/s]Epoch 3/10:   9%|▉         | 7/75 [00:00<00:07,  9.43it/s]Epoch 3/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 3/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 3/10:  17%|█▋        | 13/75 [00:01<00:06, 10.12it/s]Epoch 3/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 3/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 3/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 3/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 3/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 3/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 3/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  39%|███▊      | 29/75 [00:02<00:04, 10.38it/s]Epoch 3/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 3/10:  47%|████▋     | 35/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  49%|████▉     | 37/75 [00:03<00:03, 10.39it/s]Epoch 3/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 3/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 3/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 3/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.39it/s]Epoch 3/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 3/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 3/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 3/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.40it/s]Epoch 3/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.81it/s]Epoch 3/10: 100%|██████████| 75/75 [00:07<00:00, 10.16it/s]
[2025-04-12 19:31:43,455][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.0273
[2025-04-12 19:31:43,763][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.0754, Metrics: {'mse': 0.07845417410135269, 'rmse': 0.2800967227608218, 'r2': -0.6864919662475586}
Epoch 4/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 4/10:   1%|▏         | 1/75 [00:00<00:16,  4.58it/s]Epoch 4/10:   4%|▍         | 3/75 [00:00<00:09,  7.81it/s]Epoch 4/10:   7%|▋         | 5/75 [00:00<00:07,  8.95it/s]Epoch 4/10:   9%|▉         | 7/75 [00:00<00:07,  9.52it/s]Epoch 4/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 4/10:  15%|█▍        | 11/75 [00:01<00:06, 10.03it/s]Epoch 4/10:  17%|█▋        | 13/75 [00:01<00:06, 10.15it/s]Epoch 4/10:  20%|██        | 15/75 [00:01<00:05, 10.23it/s]Epoch 4/10:  23%|██▎       | 17/75 [00:01<00:05, 10.29it/s]Epoch 4/10:  25%|██▌       | 19/75 [00:01<00:05, 10.33it/s]Epoch 4/10:  28%|██▊       | 21/75 [00:02<00:05, 10.35it/s]Epoch 4/10:  31%|███       | 23/75 [00:02<00:05, 10.37it/s]Epoch 4/10:  33%|███▎      | 25/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 4/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 4/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 4/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 4/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 4/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 4/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 4/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 4/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 4/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 4/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 4/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 4/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 4/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 19:31:51,128][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.0175
[2025-04-12 19:31:51,424][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.0243, Metrics: {'mse': 0.02160707488656044, 'rmse': 0.14699345184925905, 'r2': 0.5355230569839478}
Epoch 5/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 5/10:   1%|▏         | 1/75 [00:00<00:17,  4.29it/s]Epoch 5/10:   4%|▍         | 3/75 [00:00<00:09,  7.58it/s]Epoch 5/10:   7%|▋         | 5/75 [00:00<00:07,  8.81it/s]Epoch 5/10:   9%|▉         | 7/75 [00:00<00:07,  9.42it/s]Epoch 5/10:  12%|█▏        | 9/75 [00:01<00:06,  9.77it/s]Epoch 5/10:  15%|█▍        | 11/75 [00:01<00:06,  9.98it/s]Epoch 5/10:  17%|█▋        | 13/75 [00:01<00:06, 10.11it/s]Epoch 5/10:  20%|██        | 15/75 [00:01<00:05, 10.21it/s]Epoch 5/10:  23%|██▎       | 17/75 [00:01<00:05, 10.27it/s]Epoch 5/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 5/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 5/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 5/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 5/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 5/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 5/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 5/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 5/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 5/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 5/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 5/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 5/10:  81%|████████▏ | 61/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 5/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 5/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 5/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 5/10: 100%|██████████| 75/75 [00:07<00:00, 10.15it/s]
[2025-04-12 19:31:59,211][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.0176
[2025-04-12 19:31:59,511][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.0275, Metrics: {'mse': 0.02430662140250206, 'rmse': 0.15590580939305007, 'r2': 0.47749215364456177}
Epoch 6/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 6/10:   1%|▏         | 1/75 [00:00<00:16,  4.48it/s]Epoch 6/10:   4%|▍         | 3/75 [00:00<00:09,  7.73it/s]Epoch 6/10:   7%|▋         | 5/75 [00:00<00:07,  8.91it/s]Epoch 6/10:   9%|▉         | 7/75 [00:00<00:07,  9.48it/s]Epoch 6/10:  12%|█▏        | 9/75 [00:00<00:06,  9.81it/s]Epoch 6/10:  15%|█▍        | 11/75 [00:01<00:06, 10.01it/s]Epoch 6/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 6/10:  20%|██        | 15/75 [00:01<00:05, 10.14it/s]Epoch 6/10:  23%|██▎       | 17/75 [00:01<00:05, 10.23it/s]Epoch 6/10:  25%|██▌       | 19/75 [00:01<00:05, 10.28it/s]Epoch 6/10:  28%|██▊       | 21/75 [00:02<00:05, 10.32it/s]Epoch 6/10:  31%|███       | 23/75 [00:02<00:05, 10.34it/s]Epoch 6/10:  33%|███▎      | 25/75 [00:02<00:04, 10.36it/s]Epoch 6/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 6/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 6/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 6/10:  44%|████▍     | 33/75 [00:03<00:04, 10.40it/s]Epoch 6/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 6/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 6/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 6/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  71%|███████   | 53/75 [00:05<00:02, 10.40it/s]Epoch 6/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 6/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 6/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 6/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 6/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 6/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 19:32:06,874][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.0130
[2025-04-12 19:32:07,172][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.0362, Metrics: {'mse': 0.03263107314705849, 'rmse': 0.1806407294799777, 'r2': 0.2985454201698303}
Epoch 7/10:   0%|          | 0/75 [00:00<?, ?it/s]Epoch 7/10:   1%|▏         | 1/75 [00:00<00:16,  4.56it/s]Epoch 7/10:   4%|▍         | 3/75 [00:00<00:09,  7.80it/s]Epoch 7/10:   7%|▋         | 5/75 [00:00<00:07,  8.95it/s]Epoch 7/10:   9%|▉         | 7/75 [00:00<00:07,  9.51it/s]Epoch 7/10:  12%|█▏        | 9/75 [00:00<00:06,  9.83it/s]Epoch 7/10:  15%|█▍        | 11/75 [00:01<00:06, 10.02it/s]Epoch 7/10:  17%|█▋        | 13/75 [00:01<00:06, 10.14it/s]Epoch 7/10:  20%|██        | 15/75 [00:01<00:05, 10.22it/s]Epoch 7/10:  23%|██▎       | 17/75 [00:01<00:05, 10.28it/s]Epoch 7/10:  25%|██▌       | 19/75 [00:01<00:05, 10.31it/s]Epoch 7/10:  28%|██▊       | 21/75 [00:02<00:05, 10.34it/s]Epoch 7/10:  31%|███       | 23/75 [00:02<00:05, 10.36it/s]Epoch 7/10:  33%|███▎      | 25/75 [00:02<00:04, 10.37it/s]Epoch 7/10:  36%|███▌      | 27/75 [00:02<00:04, 10.38it/s]Epoch 7/10:  39%|███▊      | 29/75 [00:02<00:04, 10.39it/s]Epoch 7/10:  41%|████▏     | 31/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  44%|████▍     | 33/75 [00:03<00:04, 10.39it/s]Epoch 7/10:  47%|████▋     | 35/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  49%|████▉     | 37/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  52%|█████▏    | 39/75 [00:03<00:03, 10.40it/s]Epoch 7/10:  55%|█████▍    | 41/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  57%|█████▋    | 43/75 [00:04<00:03, 10.40it/s]Epoch 7/10:  60%|██████    | 45/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  63%|██████▎   | 47/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  65%|██████▌   | 49/75 [00:04<00:02, 10.40it/s]Epoch 7/10:  68%|██████▊   | 51/75 [00:05<00:02, 10.40it/s]Epoch 7/10:  71%|███████   | 53/75 [00:05<00:02, 10.39it/s]Epoch 7/10:  73%|███████▎  | 55/75 [00:05<00:01, 10.39it/s]Epoch 7/10:  76%|███████▌  | 57/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  79%|███████▊  | 59/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  81%|████████▏ | 61/75 [00:05<00:01, 10.40it/s]Epoch 7/10:  84%|████████▍ | 63/75 [00:06<00:01, 10.40it/s]Epoch 7/10:  87%|████████▋ | 65/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  89%|████████▉ | 67/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  92%|█████████▏| 69/75 [00:06<00:00, 10.40it/s]Epoch 7/10:  95%|█████████▍| 71/75 [00:06<00:00, 10.41it/s]Epoch 7/10:  97%|█████████▋| 73/75 [00:07<00:00, 10.41it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.82it/s]Epoch 7/10: 100%|██████████| 75/75 [00:07<00:00, 10.19it/s]
[2025-04-12 19:32:14,537][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.0106
[2025-04-12 19:32:14,847][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.0558, Metrics: {'mse': 0.054593343287706375, 'rmse': 0.23365218442742275, 'r2': -0.1735670566558838}
[2025-04-12 19:32:14,848][src.training.lm_trainer][INFO] - Early stopping at epoch 7
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▁
wandb:     best_val_mse █▄▁
wandb:      best_val_r2 ▁▅█
wandb:    best_val_rmse █▅▁
wandb:            epoch ▁▁▂▂▃▃▅▅▆▆▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁▁▁▁▁▁▁
wandb:       train_loss █▃▂▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄█▁▁▃▅
wandb:          val_mse ▇▄█▁▁▂▅
wandb:           val_r2 ▂▅▁██▇▄
wandb:         val_rmse ▇▄█▁▁▃▆
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.02434
wandb:     best_val_mse 0.02161
wandb:      best_val_r2 0.53552
wandb:    best_val_rmse 0.14699
wandb:            epoch 7
wandb:   final_test_mse 0.03289
wandb:    final_test_r2 -0.02136
wandb:  final_test_rmse 0.18137
wandb:  final_train_mse 0.01532
wandb:   final_train_r2 0.23148
wandb: final_train_rmse 0.12378
wandb:    final_val_mse 0.02161
wandb:     final_val_r2 0.53552
wandb:   final_val_rmse 0.14699
wandb:    learning_rate 1e-05
wandb:       train_loss 0.01064
wandb:       train_time 56.37396
wandb:         val_loss 0.05576
wandb:          val_mse 0.05459
wandb:           val_r2 -0.17357
wandb:         val_rmse 0.23365
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_193103-b99h8pln
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250412_193103-b99h8pln/logs
Cross-lingual experiment for complexity (ru → ko) completed successfully
Cross-lingual experiments completed
