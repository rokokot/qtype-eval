SLURM_JOB_ID: 64435712
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: finetune_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Thu May  1 12:09:03 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main finetuning experiments (non-control)...
Running experiment: finetune_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/question_type/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 12:09:17,467][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar
experiment_name: finetune_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: classification
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-01 12:09:17,467][__main__][INFO] - Normalized task: question_type
[2025-05-01 12:09:17,467][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-01 12:09:17,467][__main__][INFO] - Determined Task Type: classification
[2025-05-01 12:09:17,471][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-01 12:09:17,472][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 12:09:19,219][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 12:09:21,457][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 12:09:21,457][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:09:21,520][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:09:21,557][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:09:21,652][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-01 12:09:21,659][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:09:21,660][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-01 12:09:21,661][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:09:21,680][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:09:21,711][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:09:21,724][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-01 12:09:21,725][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:09:21,725][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-01 12:09:21,726][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:09:21,747][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:09:21,777][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:09:21,788][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-01 12:09:21,790][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:09:21,790][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-01 12:09:21,791][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-01 12:09:21,791][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:09:21,792][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:09:21,792][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:09:21,792][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:09:21,792][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-01 12:09:21,792][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-01 12:09:21,792][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-01 12:09:21,792][src.data.datasets][INFO] - Sample label: 1
[2025-05-01 12:09:21,792][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:09:21,792][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:09:21,793][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:09:21,793][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:09:21,793][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-01 12:09:21,793][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-01 12:09:21,793][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-01 12:09:21,793][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 12:09:21,793][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-01 12:09:21,793][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-01 12:09:21,793][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-01 12:09:21,793][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-01 12:09:21,794][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-01 12:09:21,794][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-01 12:09:21,794][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-01 12:09:21,794][src.data.datasets][INFO] - Sample label: 0
[2025-05-01 12:09:21,794][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-01 12:09:21,794][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 12:09:21,794][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 12:09:21,794][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-01 12:09:21,795][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 12:09:25,856][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 12:09:25,857][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 12:09:25,857][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 12:09:25,857][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 12:09:25,862][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 12:09:25,863][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 12:09:25,863][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 12:09:25,863][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-01 12:09:25,864][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 12:09:25,864][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.8145Epoch 1/10: [                              ] 2/63 batches, loss: 0.8446Epoch 1/10: [=                             ] 3/63 batches, loss: 0.7718Epoch 1/10: [=                             ] 4/63 batches, loss: 0.7524Epoch 1/10: [==                            ] 5/63 batches, loss: 0.7532Epoch 1/10: [==                            ] 6/63 batches, loss: 0.7585Epoch 1/10: [===                           ] 7/63 batches, loss: 0.7449Epoch 1/10: [===                           ] 8/63 batches, loss: 0.7424Epoch 1/10: [====                          ] 9/63 batches, loss: 0.7300Epoch 1/10: [====                          ] 10/63 batches, loss: 0.7407Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.7360Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.7349Epoch 1/10: [======                        ] 13/63 batches, loss: 0.7268Epoch 1/10: [======                        ] 14/63 batches, loss: 0.7327Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.7320Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.7330Epoch 1/10: [========                      ] 17/63 batches, loss: 0.7372Epoch 1/10: [========                      ] 18/63 batches, loss: 0.7427Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.7398Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.7346Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.7308Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.7302Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.7258Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.7253Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.7248Epoch 1/10: [============                  ] 26/63 batches, loss: 0.7245Epoch 1/10: [============                  ] 27/63 batches, loss: 0.7265Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.7232Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.7250Epoch 1/10: [==============                ] 30/63 batches, loss: 0.7245Epoch 1/10: [==============                ] 31/63 batches, loss: 0.7214Epoch 1/10: [===============               ] 32/63 batches, loss: 0.7210Epoch 1/10: [===============               ] 33/63 batches, loss: 0.7208Epoch 1/10: [================              ] 34/63 batches, loss: 0.7221Epoch 1/10: [================              ] 35/63 batches, loss: 0.7219Epoch 1/10: [=================             ] 36/63 batches, loss: 0.7234Epoch 1/10: [=================             ] 37/63 batches, loss: 0.7241Epoch 1/10: [==================            ] 38/63 batches, loss: 0.7213Epoch 1/10: [==================            ] 39/63 batches, loss: 0.7219Epoch 1/10: [===================           ] 40/63 batches, loss: 0.7217Epoch 1/10: [===================           ] 41/63 batches, loss: 0.7207Epoch 1/10: [====================          ] 42/63 batches, loss: 0.7211Epoch 1/10: [====================          ] 43/63 batches, loss: 0.7201Epoch 1/10: [====================          ] 44/63 batches, loss: 0.7200Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.7206Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.7215Epoch 1/10: [======================        ] 47/63 batches, loss: 0.7212Epoch 1/10: [======================        ] 48/63 batches, loss: 0.7190Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.7185Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.7166Epoch 1/10: [========================      ] 51/63 batches, loss: 0.7153Epoch 1/10: [========================      ] 52/63 batches, loss: 0.7172Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.7159Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.7157Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.7160Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.7148Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.7164Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.7147Epoch 1/10: [============================  ] 59/63 batches, loss: 0.7147Epoch 1/10: [============================  ] 60/63 batches, loss: 0.7137Epoch 1/10: [============================= ] 61/63 batches, loss: 0.7126Epoch 1/10: [============================= ] 62/63 batches, loss: 0.7117Epoch 1/10: [==============================] 63/63 batches, loss: 0.7132
[2025-05-01 12:09:34,712][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.7132
[2025-05-01 12:09:34,891][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.7450, Metrics: {'accuracy': 0.75, 'f1': 0.7843137254901961, 'precision': 0.6451612903225806, 'recall': 1.0}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.7625Epoch 2/10: [                              ] 2/63 batches, loss: 0.7733Epoch 2/10: [=                             ] 3/63 batches, loss: 0.7537Epoch 2/10: [=                             ] 4/63 batches, loss: 0.7262Epoch 2/10: [==                            ] 5/63 batches, loss: 0.7238Epoch 2/10: [==                            ] 6/63 batches, loss: 0.7200Epoch 2/10: [===                           ] 7/63 batches, loss: 0.7277Epoch 2/10: [===                           ] 8/63 batches, loss: 0.7272Epoch 2/10: [====                          ] 9/63 batches, loss: 0.7212Epoch 2/10: [====                          ] 10/63 batches, loss: 0.7156Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.7228Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.7157Epoch 2/10: [======                        ] 13/63 batches, loss: 0.7044Epoch 2/10: [======                        ] 14/63 batches, loss: 0.6984Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.6949Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.6984Epoch 2/10: [========                      ] 17/63 batches, loss: 0.6938Epoch 2/10: [========                      ] 18/63 batches, loss: 0.6917Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.6919Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.6906Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.6866Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.6902Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.6886Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.6851Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.6877Epoch 2/10: [============                  ] 26/63 batches, loss: 0.6925Epoch 2/10: [============                  ] 27/63 batches, loss: 0.6933Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.6917Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.6880Epoch 2/10: [==============                ] 30/63 batches, loss: 0.6829Epoch 2/10: [==============                ] 31/63 batches, loss: 0.6802Epoch 2/10: [===============               ] 32/63 batches, loss: 0.6788Epoch 2/10: [===============               ] 33/63 batches, loss: 0.6767Epoch 2/10: [================              ] 34/63 batches, loss: 0.6758Epoch 2/10: [================              ] 35/63 batches, loss: 0.6757Epoch 2/10: [=================             ] 36/63 batches, loss: 0.6719Epoch 2/10: [=================             ] 37/63 batches, loss: 0.6684Epoch 2/10: [==================            ] 38/63 batches, loss: 0.6657Epoch 2/10: [==================            ] 39/63 batches, loss: 0.6629Epoch 2/10: [===================           ] 40/63 batches, loss: 0.6609Epoch 2/10: [===================           ] 41/63 batches, loss: 0.6594Epoch 2/10: [====================          ] 42/63 batches, loss: 0.6601Epoch 2/10: [====================          ] 43/63 batches, loss: 0.6576Epoch 2/10: [====================          ] 44/63 batches, loss: 0.6539Epoch 2/10: [=====================         ] 45/63 batches, loss: 0.6510Epoch 2/10: [=====================         ] 46/63 batches, loss: 0.6495Epoch 2/10: [======================        ] 47/63 batches, loss: 0.6459Epoch 2/10: [======================        ] 48/63 batches, loss: 0.6424Epoch 2/10: [=======================       ] 49/63 batches, loss: 0.6424Epoch 2/10: [=======================       ] 50/63 batches, loss: 0.6408Epoch 2/10: [========================      ] 51/63 batches, loss: 0.6394Epoch 2/10: [========================      ] 52/63 batches, loss: 0.6372Epoch 2/10: [=========================     ] 53/63 batches, loss: 0.6370Epoch 2/10: [=========================     ] 54/63 batches, loss: 0.6357Epoch 2/10: [==========================    ] 55/63 batches, loss: 0.6345Epoch 2/10: [==========================    ] 56/63 batches, loss: 0.6338Epoch 2/10: [===========================   ] 57/63 batches, loss: 0.6323Epoch 2/10: [===========================   ] 58/63 batches, loss: 0.6305Epoch 2/10: [============================  ] 59/63 batches, loss: 0.6284Epoch 2/10: [============================  ] 60/63 batches, loss: 0.6270Epoch 2/10: [============================= ] 61/63 batches, loss: 0.6262Epoch 2/10: [============================= ] 62/63 batches, loss: 0.6235Epoch 2/10: [==============================] 63/63 batches, loss: 0.6238
[2025-05-01 12:09:41,623][src.training.lm_trainer][INFO] - Epoch 2/10, Train Loss: 0.6238
[2025-05-01 12:09:41,803][src.training.lm_trainer][INFO] - Epoch 2/10, Val Loss: 0.5717, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 3/10: [Epoch 3/10: [                              ] 1/63 batches, loss: 0.6418Epoch 3/10: [                              ] 2/63 batches, loss: 0.6040Epoch 3/10: [=                             ] 3/63 batches, loss: 0.5731Epoch 3/10: [=                             ] 4/63 batches, loss: 0.5429Epoch 3/10: [==                            ] 5/63 batches, loss: 0.5245Epoch 3/10: [==                            ] 6/63 batches, loss: 0.5065Epoch 3/10: [===                           ] 7/63 batches, loss: 0.5174Epoch 3/10: [===                           ] 8/63 batches, loss: 0.5236Epoch 3/10: [====                          ] 9/63 batches, loss: 0.5277Epoch 3/10: [====                          ] 10/63 batches, loss: 0.5290Epoch 3/10: [=====                         ] 11/63 batches, loss: 0.5317Epoch 3/10: [=====                         ] 12/63 batches, loss: 0.5225Epoch 3/10: [======                        ] 13/63 batches, loss: 0.5197Epoch 3/10: [======                        ] 14/63 batches, loss: 0.5229Epoch 3/10: [=======                       ] 15/63 batches, loss: 0.5190Epoch 3/10: [=======                       ] 16/63 batches, loss: 0.5170Epoch 3/10: [========                      ] 17/63 batches, loss: 0.5166Epoch 3/10: [========                      ] 18/63 batches, loss: 0.5137Epoch 3/10: [=========                     ] 19/63 batches, loss: 0.5125Epoch 3/10: [=========                     ] 20/63 batches, loss: 0.5101Epoch 3/10: [==========                    ] 21/63 batches, loss: 0.5125Epoch 3/10: [==========                    ] 22/63 batches, loss: 0.5125Epoch 3/10: [==========                    ] 23/63 batches, loss: 0.5134Epoch 3/10: [===========                   ] 24/63 batches, loss: 0.5132Epoch 3/10: [===========                   ] 25/63 batches, loss: 0.5148Epoch 3/10: [============                  ] 26/63 batches, loss: 0.5146Epoch 3/10: [============                  ] 27/63 batches, loss: 0.5160Epoch 3/10: [=============                 ] 28/63 batches, loss: 0.5191Epoch 3/10: [=============                 ] 29/63 batches, loss: 0.5195Epoch 3/10: [==============                ] 30/63 batches, loss: 0.5167Epoch 3/10: [==============                ] 31/63 batches, loss: 0.5141Epoch 3/10: [===============               ] 32/63 batches, loss: 0.5139Epoch 3/10: [===============               ] 33/63 batches, loss: 0.5144Epoch 3/10: [================              ] 34/63 batches, loss: 0.5156Epoch 3/10: [================              ] 35/63 batches, loss: 0.5174Epoch 3/10: [=================             ] 36/63 batches, loss: 0.5164Epoch 3/10: [=================             ] 37/63 batches, loss: 0.5162Epoch 3/10: [==================            ] 38/63 batches, loss: 0.5159Epoch 3/10: [==================            ] 39/63 batches, loss: 0.5138Epoch 3/10: [===================           ] 40/63 batches, loss: 0.5137Epoch 3/10: [===================           ] 41/63 batches, loss: 0.5140Epoch 3/10: [====================          ] 42/63 batches, loss: 0.5139Epoch 3/10: [====================          ] 43/63 batches, loss: 0.5125Epoch 3/10: [====================          ] 44/63 batches, loss: 0.5146Epoch 3/10: [=====================         ] 45/63 batches, loss: 0.5133Epoch 3/10: [=====================         ] 46/63 batches, loss: 0.5137Epoch 3/10: [======================        ] 47/63 batches, loss: 0.5130Epoch 3/10: [======================        ] 48/63 batches, loss: 0.5138Epoch 3/10: [=======================       ] 49/63 batches, loss: 0.5137Epoch 3/10: [=======================       ] 50/63 batches, loss: 0.5130Epoch 3/10: [========================      ] 51/63 batches, loss: 0.5115Epoch 3/10: [========================      ] 52/63 batches, loss: 0.5114Epoch 3/10: [=========================     ] 53/63 batches, loss: 0.5122Epoch 3/10: [=========================     ] 54/63 batches, loss: 0.5120Epoch 3/10: [==========================    ] 55/63 batches, loss: 0.5136Epoch 3/10: [==========================    ] 56/63 batches, loss: 0.5131Epoch 3/10: [===========================   ] 57/63 batches, loss: 0.5125Epoch 3/10: [===========================   ] 58/63 batches, loss: 0.5123Epoch 3/10: [============================  ] 59/63 batches, loss: 0.5126Epoch 3/10: [============================  ] 60/63 batches, loss: 0.5121Epoch 3/10: [============================= ] 61/63 batches, loss: 0.5128Epoch 3/10: [============================= ] 62/63 batches, loss: 0.5115Epoch 3/10: [==============================] 63/63 batches, loss: 0.5144
[2025-05-01 12:09:48,562][src.training.lm_trainer][INFO] - Epoch 3/10, Train Loss: 0.5144
[2025-05-01 12:09:48,752][src.training.lm_trainer][INFO] - Epoch 3/10, Val Loss: 0.5252, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 4/10: [Epoch 4/10: [                              ] 1/63 batches, loss: 0.4093Epoch 4/10: [                              ] 2/63 batches, loss: 0.4450Epoch 4/10: [=                             ] 3/63 batches, loss: 0.4568Epoch 4/10: [=                             ] 4/63 batches, loss: 0.4627Epoch 4/10: [==                            ] 5/63 batches, loss: 0.4666Epoch 4/10: [==                            ] 6/63 batches, loss: 0.4730Epoch 4/10: [===                           ] 7/63 batches, loss: 0.4810Epoch 4/10: [===                           ] 8/63 batches, loss: 0.4779Epoch 4/10: [====                          ] 9/63 batches, loss: 0.4808Epoch 4/10: [====                          ] 10/63 batches, loss: 0.4856Epoch 4/10: [=====                         ] 11/63 batches, loss: 0.4808Epoch 4/10: [=====                         ] 12/63 batches, loss: 0.4828Epoch 4/10: [======                        ] 13/63 batches, loss: 0.4863Epoch 4/10: [======                        ] 14/63 batches, loss: 0.4876Epoch 4/10: [=======                       ] 15/63 batches, loss: 0.4887Epoch 4/10: [=======                       ] 16/63 batches, loss: 0.4881Epoch 4/10: [========                      ] 17/63 batches, loss: 0.4878Epoch 4/10: [========                      ] 18/63 batches, loss: 0.4860Epoch 4/10: [=========                     ] 19/63 batches, loss: 0.4882Epoch 4/10: [=========                     ] 20/63 batches, loss: 0.4878Epoch 4/10: [==========                    ] 21/63 batches, loss: 0.4897Epoch 4/10: [==========                    ] 22/63 batches, loss: 0.4936Epoch 4/10: [==========                    ] 23/63 batches, loss: 0.4974Epoch 4/10: [===========                   ] 24/63 batches, loss: 0.4967Epoch 4/10: [===========                   ] 25/63 batches, loss: 0.4989Epoch 4/10: [============                  ] 26/63 batches, loss: 0.4973Epoch 4/10: [============                  ] 27/63 batches, loss: 0.4967Epoch 4/10: [=============                 ] 28/63 batches, loss: 0.5004Epoch 4/10: [=============                 ] 29/63 batches, loss: 0.4981Epoch 4/10: [==============                ] 30/63 batches, loss: 0.4991Epoch 4/10: [==============                ] 31/63 batches, loss: 0.5000Epoch 4/10: [===============               ] 32/63 batches, loss: 0.4994Epoch 4/10: [===============               ] 33/63 batches, loss: 0.4989Epoch 4/10: [================              ] 34/63 batches, loss: 0.5006Epoch 4/10: [================              ] 35/63 batches, loss: 0.5007Epoch 4/10: [=================             ] 36/63 batches, loss: 0.5008Epoch 4/10: [=================             ] 37/63 batches, loss: 0.5028Epoch 4/10: [==================            ] 38/63 batches, loss: 0.5029Epoch 4/10: [==================            ] 39/63 batches, loss: 0.5041Epoch 4/10: [===================           ] 40/63 batches, loss: 0.5036Epoch 4/10: [===================           ] 41/63 batches, loss: 0.5053Epoch 4/10: [====================          ] 42/63 batches, loss: 0.5047Epoch 4/10: [====================          ] 43/63 batches, loss: 0.5059Epoch 4/10: [====================          ] 44/63 batches, loss: 0.5064Epoch 4/10: [=====================         ] 45/63 batches, loss: 0.5074Epoch 4/10: [=====================         ] 46/63 batches, loss: 0.5089Epoch 4/10: [======================        ] 47/63 batches, loss: 0.5073Epoch 4/10: [======================        ] 48/63 batches, loss: 0.5052Epoch 4/10: [=======================       ] 49/63 batches, loss: 0.5047Epoch 4/10: [=======================       ] 50/63 batches, loss: 0.5047Epoch 4/10: [========================      ] 51/63 batches, loss: 0.5056Epoch 4/10: [========================      ] 52/63 batches, loss: 0.5042Epoch 4/10: [=========================     ] 53/63 batches, loss: 0.5047Epoch 4/10: [=========================     ] 54/63 batches, loss: 0.5051Epoch 4/10: [==========================    ] 55/63 batches, loss: 0.5047Epoch 4/10: [==========================    ] 56/63 batches, loss: 0.5051Epoch 4/10: [===========================   ] 57/63 batches, loss: 0.5063Epoch 4/10: [===========================   ] 58/63 batches, loss: 0.5063Epoch 4/10: [============================  ] 59/63 batches, loss: 0.5067Epoch 4/10: [============================  ] 60/63 batches, loss: 0.5066Epoch 4/10: [============================= ] 61/63 batches, loss: 0.5058Epoch 4/10: [============================= ] 62/63 batches, loss: 0.5058Epoch 4/10: [==============================] 63/63 batches, loss: 0.5067
[2025-05-01 12:09:55,486][src.training.lm_trainer][INFO] - Epoch 4/10, Train Loss: 0.5067
[2025-05-01 12:09:55,684][src.training.lm_trainer][INFO] - Epoch 4/10, Val Loss: 0.5243, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 5/10: [Epoch 5/10: [                              ] 1/63 batches, loss: 0.6225Epoch 5/10: [                              ] 2/63 batches, loss: 0.6108Epoch 5/10: [=                             ] 3/63 batches, loss: 0.5751Epoch 5/10: [=                             ] 4/63 batches, loss: 0.5577Epoch 5/10: [==                            ] 5/63 batches, loss: 0.5426Epoch 5/10: [==                            ] 6/63 batches, loss: 0.5322Epoch 5/10: [===                           ] 7/63 batches, loss: 0.5180Epoch 5/10: [===                           ] 8/63 batches, loss: 0.5192Epoch 5/10: [====                          ] 9/63 batches, loss: 0.5176Epoch 5/10: [====                          ] 10/63 batches, loss: 0.5186Epoch 5/10: [=====                         ] 11/63 batches, loss: 0.5152Epoch 5/10: [=====                         ] 12/63 batches, loss: 0.5143Epoch 5/10: [======                        ] 13/63 batches, loss: 0.5135Epoch 5/10: [======                        ] 14/63 batches, loss: 0.5128Epoch 5/10: [=======                       ] 15/63 batches, loss: 0.5075Epoch 5/10: [=======                       ] 16/63 batches, loss: 0.5072Epoch 5/10: [========                      ] 17/63 batches, loss: 0.5112Epoch 5/10: [========                      ] 18/63 batches, loss: 0.5095Epoch 5/10: [=========                     ] 19/63 batches, loss: 0.5079Epoch 5/10: [=========                     ] 20/63 batches, loss: 0.5018Epoch 5/10: [==========                    ] 21/63 batches, loss: 0.5019Epoch 5/10: [==========                    ] 22/63 batches, loss: 0.4998Epoch 5/10: [==========                    ] 23/63 batches, loss: 0.5021Epoch 5/10: [===========                   ] 24/63 batches, loss: 0.5022Epoch 5/10: [===========                   ] 25/63 batches, loss: 0.5013Epoch 5/10: [============                  ] 26/63 batches, loss: 0.5023Epoch 5/10: [============                  ] 27/63 batches, loss: 0.5024Epoch 5/10: [=============                 ] 28/63 batches, loss: 0.5016Epoch 5/10: [=============                 ] 29/63 batches, loss: 0.5041Epoch 5/10: [==============                ] 30/63 batches, loss: 0.5065Epoch 5/10: [==============                ] 31/63 batches, loss: 0.5087Epoch 5/10: [===============               ] 32/63 batches, loss: 0.5093Epoch 5/10: [===============               ] 33/63 batches, loss: 0.5084Epoch 5/10: [================              ] 34/63 batches, loss: 0.5082Epoch 5/10: [================              ] 35/63 batches, loss: 0.5095Epoch 5/10: [=================             ] 36/63 batches, loss: 0.5060Epoch 5/10: [=================             ] 37/63 batches, loss: 0.5053Epoch 5/10: [==================            ] 38/63 batches, loss: 0.5046Epoch 5/10: [==================            ] 39/63 batches, loss: 0.5058Epoch 5/10: [===================           ] 40/63 batches, loss: 0.5058Epoch 5/10: [===================           ] 41/63 batches, loss: 0.5057Epoch 5/10: [====================          ] 42/63 batches, loss: 0.5046Epoch 5/10: [====================          ] 43/63 batches, loss: 0.5023Epoch 5/10: [====================          ] 44/63 batches, loss: 0.5040Epoch 5/10: [=====================         ] 45/63 batches, loss: 0.5035Epoch 5/10: [=====================         ] 46/63 batches, loss: 0.5040Epoch 5/10: [======================        ] 47/63 batches, loss: 0.5045Epoch 5/10: [======================        ] 48/63 batches, loss: 0.5055Epoch 5/10: [=======================       ] 49/63 batches, loss: 0.5050Epoch 5/10: [=======================       ] 50/63 batches, loss: 0.5040Epoch 5/10: [========================      ] 51/63 batches, loss: 0.5049Epoch 5/10: [========================      ] 52/63 batches, loss: 0.5063Epoch 5/10: [=========================     ] 53/63 batches, loss: 0.5073Epoch 5/10: [=========================     ] 54/63 batches, loss: 0.5068Epoch 5/10: [==========================    ] 55/63 batches, loss: 0.5063Epoch 5/10: [==========================    ] 56/63 batches, loss: 0.5062Epoch 5/10: [===========================   ] 57/63 batches, loss: 0.5074Epoch 5/10: [===========================   ] 58/63 batches, loss: 0.5070Epoch 5/10: [============================  ] 59/63 batches, loss: 0.5065Epoch 5/10: [============================  ] 60/63 batches, loss: 0.5061Epoch 5/10: [============================= ] 61/63 batches, loss: 0.5053Epoch 5/10: [============================= ] 62/63 batches, loss: 0.5053Epoch 5/10: [==============================] 63/63 batches, loss: 0.5042
[2025-05-01 12:10:02,403][src.training.lm_trainer][INFO] - Epoch 5/10, Train Loss: 0.5042
[2025-05-01 12:10:02,606][src.training.lm_trainer][INFO] - Epoch 5/10, Val Loss: 0.5520, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523, 'precision': 0.9090909090909091, 'recall': 1.0}
[2025-05-01 12:10:02,607][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 6/10: [Epoch 6/10: [                              ] 1/63 batches, loss: 0.5647Epoch 6/10: [                              ] 2/63 batches, loss: 0.5578Epoch 6/10: [=                             ] 3/63 batches, loss: 0.5329Epoch 6/10: [=                             ] 4/63 batches, loss: 0.5315Epoch 6/10: [==                            ] 5/63 batches, loss: 0.5164Epoch 6/10: [==                            ] 6/63 batches, loss: 0.5104Epoch 6/10: [===                           ] 7/63 batches, loss: 0.5196Epoch 6/10: [===                           ] 8/63 batches, loss: 0.5116Epoch 6/10: [====                          ] 9/63 batches, loss: 0.5071Epoch 6/10: [====                          ] 10/63 batches, loss: 0.5068Epoch 6/10: [=====                         ] 11/63 batches, loss: 0.5003Epoch 6/10: [=====                         ] 12/63 batches, loss: 0.5026Epoch 6/10: [======                        ] 13/63 batches, loss: 0.5045Epoch 6/10: [======                        ] 14/63 batches, loss: 0.5044Epoch 6/10: [=======                       ] 15/63 batches, loss: 0.5075Epoch 6/10: [=======                       ] 16/63 batches, loss: 0.5087Epoch 6/10: [========                      ] 17/63 batches, loss: 0.5070Epoch 6/10: [========                      ] 18/63 batches, loss: 0.5082Epoch 6/10: [=========                     ] 19/63 batches, loss: 0.5117Epoch 6/10: [=========                     ] 20/63 batches, loss: 0.5137Epoch 6/10: [==========                    ] 21/63 batches, loss: 0.5155Epoch 6/10: [==========                    ] 22/63 batches, loss: 0.5139Epoch 6/10: [==========                    ] 23/63 batches, loss: 0.5134Epoch 6/10: [===========                   ] 24/63 batches, loss: 0.5120Epoch 6/10: [===========                   ] 25/63 batches, loss: 0.5079Epoch 6/10: [============                  ] 26/63 batches, loss: 0.5059Epoch 6/10: [============                  ] 27/63 batches, loss: 0.5076Epoch 6/10: [=============                 ] 28/63 batches, loss: 0.5074Epoch 6/10: [=============                 ] 29/63 batches, loss: 0.5089Epoch 6/10: [==============                ] 30/63 batches, loss: 0.5056Epoch 6/10: [==============                ] 31/63 batches, loss: 0.5040Epoch 6/10: [===============               ] 32/63 batches, loss: 0.5062Epoch 6/10: [===============               ] 33/63 batches, loss: 0.5061Epoch 6/10: [================              ] 34/63 batches, loss: 0.5089Epoch 6/10: [================              ] 35/63 batches, loss: 0.5094Epoch 6/10: [=================             ] 36/63 batches, loss: 0.5099Epoch 6/10: [=================             ] 37/63 batches, loss: 0.5097Epoch 6/10: [==================            ] 38/63 batches, loss: 0.5102Epoch 6/10: [==================            ] 39/63 batches, loss: 0.5088Epoch 6/10: [===================           ] 40/63 batches, loss: 0.5093Epoch 6/10: [===================           ] 41/63 batches, loss: 0.5097Epoch 6/10: [====================          ] 42/63 batches, loss: 0.5096Epoch 6/10: [====================          ] 43/63 batches, loss: 0.5095Epoch 6/10: [====================          ] 44/63 batches, loss: 0.5099Epoch 6/10: [=====================         ] 45/63 batches, loss: 0.5102Epoch 6/10: [=====================         ] 46/63 batches, loss: 0.5101Epoch 6/10: [======================        ] 47/63 batches, loss: 0.5115Epoch 6/10: [======================        ] 48/63 batches, loss: 0.5118Epoch 6/10: [=======================       ] 49/63 batches, loss: 0.5107Epoch 6/10: [=======================       ] 50/63 batches, loss: 0.5105Epoch 6/10: [========================      ] 51/63 batches, loss: 0.5099Epoch 6/10: [========================      ] 52/63 batches, loss: 0.5084Epoch 6/10: [=========================     ] 53/63 batches, loss: 0.5074Epoch 6/10: [=========================     ] 54/63 batches, loss: 0.5069Epoch 6/10: [==========================    ] 55/63 batches, loss: 0.5064Epoch 6/10: [==========================    ] 56/63 batches, loss: 0.5077Epoch 6/10: [===========================   ] 57/63 batches, loss: 0.5080Epoch 6/10: [===========================   ] 58/63 batches, loss: 0.5059Epoch 6/10: [============================  ] 59/63 batches, loss: 0.5055Epoch 6/10: [============================  ] 60/63 batches, loss: 0.5050Epoch 6/10: [============================= ] 61/63 batches, loss: 0.5050Epoch 6/10: [============================= ] 62/63 batches, loss: 0.5054Epoch 6/10: [==============================] 63/63 batches, loss: 0.5064
[2025-05-01 12:10:08,979][src.training.lm_trainer][INFO] - Epoch 6/10, Train Loss: 0.5064
[2025-05-01 12:10:09,175][src.training.lm_trainer][INFO] - Epoch 6/10, Val Loss: 0.5391, Metrics: {'accuracy': 0.9772727272727273, 'f1': 0.975609756097561, 'precision': 0.9523809523809523, 'recall': 1.0}
[2025-05-01 12:10:09,176][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 7/10: [Epoch 7/10: [                              ] 1/63 batches, loss: 0.4559Epoch 7/10: [                              ] 2/63 batches, loss: 0.5157Epoch 7/10: [=                             ] 3/63 batches, loss: 0.5037Epoch 7/10: [=                             ] 4/63 batches, loss: 0.5037Epoch 7/10: [==                            ] 5/63 batches, loss: 0.4941Epoch 7/10: [==                            ] 6/63 batches, loss: 0.4918Epoch 7/10: [===                           ] 7/63 batches, loss: 0.4907Epoch 7/10: [===                           ] 8/63 batches, loss: 0.4923Epoch 7/10: [====                          ] 9/63 batches, loss: 0.4883Epoch 7/10: [====                          ] 10/63 batches, loss: 0.4947Epoch 7/10: [=====                         ] 11/63 batches, loss: 0.4976Epoch 7/10: [=====                         ] 12/63 batches, loss: 0.5061Epoch 7/10: [======                        ] 13/63 batches, loss: 0.5095Epoch 7/10: [======                        ] 14/63 batches, loss: 0.5041Epoch 7/10: [=======                       ] 15/63 batches, loss: 0.5025Epoch 7/10: [=======                       ] 16/63 batches, loss: 0.4996Epoch 7/10: [========                      ] 17/63 batches, loss: 0.5013Epoch 7/10: [========                      ] 18/63 batches, loss: 0.4987Epoch 7/10: [=========                     ] 19/63 batches, loss: 0.4965Epoch 7/10: [=========                     ] 20/63 batches, loss: 0.4968Epoch 7/10: [==========                    ] 21/63 batches, loss: 0.4960Epoch 7/10: [==========                    ] 22/63 batches, loss: 0.4974Epoch 7/10: [==========                    ] 23/63 batches, loss: 0.4967Epoch 7/10: [===========                   ] 24/63 batches, loss: 0.4990Epoch 7/10: [===========                   ] 25/63 batches, loss: 0.5011Epoch 7/10: [============                  ] 26/63 batches, loss: 0.5011Epoch 7/10: [============                  ] 27/63 batches, loss: 0.5012Epoch 7/10: [=============                 ] 28/63 batches, loss: 0.5013Epoch 7/10: [=============                 ] 29/63 batches, loss: 0.5030Epoch 7/10: [==============                ] 30/63 batches, loss: 0.5046Epoch 7/10: [==============                ] 31/63 batches, loss: 0.5061Epoch 7/10: [===============               ] 32/63 batches, loss: 0.5069Epoch 7/10: [===============               ] 33/63 batches, loss: 0.5090Epoch 7/10: [================              ] 34/63 batches, loss: 0.5074Epoch 7/10: [================              ] 35/63 batches, loss: 0.5060Epoch 7/10: [=================             ] 36/63 batches, loss: 0.5073Epoch 7/10: [=================             ] 37/63 batches, loss: 0.5065Epoch 7/10: [==================            ] 38/63 batches, loss: 0.5046Epoch 7/10: [==================            ] 39/63 batches, loss: 0.5045Epoch 7/10: [===================           ] 40/63 batches, loss: 0.5057Epoch 7/10: [===================           ] 41/63 batches, loss: 0.5051Epoch 7/10: [====================          ] 42/63 batches, loss: 0.5067Epoch 7/10: [====================          ] 43/63 batches, loss: 0.5072Epoch 7/10: [====================          ] 44/63 batches, loss: 0.5077Epoch 7/10: [=====================         ] 45/63 batches, loss: 0.5092Epoch 7/10: [=====================         ] 46/63 batches, loss: 0.5080Epoch 7/10: [======================        ] 47/63 batches, loss: 0.5089Epoch 7/10: [======================        ] 48/63 batches, loss: 0.5083Epoch 7/10: [=======================       ] 49/63 batches, loss: 0.5077Epoch 7/10: [=======================       ] 50/63 batches, loss: 0.5072Epoch 7/10: [========================      ] 51/63 batches, loss: 0.5066Epoch 7/10: [========================      ] 52/63 batches, loss: 0.5066Epoch 7/10: [=========================     ] 53/63 batches, loss: 0.5061Epoch 7/10: [=========================     ] 54/63 batches, loss: 0.5047Epoch 7/10: [==========================    ] 55/63 batches, loss: 0.5051Epoch 7/10: [==========================    ] 56/63 batches, loss: 0.5042Epoch 7/10: [===========================   ] 57/63 batches, loss: 0.5046Epoch 7/10: [===========================   ] 58/63 batches, loss: 0.5050Epoch 7/10: [============================  ] 59/63 batches, loss: 0.5046Epoch 7/10: [============================  ] 60/63 batches, loss: 0.5042Epoch 7/10: [============================= ] 61/63 batches, loss: 0.5046Epoch 7/10: [============================= ] 62/63 batches, loss: 0.5038Epoch 7/10: [==============================] 63/63 batches, loss: 0.5048
[2025-05-01 12:10:15,563][src.training.lm_trainer][INFO] - Epoch 7/10, Train Loss: 0.5048
[2025-05-01 12:10:15,753][src.training.lm_trainer][INFO] - Epoch 7/10, Val Loss: 0.5218, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
Epoch 8/10: [Epoch 8/10: [                              ] 1/63 batches, loss: 0.5040Epoch 8/10: [                              ] 2/63 batches, loss: 0.5392Epoch 8/10: [=                             ] 3/63 batches, loss: 0.5116Epoch 8/10: [=                             ] 4/63 batches, loss: 0.5333Epoch 8/10: [==                            ] 5/63 batches, loss: 0.5131Epoch 8/10: [==                            ] 6/63 batches, loss: 0.5114Epoch 8/10: [===                           ] 7/63 batches, loss: 0.5137Epoch 8/10: [===                           ] 8/63 batches, loss: 0.5065Epoch 8/10: [====                          ] 9/63 batches, loss: 0.5009Epoch 8/10: [====                          ] 10/63 batches, loss: 0.4988Epoch 8/10: [=====                         ] 11/63 batches, loss: 0.4970Epoch 8/10: [=====                         ] 12/63 batches, loss: 0.4996Epoch 8/10: [======                        ] 13/63 batches, loss: 0.4926Epoch 8/10: [======                        ] 14/63 batches, loss: 0.4917Epoch 8/10: [=======                       ] 15/63 batches, loss: 0.4941Epoch 8/10: [=======                       ] 16/63 batches, loss: 0.4888Epoch 8/10: [========                      ] 17/63 batches, loss: 0.4883Epoch 8/10: [========                      ] 18/63 batches, loss: 0.4931Epoch 8/10: [=========                     ] 19/63 batches, loss: 0.4936Epoch 8/10: [=========                     ] 20/63 batches, loss: 0.4977Epoch 8/10: [==========                    ] 21/63 batches, loss: 0.4979Epoch 8/10: [==========                    ] 22/63 batches, loss: 0.4960Epoch 8/10: [==========                    ] 23/63 batches, loss: 0.4974Epoch 8/10: [===========                   ] 24/63 batches, loss: 0.4947Epoch 8/10: [===========                   ] 25/63 batches, loss: 0.4969Epoch 8/10: [============                  ] 26/63 batches, loss: 0.4954Epoch 8/10: [============                  ] 27/63 batches, loss: 0.4948Epoch 8/10: [=============                 ] 28/63 batches, loss: 0.4952Epoch 8/10: [=============                 ] 29/63 batches, loss: 0.4963Epoch 8/10: [==============                ] 30/63 batches, loss: 0.4963Epoch 8/10: [==============                ] 31/63 batches, loss: 0.4973Epoch 8/10: [===============               ] 32/63 batches, loss: 0.4960Epoch 8/10: [===============               ] 33/63 batches, loss: 0.4941Epoch 8/10: [================              ] 34/63 batches, loss: 0.4951Epoch 8/10: [================              ] 35/63 batches, loss: 0.4953Epoch 8/10: [=================             ] 36/63 batches, loss: 0.4949Epoch 8/10: [=================             ] 37/63 batches, loss: 0.4964Epoch 8/10: [==================            ] 38/63 batches, loss: 0.4972Epoch 8/10: [==================            ] 39/63 batches, loss: 0.4992Epoch 8/10: [===================           ] 40/63 batches, loss: 0.5011Epoch 8/10: [===================           ] 41/63 batches, loss: 0.5012Epoch 8/10: [====================          ] 42/63 batches, loss: 0.5013Epoch 8/10: [====================          ] 43/63 batches, loss: 0.5019Epoch 8/10: [====================          ] 44/63 batches, loss: 0.5030Epoch 8/10: [=====================         ] 45/63 batches, loss: 0.5036Epoch 8/10: [=====================         ] 46/63 batches, loss: 0.5025Epoch 8/10: [======================        ] 47/63 batches, loss: 0.5036Epoch 8/10: [======================        ] 48/63 batches, loss: 0.5036Epoch 8/10: [=======================       ] 49/63 batches, loss: 0.5022Epoch 8/10: [=======================       ] 50/63 batches, loss: 0.5013Epoch 8/10: [========================      ] 51/63 batches, loss: 0.5024Epoch 8/10: [========================      ] 52/63 batches, loss: 0.5019Epoch 8/10: [=========================     ] 53/63 batches, loss: 0.5015Epoch 8/10: [=========================     ] 54/63 batches, loss: 0.5011Epoch 8/10: [==========================    ] 55/63 batches, loss: 0.5020Epoch 8/10: [==========================    ] 56/63 batches, loss: 0.5016Epoch 8/10: [===========================   ] 57/63 batches, loss: 0.5008Epoch 8/10: [===========================   ] 58/63 batches, loss: 0.5009Epoch 8/10: [============================  ] 59/63 batches, loss: 0.5013Epoch 8/10: [============================  ] 60/63 batches, loss: 0.5017Epoch 8/10: [============================= ] 61/63 batches, loss: 0.5037Epoch 8/10: [============================= ] 62/63 batches, loss: 0.5041Epoch 8/10: [==============================] 63/63 batches, loss: 0.5071
[2025-05-01 12:10:22,556][src.training.lm_trainer][INFO] - Epoch 8/10, Train Loss: 0.5071
[2025-05-01 12:10:22,756][src.training.lm_trainer][INFO] - Epoch 8/10, Val Loss: 0.5239, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 12:10:22,756][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 9/10: [Epoch 9/10: [                              ] 1/63 batches, loss: 0.4561Epoch 9/10: [                              ] 2/63 batches, loss: 0.5154Epoch 9/10: [=                             ] 3/63 batches, loss: 0.5036Epoch 9/10: [=                             ] 4/63 batches, loss: 0.5214Epoch 9/10: [==                            ] 5/63 batches, loss: 0.5225Epoch 9/10: [==                            ] 6/63 batches, loss: 0.5272Epoch 9/10: [===                           ] 7/63 batches, loss: 0.5102Epoch 9/10: [===                           ] 8/63 batches, loss: 0.5005Epoch 9/10: [====                          ] 9/63 batches, loss: 0.5061Epoch 9/10: [====                          ] 10/63 batches, loss: 0.5034Epoch 9/10: [=====                         ] 11/63 batches, loss: 0.5014Epoch 9/10: [=====                         ] 12/63 batches, loss: 0.5075Epoch 9/10: [======                        ] 13/63 batches, loss: 0.5090Epoch 9/10: [======                        ] 14/63 batches, loss: 0.5069Epoch 9/10: [=======                       ] 15/63 batches, loss: 0.5082Epoch 9/10: [=======                       ] 16/63 batches, loss: 0.5079Epoch 9/10: [========                      ] 17/63 batches, loss: 0.5077Epoch 9/10: [========                      ] 18/63 batches, loss: 0.5062Epoch 9/10: [=========                     ] 19/63 batches, loss: 0.5073Epoch 9/10: [=========                     ] 20/63 batches, loss: 0.5083Epoch 9/10: [==========                    ] 21/63 batches, loss: 0.5092Epoch 9/10: [==========                    ] 22/63 batches, loss: 0.5100Epoch 9/10: [==========                    ] 23/63 batches, loss: 0.5107Epoch 9/10: [===========                   ] 24/63 batches, loss: 0.5075Epoch 9/10: [===========                   ] 25/63 batches, loss: 0.5054Epoch 9/10: [============                  ] 26/63 batches, loss: 0.5044Epoch 9/10: [============                  ] 27/63 batches, loss: 0.5026Epoch 9/10: [=============                 ] 28/63 batches, loss: 0.5035Epoch 9/10: [=============                 ] 29/63 batches, loss: 0.5018Epoch 9/10: [==============                ] 30/63 batches, loss: 0.5019Epoch 9/10: [==============                ] 31/63 batches, loss: 0.5019Epoch 9/10: [===============               ] 32/63 batches, loss: 0.5042Epoch 9/10: [===============               ] 33/63 batches, loss: 0.5049Epoch 9/10: [================              ] 34/63 batches, loss: 0.5070Epoch 9/10: [================              ] 35/63 batches, loss: 0.5055Epoch 9/10: [=================             ] 36/63 batches, loss: 0.5081Epoch 9/10: [=================             ] 37/63 batches, loss: 0.5079Epoch 9/10: [==================            ] 38/63 batches, loss: 0.5091Epoch 9/10: [==================            ] 39/63 batches, loss: 0.5077Epoch 9/10: [===================           ] 40/63 batches, loss: 0.5094Epoch 9/10: [===================           ] 41/63 batches, loss: 0.5104Epoch 9/10: [====================          ] 42/63 batches, loss: 0.5096Epoch 9/10: [====================          ] 43/63 batches, loss: 0.5084Epoch 9/10: [====================          ] 44/63 batches, loss: 0.5094Epoch 9/10: [=====================         ] 45/63 batches, loss: 0.5098Epoch 9/10: [=====================         ] 46/63 batches, loss: 0.5096Epoch 9/10: [======================        ] 47/63 batches, loss: 0.5085Epoch 9/10: [======================        ] 48/63 batches, loss: 0.5079Epoch 9/10: [=======================       ] 49/63 batches, loss: 0.5078Epoch 9/10: [=======================       ] 50/63 batches, loss: 0.5072Epoch 9/10: [========================      ] 51/63 batches, loss: 0.5072Epoch 9/10: [========================      ] 52/63 batches, loss: 0.5067Epoch 9/10: [=========================     ] 53/63 batches, loss: 0.5062Epoch 9/10: [=========================     ] 54/63 batches, loss: 0.5061Epoch 9/10: [==========================    ] 55/63 batches, loss: 0.5052Epoch 9/10: [==========================    ] 56/63 batches, loss: 0.5052Epoch 9/10: [===========================   ] 57/63 batches, loss: 0.5060Epoch 9/10: [===========================   ] 58/63 batches, loss: 0.5052Epoch 9/10: [============================  ] 59/63 batches, loss: 0.5048Epoch 9/10: [============================  ] 60/63 batches, loss: 0.5047Epoch 9/10: [============================= ] 61/63 batches, loss: 0.5043Epoch 9/10: [============================= ] 62/63 batches, loss: 0.5036Epoch 9/10: [==============================] 63/63 batches, loss: 0.5046
[2025-05-01 12:10:29,119][src.training.lm_trainer][INFO] - Epoch 9/10, Train Loss: 0.5046
[2025-05-01 12:10:29,315][src.training.lm_trainer][INFO] - Epoch 9/10, Val Loss: 0.5224, Metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 12:10:29,316][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 10/10: [Epoch 10/10: [                              ] 1/63 batches, loss: 0.5508Epoch 10/10: [                              ] 2/63 batches, loss: 0.5033Epoch 10/10: [=                             ] 3/63 batches, loss: 0.5191Epoch 10/10: [=                             ] 4/63 batches, loss: 0.5211Epoch 10/10: [==                            ] 5/63 batches, loss: 0.5081Epoch 10/10: [==                            ] 6/63 batches, loss: 0.5073Epoch 10/10: [===                           ] 7/63 batches, loss: 0.5000Epoch 10/10: [===                           ] 8/63 batches, loss: 0.5123Epoch 10/10: [====                          ] 9/63 batches, loss: 0.5140Epoch 10/10: [====                          ] 10/63 batches, loss: 0.5153Epoch 10/10: [=====                         ] 11/63 batches, loss: 0.5056Epoch 10/10: [=====                         ] 12/63 batches, loss: 0.4995Epoch 10/10: [======                        ] 13/63 batches, loss: 0.5017Epoch 10/10: [======                        ] 14/63 batches, loss: 0.4968Epoch 10/10: [=======                       ] 15/63 batches, loss: 0.5035Epoch 10/10: [=======                       ] 16/63 batches, loss: 0.5006Epoch 10/10: [========                      ] 17/63 batches, loss: 0.5035Epoch 10/10: [========                      ] 18/63 batches, loss: 0.5035Epoch 10/10: [=========                     ] 19/63 batches, loss: 0.5035Epoch 10/10: [=========                     ] 20/63 batches, loss: 0.5035Epoch 10/10: [==========                    ] 21/63 batches, loss: 0.5058Epoch 10/10: [==========                    ] 22/63 batches, loss: 0.5057Epoch 10/10: [==========                    ] 23/63 batches, loss: 0.5077Epoch 10/10: [===========                   ] 24/63 batches, loss: 0.5085Epoch 10/10: [===========                   ] 25/63 batches, loss: 0.5092Epoch 10/10: [============                  ] 26/63 batches, loss: 0.5090Epoch 10/10: [============                  ] 27/63 batches, loss: 0.5097Epoch 10/10: [=============                 ] 28/63 batches, loss: 0.5078Epoch 10/10: [=============                 ] 29/63 batches, loss: 0.5061Epoch 10/10: [==============                ] 30/63 batches, loss: 0.5044Epoch 10/10: [==============                ] 31/63 batches, loss: 0.5051Epoch 10/10: [===============               ] 32/63 batches, loss: 0.5036Epoch 10/10: [===============               ] 33/63 batches, loss: 0.5043Epoch 10/10: [================              ] 34/63 batches, loss: 0.5029Epoch 10/10: [================              ] 35/63 batches, loss: 0.5022Epoch 10/10: [=================             ] 36/63 batches, loss: 0.5023Epoch 10/10: [=================             ] 37/63 batches, loss: 0.5017Epoch 10/10: [==================            ] 38/63 batches, loss: 0.5023Epoch 10/10: [==================            ] 39/63 batches, loss: 0.4993Epoch 10/10: [===================           ] 40/63 batches, loss: 0.4988Epoch 10/10: [===================           ] 41/63 batches, loss: 0.4984Epoch 10/10: [====================          ] 42/63 batches, loss: 0.4996Epoch 10/10: [====================          ] 43/63 batches, loss: 0.4997Epoch 10/10: [====================          ] 44/63 batches, loss: 0.4998Epoch 10/10: [=====================         ] 45/63 batches, loss: 0.5004Epoch 10/10: [=====================         ] 46/63 batches, loss: 0.5005Epoch 10/10: [======================        ] 47/63 batches, loss: 0.5005Epoch 10/10: [======================        ] 48/63 batches, loss: 0.5016Epoch 10/10: [=======================       ] 49/63 batches, loss: 0.5018Epoch 10/10: [=======================       ] 50/63 batches, loss: 0.5027Epoch 10/10: [========================      ] 51/63 batches, loss: 0.5023Epoch 10/10: [========================      ] 52/63 batches, loss: 0.5005Epoch 10/10: [=========================     ] 53/63 batches, loss: 0.5001Epoch 10/10: [=========================     ] 54/63 batches, loss: 0.5006Epoch 10/10: [==========================    ] 55/63 batches, loss: 0.5020Epoch 10/10: [==========================    ] 56/63 batches, loss: 0.5020Epoch 10/10: [===========================   ] 57/63 batches, loss: 0.5037Epoch 10/10: [===========================   ] 58/63 batches, loss: 0.5037Epoch 10/10: [============================  ] 59/63 batches, loss: 0.5037Epoch 10/10: [============================  ] 60/63 batches, loss: 0.5037Epoch 10/10: [============================= ] 61/63 batches, loss: 0.5033Epoch 10/10: [============================= ] 62/63 batches, loss: 0.5041Epoch 10/10: [==============================] 63/63 batches, loss: 0.5030
[2025-05-01 12:10:35,700][src.training.lm_trainer][INFO] - Epoch 10/10, Train Loss: 0.5030
[2025-05-01 12:10:35,892][src.training.lm_trainer][INFO] - Epoch 10/10, Val Loss: 0.5531, Metrics: {'accuracy': 0.9545454545454546, 'f1': 0.9523809523809523, 'precision': 0.9090909090909091, 'recall': 1.0}
[2025-05-01 12:10:35,893][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-01 12:10:35,893][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 10
[2025-05-01 12:10:35,893][src.training.lm_trainer][INFO] - Training completed in 68.43 seconds
[2025-05-01 12:10:35,894][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-01 12:10:38,311][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 12:10:38,311][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-01 12:10:38,312][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.922077922077922, 'f1': 0.875, 'precision': 0.8076923076923077, 'recall': 0.9545454545454546}
[2025-05-01 12:10:40,055][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/ar/model.pt
[2025-05-01 12:10:40,061][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▆███
wandb:           best_val_f1 ▁▆███
wandb:         best_val_loss █▃▁▁▁
wandb:    best_val_precision ▁▅███
wandb:       best_val_recall ▁▁▁▁▁
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▄▅▅▄▄▅▅▅
wandb:            train_loss █▅▁▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▆██▇▇███▇
wandb:                val_f1 ▁▆██▆▇███▆
wandb:              val_loss █▃▁▁▂▂▁▁▁▂
wandb:         val_precision ▁▅██▆▇███▆
wandb:            val_recall ▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 1
wandb:           best_val_f1 1
wandb:         best_val_loss 0.5218
wandb:    best_val_precision 1
wandb:       best_val_recall 1
wandb:      early_stop_epoch 10
wandb:                 epoch 10
wandb:   final_test_accuracy 0.92208
wandb:         final_test_f1 0.875
wandb:  final_test_precision 0.80769
wandb:     final_test_recall 0.95455
wandb:  final_train_accuracy 1
wandb:        final_train_f1 1
wandb: final_train_precision 1
wandb:    final_train_recall 1
wandb:    final_val_accuracy 1
wandb:          final_val_f1 1
wandb:   final_val_precision 1
wandb:      final_val_recall 1
wandb:         learning_rate 2e-05
wandb:            train_loss 0.50304
wandb:            train_time 68.4284
wandb:          val_accuracy 0.95455
wandb:                val_f1 0.95238
wandb:              val_loss 0.55315
wandb:         val_precision 0.90909
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_120917-yhcni72c
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250501_120917-yhcni72c/logs
Experiment finetune_question_type_ar completed successfully
Warning: Results file not found: /scratch/leuven/371/vsc37132/finetune_output/question_type/ar/results.json
Running experiment: finetune_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "model=glot500_finetune"         "model.lm_name=cis-lmu/glot500-base"         "model.dropout=0.1"         "model.freeze_model=false"         "model.finetune=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=10"         "training.batch_size=16"         "training.lr=2e-5"         "+training.gradient_accumulation_steps=2"                  "experiment_name=finetune_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/finetune_output/complexity/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-01 12:10:52,236][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_output/complexity/ar
experiment_name: finetune_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_finetune
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  layer_wise: false
  freeze_model: false
  layer_index: -1
  num_outputs: 1
  head_hidden_size: 768
  head_layers: 2
  finetune: true
training:
  task_type: regression
  batch_size: 16
  num_epochs: 10
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-01 12:10:52,236][__main__][INFO] - Normalized task: complexity
[2025-05-01 12:10:52,236][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-01 12:10:52,236][__main__][INFO] - Determined Task Type: regression
[2025-05-01 12:10:52,241][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-01 12:10:52,241][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-01 12:10:53,794][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-01 12:10:56,034][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-01 12:10:56,035][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:10:56,102][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:10:56,137][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:10:56,262][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-01 12:10:56,269][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:10:56,270][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-01 12:10:56,271][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:10:56,298][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:10:56,341][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:10:56,355][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-01 12:10:56,356][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:10:56,356][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-01 12:10:56,357][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-01 12:10:56,381][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:10:56,414][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-01 12:10:56,429][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-01 12:10:56,430][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-01 12:10:56,430][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-01 12:10:56,431][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-01 12:10:56,432][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:10:56,432][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:10:56,432][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:10:56,432][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:10:56,432][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:10:56,433][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-01 12:10:56,433][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-01 12:10:56,433][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-01 12:10:56,433][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:10:56,433][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:10:56,433][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:10:56,433][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:10:56,433][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:10:56,433][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-01 12:10:56,434][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-01 12:10:56,434][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-01 12:10:56,434][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-01 12:10:56,434][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-01 12:10:56,434][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-01 12:10:56,434][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-01 12:10:56,434][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-01 12:10:56,434][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-01 12:10:56,434][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-01 12:10:56,434][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-01 12:10:56,434][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-01 12:10:56,435][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-01 12:10:56,435][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-01 12:10:56,435][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-01 12:10:56,435][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-01 12:11:00,789][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-01 12:11:00,790][src.models.model_factory][INFO] - Language model parameters trainable
[2025-05-01 12:11:00,790][src.models.model_factory][INFO] - Base model configuration: layer-wise=False, layer_index=-1, freeze_model=False
[2025-05-01 12:11:00,790][src.models.model_factory][INFO] - Using head_hidden_size: 768 for fine-tuning
[2025-05-01 12:11:00,794][src.models.model_factory][INFO] - Model has 394,712,833 trainable parameters out of 394,712,833 total parameters
[2025-05-01 12:11:00,795][src.models.model_factory][INFO] - Encoder: 394,121,472 trainable parameters, Head: 591,361 trainable parameters
[2025-05-01 12:11:00,795][src.models.model_factory][INFO] - Fine-tuning head configuration: hidden_size=768, layers=2, dropout=0.1
[2025-05-01 12:11:00,795][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-01 12:11:00,796][__main__][INFO] - Total parameters: 394,712,833
[2025-05-01 12:11:00,796][__main__][INFO] - Trainable parameters: 394,712,833 (100.00%)
Epoch 1/10: [Epoch 1/10: [                              ] 1/63 batches, loss: 0.2303Epoch 1/10: [                              ] 2/63 batches, loss: 0.2078Epoch 1/10: [=                             ] 3/63 batches, loss: 0.2220Epoch 1/10: [=                             ] 4/63 batches, loss: 0.2301Epoch 1/10: [==                            ] 5/63 batches, loss: 0.2037Epoch 1/10: [==                            ] 6/63 batches, loss: 0.1926Epoch 1/10: [===                           ] 7/63 batches, loss: 0.1881Epoch 1/10: [===                           ] 8/63 batches, loss: 0.1857Epoch 1/10: [====                          ] 9/63 batches, loss: 0.1875Epoch 1/10: [====                          ] 10/63 batches, loss: 0.1871Epoch 1/10: [=====                         ] 11/63 batches, loss: 0.1827Epoch 1/10: [=====                         ] 12/63 batches, loss: 0.1822Epoch 1/10: [======                        ] 13/63 batches, loss: 0.1838Epoch 1/10: [======                        ] 14/63 batches, loss: 0.1789Epoch 1/10: [=======                       ] 15/63 batches, loss: 0.1721Epoch 1/10: [=======                       ] 16/63 batches, loss: 0.1689Epoch 1/10: [========                      ] 17/63 batches, loss: 0.1630Epoch 1/10: [========                      ] 18/63 batches, loss: 0.1590Epoch 1/10: [=========                     ] 19/63 batches, loss: 0.1582Epoch 1/10: [=========                     ] 20/63 batches, loss: 0.1561Epoch 1/10: [==========                    ] 21/63 batches, loss: 0.1553Epoch 1/10: [==========                    ] 22/63 batches, loss: 0.1519Epoch 1/10: [==========                    ] 23/63 batches, loss: 0.1510Epoch 1/10: [===========                   ] 24/63 batches, loss: 0.1471Epoch 1/10: [===========                   ] 25/63 batches, loss: 0.1437Epoch 1/10: [============                  ] 26/63 batches, loss: 0.1411Epoch 1/10: [============                  ] 27/63 batches, loss: 0.1407Epoch 1/10: [=============                 ] 28/63 batches, loss: 0.1398Epoch 1/10: [=============                 ] 29/63 batches, loss: 0.1378Epoch 1/10: [==============                ] 30/63 batches, loss: 0.1350Epoch 1/10: [==============                ] 31/63 batches, loss: 0.1325Epoch 1/10: [===============               ] 32/63 batches, loss: 0.1296Epoch 1/10: [===============               ] 33/63 batches, loss: 0.1272Epoch 1/10: [================              ] 34/63 batches, loss: 0.1249Epoch 1/10: [================              ] 35/63 batches, loss: 0.1232Epoch 1/10: [=================             ] 36/63 batches, loss: 0.1213Epoch 1/10: [=================             ] 37/63 batches, loss: 0.1195Epoch 1/10: [==================            ] 38/63 batches, loss: 0.1187Epoch 1/10: [==================            ] 39/63 batches, loss: 0.1166Epoch 1/10: [===================           ] 40/63 batches, loss: 0.1147Epoch 1/10: [===================           ] 41/63 batches, loss: 0.1134Epoch 1/10: [====================          ] 42/63 batches, loss: 0.1116Epoch 1/10: [====================          ] 43/63 batches, loss: 0.1112Epoch 1/10: [====================          ] 44/63 batches, loss: 0.1099Epoch 1/10: [=====================         ] 45/63 batches, loss: 0.1088Epoch 1/10: [=====================         ] 46/63 batches, loss: 0.1069Epoch 1/10: [======================        ] 47/63 batches, loss: 0.1060Epoch 1/10: [======================        ] 48/63 batches, loss: 0.1051Epoch 1/10: [=======================       ] 49/63 batches, loss: 0.1037Epoch 1/10: [=======================       ] 50/63 batches, loss: 0.1024Epoch 1/10: [========================      ] 51/63 batches, loss: 0.1008Epoch 1/10: [========================      ] 52/63 batches, loss: 0.0993Epoch 1/10: [=========================     ] 53/63 batches, loss: 0.0983Epoch 1/10: [=========================     ] 54/63 batches, loss: 0.0969Epoch 1/10: [==========================    ] 55/63 batches, loss: 0.0962Epoch 1/10: [==========================    ] 56/63 batches, loss: 0.0953Epoch 1/10: [===========================   ] 57/63 batches, loss: 0.0942Epoch 1/10: [===========================   ] 58/63 batches, loss: 0.0937Epoch 1/10: [============================  ] 59/63 batches, loss: 0.0928Epoch 1/10: [============================  ] 60/63 batches, loss: 0.0916Epoch 1/10: [============================= ] 61/63 batches, loss: 0.0906Epoch 1/10: [============================= ] 62/63 batches, loss: 0.0896Epoch 1/10: [==============================] 63/63 batches, loss: 0.0889
[2025-05-01 12:11:10,101][src.training.lm_trainer][INFO] - Epoch 1/10, Train Loss: 0.0889
[2025-05-01 12:11:10,285][src.training.lm_trainer][INFO] - Epoch 1/10, Val Loss: 0.0799, Metrics: {'mse': 0.08098221570253372, 'rmse': 0.2845737438741208, 'r2': -0.248205304145813}
Epoch 2/10: [Epoch 2/10: [                              ] 1/63 batches, loss: 0.0437Epoch 2/10: [                              ] 2/63 batches, loss: 0.0360Epoch 2/10: [=                             ] 3/63 batches, loss: 0.0361Epoch 2/10: [=                             ] 4/63 batches, loss: 0.0326Epoch 2/10: [==                            ] 5/63 batches, loss: 0.0318Epoch 2/10: [==                            ] 6/63 batches, loss: 0.0303Epoch 2/10: [===                           ] 7/63 batches, loss: 0.0295Epoch 2/10: [===                           ] 8/63 batches, loss: 0.0285Epoch 2/10: [====                          ] 9/63 batches, loss: 0.0291Epoch 2/10: [====                          ] 10/63 batches, loss: 0.0275Epoch 2/10: [=====                         ] 11/63 batches, loss: 0.0278Epoch 2/10: [=====                         ] 12/63 batches, loss: 0.0271Epoch 2/10: [======                        ] 13/63 batches, loss: 0.0285Epoch 2/10: [======                        ] 14/63 batches, loss: 0.0289Epoch 2/10: [=======                       ] 15/63 batches, loss: 0.0293Epoch 2/10: [=======                       ] 16/63 batches, loss: 0.0286Epoch 2/10: [========                      ] 17/63 batches, loss: 0.0287Epoch 2/10: [========                      ] 18/63 batches, loss: 0.0277Epoch 2/10: [=========                     ] 19/63 batches, loss: 0.0273Epoch 2/10: [=========                     ] 20/63 batches, loss: 0.0269Epoch 2/10: [==========                    ] 21/63 batches, loss: 0.0263Epoch 2/10: [==========                    ] 22/63 batches, loss: 0.0260Epoch 2/10: [==========                    ] 23/63 batches, loss: 0.0256Epoch 2/10: [===========                   ] 24/63 batches, loss: 0.0252Epoch 2/10: [===========                   ] 25/63 batches, loss: 0.0252Epoch 2/10: [============                  ] 26/63 batches, loss: 0.0251Epoch 2/10: [============                  ] 27/63 batches, loss: 0.0251Epoch 2/10: [=============                 ] 28/63 batches, loss: 0.0250Epoch 2/10: [=============                 ] 29/63 batches, loss: 0.0250Epoch 2/10: [==============                ] 30/63 batches, loss: 0.0247Epoch 2/10: [==============                ] 31/63 batches, loss: 0.0247slurmstepd: error: *** JOB 64435712 ON k28i22 CANCELLED AT 2025-05-01T12:11:13 ***
