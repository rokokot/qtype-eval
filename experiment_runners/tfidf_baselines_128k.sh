#!/bin/bash
#SBATCH --job-name=clean_tfidf_128k
#SBATCH --clusters=wice
#SBATCH --time=04:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=64G
#SBATCH --partition=batch
#SBATCH --account=intro_vsc37132

echo "========================================================================="
echo "SLURM Job Information:"
echo "SLURM_JOB_ID: $SLURM_JOB_ID"
echo "SLURM_JOB_USER: $SLURM_JOB_USER"
echo "SLURM_JOB_ACCOUNT: $SLURM_JOB_ACCOUNT"
echo "SLURM_JOB_NAME: $SLURM_JOB_NAME"
echo "SLURM_CLUSTER_NAME: $SLURM_CLUSTER_NAME"
echo "SLURM_JOB_PARTITION: $SLURM_JOB_PARTITION"
echo "SLURM_NNODES: $SLURM_NNODES"
echo "SLURM_NODELIST: $SLURM_NODELIST"
echo "SLURM_JOB_CPUS_PER_NODE: $SLURM_JOB_CPUS_PER_NODE"
echo "Date: $(date)"
echo "Walltime: $SLURM_TIMELIMIT"
echo "========================================================================="

# Activate the clean environment
echo "ðŸ”„ Activating clean qtype-eval environment..."

# Load Miniconda and activate environment
module --force purge
module load cluster/wice/oldhierarchy

# Try to load Miniconda module, with fallback
if module load Miniconda3/py310_22.11.1-1 2>/dev/null; then
    echo "âœ… Miniconda module loaded successfully"
    # Initialize conda
    if [ -f "$EBROOTMINICONDA3/etc/profile.d/conda.sh" ]; then
        source $EBROOTMINICONDA3/etc/profile.d/conda.sh
        conda activate qtype-eval
    else
        echo "âŒ Conda initialization script not found at expected path"
        exit 1
    fi
else
    echo "âš ï¸ Miniconda module failed to load, checking if conda is already available..."
    if command -v conda >/dev/null 2>&1; then
        echo "âœ… Conda found in PATH, initializing and activating qtype-eval environment..."
        # Initialize conda shell integration
        eval "$(conda shell.bash hook)"
        conda activate qtype-eval
    else
        echo "âŒ Neither module nor conda available, exiting..."
        exit 1
    fi
fi

# Set up environment (keeping HF settings)
export PYTHONPATH="$PWD:$PYTHONPATH"
export HF_HOME="$VSC_DATA/hf_cache"
export HF_DATASETS_OFFLINE=1
export TRANSFORMERS_OFFLINE=1
export HYDRA_JOB_CHDIR=False
export HYDRA_FULL_ERROR=1

echo "ðŸ Environment verification:"
echo "  Python path: $(which python)"
echo "  Python version: $(python --version)"
echo "  Conda environment: $CONDA_DEFAULT_ENV"
echo "  Conda prefix: $CONDA_PREFIX"

# Quick package check
echo "ðŸ“¦ Checking required packages..."
python3 -c "
required = ['numpy', 'pandas', 'sklearn', 'xgboost', 'transformers', 'datasets', 'hydra_core']
missing = []
for pkg in required:
    try:
        __import__(pkg)
        print(f'âœ… {pkg}')
    except ImportError:
        print(f'âŒ {pkg}')
        missing.append(pkg)

if missing:
    print(f'Installing missing packages: {missing}')
    import subprocess
    import sys
    for pkg in missing:
        subprocess.run([sys.executable, '-m', 'pip', 'install', pkg])
"

# Initialize experiment setup
LANGUAGES=("ar" "en" "fi" "id" "ja" "ko" "ru")
MODELS=("dummy" "logistic" "ridge" "xgboost")  
TASKS=("question_type" "complexity")

OUTPUT_BASE_DIR="$VSC_SCRATCH/clean_tfidf_128k_output"
mkdir -p "$OUTPUT_BASE_DIR"

RESULTS_TRACKER="${OUTPUT_BASE_DIR}/results.csv"
echo "experiment,language,task,model,metric,value,status" > "$RESULTS_TRACKER"

FEATURES_DIR="./data/tfidf_features_128k"

# Generate TF-IDF features if needed
echo "ðŸ” Checking TF-IDF features..."
if [ ! -f "${FEATURES_DIR}/metadata.json" ]; then
    echo "âš™ï¸ Generating TF-IDF features with 128k max_features..."
    python3 scripts/generate_tfidf_glot500.py \
        --output-dir "${FEATURES_DIR}" \
        --model-name "cis-lmu/glot500-base" \
        --max-features 128000 \
        --verify
    
    if [ $? -ne 0 ]; then
        echo "âŒ TF-IDF generation failed"
        exit 1
    fi
else
    echo "âœ… TF-IDF features found"
fi

# Function to run a single experiment
run_experiment() {
    local LANG=$1
    local TASK=$2
    local MODEL=$3
    
    local EXP_NAME="${MODEL}_${TASK}_${LANG}"
    local TASK_TYPE="classification"
    [ "$TASK" == "complexity" ] && TASK_TYPE="regression"
    
    local OUTPUT_DIR="${OUTPUT_BASE_DIR}/${EXP_NAME}"
    mkdir -p "$OUTPUT_DIR"
    
    echo "ðŸ”¬ Running: $EXP_NAME"
    
    # Check if already completed
    if [ -f "${OUTPUT_DIR}/results.json" ]; then
        echo "  âœ… Already completed"
        return 0
    fi
    
    # Run the experiment using Python script
    python3 -c "
import sys
sys.path.append('.')

from src.data.datasets import load_sklearn_data
from src.models.tfidf_baselines import create_tfidf_baseline_model
from src.training.sklearn_trainer import SklearnTrainer
import json
import traceback

try:
    # Load data
    print('  ðŸ“Š Loading data...')
    (X_train, y_train), (X_val, y_val), (X_test, y_test) = load_sklearn_data(
        task='$TASK',
        languages=['$LANG'],
        tfidf_features_dir='$FEATURES_DIR',
        use_tfidf_loader=True
    )
    print(f'    Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}')
    
    # Create model
    print('  ðŸ¤– Creating model...')
    model = create_tfidf_baseline_model(
        model_type='$MODEL',
        task_type='$TASK_TYPE',
        tfidf_features_dir='$FEATURES_DIR',
        target_languages=['$LANG']
    )
    
    # Train
    print('  ðŸ‹ï¸ Training...')
    trainer = SklearnTrainer(
        model=model.model,
        task_type='$TASK_TYPE',
        output_dir='$OUTPUT_DIR'
    )
    
    results = trainer.train(
        train_data=(X_train, y_train),
        val_data=(X_val, y_val),
        test_data=(X_test, y_test)
    )
    
    # Save results
    results['experiment_name'] = '$EXP_NAME'
    results['language'] = '$LANG'
    results['task'] = '$TASK'
    results['model'] = '$MODEL'
    results['max_features'] = 128000
    
    with open('$OUTPUT_DIR/results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    # Log to tracker
    test_metrics = results.get('test_metrics', {})
    for metric, value in test_metrics.items():
        print(f'$EXP_NAME,$LANG,$TASK,$MODEL,{metric},{value},success')
    
    print('  âœ… Completed successfully')

except Exception as e:
    print(f'  âŒ Failed: {e}')
    print(f'$EXP_NAME,$LANG,$TASK,$MODEL,error,{str(e)},failed')
    traceback.print_exc()
    sys.exit(1)
" 2>&1 | tee -a "${OUTPUT_DIR}/log.txt"
    
    return ${PIPESTATUS[0]}
}

# Run all experiments
echo "ðŸš€ Starting TF-IDF experiments with 128k features..."
total=0
completed=0

for lang in "${LANGUAGES[@]}"; do
    for task in "${TASKS[@]}"; do
        for model in "${MODELS[@]}"; do
            # Skip incompatible combinations
            if [[ "$model" == "logistic" && "$task" == "complexity" ]]; then
                echo "â­ï¸ Skipping $model + $task (incompatible)"
                continue
            fi
            
            ((total++))
            
            echo ""
            echo "ðŸ“‹ Experiment $total: $model + $task + $lang"
            echo "----------------------------------------"
            
            if run_experiment "$lang" "$task" "$model"; then
                ((completed++))
                echo "âœ… Success ($completed/$total)"
            else
                echo "âŒ Failed ($completed/$total)"
            fi
        done
    done
done

echo ""
echo "========================================================================="
echo "ðŸ TF-IDF Experiments Completed!"
echo "========================================================================="
echo "Total experiments: $total"
echo "Completed successfully: $completed"
echo "Failed: $((total - completed))"
echo "Results directory: $OUTPUT_BASE_DIR"
echo "========================================================================="

# Create summary
echo "ðŸ“Š Creating summary..."
python3 -c "
import json
import pandas as pd
from pathlib import Path

output_dir = Path('$OUTPUT_BASE_DIR')
results = []

for result_file in output_dir.rglob('results.json'):
    try:
        with open(result_file) as f:
            data = json.load(f)
        
        exp_name = data.get('experiment_name', 'unknown')
        test_metrics = data.get('test_metrics', {})
        
        for metric, value in test_metrics.items():
            results.append({
                'experiment': exp_name,
                'language': data.get('language', 'unknown'),
                'task': data.get('task', 'unknown'),
                'model': data.get('model', 'unknown'),
                'metric': metric,
                'value': value,
                'max_features': data.get('max_features', 128000)
            })
    except Exception as e:
        print(f'Error processing {result_file}: {e}')

if results:
    df = pd.DataFrame(results)
    df.to_csv('$OUTPUT_BASE_DIR/summary.csv', index=False)
    print(f'Summary saved: {len(results)} result entries')
    
    # Print best results
    if 'accuracy' in df['metric'].values:
        # Convert value to numeric, handling any non-numeric values
        accuracy_df = df[df['metric'] == 'accuracy'].copy()
        accuracy_df['value'] = pd.to_numeric(accuracy_df['value'], errors='coerce')
        accuracy_df = accuracy_df.dropna(subset=['value'])
        
        if len(accuracy_df) > 0:
            best_acc = accuracy_df.nlargest(5, 'value')
            print('\\nTop 5 Accuracy Results (128k features):')
            for _, row in best_acc.iterrows():
                print(f'  {row[\"experiment\"]}: {row[\"value\"]:.3f}')
        else:
            print('\\nNo valid accuracy results found')
else:
    print('No results found')
"

echo "âœ… All done!"