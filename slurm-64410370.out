SLURM_JOB_ID: 64410370
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: finetune_test
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Mon Apr 28 15:31:45 CEST 2025
Walltime: 00-00:30:00
========================================================================
Channels:
 - pytorch
 - nvidia
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): ...working... done
Solving environment: ...working... done


==> WARNING: A newer version of conda exists. <==
    current version: 25.1.1
    latest version: 25.3.1

Please update conda by running

    $ conda update -n base -c defaults conda



# All requested packages already installed.

Requirement already satisfied: hydra-core in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (1.3.2)
Requirement already satisfied: hydra-submitit-launcher in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (1.2.0)
Requirement already satisfied: omegaconf<2.4,>=2.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (2.3.0)
Requirement already satisfied: antlr4-python3-runtime==4.9.* in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (4.9.3)
Requirement already satisfied: packaging in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-core) (24.2)
Requirement already satisfied: submitit>=1.3.3 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from hydra-submitit-launcher) (1.5.2)
Requirement already satisfied: PyYAML>=5.1.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.2)
Requirement already satisfied: cloudpickle>=1.2.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from submitit>=1.3.3->hydra-submitit-launcher) (3.1.1)
Requirement already satisfied: typing_extensions>=3.7.4.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from submitit>=1.3.3->hydra-submitit-launcher) (4.12.2)
Requirement already satisfied: transformers<4.36.0,>=4.30.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (4.35.2)
Requirement already satisfied: torch in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (2.5.1)
Requirement already satisfied: datasets in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (3.5.0)
Requirement already satisfied: wandb in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (0.19.9)
Requirement already satisfied: filelock in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (3.18.0)
Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.30.1)
Requirement already satisfied: numpy>=1.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (1.26.4)
Requirement already satisfied: packaging>=20.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (24.2)
Requirement already satisfied: pyyaml>=5.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (2024.11.6)
Requirement already satisfied: requests in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (2.32.3)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.15.2)
Requirement already satisfied: safetensors>=0.3.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (0.5.3)
Requirement already satisfied: tqdm>=4.27 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from transformers<4.36.0,>=4.30.0) (4.67.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (4.12.2)
Requirement already satisfied: networkx in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (3.2.1)
Requirement already satisfied: jinja2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (3.1.6)
Requirement already satisfied: fsspec in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (2024.12.0)
Requirement already satisfied: sympy==1.13.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from torch) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)
Requirement already satisfied: pyarrow>=15.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (19.0.1)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (0.3.8)
Requirement already satisfied: pandas in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (2.2.3)
Requirement already satisfied: xxhash in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (3.5.0)
Requirement already satisfied: multiprocess<0.70.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (0.70.16)
Requirement already satisfied: aiohttp in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from datasets) (3.11.16)
Requirement already satisfied: click!=8.0.0,>=7.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (8.1.8)
Requirement already satisfied: docker-pycreds>=0.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (0.4.0)
Requirement already satisfied: eval-type-backport in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (0.2.2)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (3.1.44)
Requirement already satisfied: platformdirs in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (4.3.7)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.15.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (5.29.4)
Requirement already satisfied: psutil>=5.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (7.0.0)
Requirement already satisfied: pydantic<3 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (2.11.1)
Requirement already satisfied: sentry-sdk>=2.0.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (2.25.0)
Requirement already satisfied: setproctitle in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (1.3.5)
Requirement already satisfied: setuptools in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from wandb) (78.1.0)
Requirement already satisfied: six>=1.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)
Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (2.6.1)
Requirement already satisfied: aiosignal>=1.1.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.2)
Requirement already satisfied: async-timeout<6.0,>=4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (5.0.1)
Requirement already satisfied: attrs>=17.3.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (6.3.1)
Requirement already satisfied: propcache>=0.2.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (0.3.1)
Requirement already satisfied: yarl<2.0,>=1.17.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from aiohttp->datasets) (1.18.3)
Requirement already satisfied: gitdb<5,>=4.0.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)
Requirement already satisfied: annotated-types>=0.6.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (0.7.0)
Requirement already satisfied: pydantic-core==2.33.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (2.33.0)
Requirement already satisfied: typing-inspection>=0.4.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pydantic<3->wandb) (0.4.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from requests->transformers<4.36.0,>=4.30.0) (2025.1.31)
Requirement already satisfied: MarkupSafe>=2.0 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)
Requirement already satisfied: python-dateutil>=2.8.2 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from pandas->datasets) (2025.2)
Requirement already satisfied: smmap<6,>=3.0.1 in /vsc-hard-mounts/leuven-data/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)
Environment variables:
PYTHONPATH=:/data/leuven/371/vsc37132/qtype-eval:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-user/371/vsc37132:/vsc-hard-mounts/leuven-data/371/vsc37132/qtype-eval
HF_HOME=/data/leuven/371/vsc37132/qtype-eval/data/cache
GPU information:
Mon Apr 28 15:32:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          On  |   00000000:17:00.0 Off |                    0 |
| N/A   35C    P0             44W /  300W |       1MiB /  81920MiB |      0%   E. Process |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Python executable: /data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/bin/python
PyTorch CUDA available: True
Testing question type fine-tuning on English...
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-28 15:32:26,892][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_test/question_type/en
experiment_name: test_finetune_qtype_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 4
  num_epochs: 1
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_finetune
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  finetune: true
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1
    lr: 2.0e-05
    batch_size: 8

[2025-04-28 15:32:26,892][__main__][INFO] - Normalized task: question_type
[2025-04-28 15:32:26,892][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-28 15:32:26,892][__main__][INFO] - Determined Task Type: classification
[2025-04-28 15:32:26,897][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-28 15:32:26,897][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-28 15:32:28,439][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-28 15:32:30,664][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-28 15:32:30,664][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-28 15:32:30,763][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:32:30,793][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:32:30,867][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-28 15:32:30,875][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-28 15:32:30,875][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-28 15:32:30,876][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-28 15:32:30,891][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:32:30,916][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:32:30,928][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-28 15:32:30,929][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-28 15:32:30,929][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-28 15:32:30,930][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-28 15:32:30,942][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:32:30,966][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:32:30,978][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-28 15:32:30,979][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-28 15:32:30,979][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-28 15:32:30,980][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-28 15:32:30,981][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-28 15:32:30,981][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-28 15:32:30,981][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-28 15:32:30,981][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-28 15:32:30,981][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-28 15:32:30,981][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-28 15:32:30,981][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-28 15:32:30,981][src.data.datasets][INFO] - Sample label: 1
[2025-04-28 15:32:30,982][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-28 15:32:30,982][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-28 15:32:30,982][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-28 15:32:30,982][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-28 15:32:30,982][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-28 15:32:30,982][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-28 15:32:30,982][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-28 15:32:30,982][src.data.datasets][INFO] - Sample label: 0
[2025-04-28 15:32:30,982][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-28 15:32:30,982][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-28 15:32:30,982][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-28 15:32:30,983][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-28 15:32:30,983][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-28 15:32:30,983][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-28 15:32:30,983][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-28 15:32:30,983][src.data.datasets][INFO] - Sample label: 0
[2025-04-28 15:32:30,983][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-28 15:32:30,983][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-28 15:32:30,983][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-28 15:32:30,984][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-28 15:32:34,962][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-28 15:32:34,962][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-28 15:32:34,964][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-28 15:32:34,964][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-28 15:32:34,965][__main__][INFO] - Successfully created model for en
[2025-04-28 15:32:34,965][__main__][INFO] - finetuning with gradient accum steps: 1
Epoch 1/1:   0%|          | 0/298 [00:00<?, ?it/s]Epoch 1/1:   0%|          | 1/298 [00:00<04:36,  1.07it/s]Epoch 1/1:   1%|▏         | 4/298 [00:01<01:03,  4.65it/s]Epoch 1/1:   2%|▏         | 7/298 [00:01<00:36,  7.95it/s]Epoch 1/1:   3%|▎         | 10/298 [00:01<00:26, 10.89it/s]Epoch 1/1:   4%|▍         | 13/298 [00:01<00:21, 13.34it/s]Epoch 1/1:   5%|▌         | 16/298 [00:01<00:18, 15.34it/s]Epoch 1/1:   6%|▋         | 19/298 [00:01<00:16, 16.89it/s]Epoch 1/1:   7%|▋         | 22/298 [00:01<00:15, 18.06it/s]Epoch 1/1:   8%|▊         | 25/298 [00:02<00:14, 18.86it/s]Epoch 1/1:   9%|▉         | 28/298 [00:02<00:13, 19.51it/s]Epoch 1/1:  10%|█         | 31/298 [00:02<00:13, 19.97it/s]Epoch 1/1:  11%|█▏        | 34/298 [00:02<00:13, 20.30it/s]Epoch 1/1:  12%|█▏        | 37/298 [00:02<00:12, 20.49it/s]Epoch 1/1:  13%|█▎        | 40/298 [00:02<00:12, 20.66it/s]Epoch 1/1:  14%|█▍        | 43/298 [00:02<00:12, 20.77it/s]Epoch 1/1:  15%|█▌        | 46/298 [00:03<00:12, 20.86it/s]Epoch 1/1:  16%|█▋        | 49/298 [00:03<00:11, 20.87it/s]Epoch 1/1:  17%|█▋        | 52/298 [00:03<00:11, 20.92it/s]Epoch 1/1:  18%|█▊        | 55/298 [00:03<00:11, 20.88it/s]Epoch 1/1:  19%|█▉        | 58/298 [00:03<00:11, 20.92it/s]Epoch 1/1:  20%|██        | 61/298 [00:03<00:11, 20.97it/s]Epoch 1/1:  21%|██▏       | 64/298 [00:03<00:11, 20.95it/s]Epoch 1/1:  22%|██▏       | 67/298 [00:04<00:11, 20.99it/s]Epoch 1/1:  23%|██▎       | 70/298 [00:04<00:10, 21.00it/s]Epoch 1/1:  24%|██▍       | 73/298 [00:04<00:10, 21.02it/s]Epoch 1/1:  26%|██▌       | 76/298 [00:04<00:10, 20.96it/s]Epoch 1/1:  27%|██▋       | 79/298 [00:04<00:10, 21.00it/s]Epoch 1/1:  28%|██▊       | 82/298 [00:04<00:10, 21.01it/s]Epoch 1/1:  29%|██▊       | 85/298 [00:04<00:10, 21.03it/s]Epoch 1/1:  30%|██▉       | 88/298 [00:05<00:09, 21.04it/s]Epoch 1/1:  31%|███       | 91/298 [00:05<00:09, 21.03it/s]Epoch 1/1:  32%|███▏      | 94/298 [00:05<00:09, 21.05it/s]Epoch 1/1:  33%|███▎      | 97/298 [00:05<00:09, 21.06it/s]Epoch 1/1:  34%|███▎      | 100/298 [00:05<00:09, 20.99it/s]Epoch 1/1:  35%|███▍      | 103/298 [00:05<00:09, 21.01it/s]Epoch 1/1:  36%|███▌      | 106/298 [00:05<00:09, 20.97it/s]Epoch 1/1:  37%|███▋      | 109/298 [00:06<00:09, 20.99it/s]Epoch 1/1:  38%|███▊      | 112/298 [00:06<00:08, 21.00it/s]Epoch 1/1:  39%|███▊      | 115/298 [00:06<00:08, 20.95it/s]Epoch 1/1:  40%|███▉      | 118/298 [00:06<00:08, 20.97it/s]Epoch 1/1:  41%|████      | 121/298 [00:06<00:08, 20.97it/s]Epoch 1/1:  42%|████▏     | 124/298 [00:06<00:08, 20.95it/s]Epoch 1/1:  43%|████▎     | 127/298 [00:06<00:08, 20.98it/s]Epoch 1/1:  44%|████▎     | 130/298 [00:07<00:07, 21.02it/s]Epoch 1/1:  45%|████▍     | 133/298 [00:07<00:07, 21.02it/s]Epoch 1/1:  46%|████▌     | 136/298 [00:07<00:07, 21.02it/s]Epoch 1/1:  47%|████▋     | 139/298 [00:07<00:07, 21.02it/s]Epoch 1/1:  48%|████▊     | 142/298 [00:07<00:07, 21.02it/s]Epoch 1/1:  49%|████▊     | 145/298 [00:07<00:07, 21.02it/s]Epoch 1/1:  50%|████▉     | 148/298 [00:07<00:07, 20.97it/s]Epoch 1/1:  51%|█████     | 151/298 [00:08<00:06, 21.00it/s]Epoch 1/1:  52%|█████▏    | 154/298 [00:08<00:06, 21.02it/s]Epoch 1/1:  53%|█████▎    | 157/298 [00:08<00:06, 21.03it/s]Epoch 1/1:  54%|█████▎    | 160/298 [00:08<00:06, 20.97it/s]Epoch 1/1:  55%|█████▍    | 163/298 [00:08<00:06, 21.01it/s]Epoch 1/1:  56%|█████▌    | 166/298 [00:08<00:06, 21.03it/s]Epoch 1/1:  57%|█████▋    | 169/298 [00:08<00:06, 21.04it/s]Epoch 1/1:  58%|█████▊    | 172/298 [00:09<00:05, 21.04it/s]Epoch 1/1:  59%|█████▊    | 175/298 [00:09<00:05, 20.98it/s]Epoch 1/1:  60%|█████▉    | 178/298 [00:09<00:05, 21.00it/s]Epoch 1/1:  61%|██████    | 181/298 [00:09<00:05, 20.97it/s]Epoch 1/1:  62%|██████▏   | 184/298 [00:09<00:05, 21.00it/s]Epoch 1/1:  63%|██████▎   | 187/298 [00:09<00:05, 21.02it/s]Epoch 1/1:  64%|██████▍   | 190/298 [00:09<00:05, 20.98it/s]Epoch 1/1:  65%|██████▍   | 193/298 [00:10<00:05, 21.00it/s]Epoch 1/1:  66%|██████▌   | 196/298 [00:10<00:04, 21.01it/s]Epoch 1/1:  67%|██████▋   | 199/298 [00:10<00:04, 21.03it/s]Epoch 1/1:  68%|██████▊   | 202/298 [00:10<00:04, 21.04it/s]Epoch 1/1:  69%|██████▉   | 205/298 [00:10<00:04, 21.03it/s]Epoch 1/1:  70%|██████▉   | 208/298 [00:10<00:04, 21.04it/s]Epoch 1/1:  71%|███████   | 211/298 [00:10<00:04, 21.05it/s]Epoch 1/1:  72%|███████▏  | 214/298 [00:11<00:03, 21.06it/s]Epoch 1/1:  73%|███████▎  | 217/298 [00:11<00:03, 21.06it/s]Epoch 1/1:  74%|███████▍  | 220/298 [00:11<00:03, 21.07it/s]Epoch 1/1:  75%|███████▍  | 223/298 [00:11<00:03, 21.05it/s]Epoch 1/1:  76%|███████▌  | 226/298 [00:11<00:03, 21.00it/s]Epoch 1/1:  77%|███████▋  | 229/298 [00:11<00:03, 21.01it/s]Epoch 1/1:  78%|███████▊  | 232/298 [00:11<00:03, 20.96it/s]Epoch 1/1:  79%|███████▉  | 235/298 [00:12<00:03, 20.98it/s]Epoch 1/1:  80%|███████▉  | 238/298 [00:12<00:02, 20.99it/s]Epoch 1/1:  81%|████████  | 241/298 [00:12<00:02, 21.00it/s]Epoch 1/1:  82%|████████▏ | 244/298 [00:12<00:02, 21.03it/s]Epoch 1/1:  83%|████████▎ | 247/298 [00:12<00:02, 20.96it/s]Epoch 1/1:  84%|████████▍ | 250/298 [00:12<00:02, 20.99it/s]Epoch 1/1:  85%|████████▍ | 253/298 [00:12<00:02, 21.02it/s]Epoch 1/1:  86%|████████▌ | 256/298 [00:13<00:02, 20.98it/s]Epoch 1/1:  87%|████████▋ | 259/298 [00:13<00:01, 21.00it/s]Epoch 1/1:  88%|████████▊ | 262/298 [00:13<00:01, 21.02it/s]Epoch 1/1:  89%|████████▉ | 265/298 [00:13<00:01, 21.03it/s]Epoch 1/1:  90%|████████▉ | 268/298 [00:13<00:01, 21.04it/s]Epoch 1/1:  91%|█████████ | 271/298 [00:13<00:01, 21.05it/s]Epoch 1/1:  92%|█████████▏| 274/298 [00:13<00:01, 21.05it/s]Epoch 1/1:  93%|█████████▎| 277/298 [00:14<00:00, 21.04it/s]Epoch 1/1:  94%|█████████▍| 280/298 [00:14<00:00, 21.05it/s]Epoch 1/1:  95%|█████████▍| 283/298 [00:14<00:00, 21.04it/s]Epoch 1/1:  96%|█████████▌| 286/298 [00:14<00:00, 21.05it/s]Epoch 1/1:  97%|█████████▋| 289/298 [00:14<00:00, 21.05it/s]Epoch 1/1:  98%|█████████▊| 292/298 [00:14<00:00, 21.08it/s]Epoch 1/1:  99%|█████████▉| 295/298 [00:14<00:00, 21.12it/s]Epoch 1/1: 100%|██████████| 298/298 [00:15<00:00, 21.16it/s]Epoch 1/1: 100%|██████████| 298/298 [00:15<00:00, 19.74it/s]
[2025-04-28 15:32:51,892][src.training.lm_trainer][INFO] - Epoch 1/1, Train Loss: 0.6167
[2025-04-28 15:32:52,163][src.training.lm_trainer][INFO] - Epoch 1/1, Val Loss: 0.5863, Metrics: {'accuracy': 0.9305555555555556, 'f1': 0.9315068493150684}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁
wandb:           train_loss ▁
wandb:           train_time ▁
wandb:         val_accuracy ▁
wandb:               val_f1 ▁
wandb:             val_loss ▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.93056
wandb:          best_val_f1 0.93151
wandb:        best_val_loss 0.58626
wandb:                epoch 1
wandb:  final_test_accuracy 0.98182
wandb:        final_test_f1 0.98182
wandb: final_train_accuracy 0.98574
wandb:       final_train_f1 0.98556
wandb:   final_val_accuracy 0.93056
wandb:         final_val_f1 0.93151
wandb:        learning_rate 2e-05
wandb:           train_loss 0.61669
wandb:           train_time 15.79126
wandb:         val_accuracy 0.93056
wandb:               val_f1 0.93151
wandb:             val_loss 0.58626
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250428_153226-99iunm3f
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250428_153226-99iunm3f/logs
Question type fine-tuning test completed successfully!
Testing control question type fine-tuning on English...
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-28 15:33:21,775][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_test/control1/en
experiment_name: test_finetune_qtype_control1_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: classification
  batch_size: 4
  num_epochs: 1
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_finetune
  tasks: question_type
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  finetune: true
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1
    lr: 2.0e-05
    batch_size: 8

[2025-04-28 15:33:21,775][__main__][INFO] - Normalized task: question_type
[2025-04-28 15:33:21,775][__main__][INFO] - Using explicit task_type from config: classification
[2025-04-28 15:33:21,775][__main__][INFO] - Determined Task Type: classification
[2025-04-28 15:33:21,780][__main__][INFO] - Running LM probe experiment for task 'question_type' (type: classification) on languages: ['en']
[2025-04-28 15:33:21,780][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-28 15:33:22,936][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-28 15:33:25,267][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-28 15:33:25,268][src.data.datasets][INFO] - Loading 'control_question_type_seed1' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-28 15:33:25,313][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-04-28 15:33:25,341][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_question_type_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_question_type_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Tue Apr  8 18:47:32 2025).
[2025-04-28 15:33:25,624][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-28 15:33:25,633][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-28 15:33:25,633][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-28 15:33:25,634][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-28 15:33:25,654][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:33:25,686][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:33:25,703][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-28 15:33:25,704][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-28 15:33:25,704][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-28 15:33:25,705][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-28 15:33:25,724][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:33:25,759][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:33:25,772][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-28 15:33:25,773][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-28 15:33:25,773][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-28 15:33:25,775][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-28 15:33:25,776][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-28 15:33:25,777][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-28 15:33:25,777][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-28 15:33:25,777][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-28 15:33:25,777][src.data.datasets][INFO] -   Label 0: 596 examples (50.0%)
[2025-04-28 15:33:25,777][src.data.datasets][INFO] -   Label 1: 596 examples (50.0%)
[2025-04-28 15:33:25,777][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-28 15:33:25,777][src.data.datasets][INFO] - Sample label: 1
[2025-04-28 15:33:25,777][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-28 15:33:25,777][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-28 15:33:25,777][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-28 15:33:25,778][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-28 15:33:25,778][src.data.datasets][INFO] -   Label 0: 36 examples (50.0%)
[2025-04-28 15:33:25,778][src.data.datasets][INFO] -   Label 1: 36 examples (50.0%)
[2025-04-28 15:33:25,778][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-28 15:33:25,778][src.data.datasets][INFO] - Sample label: 0
[2025-04-28 15:33:25,778][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-04-28 15:33:25,778][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-04-28 15:33:25,778][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-04-28 15:33:25,778][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-04-28 15:33:25,778][src.data.datasets][INFO] -   Label 0: 55 examples (50.0%)
[2025-04-28 15:33:25,779][src.data.datasets][INFO] -   Label 1: 55 examples (50.0%)
[2025-04-28 15:33:25,779][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-28 15:33:25,779][src.data.datasets][INFO] - Sample label: 0
[2025-04-28 15:33:25,779][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-28 15:33:25,779][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-28 15:33:25,779][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-28 15:33:25,779][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-28 15:33:29,457][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-28 15:33:29,457][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-28 15:33:29,459][src.models.model_factory][INFO] - Created classification head with 1 outputs
[2025-04-28 15:33:29,459][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-28 15:33:29,460][__main__][INFO] - Successfully created model for en
[2025-04-28 15:33:29,460][__main__][INFO] - finetuning with gradient accum steps: 1
Epoch 1/1:   0%|          | 0/298 [00:00<?, ?it/s]Epoch 1/1:   0%|          | 1/298 [00:01<05:12,  1.05s/it]Epoch 1/1:   1%|▏         | 4/298 [00:01<01:10,  4.19it/s]Epoch 1/1:   2%|▏         | 7/298 [00:01<00:39,  7.36it/s]Epoch 1/1:   3%|▎         | 10/298 [00:01<00:28, 10.26it/s]Epoch 1/1:   4%|▍         | 13/298 [00:01<00:22, 12.76it/s]Epoch 1/1:   5%|▌         | 16/298 [00:01<00:19, 14.83it/s]Epoch 1/1:   6%|▋         | 19/298 [00:01<00:16, 16.48it/s]Epoch 1/1:   7%|▋         | 22/298 [00:02<00:15, 17.68it/s]Epoch 1/1:   8%|▊         | 25/298 [00:02<00:14, 18.54it/s]Epoch 1/1:   9%|▉         | 28/298 [00:02<00:14, 19.27it/s]Epoch 1/1:  10%|█         | 31/298 [00:02<00:13, 19.79it/s]Epoch 1/1:  11%|█▏        | 34/298 [00:02<00:13, 20.19it/s]Epoch 1/1:  12%|█▏        | 37/298 [00:02<00:12, 20.44it/s]Epoch 1/1:  13%|█▎        | 40/298 [00:02<00:12, 20.62it/s]Epoch 1/1:  14%|█▍        | 43/298 [00:03<00:12, 20.73it/s]Epoch 1/1:  15%|█▌        | 46/298 [00:03<00:12, 20.78it/s]Epoch 1/1:  16%|█▋        | 49/298 [00:03<00:11, 20.87it/s]Epoch 1/1:  17%|█▋        | 52/298 [00:03<00:11, 20.92it/s]Epoch 1/1:  18%|█▊        | 55/298 [00:03<00:11, 20.97it/s]Epoch 1/1:  19%|█▉        | 58/298 [00:03<00:11, 20.96it/s]Epoch 1/1:  20%|██        | 61/298 [00:03<00:11, 21.00it/s]Epoch 1/1:  21%|██▏       | 64/298 [00:04<00:11, 21.02it/s]Epoch 1/1:  22%|██▏       | 67/298 [00:04<00:10, 21.01it/s]Epoch 1/1:  23%|██▎       | 70/298 [00:04<00:10, 20.98it/s]Epoch 1/1:  24%|██▍       | 73/298 [00:04<00:10, 20.93it/s]Epoch 1/1:  26%|██▌       | 76/298 [00:04<00:10, 21.00it/s]Epoch 1/1:  27%|██▋       | 79/298 [00:04<00:10, 21.01it/s]Epoch 1/1:  28%|██▊       | 82/298 [00:04<00:10, 21.05it/s]Epoch 1/1:  29%|██▊       | 85/298 [00:05<00:10, 21.08it/s]Epoch 1/1:  30%|██▉       | 88/298 [00:05<00:09, 21.02it/s]Epoch 1/1:  31%|███       | 91/298 [00:05<00:09, 21.04it/s]Epoch 1/1:  32%|███▏      | 94/298 [00:05<00:09, 21.08it/s]Epoch 1/1:  33%|███▎      | 97/298 [00:05<00:09, 21.10it/s]Epoch 1/1:  34%|███▎      | 100/298 [00:05<00:09, 21.11it/s]Epoch 1/1:  35%|███▍      | 103/298 [00:05<00:09, 21.06it/s]Epoch 1/1:  36%|███▌      | 106/298 [00:06<00:09, 21.08it/s]Epoch 1/1:  37%|███▋      | 109/298 [00:06<00:08, 21.07it/s]Epoch 1/1:  38%|███▊      | 112/298 [00:06<00:08, 21.09it/s]Epoch 1/1:  39%|███▊      | 115/298 [00:06<00:08, 21.04it/s]Epoch 1/1:  40%|███▉      | 118/298 [00:06<00:08, 21.07it/s]Epoch 1/1:  41%|████      | 121/298 [00:06<00:08, 21.02it/s]Epoch 1/1:  42%|████▏     | 124/298 [00:06<00:08, 21.05it/s]Epoch 1/1:  43%|████▎     | 127/298 [00:07<00:08, 21.06it/s]Epoch 1/1:  44%|████▎     | 130/298 [00:07<00:07, 21.09it/s]Epoch 1/1:  45%|████▍     | 133/298 [00:07<00:07, 21.11it/s]Epoch 1/1:  46%|████▌     | 136/298 [00:07<00:07, 21.13it/s]Epoch 1/1:  47%|████▋     | 139/298 [00:07<00:07, 21.12it/s]Epoch 1/1:  48%|████▊     | 142/298 [00:07<00:07, 21.08it/s]Epoch 1/1:  49%|████▊     | 145/298 [00:07<00:07, 21.09it/s]Epoch 1/1:  50%|████▉     | 148/298 [00:08<00:07, 21.02it/s]Epoch 1/1:  51%|█████     | 151/298 [00:08<00:06, 21.03it/s]Epoch 1/1:  52%|█████▏    | 154/298 [00:08<00:06, 21.06it/s]Epoch 1/1:  53%|█████▎    | 157/298 [00:08<00:06, 21.07it/s]Epoch 1/1:  54%|█████▎    | 160/298 [00:08<00:06, 21.08it/s]Epoch 1/1:  55%|█████▍    | 163/298 [00:08<00:06, 21.10it/s]Epoch 1/1:  56%|█████▌    | 166/298 [00:08<00:06, 21.06it/s]Epoch 1/1:  57%|█████▋    | 169/298 [00:09<00:06, 21.07it/s]Epoch 1/1:  58%|█████▊    | 172/298 [00:09<00:05, 21.09it/s]Epoch 1/1:  59%|█████▊    | 175/298 [00:09<00:05, 21.08it/s]Epoch 1/1:  60%|█████▉    | 178/298 [00:09<00:05, 21.09it/s]Epoch 1/1:  61%|██████    | 181/298 [00:09<00:05, 21.08it/s]Epoch 1/1:  62%|██████▏   | 184/298 [00:09<00:05, 21.03it/s]Epoch 1/1:  63%|██████▎   | 187/298 [00:09<00:05, 21.05it/s]Epoch 1/1:  64%|██████▍   | 190/298 [00:10<00:05, 21.08it/s]Epoch 1/1:  65%|██████▍   | 193/298 [00:10<00:04, 21.10it/s]Epoch 1/1:  66%|██████▌   | 196/298 [00:10<00:04, 21.09it/s]Epoch 1/1:  67%|██████▋   | 199/298 [00:10<00:04, 21.08it/s]Epoch 1/1:  68%|██████▊   | 202/298 [00:10<00:04, 21.09it/s]Epoch 1/1:  69%|██████▉   | 205/298 [00:10<00:04, 21.09it/s]Epoch 1/1:  70%|██████▉   | 208/298 [00:10<00:04, 21.07it/s]Epoch 1/1:  71%|███████   | 211/298 [00:11<00:04, 21.06it/s]Epoch 1/1:  72%|███████▏  | 214/298 [00:11<00:03, 21.08it/s]Epoch 1/1:  73%|███████▎  | 217/298 [00:11<00:03, 21.09it/s]Epoch 1/1:  74%|███████▍  | 220/298 [00:11<00:03, 21.07it/s]Epoch 1/1:  75%|███████▍  | 223/298 [00:11<00:03, 21.08it/s]Epoch 1/1:  76%|███████▌  | 226/298 [00:11<00:03, 21.04it/s]Epoch 1/1:  77%|███████▋  | 229/298 [00:11<00:03, 21.07it/s]Epoch 1/1:  78%|███████▊  | 232/298 [00:12<00:03, 21.01it/s]Epoch 1/1:  79%|███████▉  | 235/298 [00:12<00:02, 21.01it/s]Epoch 1/1:  80%|███████▉  | 238/298 [00:12<00:02, 21.05it/s]Epoch 1/1:  81%|████████  | 241/298 [00:12<00:02, 20.97it/s]Epoch 1/1:  82%|████████▏ | 244/298 [00:12<00:02, 20.99it/s]Epoch 1/1:  83%|████████▎ | 247/298 [00:12<00:02, 21.00it/s]Epoch 1/1:  84%|████████▍ | 250/298 [00:12<00:02, 20.92it/s]Epoch 1/1:  85%|████████▍ | 253/298 [00:13<00:02, 20.99it/s]Epoch 1/1:  86%|████████▌ | 256/298 [00:13<00:02, 20.97it/s]Epoch 1/1:  87%|████████▋ | 259/298 [00:13<00:01, 21.01it/s]Epoch 1/1:  88%|████████▊ | 262/298 [00:13<00:01, 21.06it/s]Epoch 1/1:  89%|████████▉ | 265/298 [00:13<00:01, 21.08it/s]Epoch 1/1:  90%|████████▉ | 268/298 [00:13<00:01, 21.11it/s]Epoch 1/1:  91%|█████████ | 271/298 [00:13<00:01, 21.13it/s]Epoch 1/1:  92%|█████████▏| 274/298 [00:14<00:01, 21.13it/s]Epoch 1/1:  93%|█████████▎| 277/298 [00:14<00:00, 21.13it/s]Epoch 1/1:  94%|█████████▍| 280/298 [00:14<00:00, 21.12it/s]Epoch 1/1:  95%|█████████▍| 283/298 [00:14<00:00, 21.12it/s]Epoch 1/1:  96%|█████████▌| 286/298 [00:14<00:00, 21.14it/s]Epoch 1/1:  97%|█████████▋| 289/298 [00:14<00:00, 21.13it/s]Epoch 1/1:  98%|█████████▊| 292/298 [00:14<00:00, 21.11it/s]Epoch 1/1:  99%|█████████▉| 295/298 [00:15<00:00, 21.16it/s]Epoch 1/1: 100%|██████████| 298/298 [00:15<00:00, 21.19it/s]Epoch 1/1: 100%|██████████| 298/298 [00:15<00:00, 19.61it/s]
[2025-04-28 15:33:47,276][src.training.lm_trainer][INFO] - Epoch 1/1, Train Loss: 0.6954
[2025-04-28 15:33:47,552][src.training.lm_trainer][INFO] - Epoch 1/1, Val Loss: 0.6932, Metrics: {'accuracy': 0.5, 'f1': 0.0}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_accuracy ▁
wandb:          best_val_f1 ▁
wandb:        best_val_loss ▁
wandb:                epoch ▁▁
wandb:  final_test_accuracy ▁
wandb:        final_test_f1 ▁
wandb: final_train_accuracy ▁
wandb:       final_train_f1 ▁
wandb:   final_val_accuracy ▁
wandb:         final_val_f1 ▁
wandb:        learning_rate ▁
wandb:           train_loss ▁
wandb:           train_time ▁
wandb:         val_accuracy ▁
wandb:               val_f1 ▁
wandb:             val_loss ▁
wandb: 
wandb: Run summary:
wandb:    best_val_accuracy 0.5
wandb:          best_val_f1 0
wandb:        best_val_loss 0.69324
wandb:                epoch 1
wandb:  final_test_accuracy 0.5
wandb:        final_test_f1 0
wandb: final_train_accuracy 0.5
wandb:       final_train_f1 0
wandb:   final_val_accuracy 0.5
wandb:         final_val_f1 0
wandb:        learning_rate 2e-05
wandb:           train_loss 0.69545
wandb:           train_time 15.87584
wandb:         val_accuracy 0.5
wandb:               val_f1 0
wandb:             val_loss 0.69324
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250428_153321-fea4524j
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250428_153321-fea4524j/logs
Control fine-tuning test completed successfully!
Testing complexity fine-tuning on English...
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-28 15:34:02,433][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_test/complexity/en
experiment_name: test_finetune_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 4
  num_epochs: 1
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_finetune
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  finetune: true
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1
    lr: 2.0e-05
    batch_size: 8

[2025-04-28 15:34:02,433][__main__][INFO] - Normalized task: complexity
[2025-04-28 15:34:02,434][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-28 15:34:02,434][__main__][INFO] - Determined Task Type: regression
[2025-04-28 15:34:02,438][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-28 15:34:02,439][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-28 15:34:03,728][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-28 15:34:06,069][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-28 15:34:06,070][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-28 15:34:06,119][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:34:06,141][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:34:06,205][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-28 15:34:06,213][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-28 15:34:06,213][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-28 15:34:06,214][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-28 15:34:06,230][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:34:06,255][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:34:06,265][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-28 15:34:06,269][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-28 15:34:06,270][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-28 15:34:06,271][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-28 15:34:06,285][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:34:06,312][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:34:06,323][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-28 15:34:06,325][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-28 15:34:06,325][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-28 15:34:06,326][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-28 15:34:06,327][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-28 15:34:06,327][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-28 15:34:06,327][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-28 15:34:06,327][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-28 15:34:06,327][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-28 15:34:06,328][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-28 15:34:06,328][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-28 15:34:06,328][src.data.datasets][INFO] - Sample label: 0.5150214433670044
[2025-04-28 15:34:06,328][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-28 15:34:06,328][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-28 15:34:06,328][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-28 15:34:06,328][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-28 15:34:06,328][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-28 15:34:06,328][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-28 15:34:06,329][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-28 15:34:06,329][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-28 15:34:06,329][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-28 15:34:06,329][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-28 15:34:06,329][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-28 15:34:06,329][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-28 15:34:06,329][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-28 15:34:06,329][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-28 15:34:06,329][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-28 15:34:06,329][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-28 15:34:06,329][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-28 15:34:06,330][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-28 15:34:06,330][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-28 15:34:06,330][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-28 15:34:09,834][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-28 15:34:09,834][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-28 15:34:09,836][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-28 15:34:09,836][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-28 15:34:09,836][__main__][INFO] - Successfully created model for en
[2025-04-28 15:34:09,837][__main__][INFO] - finetuning with gradient accum steps: 1
Epoch 1/1:   0%|          | 0/298 [00:00<?, ?it/s]Epoch 1/1:   0%|          | 1/298 [00:00<04:48,  1.03it/s]Epoch 1/1:   1%|▏         | 4/298 [00:01<01:05,  4.48it/s]Epoch 1/1:   2%|▏         | 7/298 [00:01<00:37,  7.77it/s]Epoch 1/1:   3%|▎         | 10/298 [00:01<00:26, 10.70it/s]Epoch 1/1:   4%|▍         | 13/298 [00:01<00:21, 13.19it/s]Epoch 1/1:   5%|▌         | 16/298 [00:01<00:18, 15.17it/s]Epoch 1/1:   6%|▋         | 19/298 [00:01<00:16, 16.75it/s]Epoch 1/1:   7%|▋         | 22/298 [00:01<00:15, 17.94it/s]Epoch 1/1:   8%|▊         | 25/298 [00:02<00:14, 18.83it/s]Epoch 1/1:   9%|▉         | 28/298 [00:02<00:13, 19.49it/s]Epoch 1/1:  10%|█         | 31/298 [00:02<00:13, 19.90it/s]Epoch 1/1:  11%|█▏        | 34/298 [00:02<00:13, 20.18it/s]Epoch 1/1:  12%|█▏        | 37/298 [00:02<00:12, 20.33it/s]Epoch 1/1:  13%|█▎        | 40/298 [00:02<00:12, 20.48it/s]Epoch 1/1:  14%|█▍        | 43/298 [00:02<00:12, 20.54it/s]Epoch 1/1:  15%|█▌        | 46/298 [00:03<00:12, 20.65it/s]Epoch 1/1:  16%|█▋        | 49/298 [00:03<00:12, 20.70it/s]Epoch 1/1:  17%|█▋        | 52/298 [00:03<00:11, 20.83it/s]Epoch 1/1:  18%|█▊        | 55/298 [00:03<00:11, 20.93it/s]Epoch 1/1:  19%|█▉        | 58/298 [00:03<00:11, 20.98it/s]Epoch 1/1:  20%|██        | 61/298 [00:03<00:11, 21.03it/s]Epoch 1/1:  21%|██▏       | 64/298 [00:03<00:11, 21.06it/s]Epoch 1/1:  22%|██▏       | 67/298 [00:04<00:10, 21.08it/s]Epoch 1/1:  23%|██▎       | 70/298 [00:04<00:10, 21.02it/s]Epoch 1/1:  24%|██▍       | 73/298 [00:04<00:10, 21.05it/s]Epoch 1/1:  26%|██▌       | 76/298 [00:04<00:10, 21.06it/s]Epoch 1/1:  27%|██▋       | 79/298 [00:04<00:10, 21.07it/s]Epoch 1/1:  28%|██▊       | 82/298 [00:04<00:10, 21.08it/s]Epoch 1/1:  29%|██▊       | 85/298 [00:04<00:10, 21.09it/s]Epoch 1/1:  30%|██▉       | 88/298 [00:05<00:09, 21.10it/s]Epoch 1/1:  31%|███       | 91/298 [00:05<00:09, 21.10it/s]Epoch 1/1:  32%|███▏      | 94/298 [00:05<00:09, 21.08it/s]Epoch 1/1:  33%|███▎      | 97/298 [00:05<00:09, 21.10it/s]Epoch 1/1:  34%|███▎      | 100/298 [00:05<00:09, 21.10it/s]Epoch 1/1:  35%|███▍      | 103/298 [00:05<00:09, 21.07it/s]Epoch 1/1:  36%|███▌      | 106/298 [00:05<00:09, 21.05it/s]Epoch 1/1:  37%|███▋      | 109/298 [00:06<00:08, 21.06it/s]Epoch 1/1:  38%|███▊      | 112/298 [00:06<00:08, 21.04it/s]Epoch 1/1:  39%|███▊      | 115/298 [00:06<00:08, 21.04it/s]Epoch 1/1:  40%|███▉      | 118/298 [00:06<00:08, 20.99it/s]Epoch 1/1:  41%|████      | 121/298 [00:06<00:08, 21.00it/s]Epoch 1/1:  42%|████▏     | 124/298 [00:06<00:08, 21.02it/s]Epoch 1/1:  43%|████▎     | 127/298 [00:06<00:08, 21.05it/s]Epoch 1/1:  44%|████▎     | 130/298 [00:07<00:07, 21.08it/s]Epoch 1/1:  45%|████▍     | 133/298 [00:07<00:07, 21.09it/s]Epoch 1/1:  46%|████▌     | 136/298 [00:07<00:07, 21.11it/s]Epoch 1/1:  47%|████▋     | 139/298 [00:07<00:07, 21.12it/s]Epoch 1/1:  48%|████▊     | 142/298 [00:07<00:07, 21.12it/s]Epoch 1/1:  49%|████▊     | 145/298 [00:07<00:07, 21.12it/s]Epoch 1/1:  50%|████▉     | 148/298 [00:07<00:07, 21.12it/s]Epoch 1/1:  51%|█████     | 151/298 [00:08<00:06, 21.13it/s]Epoch 1/1:  52%|█████▏    | 154/298 [00:08<00:06, 21.13it/s]Epoch 1/1:  53%|█████▎    | 157/298 [00:08<00:06, 21.12it/s]Epoch 1/1:  54%|█████▎    | 160/298 [00:08<00:06, 21.11it/s]Epoch 1/1:  55%|█████▍    | 163/298 [00:08<00:06, 21.14it/s]Epoch 1/1:  56%|█████▌    | 166/298 [00:08<00:06, 21.05it/s]Epoch 1/1:  57%|█████▋    | 169/298 [00:08<00:06, 21.03it/s]Epoch 1/1:  58%|█████▊    | 172/298 [00:09<00:05, 21.03it/s]Epoch 1/1:  59%|█████▊    | 175/298 [00:09<00:05, 21.05it/s]Epoch 1/1:  60%|█████▉    | 178/298 [00:09<00:05, 21.09it/s]Epoch 1/1:  61%|██████    | 181/298 [00:09<00:05, 21.09it/s]Epoch 1/1:  62%|██████▏   | 184/298 [00:09<00:05, 21.11it/s]Epoch 1/1:  63%|██████▎   | 187/298 [00:09<00:05, 21.11it/s]Epoch 1/1:  64%|██████▍   | 190/298 [00:09<00:05, 21.12it/s]Epoch 1/1:  65%|██████▍   | 193/298 [00:10<00:04, 21.12it/s]Epoch 1/1:  66%|██████▌   | 196/298 [00:10<00:04, 21.13it/s]Epoch 1/1:  67%|██████▋   | 199/298 [00:10<00:04, 21.12it/s]Epoch 1/1:  68%|██████▊   | 202/298 [00:10<00:04, 21.11it/s]Epoch 1/1:  69%|██████▉   | 205/298 [00:10<00:04, 20.96it/s]Epoch 1/1:  70%|██████▉   | 208/298 [00:10<00:04, 20.98it/s]Epoch 1/1:  71%|███████   | 211/298 [00:10<00:04, 21.00it/s]Epoch 1/1:  72%|███████▏  | 214/298 [00:11<00:04, 20.97it/s]Epoch 1/1:  73%|███████▎  | 217/298 [00:11<00:03, 20.90it/s]Epoch 1/1:  74%|███████▍  | 220/298 [00:11<00:03, 20.97it/s]Epoch 1/1:  75%|███████▍  | 223/298 [00:11<00:03, 20.87it/s]Epoch 1/1:  76%|███████▌  | 226/298 [00:11<00:03, 20.94it/s]Epoch 1/1:  77%|███████▋  | 229/298 [00:11<00:03, 20.99it/s]Epoch 1/1:  78%|███████▊  | 232/298 [00:11<00:03, 21.03it/s]Epoch 1/1:  79%|███████▉  | 235/298 [00:12<00:02, 21.01it/s]Epoch 1/1:  80%|███████▉  | 238/298 [00:12<00:02, 21.06it/s]Epoch 1/1:  81%|████████  | 241/298 [00:12<00:02, 21.09it/s]Epoch 1/1:  82%|████████▏ | 244/298 [00:12<00:02, 21.10it/s]Epoch 1/1:  83%|████████▎ | 247/298 [00:12<00:02, 21.11it/s]Epoch 1/1:  84%|████████▍ | 250/298 [00:12<00:02, 21.14it/s]Epoch 1/1:  85%|████████▍ | 253/298 [00:12<00:02, 21.13it/s]Epoch 1/1:  86%|████████▌ | 256/298 [00:13<00:01, 21.14it/s]Epoch 1/1:  87%|████████▋ | 259/298 [00:13<00:01, 21.06it/s]Epoch 1/1:  88%|████████▊ | 262/298 [00:13<00:01, 21.09it/s]Epoch 1/1:  89%|████████▉ | 265/298 [00:13<00:01, 21.10it/s]Epoch 1/1:  90%|████████▉ | 268/298 [00:13<00:01, 21.08it/s]Epoch 1/1:  91%|█████████ | 271/298 [00:13<00:01, 21.10it/s]Epoch 1/1:  92%|█████████▏| 274/298 [00:13<00:01, 21.11it/s]Epoch 1/1:  93%|█████████▎| 277/298 [00:14<00:00, 21.09it/s]Epoch 1/1:  94%|█████████▍| 280/298 [00:14<00:00, 21.10it/s]Epoch 1/1:  95%|█████████▍| 283/298 [00:14<00:00, 21.10it/s]Epoch 1/1:  96%|█████████▌| 286/298 [00:14<00:00, 21.12it/s]Epoch 1/1:  97%|█████████▋| 289/298 [00:14<00:00, 21.11it/s]Epoch 1/1:  98%|█████████▊| 292/298 [00:14<00:00, 21.13it/s]Epoch 1/1:  99%|█████████▉| 295/298 [00:14<00:00, 21.16it/s]Epoch 1/1: 100%|██████████| 298/298 [00:15<00:00, 21.19it/s]Epoch 1/1: 100%|██████████| 298/298 [00:15<00:00, 19.72it/s]
[2025-04-28 15:34:26,408][src.training.lm_trainer][INFO] - Epoch 1/1, Train Loss: 0.0548
[2025-04-28 15:34:26,689][src.training.lm_trainer][INFO] - Epoch 1/1, Val Loss: 0.0456, Metrics: {'mse': 0.04560742527246475, 'rmse': 0.2135589503450154, 'r2': -0.08975160121917725}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb:            epoch ▁▁
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁
wandb:       train_loss ▁
wandb:       train_time ▁
wandb:         val_loss ▁
wandb:          val_mse ▁
wandb:           val_r2 ▁
wandb:         val_rmse ▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04561
wandb:     best_val_mse 0.04561
wandb:      best_val_r2 -0.08975
wandb:    best_val_rmse 0.21356
wandb:            epoch 1
wandb:   final_test_mse 0.02736
wandb:    final_test_r2 0.29018
wandb:  final_test_rmse 0.16539
wandb:  final_train_mse 0.02139
wandb:   final_train_r2 0.20284
wandb: final_train_rmse 0.14624
wandb:    final_val_mse 0.04561
wandb:     final_val_r2 -0.08975
wandb:   final_val_rmse 0.21356
wandb:    learning_rate 2e-05
wandb:       train_loss 0.0548
wandb:       train_time 15.81374
wandb:         val_loss 0.04561
wandb:          val_mse 0.04561
wandb:           val_r2 -0.08975
wandb:         val_rmse 0.21356
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250428_153402-bken9x38
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250428_153402-bken9x38/logs
Complexity fine-tuning test completed successfully!
Testing control complexity fine-tuning on English...
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-28 15:34:57,377][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_test/control1/en
experiment_name: test_finetune_complexity_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 4
  num_epochs: 1
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_finetune
  tasks: complexity
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  finetune: true
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1
    lr: 2.0e-05
    batch_size: 8

[2025-04-28 15:34:57,377][__main__][INFO] - Normalized task: complexity
[2025-04-28 15:34:57,377][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-28 15:34:57,377][__main__][INFO] - Determined Task Type: regression
[2025-04-28 15:34:57,381][__main__][INFO] - Running LM probe experiment for task 'complexity' (type: regression) on languages: ['en']
[2025-04-28 15:34:57,381][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-28 15:34:58,529][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-28 15:35:00,714][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-28 15:35:00,715][src.data.datasets][INFO] - Loading 'control_complexity_seed1' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-28 15:35:00,732][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-04-28 15:35:00,750][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'control_complexity_seed1' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/control_complexity_seed1/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Wed Apr  9 01:56:26 2025).
[2025-04-28 15:35:01,474][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-28 15:35:01,482][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-28 15:35:01,482][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-28 15:35:01,483][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-28 15:35:01,496][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:35:01,516][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:35:01,524][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-28 15:35:01,525][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-28 15:35:01,525][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-28 15:35:01,526][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-28 15:35:01,536][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:35:01,555][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:35:01,563][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-28 15:35:01,565][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-28 15:35:01,565][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-28 15:35:01,565][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-28 15:35:01,566][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-28 15:35:01,566][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-28 15:35:01,566][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-28 15:35:01,566][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-28 15:35:01,566][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-28 15:35:01,566][src.data.datasets][INFO] -   Mean: 0.3875, Std: 0.1638
[2025-04-28 15:35:01,566][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-28 15:35:01,566][src.data.datasets][INFO] - Sample label: 0.4486629366874695
[2025-04-28 15:35:01,567][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-28 15:35:01,567][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-28 15:35:01,567][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-28 15:35:01,567][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-28 15:35:01,567][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-28 15:35:01,567][src.data.datasets][INFO] -   Mean: 0.3150, Std: 0.2046
[2025-04-28 15:35:01,567][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-28 15:35:01,567][src.data.datasets][INFO] - Sample label: 0.8405253291130066
[2025-04-28 15:35:01,567][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-04-28 15:35:01,567][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-04-28 15:35:01,567][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-04-28 15:35:01,568][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-04-28 15:35:01,568][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-28 15:35:01,568][src.data.datasets][INFO] -   Mean: 0.3955, Std: 0.1963
[2025-04-28 15:35:01,568][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-28 15:35:01,568][src.data.datasets][INFO] - Sample label: 0.03787878900766373
[2025-04-28 15:35:01,568][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-28 15:35:01,568][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-28 15:35:01,568][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-28 15:35:01,569][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-28 15:35:04,899][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-28 15:35:04,899][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-28 15:35:04,901][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-28 15:35:04,901][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-28 15:35:04,901][__main__][INFO] - Successfully created model for en
[2025-04-28 15:35:04,902][__main__][INFO] - finetuning with gradient accum steps: 1
Epoch 1/1:   0%|          | 0/298 [00:00<?, ?it/s]Epoch 1/1:   0%|          | 1/298 [00:01<04:58,  1.01s/it]Epoch 1/1:   1%|▏         | 4/298 [00:01<01:07,  4.37it/s]Epoch 1/1:   2%|▏         | 7/298 [00:01<00:38,  7.60it/s]Epoch 1/1:   3%|▎         | 10/298 [00:01<00:27, 10.51it/s]Epoch 1/1:   4%|▍         | 13/298 [00:01<00:21, 13.00it/s]Epoch 1/1:   5%|▌         | 16/298 [00:01<00:18, 14.98it/s]Epoch 1/1:   6%|▋         | 19/298 [00:01<00:16, 16.55it/s]Epoch 1/1:   7%|▋         | 22/298 [00:02<00:15, 17.76it/s]Epoch 1/1:   8%|▊         | 25/298 [00:02<00:14, 18.70it/s]Epoch 1/1:   9%|▉         | 28/298 [00:02<00:13, 19.38it/s]Epoch 1/1:  10%|█         | 31/298 [00:02<00:13, 19.89it/s]Epoch 1/1:  11%|█▏        | 34/298 [00:02<00:13, 20.07it/s]Epoch 1/1:  12%|█▏        | 37/298 [00:02<00:12, 20.26it/s]Epoch 1/1:  13%|█▎        | 40/298 [00:02<00:12, 20.40it/s]Epoch 1/1:  14%|█▍        | 43/298 [00:03<00:12, 20.59it/s]Epoch 1/1:  15%|█▌        | 46/298 [00:03<00:12, 20.73it/s]Epoch 1/1:  16%|█▋        | 49/298 [00:03<00:11, 20.76it/s]Epoch 1/1:  17%|█▋        | 52/298 [00:03<00:11, 20.84it/s]Epoch 1/1:  18%|█▊        | 55/298 [00:03<00:11, 20.91it/s]Epoch 1/1:  19%|█▉        | 58/298 [00:03<00:11, 20.95it/s]Epoch 1/1:  20%|██        | 61/298 [00:03<00:11, 20.86it/s]Epoch 1/1:  21%|██▏       | 64/298 [00:04<00:11, 20.91it/s]Epoch 1/1:  22%|██▏       | 67/298 [00:04<00:11, 20.95it/s]Epoch 1/1:  23%|██▎       | 70/298 [00:04<00:10, 20.97it/s]Epoch 1/1:  24%|██▍       | 73/298 [00:04<00:10, 20.98it/s]Epoch 1/1:  26%|██▌       | 76/298 [00:04<00:10, 21.02it/s]Epoch 1/1:  27%|██▋       | 79/298 [00:04<00:10, 21.02it/s]Epoch 1/1:  28%|██▊       | 82/298 [00:04<00:10, 20.97it/s]Epoch 1/1:  29%|██▊       | 85/298 [00:05<00:10, 20.98it/s]Epoch 1/1:  30%|██▉       | 88/298 [00:05<00:10, 21.00it/s]Epoch 1/1:  31%|███       | 91/298 [00:05<00:09, 20.96it/s]Epoch 1/1:  32%|███▏      | 94/298 [00:05<00:09, 20.99it/s]Epoch 1/1:  33%|███▎      | 97/298 [00:05<00:09, 20.95it/s]Epoch 1/1:  34%|███▎      | 100/298 [00:05<00:09, 20.99it/s]Epoch 1/1:  35%|███▍      | 103/298 [00:05<00:09, 21.02it/s]Epoch 1/1:  36%|███▌      | 106/298 [00:06<00:09, 21.05it/s]Epoch 1/1:  37%|███▋      | 109/298 [00:06<00:08, 21.05it/s]Epoch 1/1:  38%|███▊      | 112/298 [00:06<00:08, 21.05it/s]Epoch 1/1:  39%|███▊      | 115/298 [00:06<00:08, 21.05it/s]Epoch 1/1:  40%|███▉      | 118/298 [00:06<00:08, 20.98it/s]Epoch 1/1:  41%|████      | 121/298 [00:06<00:08, 20.95it/s]Epoch 1/1:  42%|████▏     | 124/298 [00:06<00:08, 20.98it/s]Epoch 1/1:  43%|████▎     | 127/298 [00:07<00:08, 20.95it/s]Epoch 1/1:  44%|████▎     | 130/298 [00:07<00:08, 20.98it/s]Epoch 1/1:  45%|████▍     | 133/298 [00:07<00:07, 21.00it/s]Epoch 1/1:  46%|████▌     | 136/298 [00:07<00:07, 20.95it/s]Epoch 1/1:  47%|████▋     | 139/298 [00:07<00:07, 20.99it/s]Epoch 1/1:  48%|████▊     | 142/298 [00:07<00:07, 21.01it/s]Epoch 1/1:  49%|████▊     | 145/298 [00:07<00:07, 21.02it/s]Epoch 1/1:  50%|████▉     | 148/298 [00:08<00:07, 21.01it/s]Epoch 1/1:  51%|█████     | 151/298 [00:08<00:06, 21.01it/s]Epoch 1/1:  52%|█████▏    | 154/298 [00:08<00:06, 21.02it/s]Epoch 1/1:  53%|█████▎    | 157/298 [00:08<00:06, 21.04it/s]Epoch 1/1:  54%|█████▎    | 160/298 [00:08<00:06, 21.04it/s]Epoch 1/1:  55%|█████▍    | 163/298 [00:08<00:06, 21.01it/s]Epoch 1/1:  56%|█████▌    | 166/298 [00:08<00:06, 21.02it/s]Epoch 1/1:  57%|█████▋    | 169/298 [00:09<00:06, 21.04it/s]Epoch 1/1:  58%|█████▊    | 172/298 [00:09<00:05, 21.04it/s]Epoch 1/1:  59%|█████▊    | 175/298 [00:09<00:05, 20.99it/s]Epoch 1/1:  60%|█████▉    | 178/298 [00:09<00:05, 21.00it/s]Epoch 1/1:  61%|██████    | 181/298 [00:09<00:05, 21.03it/s]Epoch 1/1:  62%|██████▏   | 184/298 [00:09<00:05, 21.04it/s]Epoch 1/1:  63%|██████▎   | 187/298 [00:09<00:05, 21.04it/s]Epoch 1/1:  64%|██████▍   | 190/298 [00:10<00:05, 21.04it/s]Epoch 1/1:  65%|██████▍   | 193/298 [00:10<00:04, 21.03it/s]Epoch 1/1:  66%|██████▌   | 196/298 [00:10<00:04, 21.05it/s]Epoch 1/1:  67%|██████▋   | 199/298 [00:10<00:04, 21.05it/s]Epoch 1/1:  68%|██████▊   | 202/298 [00:10<00:04, 21.00it/s]Epoch 1/1:  69%|██████▉   | 205/298 [00:10<00:04, 21.02it/s]Epoch 1/1:  70%|██████▉   | 208/298 [00:10<00:04, 21.03it/s]Epoch 1/1:  71%|███████   | 211/298 [00:11<00:04, 21.05it/s]Epoch 1/1:  72%|███████▏  | 214/298 [00:11<00:03, 21.05it/s]Epoch 1/1:  73%|███████▎  | 217/298 [00:11<00:03, 21.07it/s]Epoch 1/1:  74%|███████▍  | 220/298 [00:11<00:03, 21.07it/s]Epoch 1/1:  75%|███████▍  | 223/298 [00:11<00:03, 21.07it/s]Epoch 1/1:  76%|███████▌  | 226/298 [00:11<00:03, 21.08it/s]Epoch 1/1:  77%|███████▋  | 229/298 [00:11<00:03, 21.08it/s]Epoch 1/1:  78%|███████▊  | 232/298 [00:12<00:03, 21.05it/s]Epoch 1/1:  79%|███████▉  | 235/298 [00:12<00:02, 21.01it/s]Epoch 1/1:  80%|███████▉  | 238/298 [00:12<00:02, 21.03it/s]Epoch 1/1:  81%|████████  | 241/298 [00:12<00:02, 21.03it/s]Epoch 1/1:  82%|████████▏ | 244/298 [00:12<00:02, 21.03it/s]Epoch 1/1:  83%|████████▎ | 247/298 [00:12<00:02, 21.03it/s]Epoch 1/1:  84%|████████▍ | 250/298 [00:12<00:02, 21.05it/s]Epoch 1/1:  85%|████████▍ | 253/298 [00:13<00:02, 21.05it/s]Epoch 1/1:  86%|████████▌ | 256/298 [00:13<00:01, 21.04it/s]Epoch 1/1:  87%|████████▋ | 259/298 [00:13<00:01, 21.04it/s]Epoch 1/1:  88%|████████▊ | 262/298 [00:13<00:01, 21.03it/s]Epoch 1/1:  89%|████████▉ | 265/298 [00:13<00:01, 20.99it/s]Epoch 1/1:  90%|████████▉ | 268/298 [00:13<00:01, 21.00it/s]Epoch 1/1:  91%|█████████ | 271/298 [00:13<00:01, 21.02it/s]Epoch 1/1:  92%|█████████▏| 274/298 [00:14<00:01, 21.01it/s]Epoch 1/1:  93%|█████████▎| 277/298 [00:14<00:00, 21.01it/s]Epoch 1/1:  94%|█████████▍| 280/298 [00:14<00:00, 21.02it/s]Epoch 1/1:  95%|█████████▍| 283/298 [00:14<00:00, 20.98it/s]Epoch 1/1:  96%|█████████▌| 286/298 [00:14<00:00, 21.01it/s]Epoch 1/1:  97%|█████████▋| 289/298 [00:14<00:00, 21.01it/s]Epoch 1/1:  98%|█████████▊| 292/298 [00:14<00:00, 21.06it/s]Epoch 1/1:  99%|█████████▉| 295/298 [00:15<00:00, 21.11it/s]Epoch 1/1: 100%|██████████| 298/298 [00:15<00:00, 21.14it/s]Epoch 1/1: 100%|██████████| 298/298 [00:15<00:00, 19.64it/s]
[2025-04-28 15:35:21,501][src.training.lm_trainer][INFO] - Epoch 1/1, Train Loss: 0.0683
[2025-04-28 15:35:21,783][src.training.lm_trainer][INFO] - Epoch 1/1, Val Loss: 0.0440, Metrics: {'mse': 0.04396254941821098, 'rmse': 0.20967248130885222, 'r2': -0.05044865608215332}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb:            epoch ▁▁
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁
wandb:       train_loss ▁
wandb:       train_time ▁
wandb:         val_loss ▁
wandb:          val_mse ▁
wandb:           val_r2 ▁
wandb:         val_rmse ▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.04396
wandb:     best_val_mse 0.04396
wandb:      best_val_r2 -0.05045
wandb:    best_val_rmse 0.20967
wandb:            epoch 1
wandb:   final_test_mse 0.03749
wandb:    final_test_r2 0.02729
wandb:  final_test_rmse 0.19362
wandb:  final_train_mse 0.02689
wandb:   final_train_r2 -0.00242
wandb: final_train_rmse 0.16399
wandb:    final_val_mse 0.04396
wandb:     final_val_r2 -0.05045
wandb:   final_val_rmse 0.20967
wandb:    learning_rate 2e-05
wandb:       train_loss 0.0683
wandb:       train_time 15.86376
wandb:         val_loss 0.04396
wandb:          val_mse 0.04396
wandb:           val_r2 -0.05045
wandb:         val_rmse 0.20967
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250428_153457-4pb98p26
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250428_153457-4pb98p26/logs
Complexity control 1 finetuning test completed successfully!
Testing control 1 submetric (avg_links_len) finetuning on English...
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-04-28 15:35:38,091][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/finetune_test/submetrics/en/avg_links_len
experiment_name: test_finetune_avg_links_len_en
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - en
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: false
  finetune: true
  layer_wise: false
  layer_index: -1
  num_outputs: 1
training:
  task_type: regression
  batch_size: 4
  num_epochs: 1
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
experiment:
  type: lm_finetune
  tasks:
  - single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: true
  control_index: 1
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  finetune: true
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1
    lr: 2.0e-05
    batch_size: 8

[2025-04-28 15:35:38,091][__main__][INFO] - Normalized task: ['single_submetric']
[2025-04-28 15:35:38,091][__main__][INFO] - Using explicit task_type from config: regression
[2025-04-28 15:35:38,091][__main__][INFO] - Determined Task Type: regression
[2025-04-28 15:35:38,096][__main__][INFO] - Running LM probe experiment for task '['single_submetric']' (type: regression) on languages: ['en']
[2025-04-28 15:35:38,097][__main__][INFO] - Processing language: en
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-04-28 15:35:39,352][src.data.datasets][INFO] - Creating dataloaders for language: 'en', task: '['single_submetric']', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-04-28 15:35:41,529][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-04-28 15:35:41,530][src.data.datasets][WARNING] - Unknown task '['single_submetric']' for control data. Using base config.
[2025-04-28 15:35:41,530][src.data.datasets][INFO] - Loading 'base' dataset for en language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-28 15:35:41,567][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:35:41,588][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:35:41,638][src.data.datasets][INFO] - Filtered from 7460 to 1192 examples for language 'en'
[2025-04-28 15:35:41,646][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-28 15:35:41,647][src.data.datasets][INFO] - Loaded 1192 examples for en (train)
[2025-04-28 15:35:41,647][src.data.datasets][INFO] - Loading 'base' dataset for en language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-28 15:35:41,664][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:35:41,685][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:35:41,695][src.data.datasets][INFO] - Filtered from 441 to 72 examples for language 'en'
[2025-04-28 15:35:41,696][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-28 15:35:41,696][src.data.datasets][INFO] - Loaded 72 examples for en (validation)
[2025-04-28 15:35:41,697][src.data.datasets][INFO] - Loading 'base' dataset for en language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-04-28 15:35:41,710][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:35:41,730][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-04-28 15:35:41,738][src.data.datasets][INFO] - Filtered from 719 to 110 examples for language 'en'
[2025-04-28 15:35:41,740][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-04-28 15:35:41,740][src.data.datasets][INFO] - Loaded 110 examples for en (test)
[2025-04-28 15:35:41,740][src.data.datasets][INFO] - Loaded datasets: train=1192, val=72, test=110 examples
[2025-04-28 15:35:41,741][src.data.datasets][INFO] - Task '['single_submetric']' is classification: False
[2025-04-28 15:35:41,741][src.data.datasets][INFO] - Getting feature name for task: '['single_submetric']', submetric: 'None'
[2025-04-28 15:35:41,741][src.data.datasets][WARNING] - Unrecognized task: '['single_submetric']'. Using default 'question_type'.
[2025-04-28 15:35:41,741][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: '['single_submetric']'
[2025-04-28 15:35:41,741][src.data.datasets][INFO] - Label statistics for ['single_submetric'] (feature: question_type):
[2025-04-28 15:35:41,741][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-28 15:35:41,741][src.data.datasets][INFO] -   Mean: 0.5000, Std: 0.5000
[2025-04-28 15:35:41,741][src.data.datasets][INFO] - Sample text: Did Nvidia skip the 800 series for graphics cards?...
[2025-04-28 15:35:41,742][src.data.datasets][INFO] - Sample label: 1.0
[2025-04-28 15:35:41,742][src.data.datasets][INFO] - Task '['single_submetric']' is classification: False
[2025-04-28 15:35:41,742][src.data.datasets][INFO] - Getting feature name for task: '['single_submetric']', submetric: 'None'
[2025-04-28 15:35:41,742][src.data.datasets][WARNING] - Unrecognized task: '['single_submetric']'. Using default 'question_type'.
[2025-04-28 15:35:41,742][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: '['single_submetric']'
[2025-04-28 15:35:41,742][src.data.datasets][INFO] - Label statistics for ['single_submetric'] (feature: question_type):
[2025-04-28 15:35:41,742][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-28 15:35:41,742][src.data.datasets][INFO] -   Mean: 0.5000, Std: 0.5000
[2025-04-28 15:35:41,742][src.data.datasets][INFO] - Sample text: We just did a deal for the rest of the month for 1...
[2025-04-28 15:35:41,742][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-28 15:35:41,743][src.data.datasets][INFO] - Task '['single_submetric']' is classification: False
[2025-04-28 15:35:41,743][src.data.datasets][INFO] - Getting feature name for task: '['single_submetric']', submetric: 'None'
[2025-04-28 15:35:41,743][src.data.datasets][WARNING] - Unrecognized task: '['single_submetric']'. Using default 'question_type'.
[2025-04-28 15:35:41,743][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: '['single_submetric']'
[2025-04-28 15:35:41,743][src.data.datasets][INFO] - Label statistics for ['single_submetric'] (feature: question_type):
[2025-04-28 15:35:41,743][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-04-28 15:35:41,743][src.data.datasets][INFO] -   Mean: 0.5000, Std: 0.5000
[2025-04-28 15:35:41,743][src.data.datasets][INFO] - Sample text: What is the problem?...
[2025-04-28 15:35:41,743][src.data.datasets][INFO] - Sample label: 0.0
[2025-04-28 15:35:41,743][src.data.datasets][INFO] - Created datasets: train=1192, val=72, test=110
[2025-04-28 15:35:41,743][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-04-28 15:35:41,744][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-04-28 15:35:41,744][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-04-28 15:35:45,127][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-04-28 15:35:45,128][src.models.model_factory][INFO] - finetuning the entire model
[2025-04-28 15:35:45,129][src.models.model_factory][INFO] - Created regression head with 1 outputs
[2025-04-28 15:35:45,130][src.models.model_factory][INFO] - layer-wise probing: False, layer index: -1
[2025-04-28 15:35:45,130][__main__][INFO] - Successfully created model for en
[2025-04-28 15:35:45,130][__main__][INFO] - finetuning with gradient accum steps: 1
Epoch 1/1:   0%|          | 0/298 [00:00<?, ?it/s]Epoch 1/1:   0%|          | 1/298 [00:01<05:14,  1.06s/it]Epoch 1/1:   1%|▏         | 4/298 [00:01<01:10,  4.18it/s]Epoch 1/1:   2%|▏         | 7/298 [00:01<00:39,  7.34it/s]Epoch 1/1:   3%|▎         | 10/298 [00:01<00:28, 10.23it/s]Epoch 1/1:   4%|▍         | 13/298 [00:01<00:22, 12.72it/s]Epoch 1/1:   5%|▌         | 16/298 [00:01<00:19, 14.76it/s]Epoch 1/1:   6%|▋         | 19/298 [00:01<00:17, 16.38it/s]Epoch 1/1:   7%|▋         | 22/298 [00:02<00:15, 17.66it/s]Epoch 1/1:   8%|▊         | 25/298 [00:02<00:14, 18.62it/s]Epoch 1/1:   9%|▉         | 28/298 [00:02<00:13, 19.34it/s]Epoch 1/1:  10%|█         | 31/298 [00:02<00:13, 19.79it/s]Epoch 1/1:  11%|█▏        | 34/298 [00:02<00:13, 20.16it/s]Epoch 1/1:  12%|█▏        | 37/298 [00:02<00:12, 20.40it/s]Epoch 1/1:  13%|█▎        | 40/298 [00:02<00:12, 20.60it/s]Epoch 1/1:  14%|█▍        | 43/298 [00:03<00:12, 20.73it/s]Epoch 1/1:  15%|█▌        | 46/298 [00:03<00:12, 20.83it/s]Epoch 1/1:  16%|█▋        | 49/298 [00:03<00:11, 20.90it/s]Epoch 1/1:  17%|█▋        | 52/298 [00:03<00:11, 20.96it/s]Epoch 1/1:  18%|█▊        | 55/298 [00:03<00:11, 20.94it/s]Epoch 1/1:  19%|█▉        | 58/298 [00:03<00:11, 20.98it/s]Epoch 1/1:  20%|██        | 61/298 [00:03<00:11, 21.00it/s]Epoch 1/1:  21%|██▏       | 64/298 [00:04<00:11, 21.02it/s]Epoch 1/1:  22%|██▏       | 67/298 [00:04<00:11, 20.97it/s]Epoch 1/1:  23%|██▎       | 70/298 [00:04<00:10, 21.00it/s]Epoch 1/1:  24%|██▍       | 73/298 [00:04<00:10, 21.03it/s]Epoch 1/1:  26%|██▌       | 76/298 [00:04<00:10, 21.04it/s]Epoch 1/1:  27%|██▋       | 79/298 [00:04<00:10, 21.00it/s]Epoch 1/1:  28%|██▊       | 82/298 [00:04<00:10, 21.03it/s]Epoch 1/1:  29%|██▊       | 85/298 [00:05<00:10, 21.04it/s]Epoch 1/1:  30%|██▉       | 88/298 [00:05<00:09, 21.06it/s]Epoch 1/1:  31%|███       | 91/298 [00:05<00:09, 21.07it/s]Epoch 1/1:  32%|███▏      | 94/298 [00:05<00:09, 21.07it/s]Epoch 1/1:  33%|███▎      | 97/298 [00:05<00:09, 21.06it/s]Epoch 1/1:  34%|███▎      | 100/298 [00:05<00:09, 21.06it/s]Epoch 1/1:  35%|███▍      | 103/298 [00:05<00:09, 21.02it/s]Epoch 1/1:  36%|███▌      | 106/298 [00:06<00:09, 21.04it/s]Epoch 1/1:  37%|███▋      | 109/298 [00:06<00:08, 21.06it/s]Epoch 1/1:  38%|███▊      | 112/298 [00:06<00:08, 21.08it/s]Epoch 1/1:  39%|███▊      | 115/298 [00:06<00:08, 21.03it/s]Epoch 1/1:  40%|███▉      | 118/298 [00:06<00:08, 21.03it/s]Epoch 1/1:  41%|████      | 121/298 [00:06<00:08, 20.97it/s]Epoch 1/1:  42%|████▏     | 124/298 [00:06<00:08, 20.98it/s]Epoch 1/1:  43%|████▎     | 127/298 [00:07<00:08, 21.01it/s]Epoch 1/1:  44%|████▎     | 130/298 [00:07<00:07, 21.04it/s]Epoch 1/1:  45%|████▍     | 133/298 [00:07<00:07, 21.05it/s]Epoch 1/1:  46%|████▌     | 136/298 [00:07<00:07, 21.04it/s]Epoch 1/1:  47%|████▋     | 139/298 [00:07<00:07, 21.07it/s]Epoch 1/1:  48%|████▊     | 142/298 [00:07<00:07, 21.07it/s]Epoch 1/1:  49%|████▊     | 145/298 [00:07<00:07, 21.07it/s]Epoch 1/1:  50%|████▉     | 148/298 [00:08<00:07, 21.07it/s]Epoch 1/1:  51%|█████     | 151/298 [00:08<00:06, 21.07it/s]Epoch 1/1:  52%|█████▏    | 154/298 [00:08<00:06, 21.05it/s]Epoch 1/1:  53%|█████▎    | 157/298 [00:08<00:06, 21.00it/s]Epoch 1/1:  54%|█████▎    | 160/298 [00:08<00:06, 21.02it/s]Epoch 1/1:  55%|█████▍    | 163/298 [00:08<00:06, 21.04it/s]Epoch 1/1:  56%|█████▌    | 166/298 [00:08<00:06, 21.05it/s]Epoch 1/1:  57%|█████▋    | 169/298 [00:09<00:06, 21.05it/s]Epoch 1/1:  58%|█████▊    | 172/298 [00:09<00:05, 21.06it/s]Epoch 1/1:  59%|█████▊    | 175/298 [00:09<00:05, 21.00it/s]Epoch 1/1:  60%|█████▉    | 178/298 [00:09<00:05, 21.01it/s]Epoch 1/1:  61%|██████    | 181/298 [00:09<00:05, 21.03it/s]Epoch 1/1:  62%|██████▏   | 184/298 [00:09<00:05, 21.02it/s]Epoch 1/1:  63%|██████▎   | 187/298 [00:09<00:05, 21.04it/s]Epoch 1/1:  64%|██████▍   | 190/298 [00:10<00:05, 21.04it/s]Epoch 1/1:  65%|██████▍   | 193/298 [00:10<00:04, 21.05it/s]Epoch 1/1:  66%|██████▌   | 196/298 [00:10<00:04, 21.05it/s]Epoch 1/1:  67%|██████▋   | 199/298 [00:10<00:04, 21.05it/s]Epoch 1/1:  68%|██████▊   | 202/298 [00:10<00:04, 20.99it/s]Epoch 1/1:  69%|██████▉   | 205/298 [00:10<00:04, 20.98it/s]Epoch 1/1:  70%|██████▉   | 208/298 [00:10<00:04, 21.02it/s]Epoch 1/1:  71%|███████   | 211/298 [00:11<00:04, 21.04it/s]Epoch 1/1:  72%|███████▏  | 214/298 [00:11<00:03, 21.05it/s]Epoch 1/1:  73%|███████▎  | 217/298 [00:11<00:03, 21.07it/s]Epoch 1/1:  74%|███████▍  | 220/298 [00:11<00:03, 21.08it/s]Epoch 1/1:  75%|███████▍  | 223/298 [00:11<00:03, 21.08it/s]Epoch 1/1:  76%|███████▌  | 226/298 [00:11<00:03, 21.03it/s]Epoch 1/1:  77%|███████▋  | 229/298 [00:11<00:03, 21.06it/s]Epoch 1/1:  78%|███████▊  | 232/298 [00:12<00:03, 21.07it/s]Epoch 1/1:  79%|███████▉  | 235/298 [00:12<00:02, 21.09it/s]Epoch 1/1:  80%|███████▉  | 238/298 [00:12<00:02, 21.08it/s]Epoch 1/1:  81%|████████  | 241/298 [00:12<00:02, 20.99it/s]Epoch 1/1:  82%|████████▏ | 244/298 [00:12<00:02, 20.96it/s]Epoch 1/1:  83%|████████▎ | 247/298 [00:12<00:02, 21.00it/s]Epoch 1/1:  84%|████████▍ | 250/298 [00:12<00:02, 20.99it/s]Epoch 1/1:  85%|████████▍ | 253/298 [00:13<00:02, 21.02it/s]Epoch 1/1:  86%|████████▌ | 256/298 [00:13<00:02, 20.99it/s]Epoch 1/1:  87%|████████▋ | 259/298 [00:13<00:01, 21.01it/s]Epoch 1/1:  88%|████████▊ | 262/298 [00:13<00:01, 20.95it/s]Epoch 1/1:  89%|████████▉ | 265/298 [00:13<00:01, 20.98it/s]Epoch 1/1:  90%|████████▉ | 268/298 [00:13<00:01, 21.00it/s]Epoch 1/1:  91%|█████████ | 271/298 [00:13<00:01, 21.03it/s]Epoch 1/1:  92%|█████████▏| 274/298 [00:14<00:01, 21.04it/s]Epoch 1/1:  93%|█████████▎| 277/298 [00:14<00:01, 20.98it/s]Epoch 1/1:  94%|█████████▍| 280/298 [00:14<00:00, 20.96it/s]Epoch 1/1:  95%|█████████▍| 283/298 [00:14<00:00, 20.98it/s]Epoch 1/1:  96%|█████████▌| 286/298 [00:14<00:00, 21.00it/s]Epoch 1/1:  97%|█████████▋| 289/298 [00:14<00:00, 21.01it/s]Epoch 1/1:  98%|█████████▊| 292/298 [00:14<00:00, 21.00it/s]Epoch 1/1:  99%|█████████▉| 295/298 [00:15<00:00, 21.08it/s]Epoch 1/1: 100%|██████████| 298/298 [00:15<00:00, 21.13it/s]Epoch 1/1: 100%|██████████| 298/298 [00:15<00:00, 19.58it/s]
[2025-04-28 15:36:01,788][src.training.lm_trainer][INFO] - Epoch 1/1, Train Loss: 0.4127
[2025-04-28 15:36:02,063][src.training.lm_trainer][INFO] - Epoch 1/1, Val Loss: 0.2689, Metrics: {'mse': 0.26887047290802, 'rmse': 0.518527215204776, 'r2': -0.07548189163208008}
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ▁
wandb:     best_val_mse ▁
wandb:      best_val_r2 ▁
wandb:    best_val_rmse ▁
wandb:            epoch ▁▁
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate ▁
wandb:       train_loss ▁
wandb:       train_time ▁
wandb:         val_loss ▁
wandb:          val_mse ▁
wandb:           val_r2 ▁
wandb:         val_rmse ▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.26887
wandb:     best_val_mse 0.26887
wandb:      best_val_r2 -0.07548
wandb:    best_val_rmse 0.51853
wandb:            epoch 1
wandb:   final_test_mse 0.27048
wandb:    final_test_r2 -0.08191
wandb:  final_test_rmse 0.52008
wandb:  final_train_mse 0.26768
wandb:   final_train_r2 -0.07073
wandb: final_train_rmse 0.51738
wandb:    final_val_mse 0.26887
wandb:     final_val_r2 -0.07548
wandb:   final_val_rmse 0.51853
wandb:    learning_rate 2e-05
wandb:       train_loss 0.41272
wandb:       train_time 15.90999
wandb:         val_loss 0.26887
wandb:          val_mse 0.26887
wandb:           val_r2 -0.07548
wandb:         val_rmse 0.51853
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250428_153538-og009wlb
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250428_153538-og009wlb/logs
Submetric control finetuning test completed successfully!
All finetuning tests completed successfully!
Results can be found in: /scratch/leuven/371/vsc37132/finetune_test
