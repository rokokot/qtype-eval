SLURM_JOB_ID: 64437535
SLURM_JOB_USER: vsc37132
SLURM_JOB_ACCOUNT: intro_vsc37132
SLURM_JOB_NAME: qtype_experiments
SLURM_CLUSTER_NAME: wice
SLURM_JOB_PARTITION: gpu_a100_debug
SLURM_NNODES: 1
SLURM_NODELIST: k28i22
SLURM_JOB_CPUS_PER_NODE: 4
SLURM_JOB_GPUS: 0
Date: Fri May  2 12:44:03 CEST 2025
Walltime: 00-00:30:00
========================================================================
Running main probing experiments (non-control)...
=======================
PROBING LAYER 1
=======================
Running experiment: probe_layer1_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=1"         "model.probe_hidden_size=128" "model.probe_depth=2" "model.dropout=0.2" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer1_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 12:44:20,529][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ar
experiment_name: probe_layer1_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 12:44:20,529][__main__][INFO] - Normalized task: question_type
[2025-05-02 12:44:20,530][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 12:44:20,530][__main__][INFO] - Determined Task Type: classification
[2025-05-02 12:44:20,574][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-02 12:44:20,574][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 12:44:22,726][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 12:44:25,165][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 12:44:25,166][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:44:25,244][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:44:25,277][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:44:25,376][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 12:44:25,383][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:44:25,384][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 12:44:25,385][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:44:25,404][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:44:25,433][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:44:25,445][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 12:44:25,446][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:44:25,446][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 12:44:25,447][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:44:25,464][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:44:25,493][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:44:25,505][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 12:44:25,506][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:44:25,507][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 12:44:25,507][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 12:44:25,508][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 12:44:25,508][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 12:44:25,508][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 12:44:25,509][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 12:44:25,509][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-02 12:44:25,509][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-02 12:44:25,509][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 12:44:25,509][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 12:44:25,509][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 12:44:25,509][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 12:44:25,509][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 12:44:25,509][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 12:44:25,509][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-02 12:44:25,510][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-02 12:44:25,510][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 12:44:25,510][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 12:44:25,510][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 12:44:25,510][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 12:44:25,510][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 12:44:25,510][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 12:44:25,510][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-02 12:44:25,510][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-02 12:44:25,510][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 12:44:25,510][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 12:44:25,511][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 12:44:25,511][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 12:44:25,511][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 12:44:25,511][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 12:44:25,511][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 12:44:30,140][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 12:44:30,141][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 12:44:30,141][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=1, freeze_model=True
[2025-05-02 12:44:30,141][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-02 12:44:30,144][src.models.model_factory][INFO] - Model has 116,865 trainable parameters out of 394,238,337 total parameters
[2025-05-02 12:44:30,145][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 116,865 trainable parameters
[2025-05-02 12:44:30,145][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=2, activation=gelu, normalization=layer
[2025-05-02 12:44:30,145][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 128 hidden size
[2025-05-02 12:44:30,145][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 12:44:30,146][__main__][INFO] - Total parameters: 394,238,337
[2025-05-02 12:44:30,146][__main__][INFO] - Trainable parameters: 116,865 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7240Epoch 1/15: [                              ] 2/63 batches, loss: 0.7344Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7343Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7319Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7295Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7177Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7066Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7102Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7054Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7006Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7013Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7012Epoch 1/15: [======                        ] 13/63 batches, loss: 0.6994Epoch 1/15: [======                        ] 14/63 batches, loss: 0.6975Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.6980Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.6960Epoch 1/15: [========                      ] 17/63 batches, loss: 0.6968Epoch 1/15: [========                      ] 18/63 batches, loss: 0.6968Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.6960Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.6952Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.6956Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.6962Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.6970Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6968Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.6990Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6962Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6966Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6974Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6971Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6990Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6994Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6998Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6996Epoch 1/15: [================              ] 34/63 batches, loss: 0.6994Epoch 1/15: [================              ] 35/63 batches, loss: 0.6989Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6989Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6984Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6982Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6980Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6971Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6973Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6966Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6966Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6963Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6953Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6950Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6953Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6943Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6941Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6932Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6922Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6917Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6918Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6916Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6909Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6909Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6906Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6907Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6898Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6896Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6897Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6895Epoch 1/15: [==============================] 63/63 batches, loss: 0.6873
[2025-05-02 12:44:34,548][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6873
[2025-05-02 12:44:34,752][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.7009, Metrics: {'accuracy': 0.5, 'f1': 0.08333333333333333, 'precision': 0.25, 'recall': 0.05}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6991Epoch 2/15: [                              ] 2/63 batches, loss: 0.6983Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6910Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6978Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6853Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6858Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6765Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6745Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6767Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6751Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6706Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6727Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6737Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6773Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6800Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6823Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6830Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6804Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6803Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6789Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6776Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6746Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6718Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6681Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6696Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6717Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6707Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6707Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6681Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6686Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6672Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6651Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6636Epoch 2/15: [================              ] 34/63 batches, loss: 0.6629Epoch 2/15: [================              ] 35/63 batches, loss: 0.6638Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6631Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6607Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6613Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6614Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6626Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6604Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6609Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6615Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6616Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6621Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6615Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6611Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6625Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6630Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6643Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6635Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6632Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6630Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6616Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6609Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6603Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6601Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6582Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6580Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6576Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6563Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6564Epoch 2/15: [==============================] 63/63 batches, loss: 0.6545
[2025-05-02 12:44:37,090][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6545
[2025-05-02 12:44:37,295][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6723, Metrics: {'accuracy': 0.6818181818181818, 'f1': 0.6111111111111112, 'precision': 0.6875, 'recall': 0.55}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6829Epoch 3/15: [                              ] 2/63 batches, loss: 0.6666Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6661Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6500Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6303Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6237Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6245Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6254Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6265Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6160Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6101Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6108Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6130Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6103Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6125Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6154Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6158Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6169Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6133Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6143Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6101Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6095Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6089Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6080Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6093Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6076Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6091Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6121Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6114Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6090Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6109Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6091Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6079Epoch 3/15: [================              ] 34/63 batches, loss: 0.6093Epoch 3/15: [================              ] 35/63 batches, loss: 0.6113Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6122Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6129Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6140Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6139Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6151Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6173Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6172Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6171Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6167Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6165Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6166Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6156Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6153Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6153Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6158Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6159Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6156Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6159Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6144Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6145Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6131Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6127Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6128Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6115Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6104Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6089Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6085Epoch 3/15: [==============================] 63/63 batches, loss: 0.6120
[2025-05-02 12:44:39,629][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6120
[2025-05-02 12:44:39,852][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6284, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8636363636363636, 'precision': 0.7916666666666666, 'recall': 0.95}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.5205Epoch 4/15: [                              ] 2/63 batches, loss: 0.5219Epoch 4/15: [=                             ] 3/63 batches, loss: 0.5524Epoch 4/15: [=                             ] 4/63 batches, loss: 0.5628Epoch 4/15: [==                            ] 5/63 batches, loss: 0.5741Epoch 4/15: [==                            ] 6/63 batches, loss: 0.5611Epoch 4/15: [===                           ] 7/63 batches, loss: 0.5691Epoch 4/15: [===                           ] 8/63 batches, loss: 0.5823Epoch 4/15: [====                          ] 9/63 batches, loss: 0.5882Epoch 4/15: [====                          ] 10/63 batches, loss: 0.5921Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.5929Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.5889Epoch 4/15: [======                        ] 13/63 batches, loss: 0.5913Epoch 4/15: [======                        ] 14/63 batches, loss: 0.5858Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.5870Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.5911Epoch 4/15: [========                      ] 17/63 batches, loss: 0.5865Epoch 4/15: [========                      ] 18/63 batches, loss: 0.5854Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.5852Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.5880Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.5851Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.5873Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.5899Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.5918Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.5919Epoch 4/15: [============                  ] 26/63 batches, loss: 0.5880Epoch 4/15: [============                  ] 27/63 batches, loss: 0.5865Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.5843Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.5833Epoch 4/15: [==============                ] 30/63 batches, loss: 0.5844Epoch 4/15: [==============                ] 31/63 batches, loss: 0.5844Epoch 4/15: [===============               ] 32/63 batches, loss: 0.5829Epoch 4/15: [===============               ] 33/63 batches, loss: 0.5801Epoch 4/15: [================              ] 34/63 batches, loss: 0.5778Epoch 4/15: [================              ] 35/63 batches, loss: 0.5770Epoch 4/15: [=================             ] 36/63 batches, loss: 0.5750Epoch 4/15: [=================             ] 37/63 batches, loss: 0.5742Epoch 4/15: [==================            ] 38/63 batches, loss: 0.5725Epoch 4/15: [==================            ] 39/63 batches, loss: 0.5737Epoch 4/15: [===================           ] 40/63 batches, loss: 0.5747Epoch 4/15: [===================           ] 41/63 batches, loss: 0.5764Epoch 4/15: [====================          ] 42/63 batches, loss: 0.5755Epoch 4/15: [====================          ] 43/63 batches, loss: 0.5764Epoch 4/15: [====================          ] 44/63 batches, loss: 0.5752Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.5769Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.5751Epoch 4/15: [======================        ] 47/63 batches, loss: 0.5747Epoch 4/15: [======================        ] 48/63 batches, loss: 0.5760Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.5764Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.5768Epoch 4/15: [========================      ] 51/63 batches, loss: 0.5778Epoch 4/15: [========================      ] 52/63 batches, loss: 0.5772Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.5791Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.5807Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.5806Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.5791Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.5787Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.5789Epoch 4/15: [============================  ] 59/63 batches, loss: 0.5793Epoch 4/15: [============================  ] 60/63 batches, loss: 0.5782Epoch 4/15: [============================= ] 61/63 batches, loss: 0.5779Epoch 4/15: [============================= ] 62/63 batches, loss: 0.5776Epoch 4/15: [==============================] 63/63 batches, loss: 0.5754
[2025-05-02 12:44:42,131][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5754
[2025-05-02 12:44:42,348][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.5964, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8780487804878049, 'precision': 0.8571428571428571, 'recall': 0.9}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.4878Epoch 5/15: [                              ] 2/63 batches, loss: 0.5081Epoch 5/15: [=                             ] 3/63 batches, loss: 0.5040Epoch 5/15: [=                             ] 4/63 batches, loss: 0.5413Epoch 5/15: [==                            ] 5/63 batches, loss: 0.5479Epoch 5/15: [==                            ] 6/63 batches, loss: 0.5377Epoch 5/15: [===                           ] 7/63 batches, loss: 0.5443Epoch 5/15: [===                           ] 8/63 batches, loss: 0.5476Epoch 5/15: [====                          ] 9/63 batches, loss: 0.5424Epoch 5/15: [====                          ] 10/63 batches, loss: 0.5480Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.5545Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.5559Epoch 5/15: [======                        ] 13/63 batches, loss: 0.5532Epoch 5/15: [======                        ] 14/63 batches, loss: 0.5559Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.5541Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.5509Epoch 5/15: [========                      ] 17/63 batches, loss: 0.5502Epoch 5/15: [========                      ] 18/63 batches, loss: 0.5494Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.5533Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.5498Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.5535Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.5550Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.5570Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.5532Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.5524Epoch 5/15: [============                  ] 26/63 batches, loss: 0.5533Epoch 5/15: [============                  ] 27/63 batches, loss: 0.5540Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.5572Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.5574Epoch 5/15: [==============                ] 30/63 batches, loss: 0.5560Epoch 5/15: [==============                ] 31/63 batches, loss: 0.5567Epoch 5/15: [===============               ] 32/63 batches, loss: 0.5554Epoch 5/15: [===============               ] 33/63 batches, loss: 0.5551Epoch 5/15: [================              ] 34/63 batches, loss: 0.5571Epoch 5/15: [================              ] 35/63 batches, loss: 0.5534Epoch 5/15: [=================             ] 36/63 batches, loss: 0.5551Epoch 5/15: [=================             ] 37/63 batches, loss: 0.5544Epoch 5/15: [==================            ] 38/63 batches, loss: 0.5554Epoch 5/15: [==================            ] 39/63 batches, loss: 0.5553Epoch 5/15: [===================           ] 40/63 batches, loss: 0.5545Epoch 5/15: [===================           ] 41/63 batches, loss: 0.5534Epoch 5/15: [====================          ] 42/63 batches, loss: 0.5535Epoch 5/15: [====================          ] 43/63 batches, loss: 0.5530Epoch 5/15: [====================          ] 44/63 batches, loss: 0.5527Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.5540Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.5561Epoch 5/15: [======================        ] 47/63 batches, loss: 0.5552Epoch 5/15: [======================        ] 48/63 batches, loss: 0.5568Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.5577Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.5577Epoch 5/15: [========================      ] 51/63 batches, loss: 0.5579Epoch 5/15: [========================      ] 52/63 batches, loss: 0.5561Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.5555Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.5552Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.5551Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.5562Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.5544Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.5534Epoch 5/15: [============================  ] 59/63 batches, loss: 0.5529Epoch 5/15: [============================  ] 60/63 batches, loss: 0.5531Epoch 5/15: [============================= ] 61/63 batches, loss: 0.5523Epoch 5/15: [============================= ] 62/63 batches, loss: 0.5519Epoch 5/15: [==============================] 63/63 batches, loss: 0.5563
[2025-05-02 12:44:44,648][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5563
[2025-05-02 12:44:44,876][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5814, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.5307Epoch 6/15: [                              ] 2/63 batches, loss: 0.5756Epoch 6/15: [=                             ] 3/63 batches, loss: 0.5668Epoch 6/15: [=                             ] 4/63 batches, loss: 0.5529Epoch 6/15: [==                            ] 5/63 batches, loss: 0.5571Epoch 6/15: [==                            ] 6/63 batches, loss: 0.5435Epoch 6/15: [===                           ] 7/63 batches, loss: 0.5445Epoch 6/15: [===                           ] 8/63 batches, loss: 0.5463Epoch 6/15: [====                          ] 9/63 batches, loss: 0.5501Epoch 6/15: [====                          ] 10/63 batches, loss: 0.5421Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.5421Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.5374Epoch 6/15: [======                        ] 13/63 batches, loss: 0.5435Epoch 6/15: [======                        ] 14/63 batches, loss: 0.5435Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.5392Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.5381Epoch 6/15: [========                      ] 17/63 batches, loss: 0.5379Epoch 6/15: [========                      ] 18/63 batches, loss: 0.5406Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.5371Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.5390Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.5415Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.5431Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.5410Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.5397Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.5373Epoch 6/15: [============                  ] 26/63 batches, loss: 0.5373Epoch 6/15: [============                  ] 27/63 batches, loss: 0.5373Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.5385Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.5384Epoch 6/15: [==============                ] 30/63 batches, loss: 0.5381Epoch 6/15: [==============                ] 31/63 batches, loss: 0.5387Epoch 6/15: [===============               ] 32/63 batches, loss: 0.5393Epoch 6/15: [===============               ] 33/63 batches, loss: 0.5396Epoch 6/15: [================              ] 34/63 batches, loss: 0.5406Epoch 6/15: [================              ] 35/63 batches, loss: 0.5398Epoch 6/15: [=================             ] 36/63 batches, loss: 0.5385Epoch 6/15: [=================             ] 37/63 batches, loss: 0.5390Epoch 6/15: [==================            ] 38/63 batches, loss: 0.5402Epoch 6/15: [==================            ] 39/63 batches, loss: 0.5412Epoch 6/15: [===================           ] 40/63 batches, loss: 0.5419Epoch 6/15: [===================           ] 41/63 batches, loss: 0.5410Epoch 6/15: [====================          ] 42/63 batches, loss: 0.5381Epoch 6/15: [====================          ] 43/63 batches, loss: 0.5374Epoch 6/15: [====================          ] 44/63 batches, loss: 0.5350Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.5356Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.5373Epoch 6/15: [======================        ] 47/63 batches, loss: 0.5384Epoch 6/15: [======================        ] 48/63 batches, loss: 0.5390Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.5391Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.5389Epoch 6/15: [========================      ] 51/63 batches, loss: 0.5402Epoch 6/15: [========================      ] 52/63 batches, loss: 0.5388Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.5387Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.5392Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.5383Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.5388Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.5398Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.5412Epoch 6/15: [============================  ] 59/63 batches, loss: 0.5416Epoch 6/15: [============================  ] 60/63 batches, loss: 0.5416Epoch 6/15: [============================= ] 61/63 batches, loss: 0.5422Epoch 6/15: [============================= ] 62/63 batches, loss: 0.5428Epoch 6/15: [==============================] 63/63 batches, loss: 0.5433
[2025-05-02 12:44:47,155][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5433
[2025-05-02 12:44:47,369][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5733, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.5252Epoch 7/15: [                              ] 2/63 batches, loss: 0.5578Epoch 7/15: [=                             ] 3/63 batches, loss: 0.5408Epoch 7/15: [=                             ] 4/63 batches, loss: 0.5258Epoch 7/15: [==                            ] 5/63 batches, loss: 0.5347Epoch 7/15: [==                            ] 6/63 batches, loss: 0.5324Epoch 7/15: [===                           ] 7/63 batches, loss: 0.5358Epoch 7/15: [===                           ] 8/63 batches, loss: 0.5265Epoch 7/15: [====                          ] 9/63 batches, loss: 0.5279Epoch 7/15: [====                          ] 10/63 batches, loss: 0.5322Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.5290Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.5291Epoch 7/15: [======                        ] 13/63 batches, loss: 0.5224Epoch 7/15: [======                        ] 14/63 batches, loss: 0.5242Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.5293Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.5317Epoch 7/15: [========                      ] 17/63 batches, loss: 0.5294Epoch 7/15: [========                      ] 18/63 batches, loss: 0.5295Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.5306Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.5297Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.5272Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.5366Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.5378Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.5383Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.5369Epoch 7/15: [============                  ] 26/63 batches, loss: 0.5344Epoch 7/15: [============                  ] 27/63 batches, loss: 0.5354Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.5396Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.5369Epoch 7/15: [==============                ] 30/63 batches, loss: 0.5343Epoch 7/15: [==============                ] 31/63 batches, loss: 0.5345Epoch 7/15: [===============               ] 32/63 batches, loss: 0.5368Epoch 7/15: [===============               ] 33/63 batches, loss: 0.5367Epoch 7/15: [================              ] 34/63 batches, loss: 0.5382Epoch 7/15: [================              ] 35/63 batches, loss: 0.5371Epoch 7/15: [=================             ] 36/63 batches, loss: 0.5387Epoch 7/15: [=================             ] 37/63 batches, loss: 0.5384Epoch 7/15: [==================            ] 38/63 batches, loss: 0.5388Epoch 7/15: [==================            ] 39/63 batches, loss: 0.5373Epoch 7/15: [===================           ] 40/63 batches, loss: 0.5352Epoch 7/15: [===================           ] 41/63 batches, loss: 0.5348Epoch 7/15: [====================          ] 42/63 batches, loss: 0.5360Epoch 7/15: [====================          ] 43/63 batches, loss: 0.5359Epoch 7/15: [====================          ] 44/63 batches, loss: 0.5351Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.5342Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.5350Epoch 7/15: [======================        ] 47/63 batches, loss: 0.5355Epoch 7/15: [======================        ] 48/63 batches, loss: 0.5367Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.5378Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.5388Epoch 7/15: [========================      ] 51/63 batches, loss: 0.5384Epoch 7/15: [========================      ] 52/63 batches, loss: 0.5394Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.5377Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.5380Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.5372Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.5381Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.5385Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.5373Epoch 7/15: [============================  ] 59/63 batches, loss: 0.5379Epoch 7/15: [============================  ] 60/63 batches, loss: 0.5369Epoch 7/15: [============================= ] 61/63 batches, loss: 0.5365Epoch 7/15: [============================= ] 62/63 batches, loss: 0.5366Epoch 7/15: [==============================] 63/63 batches, loss: 0.5373
[2025-05-02 12:44:49,654][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5373
[2025-05-02 12:44:49,876][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5712, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.5715Epoch 8/15: [                              ] 2/63 batches, loss: 0.5316Epoch 8/15: [=                             ] 3/63 batches, loss: 0.5174Epoch 8/15: [=                             ] 4/63 batches, loss: 0.5076Epoch 8/15: [==                            ] 5/63 batches, loss: 0.5215Epoch 8/15: [==                            ] 6/63 batches, loss: 0.5274Epoch 8/15: [===                           ] 7/63 batches, loss: 0.5300Epoch 8/15: [===                           ] 8/63 batches, loss: 0.5246Epoch 8/15: [====                          ] 9/63 batches, loss: 0.5288Epoch 8/15: [====                          ] 10/63 batches, loss: 0.5346Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.5321Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.5350Epoch 8/15: [======                        ] 13/63 batches, loss: 0.5297Epoch 8/15: [======                        ] 14/63 batches, loss: 0.5300Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.5322Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.5306Epoch 8/15: [========                      ] 17/63 batches, loss: 0.5319Epoch 8/15: [========                      ] 18/63 batches, loss: 0.5276Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.5267Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.5231Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.5240Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.5224Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.5220Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.5224Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.5234Epoch 8/15: [============                  ] 26/63 batches, loss: 0.5207Epoch 8/15: [============                  ] 27/63 batches, loss: 0.5203Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.5203Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.5212Epoch 8/15: [==============                ] 30/63 batches, loss: 0.5213Epoch 8/15: [==============                ] 31/63 batches, loss: 0.5218Epoch 8/15: [===============               ] 32/63 batches, loss: 0.5217Epoch 8/15: [===============               ] 33/63 batches, loss: 0.5203Epoch 8/15: [================              ] 34/63 batches, loss: 0.5203Epoch 8/15: [================              ] 35/63 batches, loss: 0.5179Epoch 8/15: [=================             ] 36/63 batches, loss: 0.5184Epoch 8/15: [=================             ] 37/63 batches, loss: 0.5201Epoch 8/15: [==================            ] 38/63 batches, loss: 0.5219Epoch 8/15: [==================            ] 39/63 batches, loss: 0.5235Epoch 8/15: [===================           ] 40/63 batches, loss: 0.5209Epoch 8/15: [===================           ] 41/63 batches, loss: 0.5212Epoch 8/15: [====================          ] 42/63 batches, loss: 0.5216Epoch 8/15: [====================          ] 43/63 batches, loss: 0.5198Epoch 8/15: [====================          ] 44/63 batches, loss: 0.5215Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.5209Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.5190Epoch 8/15: [======================        ] 47/63 batches, loss: 0.5180Epoch 8/15: [======================        ] 48/63 batches, loss: 0.5196Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.5224Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.5224Epoch 8/15: [========================      ] 51/63 batches, loss: 0.5247Epoch 8/15: [========================      ] 52/63 batches, loss: 0.5246Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.5251Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.5253Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.5270Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.5276Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.5286Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.5280Epoch 8/15: [============================  ] 59/63 batches, loss: 0.5272Epoch 8/15: [============================  ] 60/63 batches, loss: 0.5282Epoch 8/15: [============================= ] 61/63 batches, loss: 0.5276Epoch 8/15: [============================= ] 62/63 batches, loss: 0.5273Epoch 8/15: [==============================] 63/63 batches, loss: 0.5259
[2025-05-02 12:44:52,243][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5259
[2025-05-02 12:44:52,478][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5692, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.5168Epoch 9/15: [                              ] 2/63 batches, loss: 0.5191Epoch 9/15: [=                             ] 3/63 batches, loss: 0.5199Epoch 9/15: [=                             ] 4/63 batches, loss: 0.5181Epoch 9/15: [==                            ] 5/63 batches, loss: 0.5273Epoch 9/15: [==                            ] 6/63 batches, loss: 0.5310Epoch 9/15: [===                           ] 7/63 batches, loss: 0.5356Epoch 9/15: [===                           ] 8/63 batches, loss: 0.5262Epoch 9/15: [====                          ] 9/63 batches, loss: 0.5323Epoch 9/15: [====                          ] 10/63 batches, loss: 0.5271Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.5298Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.5261Epoch 9/15: [======                        ] 13/63 batches, loss: 0.5356Epoch 9/15: [======                        ] 14/63 batches, loss: 0.5365Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.5339Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.5370Epoch 9/15: [========                      ] 17/63 batches, loss: 0.5370Epoch 9/15: [========                      ] 18/63 batches, loss: 0.5360Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.5363Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.5352Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.5341Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.5320Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.5293Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.5295Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.5293Epoch 9/15: [============                  ] 26/63 batches, loss: 0.5283Epoch 9/15: [============                  ] 27/63 batches, loss: 0.5288Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.5262Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.5265Epoch 9/15: [==============                ] 30/63 batches, loss: 0.5267Epoch 9/15: [==============                ] 31/63 batches, loss: 0.5271Epoch 9/15: [===============               ] 32/63 batches, loss: 0.5236Epoch 9/15: [===============               ] 33/63 batches, loss: 0.5247Epoch 9/15: [================              ] 34/63 batches, loss: 0.5244Epoch 9/15: [================              ] 35/63 batches, loss: 0.5251Epoch 9/15: [=================             ] 36/63 batches, loss: 0.5227Epoch 9/15: [=================             ] 37/63 batches, loss: 0.5243Epoch 9/15: [==================            ] 38/63 batches, loss: 0.5268Epoch 9/15: [==================            ] 39/63 batches, loss: 0.5262Epoch 9/15: [===================           ] 40/63 batches, loss: 0.5280Epoch 9/15: [===================           ] 41/63 batches, loss: 0.5293Epoch 9/15: [====================          ] 42/63 batches, loss: 0.5308Epoch 9/15: [====================          ] 43/63 batches, loss: 0.5302Epoch 9/15: [====================          ] 44/63 batches, loss: 0.5301Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.5304Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.5326Epoch 9/15: [======================        ] 47/63 batches, loss: 0.5310Epoch 9/15: [======================        ] 48/63 batches, loss: 0.5320Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.5326Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.5314Epoch 9/15: [========================      ] 51/63 batches, loss: 0.5311Epoch 9/15: [========================      ] 52/63 batches, loss: 0.5297Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.5306Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.5294Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.5304Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.5309Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.5300Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.5305Epoch 9/15: [============================  ] 59/63 batches, loss: 0.5292Epoch 9/15: [============================  ] 60/63 batches, loss: 0.5273Epoch 9/15: [============================= ] 61/63 batches, loss: 0.5269Epoch 9/15: [============================= ] 62/63 batches, loss: 0.5268Epoch 9/15: [==============================] 63/63 batches, loss: 0.5235
[2025-05-02 12:44:54,814][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5235
[2025-05-02 12:44:55,043][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5683, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.5051Epoch 10/15: [                              ] 2/63 batches, loss: 0.4853Epoch 10/15: [=                             ] 3/63 batches, loss: 0.5165Epoch 10/15: [=                             ] 4/63 batches, loss: 0.5067Epoch 10/15: [==                            ] 5/63 batches, loss: 0.5133Epoch 10/15: [==                            ] 6/63 batches, loss: 0.5021Epoch 10/15: [===                           ] 7/63 batches, loss: 0.4970Epoch 10/15: [===                           ] 8/63 batches, loss: 0.5020Epoch 10/15: [====                          ] 9/63 batches, loss: 0.4923Epoch 10/15: [====                          ] 10/63 batches, loss: 0.4952Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.4943Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.5006Epoch 10/15: [======                        ] 13/63 batches, loss: 0.5048Epoch 10/15: [======                        ] 14/63 batches, loss: 0.5082Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.5104Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.5102Epoch 10/15: [========                      ] 17/63 batches, loss: 0.5138Epoch 10/15: [========                      ] 18/63 batches, loss: 0.5230Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.5200Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.5175Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.5221Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.5212Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.5227Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.5237Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.5234Epoch 10/15: [============                  ] 26/63 batches, loss: 0.5210Epoch 10/15: [============                  ] 27/63 batches, loss: 0.5229Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.5214Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.5219Epoch 10/15: [==============                ] 30/63 batches, loss: 0.5218Epoch 10/15: [==============                ] 31/63 batches, loss: 0.5223Epoch 10/15: [===============               ] 32/63 batches, loss: 0.5241Epoch 10/15: [===============               ] 33/63 batches, loss: 0.5229Epoch 10/15: [================              ] 34/63 batches, loss: 0.5233Epoch 10/15: [================              ] 35/63 batches, loss: 0.5228Epoch 10/15: [=================             ] 36/63 batches, loss: 0.5224Epoch 10/15: [=================             ] 37/63 batches, loss: 0.5205Epoch 10/15: [==================            ] 38/63 batches, loss: 0.5206Epoch 10/15: [==================            ] 39/63 batches, loss: 0.5218Epoch 10/15: [===================           ] 40/63 batches, loss: 0.5235Epoch 10/15: [===================           ] 41/63 batches, loss: 0.5203Epoch 10/15: [====================          ] 42/63 batches, loss: 0.5217Epoch 10/15: [====================          ] 43/63 batches, loss: 0.5216Epoch 10/15: [====================          ] 44/63 batches, loss: 0.5220Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.5211Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.5202Epoch 10/15: [======================        ] 47/63 batches, loss: 0.5201Epoch 10/15: [======================        ] 48/63 batches, loss: 0.5215Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.5208Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.5217Epoch 10/15: [========================      ] 51/63 batches, loss: 0.5206Epoch 10/15: [========================      ] 52/63 batches, loss: 0.5205Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.5224Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.5225Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.5222Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.5210Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.5210Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.5218Epoch 10/15: [============================  ] 59/63 batches, loss: 0.5216Epoch 10/15: [============================  ] 60/63 batches, loss: 0.5214Epoch 10/15: [============================= ] 61/63 batches, loss: 0.5216Epoch 10/15: [============================= ] 62/63 batches, loss: 0.5220Epoch 10/15: [==============================] 63/63 batches, loss: 0.5242
[2025-05-02 12:44:57,358][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5242
[2025-05-02 12:44:57,608][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5671, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.5057Epoch 11/15: [                              ] 2/63 batches, loss: 0.5316Epoch 11/15: [=                             ] 3/63 batches, loss: 0.5407Epoch 11/15: [=                             ] 4/63 batches, loss: 0.5386Epoch 11/15: [==                            ] 5/63 batches, loss: 0.5510Epoch 11/15: [==                            ] 6/63 batches, loss: 0.5457Epoch 11/15: [===                           ] 7/63 batches, loss: 0.5492Epoch 11/15: [===                           ] 8/63 batches, loss: 0.5559Epoch 11/15: [====                          ] 9/63 batches, loss: 0.5523Epoch 11/15: [====                          ] 10/63 batches, loss: 0.5433Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.5364Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.5382Epoch 11/15: [======                        ] 13/63 batches, loss: 0.5403Epoch 11/15: [======                        ] 14/63 batches, loss: 0.5422Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.5354Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.5330Epoch 11/15: [========                      ] 17/63 batches, loss: 0.5322Epoch 11/15: [========                      ] 18/63 batches, loss: 0.5307Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.5294Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.5307Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.5331Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.5320Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.5303Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.5325Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.5318Epoch 11/15: [============                  ] 26/63 batches, loss: 0.5298Epoch 11/15: [============                  ] 27/63 batches, loss: 0.5274Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.5296Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.5289Epoch 11/15: [==============                ] 30/63 batches, loss: 0.5276Epoch 11/15: [==============                ] 31/63 batches, loss: 0.5259Epoch 11/15: [===============               ] 32/63 batches, loss: 0.5256Epoch 11/15: [===============               ] 33/63 batches, loss: 0.5229Epoch 11/15: [================              ] 34/63 batches, loss: 0.5202Epoch 11/15: [================              ] 35/63 batches, loss: 0.5220Epoch 11/15: [=================             ] 36/63 batches, loss: 0.5216Epoch 11/15: [=================             ] 37/63 batches, loss: 0.5199Epoch 11/15: [==================            ] 38/63 batches, loss: 0.5213Epoch 11/15: [==================            ] 39/63 batches, loss: 0.5203Epoch 11/15: [===================           ] 40/63 batches, loss: 0.5201Epoch 11/15: [===================           ] 41/63 batches, loss: 0.5193Epoch 11/15: [====================          ] 42/63 batches, loss: 0.5190Epoch 11/15: [====================          ] 43/63 batches, loss: 0.5172Epoch 11/15: [====================          ] 44/63 batches, loss: 0.5175Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.5181Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.5169Epoch 11/15: [======================        ] 47/63 batches, loss: 0.5178Epoch 11/15: [======================        ] 48/63 batches, loss: 0.5171Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.5181Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.5159Epoch 11/15: [========================      ] 51/63 batches, loss: 0.5177Epoch 11/15: [========================      ] 52/63 batches, loss: 0.5187Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.5185Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.5187Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.5181Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.5196Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.5204Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.5207Epoch 11/15: [============================  ] 59/63 batches, loss: 0.5206Epoch 11/15: [============================  ] 60/63 batches, loss: 0.5196Epoch 11/15: [============================= ] 61/63 batches, loss: 0.5194Epoch 11/15: [============================= ] 62/63 batches, loss: 0.5188Epoch 11/15: [==============================] 63/63 batches, loss: 0.5198
[2025-05-02 12:44:59,948][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5198
[2025-05-02 12:45:00,189][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5670, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.4615Epoch 12/15: [                              ] 2/63 batches, loss: 0.5528Epoch 12/15: [=                             ] 3/63 batches, loss: 0.5420Epoch 12/15: [=                             ] 4/63 batches, loss: 0.5435Epoch 12/15: [==                            ] 5/63 batches, loss: 0.5256Epoch 12/15: [==                            ] 6/63 batches, loss: 0.5317Epoch 12/15: [===                           ] 7/63 batches, loss: 0.5347Epoch 12/15: [===                           ] 8/63 batches, loss: 0.5311Epoch 12/15: [====                          ] 9/63 batches, loss: 0.5319Epoch 12/15: [====                          ] 10/63 batches, loss: 0.5173Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.5119Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.5055Epoch 12/15: [======                        ] 13/63 batches, loss: 0.5108Epoch 12/15: [======                        ] 14/63 batches, loss: 0.5064Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.5023Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.5029Epoch 12/15: [========                      ] 17/63 batches, loss: 0.5072Epoch 12/15: [========                      ] 18/63 batches, loss: 0.5071Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.5088Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.5056Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.5087Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.5092Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.5081Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.5070Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.5051Epoch 12/15: [============                  ] 26/63 batches, loss: 0.5054Epoch 12/15: [============                  ] 27/63 batches, loss: 0.5055Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.5055Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.5051Epoch 12/15: [==============                ] 30/63 batches, loss: 0.5062Epoch 12/15: [==============                ] 31/63 batches, loss: 0.5071Epoch 12/15: [===============               ] 32/63 batches, loss: 0.5080Epoch 12/15: [===============               ] 33/63 batches, loss: 0.5088Epoch 12/15: [================              ] 34/63 batches, loss: 0.5099Epoch 12/15: [================              ] 35/63 batches, loss: 0.5109Epoch 12/15: [=================             ] 36/63 batches, loss: 0.5125Epoch 12/15: [=================             ] 37/63 batches, loss: 0.5143Epoch 12/15: [==================            ] 38/63 batches, loss: 0.5142Epoch 12/15: [==================            ] 39/63 batches, loss: 0.5135Epoch 12/15: [===================           ] 40/63 batches, loss: 0.5153Epoch 12/15: [===================           ] 41/63 batches, loss: 0.5135Epoch 12/15: [====================          ] 42/63 batches, loss: 0.5140Epoch 12/15: [====================          ] 43/63 batches, loss: 0.5136Epoch 12/15: [====================          ] 44/63 batches, loss: 0.5151Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.5150Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.5155Epoch 12/15: [======================        ] 47/63 batches, loss: 0.5152Epoch 12/15: [======================        ] 48/63 batches, loss: 0.5155Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.5177Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.5185Epoch 12/15: [========================      ] 51/63 batches, loss: 0.5159Epoch 12/15: [========================      ] 52/63 batches, loss: 0.5166Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.5177Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.5185Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.5191Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.5185Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.5196Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.5206Epoch 12/15: [============================  ] 59/63 batches, loss: 0.5212Epoch 12/15: [============================  ] 60/63 batches, loss: 0.5212Epoch 12/15: [============================= ] 61/63 batches, loss: 0.5217Epoch 12/15: [============================= ] 62/63 batches, loss: 0.5219Epoch 12/15: [==============================] 63/63 batches, loss: 0.5212
[2025-05-02 12:45:02,523][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5212
[2025-05-02 12:45:02,767][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.5656, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.5009Epoch 13/15: [                              ] 2/63 batches, loss: 0.5040Epoch 13/15: [=                             ] 3/63 batches, loss: 0.5083Epoch 13/15: [=                             ] 4/63 batches, loss: 0.5150Epoch 13/15: [==                            ] 5/63 batches, loss: 0.5142Epoch 13/15: [==                            ] 6/63 batches, loss: 0.5232Epoch 13/15: [===                           ] 7/63 batches, loss: 0.5190Epoch 13/15: [===                           ] 8/63 batches, loss: 0.5204Epoch 13/15: [====                          ] 9/63 batches, loss: 0.5231Epoch 13/15: [====                          ] 10/63 batches, loss: 0.5167Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.5143Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.5249Epoch 13/15: [======                        ] 13/63 batches, loss: 0.5254Epoch 13/15: [======                        ] 14/63 batches, loss: 0.5225Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.5173Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.5141Epoch 13/15: [========                      ] 17/63 batches, loss: 0.5086Epoch 13/15: [========                      ] 18/63 batches, loss: 0.5111Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.5115Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.5099Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.5097Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.5109Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.5097Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.5100Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.5089Epoch 13/15: [============                  ] 26/63 batches, loss: 0.5071Epoch 13/15: [============                  ] 27/63 batches, loss: 0.5088Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.5081Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.5102Epoch 13/15: [==============                ] 30/63 batches, loss: 0.5106Epoch 13/15: [==============                ] 31/63 batches, loss: 0.5107Epoch 13/15: [===============               ] 32/63 batches, loss: 0.5122Epoch 13/15: [===============               ] 33/63 batches, loss: 0.5094Epoch 13/15: [================              ] 34/63 batches, loss: 0.5123Epoch 13/15: [================              ] 35/63 batches, loss: 0.5148Epoch 13/15: [=================             ] 36/63 batches, loss: 0.5146Epoch 13/15: [=================             ] 37/63 batches, loss: 0.5136Epoch 13/15: [==================            ] 38/63 batches, loss: 0.5146Epoch 13/15: [==================            ] 39/63 batches, loss: 0.5144Epoch 13/15: [===================           ] 40/63 batches, loss: 0.5135Epoch 13/15: [===================           ] 41/63 batches, loss: 0.5133Epoch 13/15: [====================          ] 42/63 batches, loss: 0.5133Epoch 13/15: [====================          ] 43/63 batches, loss: 0.5119Epoch 13/15: [====================          ] 44/63 batches, loss: 0.5117Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.5131Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.5121Epoch 13/15: [======================        ] 47/63 batches, loss: 0.5129Epoch 13/15: [======================        ] 48/63 batches, loss: 0.5148Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.5137Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.5138Epoch 13/15: [========================      ] 51/63 batches, loss: 0.5133Epoch 13/15: [========================      ] 52/63 batches, loss: 0.5138Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.5134Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.5138Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.5141Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.5145Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.5145Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.5151Epoch 13/15: [============================  ] 59/63 batches, loss: 0.5153Epoch 13/15: [============================  ] 60/63 batches, loss: 0.5156Epoch 13/15: [============================= ] 61/63 batches, loss: 0.5143Epoch 13/15: [============================= ] 62/63 batches, loss: 0.5159Epoch 13/15: [==============================] 63/63 batches, loss: 0.5167
[2025-05-02 12:45:05,090][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5167
[2025-05-02 12:45:05,336][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.5652, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.5069Epoch 14/15: [                              ] 2/63 batches, loss: 0.5224Epoch 14/15: [=                             ] 3/63 batches, loss: 0.5196Epoch 14/15: [=                             ] 4/63 batches, loss: 0.5139Epoch 14/15: [==                            ] 5/63 batches, loss: 0.5111Epoch 14/15: [==                            ] 6/63 batches, loss: 0.5175Epoch 14/15: [===                           ] 7/63 batches, loss: 0.5232Epoch 14/15: [===                           ] 8/63 batches, loss: 0.5257Epoch 14/15: [====                          ] 9/63 batches, loss: 0.5194Epoch 14/15: [====                          ] 10/63 batches, loss: 0.5220Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.5101Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.5071Epoch 14/15: [======                        ] 13/63 batches, loss: 0.5069Epoch 14/15: [======                        ] 14/63 batches, loss: 0.5157Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.5135Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.5104Epoch 14/15: [========                      ] 17/63 batches, loss: 0.5100Epoch 14/15: [========                      ] 18/63 batches, loss: 0.5137Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.5085Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.5109Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.5108Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.5107Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.5130Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.5139Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.5109Epoch 14/15: [============                  ] 26/63 batches, loss: 0.5139Epoch 14/15: [============                  ] 27/63 batches, loss: 0.5136Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.5135Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.5178Epoch 14/15: [==============                ] 30/63 batches, loss: 0.5168Epoch 14/15: [==============                ] 31/63 batches, loss: 0.5145Epoch 14/15: [===============               ] 32/63 batches, loss: 0.5121Epoch 14/15: [===============               ] 33/63 batches, loss: 0.5113Epoch 14/15: [================              ] 34/63 batches, loss: 0.5138Epoch 14/15: [================              ] 35/63 batches, loss: 0.5134Epoch 14/15: [=================             ] 36/63 batches, loss: 0.5132Epoch 14/15: [=================             ] 37/63 batches, loss: 0.5136Epoch 14/15: [==================            ] 38/63 batches, loss: 0.5123Epoch 14/15: [==================            ] 39/63 batches, loss: 0.5105Epoch 14/15: [===================           ] 40/63 batches, loss: 0.5091Epoch 14/15: [===================           ] 41/63 batches, loss: 0.5084Epoch 14/15: [====================          ] 42/63 batches, loss: 0.5071Epoch 14/15: [====================          ] 43/63 batches, loss: 0.5049Epoch 14/15: [====================          ] 44/63 batches, loss: 0.5044Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.5057Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.5045Epoch 14/15: [======================        ] 47/63 batches, loss: 0.5035Epoch 14/15: [======================        ] 48/63 batches, loss: 0.5048Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.5068Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.5070Epoch 14/15: [========================      ] 51/63 batches, loss: 0.5061Epoch 14/15: [========================      ] 52/63 batches, loss: 0.5085Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.5094Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.5106Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.5105Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.5115Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.5119Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.5130Epoch 14/15: [============================  ] 59/63 batches, loss: 0.5125Epoch 14/15: [============================  ] 60/63 batches, loss: 0.5140Epoch 14/15: [============================= ] 61/63 batches, loss: 0.5152Epoch 14/15: [============================= ] 62/63 batches, loss: 0.5155Epoch 14/15: [==============================] 63/63 batches, loss: 0.5123
[2025-05-02 12:45:07,720][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.5123
[2025-05-02 12:45:07,946][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.5652, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.5047Epoch 15/15: [                              ] 2/63 batches, loss: 0.4507Epoch 15/15: [=                             ] 3/63 batches, loss: 0.4785Epoch 15/15: [=                             ] 4/63 batches, loss: 0.4997Epoch 15/15: [==                            ] 5/63 batches, loss: 0.5113Epoch 15/15: [==                            ] 6/63 batches, loss: 0.5114Epoch 15/15: [===                           ] 7/63 batches, loss: 0.4983Epoch 15/15: [===                           ] 8/63 batches, loss: 0.5027Epoch 15/15: [====                          ] 9/63 batches, loss: 0.5051Epoch 15/15: [====                          ] 10/63 batches, loss: 0.5050Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.5045Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.5067Epoch 15/15: [======                        ] 13/63 batches, loss: 0.5050Epoch 15/15: [======                        ] 14/63 batches, loss: 0.5033Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.5039Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.5038Epoch 15/15: [========                      ] 17/63 batches, loss: 0.5058Epoch 15/15: [========                      ] 18/63 batches, loss: 0.5097Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.5138Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.5121Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.5115Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.5119Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.5127Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.5154Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.5154Epoch 15/15: [============                  ] 26/63 batches, loss: 0.5159Epoch 15/15: [============                  ] 27/63 batches, loss: 0.5161Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.5195Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.5173Epoch 15/15: [==============                ] 30/63 batches, loss: 0.5179Epoch 15/15: [==============                ] 31/63 batches, loss: 0.5198Epoch 15/15: [===============               ] 32/63 batches, loss: 0.5219Epoch 15/15: [===============               ] 33/63 batches, loss: 0.5235Epoch 15/15: [================              ] 34/63 batches, loss: 0.5241Epoch 15/15: [================              ] 35/63 batches, loss: 0.5237Epoch 15/15: [=================             ] 36/63 batches, loss: 0.5236Epoch 15/15: [=================             ] 37/63 batches, loss: 0.5207Epoch 15/15: [==================            ] 38/63 batches, loss: 0.5231Epoch 15/15: [==================            ] 39/63 batches, loss: 0.5228Epoch 15/15: [===================           ] 40/63 batches, loss: 0.5200Epoch 15/15: [===================           ] 41/63 batches, loss: 0.5208Epoch 15/15: [====================          ] 42/63 batches, loss: 0.5193Epoch 15/15: [====================          ] 43/63 batches, loss: 0.5185Epoch 15/15: [====================          ] 44/63 batches, loss: 0.5182Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.5165Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.5166Epoch 15/15: [======================        ] 47/63 batches, loss: 0.5168Epoch 15/15: [======================        ] 48/63 batches, loss: 0.5144Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.5133Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.5136Epoch 15/15: [========================      ] 51/63 batches, loss: 0.5144Epoch 15/15: [========================      ] 52/63 batches, loss: 0.5158Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.5170Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.5178Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.5155Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.5149Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.5154Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.5143Epoch 15/15: [============================  ] 59/63 batches, loss: 0.5148Epoch 15/15: [============================  ] 60/63 batches, loss: 0.5161Epoch 15/15: [============================= ] 61/63 batches, loss: 0.5162Epoch 15/15: [============================= ] 62/63 batches, loss: 0.5172Epoch 15/15: [==============================] 63/63 batches, loss: 0.5140
[2025-05-02 12:45:10,352][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.5140
[2025-05-02 12:45:10,585][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.5659, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
[2025-05-02 12:45:10,586][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
[2025-05-02 12:45:10,586][src.training.lm_trainer][INFO] - Training completed in 38.64 seconds
[2025-05-02 12:45:10,586][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 12:45:13,185][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 1.0, 'f1': 1.0, 'precision': 1.0, 'recall': 1.0}
[2025-05-02 12:45:13,185][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
[2025-05-02 12:45:13,185][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7142857142857143, 'f1': 0.6666666666666666, 'precision': 0.5, 'recall': 1.0}
[2025-05-02 12:45:14,871][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ar/ar/model.pt
[2025-05-02 12:45:14,873][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▄▇▇██████████
wandb:           best_val_f1 ▁▅▇███████████
wandb:         best_val_loss █▇▄▃▂▁▁▁▁▁▁▁▁▁
wandb:    best_val_precision ▁▆▇███████████
wandb:       best_val_recall ▁▅█▇██████████
wandb:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▂▂▃▃▃▃▃▃▃▃▃▃▃
wandb:            train_loss █▇▅▄▃▂▂▂▁▁▁▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▄▇▇███████████
wandb:                val_f1 ▁▅▇████████████
wandb:              val_loss █▇▄▃▂▁▁▁▁▁▁▁▁▁▁
wandb:         val_precision ▁▆▇████████████
wandb:            val_recall ▁▅█▇███████████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.93182
wandb:           best_val_f1 0.93023
wandb:         best_val_loss 0.56522
wandb:    best_val_precision 0.86957
wandb:       best_val_recall 1
wandb:                 epoch 15
wandb:   final_test_accuracy 0.71429
wandb:         final_test_f1 0.66667
wandb:  final_test_precision 0.5
wandb:     final_test_recall 1
wandb:  final_train_accuracy 1
wandb:        final_train_f1 1
wandb: final_train_precision 1
wandb:    final_train_recall 1
wandb:    final_val_accuracy 0.93182
wandb:          final_val_f1 0.93023
wandb:   final_val_precision 0.86957
wandb:      final_val_recall 1
wandb:         learning_rate 0.0001
wandb:            train_loss 0.51401
wandb:            train_time 38.64126
wandb:          val_accuracy 0.93182
wandb:                val_f1 0.93023
wandb:              val_loss 0.56586
wandb:         val_precision 0.86957
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_124420-fxneakfx
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_124420-fxneakfx/logs
Experiment probe_layer1_question_type_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer1/ar/ar/results.json for layer 1
Running experiment: probe_layer1_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=1"         "model.probe_hidden_size=96" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer1_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 12:45:26,538][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ar
experiment_name: probe_layer1_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
  probe_hidden_size: 96
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-02 12:45:26,538][__main__][INFO] - Normalized task: complexity
[2025-05-02 12:45:26,538][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 12:45:26,539][__main__][INFO] - Determined Task Type: regression
[2025-05-02 12:45:26,543][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-02 12:45:26,543][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 12:45:27,940][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 12:45:30,167][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 12:45:30,167][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:45:30,209][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:45:30,235][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:45:30,322][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 12:45:30,329][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:45:30,329][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 12:45:30,330][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:45:30,350][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:45:30,380][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:45:30,393][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 12:45:30,394][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:45:30,394][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 12:45:30,395][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:45:30,412][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:45:30,453][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:45:30,478][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 12:45:30,479][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:45:30,480][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 12:45:30,480][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 12:45:30,481][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 12:45:30,481][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 12:45:30,481][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 12:45:30,481][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 12:45:30,482][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:45:30,482][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-02 12:45:30,482][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 12:45:30,482][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-02 12:45:30,482][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 12:45:30,482][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 12:45:30,482][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 12:45:30,482][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 12:45:30,482][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:45:30,483][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-02 12:45:30,483][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 12:45:30,483][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-02 12:45:30,483][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 12:45:30,483][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 12:45:30,483][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 12:45:30,483][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 12:45:30,483][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:45:30,483][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-02 12:45:30,484][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 12:45:30,484][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-02 12:45:30,484][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 12:45:30,484][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 12:45:30,484][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 12:45:30,484][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-02 12:45:30,484][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 12:45:35,131][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 12:45:35,132][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 12:45:35,132][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=1, freeze_model=True
[2025-05-02 12:45:35,132][src.models.model_factory][INFO] - Using provided probe_hidden_size: 96
[2025-05-02 12:45:35,135][src.models.model_factory][INFO] - Model has 84,961 trainable parameters out of 394,206,433 total parameters
[2025-05-02 12:45:35,135][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 84,961 trainable parameters
[2025-05-02 12:45:35,135][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=96, depth=2, activation=silu, normalization=layer
[2025-05-02 12:45:35,135][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 96 hidden size
[2025-05-02 12:45:35,136][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 12:45:35,136][__main__][INFO] - Total parameters: 394,206,433
[2025-05-02 12:45:35,136][__main__][INFO] - Trainable parameters: 84,961 (0.02%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 2.5498Epoch 1/15: [                              ] 2/63 batches, loss: 2.7839Epoch 1/15: [=                             ] 3/63 batches, loss: 2.9872Epoch 1/15: [=                             ] 4/63 batches, loss: 3.2469Epoch 1/15: [==                            ] 5/63 batches, loss: 3.2605Epoch 1/15: [==                            ] 6/63 batches, loss: 3.1708Epoch 1/15: [===                           ] 7/63 batches, loss: 3.1050Epoch 1/15: [===                           ] 8/63 batches, loss: 3.0248Epoch 1/15: [====                          ] 9/63 batches, loss: 2.9032Epoch 1/15: [====                          ] 10/63 batches, loss: 2.9141Epoch 1/15: [=====                         ] 11/63 batches, loss: 2.8752Epoch 1/15: [=====                         ] 12/63 batches, loss: 2.8226Epoch 1/15: [======                        ] 13/63 batches, loss: 2.7173Epoch 1/15: [======                        ] 14/63 batches, loss: 2.6492Epoch 1/15: [=======                       ] 15/63 batches, loss: 2.5674Epoch 1/15: [=======                       ] 16/63 batches, loss: 2.5049Epoch 1/15: [========                      ] 17/63 batches, loss: 2.4410Epoch 1/15: [========                      ] 18/63 batches, loss: 2.3736Epoch 1/15: [=========                     ] 19/63 batches, loss: 2.3133Epoch 1/15: [=========                     ] 20/63 batches, loss: 2.2589Epoch 1/15: [==========                    ] 21/63 batches, loss: 2.1965Epoch 1/15: [==========                    ] 22/63 batches, loss: 2.1391Epoch 1/15: [==========                    ] 23/63 batches, loss: 2.1077Epoch 1/15: [===========                   ] 24/63 batches, loss: 2.0511Epoch 1/15: [===========                   ] 25/63 batches, loss: 2.0044Epoch 1/15: [============                  ] 26/63 batches, loss: 1.9595Epoch 1/15: [============                  ] 27/63 batches, loss: 1.9235Epoch 1/15: [=============                 ] 28/63 batches, loss: 1.8757Epoch 1/15: [=============                 ] 29/63 batches, loss: 1.8328Epoch 1/15: [==============                ] 30/63 batches, loss: 1.7960Epoch 1/15: [==============                ] 31/63 batches, loss: 1.7581Epoch 1/15: [===============               ] 32/63 batches, loss: 1.7166Epoch 1/15: [===============               ] 33/63 batches, loss: 1.6856Epoch 1/15: [================              ] 34/63 batches, loss: 1.6440Epoch 1/15: [================              ] 35/63 batches, loss: 1.6227Epoch 1/15: [=================             ] 36/63 batches, loss: 1.5892Epoch 1/15: [=================             ] 37/63 batches, loss: 1.5616Epoch 1/15: [==================            ] 38/63 batches, loss: 1.5428Epoch 1/15: [==================            ] 39/63 batches, loss: 1.5268Epoch 1/15: [===================           ] 40/63 batches, loss: 1.5128Epoch 1/15: [===================           ] 41/63 batches, loss: 1.4883Epoch 1/15: [====================          ] 42/63 batches, loss: 1.4731Epoch 1/15: [====================          ] 43/63 batches, loss: 1.4463Epoch 1/15: [====================          ] 44/63 batches, loss: 1.4300Epoch 1/15: [=====================         ] 45/63 batches, loss: 1.4054Epoch 1/15: [=====================         ] 46/63 batches, loss: 1.3877Epoch 1/15: [======================        ] 47/63 batches, loss: 1.3711Epoch 1/15: [======================        ] 48/63 batches, loss: 1.3474Epoch 1/15: [=======================       ] 49/63 batches, loss: 1.3290Epoch 1/15: [=======================       ] 50/63 batches, loss: 1.3142Epoch 1/15: [========================      ] 51/63 batches, loss: 1.2978Epoch 1/15: [========================      ] 52/63 batches, loss: 1.2800Epoch 1/15: [=========================     ] 53/63 batches, loss: 1.2623Epoch 1/15: [=========================     ] 54/63 batches, loss: 1.2496Epoch 1/15: [==========================    ] 55/63 batches, loss: 1.2353Epoch 1/15: [==========================    ] 56/63 batches, loss: 1.2238Epoch 1/15: [===========================   ] 57/63 batches, loss: 1.2135Epoch 1/15: [===========================   ] 58/63 batches, loss: 1.1994Epoch 1/15: [============================  ] 59/63 batches, loss: 1.1837Epoch 1/15: [============================  ] 60/63 batches, loss: 1.1716Epoch 1/15: [============================= ] 61/63 batches, loss: 1.1594Epoch 1/15: [============================= ] 62/63 batches, loss: 1.1473Epoch 1/15: [==============================] 63/63 batches, loss: 1.1341
[2025-05-02 12:45:39,650][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 1.1341
[2025-05-02 12:45:39,838][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.4722, Metrics: {'mse': 0.4513513743877411, 'rmse': 0.6718268931709575, 'r2': -5.9568257331848145}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.3925Epoch 2/15: [                              ] 2/63 batches, loss: 0.3650Epoch 2/15: [=                             ] 3/63 batches, loss: 0.3474Epoch 2/15: [=                             ] 4/63 batches, loss: 0.4019Epoch 2/15: [==                            ] 5/63 batches, loss: 0.4154Epoch 2/15: [==                            ] 6/63 batches, loss: 0.4163Epoch 2/15: [===                           ] 7/63 batches, loss: 0.4050Epoch 2/15: [===                           ] 8/63 batches, loss: 0.4186Epoch 2/15: [====                          ] 9/63 batches, loss: 0.4251Epoch 2/15: [====                          ] 10/63 batches, loss: 0.4677Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.4731Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.4782Epoch 2/15: [======                        ] 13/63 batches, loss: 0.4880Epoch 2/15: [======                        ] 14/63 batches, loss: 0.4871Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.4872Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.4878Epoch 2/15: [========                      ] 17/63 batches, loss: 0.4869Epoch 2/15: [========                      ] 18/63 batches, loss: 0.4830Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.4878Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.4982Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.4878Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.4835Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.4796Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.4810Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.4850Epoch 2/15: [============                  ] 26/63 batches, loss: 0.4875Epoch 2/15: [============                  ] 27/63 batches, loss: 0.4856Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.4880Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.4811Epoch 2/15: [==============                ] 30/63 batches, loss: 0.4882Epoch 2/15: [==============                ] 31/63 batches, loss: 0.4941Epoch 2/15: [===============               ] 32/63 batches, loss: 0.5042Epoch 2/15: [===============               ] 33/63 batches, loss: 0.5041Epoch 2/15: [================              ] 34/63 batches, loss: 0.5013Epoch 2/15: [================              ] 35/63 batches, loss: 0.4969Epoch 2/15: [=================             ] 36/63 batches, loss: 0.4931Epoch 2/15: [=================             ] 37/63 batches, loss: 0.4883Epoch 2/15: [==================            ] 38/63 batches, loss: 0.4880Epoch 2/15: [==================            ] 39/63 batches, loss: 0.4841Epoch 2/15: [===================           ] 40/63 batches, loss: 0.4855Epoch 2/15: [===================           ] 41/63 batches, loss: 0.4848Epoch 2/15: [====================          ] 42/63 batches, loss: 0.4778Epoch 2/15: [====================          ] 43/63 batches, loss: 0.4752Epoch 2/15: [====================          ] 44/63 batches, loss: 0.4748Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.4680Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.4685Epoch 2/15: [======================        ] 47/63 batches, loss: 0.4655Epoch 2/15: [======================        ] 48/63 batches, loss: 0.4700Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.4698Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.4687Epoch 2/15: [========================      ] 51/63 batches, loss: 0.4667Epoch 2/15: [========================      ] 52/63 batches, loss: 0.4641Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.4626Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.4580Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.4595Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.4613Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.4607Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.4592Epoch 2/15: [============================  ] 59/63 batches, loss: 0.4568Epoch 2/15: [============================  ] 60/63 batches, loss: 0.4525Epoch 2/15: [============================= ] 61/63 batches, loss: 0.4494Epoch 2/15: [============================= ] 62/63 batches, loss: 0.4465Epoch 2/15: [==============================] 63/63 batches, loss: 0.4430
[2025-05-02 12:45:42,174][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.4430
[2025-05-02 12:45:42,384][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.4027, Metrics: {'mse': 0.3837807774543762, 'rmse': 0.6195004257096005, 'r2': -4.915338039398193}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.3710Epoch 3/15: [                              ] 2/63 batches, loss: 0.3874Epoch 3/15: [=                             ] 3/63 batches, loss: 0.3697Epoch 3/15: [=                             ] 4/63 batches, loss: 0.3770Epoch 3/15: [==                            ] 5/63 batches, loss: 0.3928Epoch 3/15: [==                            ] 6/63 batches, loss: 0.4036Epoch 3/15: [===                           ] 7/63 batches, loss: 0.4056Epoch 3/15: [===                           ] 8/63 batches, loss: 0.3972Epoch 3/15: [====                          ] 9/63 batches, loss: 0.4089Epoch 3/15: [====                          ] 10/63 batches, loss: 0.4580Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.4610Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.4673Epoch 3/15: [======                        ] 13/63 batches, loss: 0.4473Epoch 3/15: [======                        ] 14/63 batches, loss: 0.4380Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.4453Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.4692Epoch 3/15: [========                      ] 17/63 batches, loss: 0.4777Epoch 3/15: [========                      ] 18/63 batches, loss: 0.4722Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.4713Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.4794Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.4876Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.4814Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.4685Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.4605Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.4553Epoch 3/15: [============                  ] 26/63 batches, loss: 0.4535Epoch 3/15: [============                  ] 27/63 batches, loss: 0.4517Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.4454Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.4378Epoch 3/15: [==============                ] 30/63 batches, loss: 0.4351Epoch 3/15: [==============                ] 31/63 batches, loss: 0.4316Epoch 3/15: [===============               ] 32/63 batches, loss: 0.4296Epoch 3/15: [===============               ] 33/63 batches, loss: 0.4372Epoch 3/15: [================              ] 34/63 batches, loss: 0.4390Epoch 3/15: [================              ] 35/63 batches, loss: 0.4335Epoch 3/15: [=================             ] 36/63 batches, loss: 0.4296Epoch 3/15: [=================             ] 37/63 batches, loss: 0.4261Epoch 3/15: [==================            ] 38/63 batches, loss: 0.4250Epoch 3/15: [==================            ] 39/63 batches, loss: 0.4212Epoch 3/15: [===================           ] 40/63 batches, loss: 0.4156Epoch 3/15: [===================           ] 41/63 batches, loss: 0.4143Epoch 3/15: [====================          ] 42/63 batches, loss: 0.4079Epoch 3/15: [====================          ] 43/63 batches, loss: 0.4098Epoch 3/15: [====================          ] 44/63 batches, loss: 0.4036Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.3986Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.3937Epoch 3/15: [======================        ] 47/63 batches, loss: 0.3918Epoch 3/15: [======================        ] 48/63 batches, loss: 0.3870Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.3910Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.3891Epoch 3/15: [========================      ] 51/63 batches, loss: 0.3866Epoch 3/15: [========================      ] 52/63 batches, loss: 0.3845Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.3853Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.3879Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.3839Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.3813Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.3815Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.3802Epoch 3/15: [============================  ] 59/63 batches, loss: 0.3774Epoch 3/15: [============================  ] 60/63 batches, loss: 0.3762Epoch 3/15: [============================= ] 61/63 batches, loss: 0.3748Epoch 3/15: [============================= ] 62/63 batches, loss: 0.3727Epoch 3/15: [==============================] 63/63 batches, loss: 0.3693
[2025-05-02 12:45:44,748][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.3693
[2025-05-02 12:45:44,965][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.3493, Metrics: {'mse': 0.33174929022789, 'rmse': 0.5759768139672725, 'r2': -4.1133599281311035}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.3489Epoch 4/15: [                              ] 2/63 batches, loss: 0.3856Epoch 4/15: [=                             ] 3/63 batches, loss: 0.3878Epoch 4/15: [=                             ] 4/63 batches, loss: 0.3661Epoch 4/15: [==                            ] 5/63 batches, loss: 0.3657Epoch 4/15: [==                            ] 6/63 batches, loss: 0.3675Epoch 4/15: [===                           ] 7/63 batches, loss: 0.3698Epoch 4/15: [===                           ] 8/63 batches, loss: 0.3683Epoch 4/15: [====                          ] 9/63 batches, loss: 0.3566Epoch 4/15: [====                          ] 10/63 batches, loss: 0.3497Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.3331Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.3245Epoch 4/15: [======                        ] 13/63 batches, loss: 0.3141Epoch 4/15: [======                        ] 14/63 batches, loss: 0.3186Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.3277Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.3229Epoch 4/15: [========                      ] 17/63 batches, loss: 0.3189Epoch 4/15: [========                      ] 18/63 batches, loss: 0.3182Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.3118Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.3081Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.3012Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.2972Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.2939Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.2955Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.3009Epoch 4/15: [============                  ] 26/63 batches, loss: 0.3105Epoch 4/15: [============                  ] 27/63 batches, loss: 0.3054Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.3093Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.3144Epoch 4/15: [==============                ] 30/63 batches, loss: 0.3131Epoch 4/15: [==============                ] 31/63 batches, loss: 0.3135Epoch 4/15: [===============               ] 32/63 batches, loss: 0.3132Epoch 4/15: [===============               ] 33/63 batches, loss: 0.3128Epoch 4/15: [================              ] 34/63 batches, loss: 0.3176Epoch 4/15: [================              ] 35/63 batches, loss: 0.3169Epoch 4/15: [=================             ] 36/63 batches, loss: 0.3169Epoch 4/15: [=================             ] 37/63 batches, loss: 0.3153Epoch 4/15: [==================            ] 38/63 batches, loss: 0.3139Epoch 4/15: [==================            ] 39/63 batches, loss: 0.3181Epoch 4/15: [===================           ] 40/63 batches, loss: 0.3215Epoch 4/15: [===================           ] 41/63 batches, loss: 0.3204Epoch 4/15: [====================          ] 42/63 batches, loss: 0.3184Epoch 4/15: [====================          ] 43/63 batches, loss: 0.3130Epoch 4/15: [====================          ] 44/63 batches, loss: 0.3191Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.3189Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.3203Epoch 4/15: [======================        ] 47/63 batches, loss: 0.3249Epoch 4/15: [======================        ] 48/63 batches, loss: 0.3273Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.3245Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.3255Epoch 4/15: [========================      ] 51/63 batches, loss: 0.3260Epoch 4/15: [========================      ] 52/63 batches, loss: 0.3261Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.3249Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.3240Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.3230Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.3217Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.3247Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.3229Epoch 4/15: [============================  ] 59/63 batches, loss: 0.3224Epoch 4/15: [============================  ] 60/63 batches, loss: 0.3191Epoch 4/15: [============================= ] 61/63 batches, loss: 0.3169Epoch 4/15: [============================= ] 62/63 batches, loss: 0.3163Epoch 4/15: [==============================] 63/63 batches, loss: 0.3165
[2025-05-02 12:45:47,236][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.3165
[2025-05-02 12:45:47,453][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.2619, Metrics: {'mse': 0.24721461534500122, 'rmse': 0.49720681345392004, 'r2': -2.8103995323181152}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.3628Epoch 5/15: [                              ] 2/63 batches, loss: 0.3136Epoch 5/15: [=                             ] 3/63 batches, loss: 0.2678Epoch 5/15: [=                             ] 4/63 batches, loss: 0.2540Epoch 5/15: [==                            ] 5/63 batches, loss: 0.2508Epoch 5/15: [==                            ] 6/63 batches, loss: 0.2811Epoch 5/15: [===                           ] 7/63 batches, loss: 0.2920Epoch 5/15: [===                           ] 8/63 batches, loss: 0.2716Epoch 5/15: [====                          ] 9/63 batches, loss: 0.2733Epoch 5/15: [====                          ] 10/63 batches, loss: 0.2743Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.2737Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.2717Epoch 5/15: [======                        ] 13/63 batches, loss: 0.2859Epoch 5/15: [======                        ] 14/63 batches, loss: 0.2925Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.2965Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.2912Epoch 5/15: [========                      ] 17/63 batches, loss: 0.2898Epoch 5/15: [========                      ] 18/63 batches, loss: 0.3159Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.3105Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.3127Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.3092Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.3060Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.3064Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.3049Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.3010Epoch 5/15: [============                  ] 26/63 batches, loss: 0.2998Epoch 5/15: [============                  ] 27/63 batches, loss: 0.3000Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.3006Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.2983Epoch 5/15: [==============                ] 30/63 batches, loss: 0.3004Epoch 5/15: [==============                ] 31/63 batches, loss: 0.3042Epoch 5/15: [===============               ] 32/63 batches, loss: 0.3020Epoch 5/15: [===============               ] 33/63 batches, loss: 0.3016Epoch 5/15: [================              ] 34/63 batches, loss: 0.3051Epoch 5/15: [================              ] 35/63 batches, loss: 0.3061Epoch 5/15: [=================             ] 36/63 batches, loss: 0.3086Epoch 5/15: [=================             ] 37/63 batches, loss: 0.3100Epoch 5/15: [==================            ] 38/63 batches, loss: 0.3063Epoch 5/15: [==================            ] 39/63 batches, loss: 0.3060Epoch 5/15: [===================           ] 40/63 batches, loss: 0.3068Epoch 5/15: [===================           ] 41/63 batches, loss: 0.3108Epoch 5/15: [====================          ] 42/63 batches, loss: 0.3093Epoch 5/15: [====================          ] 43/63 batches, loss: 0.3101Epoch 5/15: [====================          ] 44/63 batches, loss: 0.3067Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.3039Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.3000Epoch 5/15: [======================        ] 47/63 batches, loss: 0.2996Epoch 5/15: [======================        ] 48/63 batches, loss: 0.2992Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.2956Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.2941Epoch 5/15: [========================      ] 51/63 batches, loss: 0.2985Epoch 5/15: [========================      ] 52/63 batches, loss: 0.2972Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.2992Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.2990Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.3020Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.3001Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.3004Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.2976Epoch 5/15: [============================  ] 59/63 batches, loss: 0.2943Epoch 5/15: [============================  ] 60/63 batches, loss: 0.2914Epoch 5/15: [============================= ] 61/63 batches, loss: 0.2976Epoch 5/15: [============================= ] 62/63 batches, loss: 0.2945Epoch 5/15: [==============================] 63/63 batches, loss: 0.2902
[2025-05-02 12:45:49,738][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.2902
[2025-05-02 12:45:49,956][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.2306, Metrics: {'mse': 0.2168370634317398, 'rmse': 0.4656576676398015, 'r2': -2.3421802520751953}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.3043Epoch 6/15: [                              ] 2/63 batches, loss: 0.2912Epoch 6/15: [=                             ] 3/63 batches, loss: 0.3104Epoch 6/15: [=                             ] 4/63 batches, loss: 0.3432Epoch 6/15: [==                            ] 5/63 batches, loss: 0.3240Epoch 6/15: [==                            ] 6/63 batches, loss: 0.2852Epoch 6/15: [===                           ] 7/63 batches, loss: 0.2755Epoch 6/15: [===                           ] 8/63 batches, loss: 0.2696Epoch 6/15: [====                          ] 9/63 batches, loss: 0.2760Epoch 6/15: [====                          ] 10/63 batches, loss: 0.2692Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.2668Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.2671Epoch 6/15: [======                        ] 13/63 batches, loss: 0.2893Epoch 6/15: [======                        ] 14/63 batches, loss: 0.2890Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.2778Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.2723Epoch 6/15: [========                      ] 17/63 batches, loss: 0.2746Epoch 6/15: [========                      ] 18/63 batches, loss: 0.2745Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.2734Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.2724Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.2789Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.2763Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.2751Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.2714Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.2768Epoch 6/15: [============                  ] 26/63 batches, loss: 0.2709Epoch 6/15: [============                  ] 27/63 batches, loss: 0.2683Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.2687Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.2661Epoch 6/15: [==============                ] 30/63 batches, loss: 0.2734Epoch 6/15: [==============                ] 31/63 batches, loss: 0.2737Epoch 6/15: [===============               ] 32/63 batches, loss: 0.2729Epoch 6/15: [===============               ] 33/63 batches, loss: 0.2738Epoch 6/15: [================              ] 34/63 batches, loss: 0.2709Epoch 6/15: [================              ] 35/63 batches, loss: 0.2788Epoch 6/15: [=================             ] 36/63 batches, loss: 0.2759Epoch 6/15: [=================             ] 37/63 batches, loss: 0.2743Epoch 6/15: [==================            ] 38/63 batches, loss: 0.2740Epoch 6/15: [==================            ] 39/63 batches, loss: 0.2714Epoch 6/15: [===================           ] 40/63 batches, loss: 0.2712Epoch 6/15: [===================           ] 41/63 batches, loss: 0.2713Epoch 6/15: [====================          ] 42/63 batches, loss: 0.2739Epoch 6/15: [====================          ] 43/63 batches, loss: 0.2720Epoch 6/15: [====================          ] 44/63 batches, loss: 0.2721Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.2717Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.2674Epoch 6/15: [======================        ] 47/63 batches, loss: 0.2670Epoch 6/15: [======================        ] 48/63 batches, loss: 0.2663Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.2650Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.2703Epoch 6/15: [========================      ] 51/63 batches, loss: 0.2696Epoch 6/15: [========================      ] 52/63 batches, loss: 0.2694Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.2697Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.2689Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.2680Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.2691Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.2677Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.2668Epoch 6/15: [============================  ] 59/63 batches, loss: 0.2658Epoch 6/15: [============================  ] 60/63 batches, loss: 0.2676Epoch 6/15: [============================= ] 61/63 batches, loss: 0.2664Epoch 6/15: [============================= ] 62/63 batches, loss: 0.2640Epoch 6/15: [==============================] 63/63 batches, loss: 0.2661
[2025-05-02 12:45:52,257][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.2661
[2025-05-02 12:45:52,485][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1722, Metrics: {'mse': 0.16112330555915833, 'rmse': 0.4014016760791593, 'r2': -1.4834461212158203}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.1818Epoch 7/15: [                              ] 2/63 batches, loss: 0.2092Epoch 7/15: [=                             ] 3/63 batches, loss: 0.2507Epoch 7/15: [=                             ] 4/63 batches, loss: 0.3038Epoch 7/15: [==                            ] 5/63 batches, loss: 0.2799Epoch 7/15: [==                            ] 6/63 batches, loss: 0.2532Epoch 7/15: [===                           ] 7/63 batches, loss: 0.2479Epoch 7/15: [===                           ] 8/63 batches, loss: 0.2413Epoch 7/15: [====                          ] 9/63 batches, loss: 0.2320Epoch 7/15: [====                          ] 10/63 batches, loss: 0.2223Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.2345Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.2249Epoch 7/15: [======                        ] 13/63 batches, loss: 0.2224Epoch 7/15: [======                        ] 14/63 batches, loss: 0.2229Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.2231Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.2194Epoch 7/15: [========                      ] 17/63 batches, loss: 0.2145Epoch 7/15: [========                      ] 18/63 batches, loss: 0.2169Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.2139Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.2164Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.2188Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.2208Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.2210Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.2192Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.2137Epoch 7/15: [============                  ] 26/63 batches, loss: 0.2120Epoch 7/15: [============                  ] 27/63 batches, loss: 0.2120Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.2129Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.2106Epoch 7/15: [==============                ] 30/63 batches, loss: 0.2095Epoch 7/15: [==============                ] 31/63 batches, loss: 0.2073Epoch 7/15: [===============               ] 32/63 batches, loss: 0.2079Epoch 7/15: [===============               ] 33/63 batches, loss: 0.2071Epoch 7/15: [================              ] 34/63 batches, loss: 0.2069Epoch 7/15: [================              ] 35/63 batches, loss: 0.2119Epoch 7/15: [=================             ] 36/63 batches, loss: 0.2128Epoch 7/15: [=================             ] 37/63 batches, loss: 0.2131Epoch 7/15: [==================            ] 38/63 batches, loss: 0.2161Epoch 7/15: [==================            ] 39/63 batches, loss: 0.2208Epoch 7/15: [===================           ] 40/63 batches, loss: 0.2222Epoch 7/15: [===================           ] 41/63 batches, loss: 0.2246Epoch 7/15: [====================          ] 42/63 batches, loss: 0.2238Epoch 7/15: [====================          ] 43/63 batches, loss: 0.2230Epoch 7/15: [====================          ] 44/63 batches, loss: 0.2250Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.2236Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.2243Epoch 7/15: [======================        ] 47/63 batches, loss: 0.2223Epoch 7/15: [======================        ] 48/63 batches, loss: 0.2224Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.2212Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.2211Epoch 7/15: [========================      ] 51/63 batches, loss: 0.2200Epoch 7/15: [========================      ] 52/63 batches, loss: 0.2189Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.2191Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.2180Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.2236Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.2246Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.2255Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.2248Epoch 7/15: [============================  ] 59/63 batches, loss: 0.2235Epoch 7/15: [============================  ] 60/63 batches, loss: 0.2236Epoch 7/15: [============================= ] 61/63 batches, loss: 0.2229Epoch 7/15: [============================= ] 62/63 batches, loss: 0.2236Epoch 7/15: [==============================] 63/63 batches, loss: 0.2206
[2025-05-02 12:45:54,798][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.2206
[2025-05-02 12:45:55,016][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1768, Metrics: {'mse': 0.16580098867416382, 'rmse': 0.4071866754624515, 'r2': -1.5555446147918701}
[2025-05-02 12:45:55,016][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.2319Epoch 8/15: [                              ] 2/63 batches, loss: 0.1929Epoch 8/15: [=                             ] 3/63 batches, loss: 0.2381Epoch 8/15: [=                             ] 4/63 batches, loss: 0.2466Epoch 8/15: [==                            ] 5/63 batches, loss: 0.2297Epoch 8/15: [==                            ] 6/63 batches, loss: 0.2251Epoch 8/15: [===                           ] 7/63 batches, loss: 0.2097Epoch 8/15: [===                           ] 8/63 batches, loss: 0.2037Epoch 8/15: [====                          ] 9/63 batches, loss: 0.2059Epoch 8/15: [====                          ] 10/63 batches, loss: 0.2027Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.1968Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.1966Epoch 8/15: [======                        ] 13/63 batches, loss: 0.1937Epoch 8/15: [======                        ] 14/63 batches, loss: 0.1887Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.1882Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.1909Epoch 8/15: [========                      ] 17/63 batches, loss: 0.1876Epoch 8/15: [========                      ] 18/63 batches, loss: 0.1849Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.1846Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.1862Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.1876Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.1846Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.1863Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.1851Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.1871Epoch 8/15: [============                  ] 26/63 batches, loss: 0.1876Epoch 8/15: [============                  ] 27/63 batches, loss: 0.1898Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.1966Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.1936Epoch 8/15: [==============                ] 30/63 batches, loss: 0.1894Epoch 8/15: [==============                ] 31/63 batches, loss: 0.1890Epoch 8/15: [===============               ] 32/63 batches, loss: 0.1883Epoch 8/15: [===============               ] 33/63 batches, loss: 0.1899Epoch 8/15: [================              ] 34/63 batches, loss: 0.1875Epoch 8/15: [================              ] 35/63 batches, loss: 0.1873Epoch 8/15: [=================             ] 36/63 batches, loss: 0.1894Epoch 8/15: [=================             ] 37/63 batches, loss: 0.1879Epoch 8/15: [==================            ] 38/63 batches, loss: 0.1860Epoch 8/15: [==================            ] 39/63 batches, loss: 0.1859Epoch 8/15: [===================           ] 40/63 batches, loss: 0.1880Epoch 8/15: [===================           ] 41/63 batches, loss: 0.1883Epoch 8/15: [====================          ] 42/63 batches, loss: 0.1865Epoch 8/15: [====================          ] 43/63 batches, loss: 0.1868Epoch 8/15: [====================          ] 44/63 batches, loss: 0.1854Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.1848Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.1853Epoch 8/15: [======================        ] 47/63 batches, loss: 0.1855Epoch 8/15: [======================        ] 48/63 batches, loss: 0.1855Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.1853Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.1869Epoch 8/15: [========================      ] 51/63 batches, loss: 0.1858Epoch 8/15: [========================      ] 52/63 batches, loss: 0.1861Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.1871Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.1877Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.1858Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.1847Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.1845Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.1837Epoch 8/15: [============================  ] 59/63 batches, loss: 0.1826Epoch 8/15: [============================  ] 60/63 batches, loss: 0.1845Epoch 8/15: [============================= ] 61/63 batches, loss: 0.1837Epoch 8/15: [============================= ] 62/63 batches, loss: 0.1835Epoch 8/15: [==============================] 63/63 batches, loss: 0.1851
[2025-05-02 12:45:56,925][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.1851
[2025-05-02 12:45:57,142][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1694, Metrics: {'mse': 0.1589212715625763, 'rmse': 0.3986493089954833, 'r2': -1.44950532913208}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1785Epoch 9/15: [                              ] 2/63 batches, loss: 0.1510Epoch 9/15: [=                             ] 3/63 batches, loss: 0.1382Epoch 9/15: [=                             ] 4/63 batches, loss: 0.1408Epoch 9/15: [==                            ] 5/63 batches, loss: 0.1477Epoch 9/15: [==                            ] 6/63 batches, loss: 0.1497Epoch 9/15: [===                           ] 7/63 batches, loss: 0.1630Epoch 9/15: [===                           ] 8/63 batches, loss: 0.1632Epoch 9/15: [====                          ] 9/63 batches, loss: 0.1667Epoch 9/15: [====                          ] 10/63 batches, loss: 0.1644Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.1810Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.1808Epoch 9/15: [======                        ] 13/63 batches, loss: 0.1818Epoch 9/15: [======                        ] 14/63 batches, loss: 0.1886Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.1868Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.1818Epoch 9/15: [========                      ] 17/63 batches, loss: 0.1768Epoch 9/15: [========                      ] 18/63 batches, loss: 0.1729Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.1714Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.1747Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.1717Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.1684Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.1650Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.1630Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.1593Epoch 9/15: [============                  ] 26/63 batches, loss: 0.1606Epoch 9/15: [============                  ] 27/63 batches, loss: 0.1617Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.1586Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.1572Epoch 9/15: [==============                ] 30/63 batches, loss: 0.1566Epoch 9/15: [==============                ] 31/63 batches, loss: 0.1577Epoch 9/15: [===============               ] 32/63 batches, loss: 0.1581Epoch 9/15: [===============               ] 33/63 batches, loss: 0.1598Epoch 9/15: [================              ] 34/63 batches, loss: 0.1587Epoch 9/15: [================              ] 35/63 batches, loss: 0.1603Epoch 9/15: [=================             ] 36/63 batches, loss: 0.1603Epoch 9/15: [=================             ] 37/63 batches, loss: 0.1603Epoch 9/15: [==================            ] 38/63 batches, loss: 0.1613Epoch 9/15: [==================            ] 39/63 batches, loss: 0.1613Epoch 9/15: [===================           ] 40/63 batches, loss: 0.1595Epoch 9/15: [===================           ] 41/63 batches, loss: 0.1621Epoch 9/15: [====================          ] 42/63 batches, loss: 0.1601Epoch 9/15: [====================          ] 43/63 batches, loss: 0.1611Epoch 9/15: [====================          ] 44/63 batches, loss: 0.1608Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.1600Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.1619Epoch 9/15: [======================        ] 47/63 batches, loss: 0.1620Epoch 9/15: [======================        ] 48/63 batches, loss: 0.1637Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.1642Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.1637Epoch 9/15: [========================      ] 51/63 batches, loss: 0.1643Epoch 9/15: [========================      ] 52/63 batches, loss: 0.1637Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.1634Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.1631Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.1627Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.1628Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.1638Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.1634Epoch 9/15: [============================  ] 59/63 batches, loss: 0.1641Epoch 9/15: [============================  ] 60/63 batches, loss: 0.1648Epoch 9/15: [============================= ] 61/63 batches, loss: 0.1632Epoch 9/15: [============================= ] 62/63 batches, loss: 0.1626Epoch 9/15: [==============================] 63/63 batches, loss: 0.1608
[2025-05-02 12:45:59,548][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.1608
[2025-05-02 12:45:59,791][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.1579, Metrics: {'mse': 0.14807890355587006, 'rmse': 0.3848102175824728, 'r2': -1.28238844871521}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.1697Epoch 10/15: [                              ] 2/63 batches, loss: 0.1377Epoch 10/15: [=                             ] 3/63 batches, loss: 0.1376Epoch 10/15: [=                             ] 4/63 batches, loss: 0.1739Epoch 10/15: [==                            ] 5/63 batches, loss: 0.1951Epoch 10/15: [==                            ] 6/63 batches, loss: 0.2088Epoch 10/15: [===                           ] 7/63 batches, loss: 0.2049Epoch 10/15: [===                           ] 8/63 batches, loss: 0.1934Epoch 10/15: [====                          ] 9/63 batches, loss: 0.1872Epoch 10/15: [====                          ] 10/63 batches, loss: 0.1826Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.1829Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.1883Epoch 10/15: [======                        ] 13/63 batches, loss: 0.1864Epoch 10/15: [======                        ] 14/63 batches, loss: 0.1816Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.1811Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.1830Epoch 10/15: [========                      ] 17/63 batches, loss: 0.1801Epoch 10/15: [========                      ] 18/63 batches, loss: 0.1759Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.1722Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.1713Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.1706Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.1672Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.1646Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.1687Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.1671Epoch 10/15: [============                  ] 26/63 batches, loss: 0.1674Epoch 10/15: [============                  ] 27/63 batches, loss: 0.1711Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.1700Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.1686Epoch 10/15: [==============                ] 30/63 batches, loss: 0.1651Epoch 10/15: [==============                ] 31/63 batches, loss: 0.1683Epoch 10/15: [===============               ] 32/63 batches, loss: 0.1681Epoch 10/15: [===============               ] 33/63 batches, loss: 0.1677Epoch 10/15: [================              ] 34/63 batches, loss: 0.1683Epoch 10/15: [================              ] 35/63 batches, loss: 0.1682Epoch 10/15: [=================             ] 36/63 batches, loss: 0.1691Epoch 10/15: [=================             ] 37/63 batches, loss: 0.1672Epoch 10/15: [==================            ] 38/63 batches, loss: 0.1650Epoch 10/15: [==================            ] 39/63 batches, loss: 0.1659Epoch 10/15: [===================           ] 40/63 batches, loss: 0.1663Epoch 10/15: [===================           ] 41/63 batches, loss: 0.1659Epoch 10/15: [====================          ] 42/63 batches, loss: 0.1666Epoch 10/15: [====================          ] 43/63 batches, loss: 0.1658Epoch 10/15: [====================          ] 44/63 batches, loss: 0.1643Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.1621Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.1617Epoch 10/15: [======================        ] 47/63 batches, loss: 0.1600Epoch 10/15: [======================        ] 48/63 batches, loss: 0.1579Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.1587Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.1587Epoch 10/15: [========================      ] 51/63 batches, loss: 0.1587Epoch 10/15: [========================      ] 52/63 batches, loss: 0.1582Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.1571Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.1560Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.1566Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.1562Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.1555Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.1542Epoch 10/15: [============================  ] 59/63 batches, loss: 0.1545Epoch 10/15: [============================  ] 60/63 batches, loss: 0.1557Epoch 10/15: [============================= ] 61/63 batches, loss: 0.1549Epoch 10/15: [============================= ] 62/63 batches, loss: 0.1553Epoch 10/15: [==============================] 63/63 batches, loss: 0.1532
[2025-05-02 12:46:02,166][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.1532
[2025-05-02 12:46:02,390][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.1411, Metrics: {'mse': 0.13233613967895508, 'rmse': 0.3637803453719773, 'r2': -1.0397400856018066}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.1783Epoch 11/15: [                              ] 2/63 batches, loss: 0.2417Epoch 11/15: [=                             ] 3/63 batches, loss: 0.1878Epoch 11/15: [=                             ] 4/63 batches, loss: 0.1779Epoch 11/15: [==                            ] 5/63 batches, loss: 0.1723Epoch 11/15: [==                            ] 6/63 batches, loss: 0.1744Epoch 11/15: [===                           ] 7/63 batches, loss: 0.1676Epoch 11/15: [===                           ] 8/63 batches, loss: 0.1576Epoch 11/15: [====                          ] 9/63 batches, loss: 0.1520Epoch 11/15: [====                          ] 10/63 batches, loss: 0.1479Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.1448Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.1515Epoch 11/15: [======                        ] 13/63 batches, loss: 0.1505Epoch 11/15: [======                        ] 14/63 batches, loss: 0.1531Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.1471Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.1447Epoch 11/15: [========                      ] 17/63 batches, loss: 0.1434Epoch 11/15: [========                      ] 18/63 batches, loss: 0.1445Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.1427Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.1438Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.1451Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.1478Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.1506Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.1563Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.1526Epoch 11/15: [============                  ] 26/63 batches, loss: 0.1510Epoch 11/15: [============                  ] 27/63 batches, loss: 0.1522Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.1519Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.1534Epoch 11/15: [==============                ] 30/63 batches, loss: 0.1548Epoch 11/15: [==============                ] 31/63 batches, loss: 0.1532Epoch 11/15: [===============               ] 32/63 batches, loss: 0.1538Epoch 11/15: [===============               ] 33/63 batches, loss: 0.1515Epoch 11/15: [================              ] 34/63 batches, loss: 0.1502Epoch 11/15: [================              ] 35/63 batches, loss: 0.1482Epoch 11/15: [=================             ] 36/63 batches, loss: 0.1467Epoch 11/15: [=================             ] 37/63 batches, loss: 0.1457Epoch 11/15: [==================            ] 38/63 batches, loss: 0.1440Epoch 11/15: [==================            ] 39/63 batches, loss: 0.1450Epoch 11/15: [===================           ] 40/63 batches, loss: 0.1463Epoch 11/15: [===================           ] 41/63 batches, loss: 0.1465Epoch 11/15: [====================          ] 42/63 batches, loss: 0.1455Epoch 11/15: [====================          ] 43/63 batches, loss: 0.1446Epoch 11/15: [====================          ] 44/63 batches, loss: 0.1436Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.1463Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.1478Epoch 11/15: [======================        ] 47/63 batches, loss: 0.1517Epoch 11/15: [======================        ] 48/63 batches, loss: 0.1511Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.1513Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.1526Epoch 11/15: [========================      ] 51/63 batches, loss: 0.1521Epoch 11/15: [========================      ] 52/63 batches, loss: 0.1505Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.1524Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.1528Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.1536Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.1528Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.1530Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.1524Epoch 11/15: [============================  ] 59/63 batches, loss: 0.1519Epoch 11/15: [============================  ] 60/63 batches, loss: 0.1528Epoch 11/15: [============================= ] 61/63 batches, loss: 0.1527Epoch 11/15: [============================= ] 62/63 batches, loss: 0.1520Epoch 11/15: [==============================] 63/63 batches, loss: 0.1510
[2025-05-02 12:46:04,715][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.1510
[2025-05-02 12:46:04,945][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.1175, Metrics: {'mse': 0.1098225861787796, 'rmse': 0.3313949097055952, 'r2': -0.6927313804626465}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.1056Epoch 12/15: [                              ] 2/63 batches, loss: 0.1418Epoch 12/15: [=                             ] 3/63 batches, loss: 0.1379Epoch 12/15: [=                             ] 4/63 batches, loss: 0.1286Epoch 12/15: [==                            ] 5/63 batches, loss: 0.1137Epoch 12/15: [==                            ] 6/63 batches, loss: 0.1309Epoch 12/15: [===                           ] 7/63 batches, loss: 0.1226Epoch 12/15: [===                           ] 8/63 batches, loss: 0.1262Epoch 12/15: [====                          ] 9/63 batches, loss: 0.1249Epoch 12/15: [====                          ] 10/63 batches, loss: 0.1234Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.1256Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.1271Epoch 12/15: [======                        ] 13/63 batches, loss: 0.1283Epoch 12/15: [======                        ] 14/63 batches, loss: 0.1294Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.1344Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.1314Epoch 12/15: [========                      ] 17/63 batches, loss: 0.1313Epoch 12/15: [========                      ] 18/63 batches, loss: 0.1293Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.1339Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.1333Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.1358Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.1346Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.1341Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.1312Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.1307Epoch 12/15: [============                  ] 26/63 batches, loss: 0.1288Epoch 12/15: [============                  ] 27/63 batches, loss: 0.1264Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.1270Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.1263Epoch 12/15: [==============                ] 30/63 batches, loss: 0.1243Epoch 12/15: [==============                ] 31/63 batches, loss: 0.1263Epoch 12/15: [===============               ] 32/63 batches, loss: 0.1272Epoch 12/15: [===============               ] 33/63 batches, loss: 0.1287Epoch 12/15: [================              ] 34/63 batches, loss: 0.1285Epoch 12/15: [================              ] 35/63 batches, loss: 0.1283Epoch 12/15: [=================             ] 36/63 batches, loss: 0.1288Epoch 12/15: [=================             ] 37/63 batches, loss: 0.1304Epoch 12/15: [==================            ] 38/63 batches, loss: 0.1280Epoch 12/15: [==================            ] 39/63 batches, loss: 0.1293Epoch 12/15: [===================           ] 40/63 batches, loss: 0.1285Epoch 12/15: [===================           ] 41/63 batches, loss: 0.1290Epoch 12/15: [====================          ] 42/63 batches, loss: 0.1267Epoch 12/15: [====================          ] 43/63 batches, loss: 0.1252Epoch 12/15: [====================          ] 44/63 batches, loss: 0.1240Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.1249Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.1234Epoch 12/15: [======================        ] 47/63 batches, loss: 0.1233Epoch 12/15: [======================        ] 48/63 batches, loss: 0.1222Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.1211Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.1221Epoch 12/15: [========================      ] 51/63 batches, loss: 0.1239Epoch 12/15: [========================      ] 52/63 batches, loss: 0.1243Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.1242Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.1252Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.1252Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.1257Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.1248Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.1236Epoch 12/15: [============================  ] 59/63 batches, loss: 0.1239Epoch 12/15: [============================  ] 60/63 batches, loss: 0.1231Epoch 12/15: [============================= ] 61/63 batches, loss: 0.1232Epoch 12/15: [============================= ] 62/63 batches, loss: 0.1226Epoch 12/15: [==============================] 63/63 batches, loss: 0.1227
[2025-05-02 12:46:07,294][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.1227
[2025-05-02 12:46:07,542][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.1212, Metrics: {'mse': 0.11372198909521103, 'rmse': 0.3372269103959692, 'r2': -0.7528340816497803}
[2025-05-02 12:46:07,543][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.1291Epoch 13/15: [                              ] 2/63 batches, loss: 0.1409Epoch 13/15: [=                             ] 3/63 batches, loss: 0.1447Epoch 13/15: [=                             ] 4/63 batches, loss: 0.1306Epoch 13/15: [==                            ] 5/63 batches, loss: 0.1296Epoch 13/15: [==                            ] 6/63 batches, loss: 0.1204Epoch 13/15: [===                           ] 7/63 batches, loss: 0.1274Epoch 13/15: [===                           ] 8/63 batches, loss: 0.1204Epoch 13/15: [====                          ] 9/63 batches, loss: 0.1164Epoch 13/15: [====                          ] 10/63 batches, loss: 0.1122Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.1140Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.1122Epoch 13/15: [======                        ] 13/63 batches, loss: 0.1210Epoch 13/15: [======                        ] 14/63 batches, loss: 0.1199Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.1226Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.1198Epoch 13/15: [========                      ] 17/63 batches, loss: 0.1201Epoch 13/15: [========                      ] 18/63 batches, loss: 0.1207Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.1234Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.1274Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.1255Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.1246Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.1229Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.1228Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.1219Epoch 13/15: [============                  ] 26/63 batches, loss: 0.1224Epoch 13/15: [============                  ] 27/63 batches, loss: 0.1227Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.1230Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.1213Epoch 13/15: [==============                ] 30/63 batches, loss: 0.1200Epoch 13/15: [==============                ] 31/63 batches, loss: 0.1201Epoch 13/15: [===============               ] 32/63 batches, loss: 0.1182Epoch 13/15: [===============               ] 33/63 batches, loss: 0.1164Epoch 13/15: [================              ] 34/63 batches, loss: 0.1142Epoch 13/15: [================              ] 35/63 batches, loss: 0.1151Epoch 13/15: [=================             ] 36/63 batches, loss: 0.1133Epoch 13/15: [=================             ] 37/63 batches, loss: 0.1130Epoch 13/15: [==================            ] 38/63 batches, loss: 0.1123Epoch 13/15: [==================            ] 39/63 batches, loss: 0.1109Epoch 13/15: [===================           ] 40/63 batches, loss: 0.1102Epoch 13/15: [===================           ] 41/63 batches, loss: 0.1117Epoch 13/15: [====================          ] 42/63 batches, loss: 0.1115Epoch 13/15: [====================          ] 43/63 batches, loss: 0.1114Epoch 13/15: [====================          ] 44/63 batches, loss: 0.1106Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.1115Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.1111Epoch 13/15: [======================        ] 47/63 batches, loss: 0.1116Epoch 13/15: [======================        ] 48/63 batches, loss: 0.1117Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.1113Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.1106Epoch 13/15: [========================      ] 51/63 batches, loss: 0.1097Epoch 13/15: [========================      ] 52/63 batches, loss: 0.1094Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.1086Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.1088Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.1088Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.1090Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.1087Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.1092Epoch 13/15: [============================  ] 59/63 batches, loss: 0.1089Epoch 13/15: [============================  ] 60/63 batches, loss: 0.1088Epoch 13/15: [============================= ] 61/63 batches, loss: 0.1097Epoch 13/15: [============================= ] 62/63 batches, loss: 0.1101Epoch 13/15: [==============================] 63/63 batches, loss: 0.1088
[2025-05-02 12:46:09,501][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.1088
[2025-05-02 12:46:09,741][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.1108, Metrics: {'mse': 0.10391947627067566, 'rmse': 0.3223654390139794, 'r2': -0.6017446517944336}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.1016Epoch 14/15: [                              ] 2/63 batches, loss: 0.1115Epoch 14/15: [=                             ] 3/63 batches, loss: 0.0946Epoch 14/15: [=                             ] 4/63 batches, loss: 0.1032Epoch 14/15: [==                            ] 5/63 batches, loss: 0.0961Epoch 14/15: [==                            ] 6/63 batches, loss: 0.1002Epoch 14/15: [===                           ] 7/63 batches, loss: 0.0969Epoch 14/15: [===                           ] 8/63 batches, loss: 0.0968Epoch 14/15: [====                          ] 9/63 batches, loss: 0.0985Epoch 14/15: [====                          ] 10/63 batches, loss: 0.0994Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.0985Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.0951Epoch 14/15: [======                        ] 13/63 batches, loss: 0.0977Epoch 14/15: [======                        ] 14/63 batches, loss: 0.0997Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.1028Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.1012Epoch 14/15: [========                      ] 17/63 batches, loss: 0.1010Epoch 14/15: [========                      ] 18/63 batches, loss: 0.1055Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.1059Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.1052Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.1079Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.1064Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.1059Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.1041Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.1058Epoch 14/15: [============                  ] 26/63 batches, loss: 0.1043Epoch 14/15: [============                  ] 27/63 batches, loss: 0.1033Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.1027Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.1037Epoch 14/15: [==============                ] 30/63 batches, loss: 0.1035Epoch 14/15: [==============                ] 31/63 batches, loss: 0.1050Epoch 14/15: [===============               ] 32/63 batches, loss: 0.1041Epoch 14/15: [===============               ] 33/63 batches, loss: 0.1016Epoch 14/15: [================              ] 34/63 batches, loss: 0.1011Epoch 14/15: [================              ] 35/63 batches, loss: 0.1009Epoch 14/15: [=================             ] 36/63 batches, loss: 0.1006Epoch 14/15: [=================             ] 37/63 batches, loss: 0.0997Epoch 14/15: [==================            ] 38/63 batches, loss: 0.0991Epoch 14/15: [==================            ] 39/63 batches, loss: 0.0986Epoch 14/15: [===================           ] 40/63 batches, loss: 0.1002Epoch 14/15: [===================           ] 41/63 batches, loss: 0.0992Epoch 14/15: [====================          ] 42/63 batches, loss: 0.0993Epoch 14/15: [====================          ] 43/63 batches, loss: 0.1003Epoch 14/15: [====================          ] 44/63 batches, loss: 0.0997Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.1003Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.1003Epoch 14/15: [======================        ] 47/63 batches, loss: 0.0998Epoch 14/15: [======================        ] 48/63 batches, loss: 0.1003Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.0991Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.1003Epoch 14/15: [========================      ] 51/63 batches, loss: 0.1023Epoch 14/15: [========================      ] 52/63 batches, loss: 0.1011Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.1012Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.1012Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.1012Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.1015Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.1015Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.1027Epoch 14/15: [============================  ] 59/63 batches, loss: 0.1029Epoch 14/15: [============================  ] 60/63 batches, loss: 0.1020Epoch 14/15: [============================= ] 61/63 batches, loss: 0.1016Epoch 14/15: [============================= ] 62/63 batches, loss: 0.1011Epoch 14/15: [==============================] 63/63 batches, loss: 0.0998
[2025-05-02 12:46:12,114][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0998
[2025-05-02 12:46:12,338][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.1114, Metrics: {'mse': 0.10482096672058105, 'rmse': 0.3237606627133399, 'r2': -0.6156396865844727}
[2025-05-02 12:46:12,338][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.1003Epoch 15/15: [                              ] 2/63 batches, loss: 0.1274Epoch 15/15: [=                             ] 3/63 batches, loss: 0.1301Epoch 15/15: [=                             ] 4/63 batches, loss: 0.1095Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0999Epoch 15/15: [==                            ] 6/63 batches, loss: 0.1044Epoch 15/15: [===                           ] 7/63 batches, loss: 0.0968Epoch 15/15: [===                           ] 8/63 batches, loss: 0.0922Epoch 15/15: [====                          ] 9/63 batches, loss: 0.0873Epoch 15/15: [====                          ] 10/63 batches, loss: 0.0909Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.0909Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.0902Epoch 15/15: [======                        ] 13/63 batches, loss: 0.0887Epoch 15/15: [======                        ] 14/63 batches, loss: 0.0913Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.0950Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.0983Epoch 15/15: [========                      ] 17/63 batches, loss: 0.0980Epoch 15/15: [========                      ] 18/63 batches, loss: 0.0982Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.1001Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.1013Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.1001Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.0967Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.0944Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.0952Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.0954Epoch 15/15: [============                  ] 26/63 batches, loss: 0.0937Epoch 15/15: [============                  ] 27/63 batches, loss: 0.0944Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.0937Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.0932Epoch 15/15: [==============                ] 30/63 batches, loss: 0.0932Epoch 15/15: [==============                ] 31/63 batches, loss: 0.0930Epoch 15/15: [===============               ] 32/63 batches, loss: 0.0924Epoch 15/15: [===============               ] 33/63 batches, loss: 0.0931Epoch 15/15: [================              ] 34/63 batches, loss: 0.0949Epoch 15/15: [================              ] 35/63 batches, loss: 0.0940Epoch 15/15: [=================             ] 36/63 batches, loss: 0.0935Epoch 15/15: [=================             ] 37/63 batches, loss: 0.0933Epoch 15/15: [==================            ] 38/63 batches, loss: 0.0932Epoch 15/15: [==================            ] 39/63 batches, loss: 0.0929Epoch 15/15: [===================           ] 40/63 batches, loss: 0.0931Epoch 15/15: [===================           ] 41/63 batches, loss: 0.0926Epoch 15/15: [====================          ] 42/63 batches, loss: 0.0918Epoch 15/15: [====================          ] 43/63 batches, loss: 0.0938Epoch 15/15: [====================          ] 44/63 batches, loss: 0.0930Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.0918Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.0918Epoch 15/15: [======================        ] 47/63 batches, loss: 0.0923Epoch 15/15: [======================        ] 48/63 batches, loss: 0.0915Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.0912Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.0917Epoch 15/15: [========================      ] 51/63 batches, loss: 0.0928Epoch 15/15: [========================      ] 52/63 batches, loss: 0.0931Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.0929Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.0929Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.0928Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.0937Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.0934Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.0939Epoch 15/15: [============================  ] 59/63 batches, loss: 0.0934Epoch 15/15: [============================  ] 60/63 batches, loss: 0.0943Epoch 15/15: [============================= ] 61/63 batches, loss: 0.0941Epoch 15/15: [============================= ] 62/63 batches, loss: 0.0941Epoch 15/15: [==============================] 63/63 batches, loss: 0.0935
[2025-05-02 12:46:14,285][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0935
[2025-05-02 12:46:14,512][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.1081, Metrics: {'mse': 0.10195743292570114, 'rmse': 0.3193077401593972, 'r2': -0.5715030431747437}
[2025-05-02 12:46:14,960][src.training.lm_trainer][INFO] - Training completed in 38.33 seconds
[2025-05-02 12:46:14,960][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 12:46:17,495][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.05439286679029465, 'rmse': 0.2332227836003478, 'r2': -0.771906852722168}
[2025-05-02 12:46:17,496][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.10195743292570114, 'rmse': 0.3193077401593972, 'r2': -0.5715030431747437}
[2025-05-02 12:46:17,496][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.0938773825764656, 'rmse': 0.3063941621122465, 'r2': -0.6184042692184448}
[2025-05-02 12:46:19,151][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ar/ar/model.pt
[2025-05-02 12:46:19,153][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▆▄▃▂▂▂▂▁▁▁
wandb:     best_val_mse █▇▆▄▃▂▂▂▂▁▁▁
wandb:      best_val_r2 ▁▂▃▅▆▇▇▇▇███
wandb:    best_val_rmse █▇▆▅▄▃▃▂▂▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▃▄▆▆▇▇▇▇▇████
wandb:       train_loss █▃▃▃▂▂▂▂▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▇▆▄▃▂▂▂▂▂▁▁▁▁▁
wandb:          val_mse █▇▆▄▃▂▂▂▂▂▁▁▁▁▁
wandb:           val_r2 ▁▂▃▅▆▇▇▇▇▇█████
wandb:         val_rmse █▇▆▅▄▃▃▃▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.10808
wandb:     best_val_mse 0.10196
wandb:      best_val_r2 -0.5715
wandb:    best_val_rmse 0.31931
wandb:            epoch 15
wandb:   final_test_mse 0.09388
wandb:    final_test_r2 -0.6184
wandb:  final_test_rmse 0.30639
wandb:  final_train_mse 0.05439
wandb:   final_train_r2 -0.77191
wandb: final_train_rmse 0.23322
wandb:    final_val_mse 0.10196
wandb:     final_val_r2 -0.5715
wandb:   final_val_rmse 0.31931
wandb:    learning_rate 2e-05
wandb:       train_loss 0.09346
wandb:       train_time 38.32673
wandb:         val_loss 0.10808
wandb:          val_mse 0.10196
wandb:           val_r2 -0.5715
wandb:         val_rmse 0.31931
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_124526-tiruth4g
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_124526-tiruth4g/logs
Experiment probe_layer1_complexity_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer1/ar/ar/results.json for layer 1
=======================
PROBING LAYER 4
=======================
Running experiment: probe_layer4_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=4"         "model.probe_hidden_size=128" "model.probe_depth=2" "model.dropout=0.2" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer4_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer4/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 12:46:30,563][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer4/ar
experiment_name: probe_layer4_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 12:46:30,563][__main__][INFO] - Normalized task: question_type
[2025-05-02 12:46:30,563][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 12:46:30,563][__main__][INFO] - Determined Task Type: classification
[2025-05-02 12:46:30,567][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-02 12:46:30,567][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 12:46:32,093][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 12:46:34,330][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 12:46:34,330][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:46:34,373][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:46:34,402][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:46:34,499][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 12:46:34,506][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:46:34,507][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 12:46:34,508][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:46:34,538][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:46:34,577][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:46:34,591][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 12:46:34,592][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:46:34,592][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 12:46:34,593][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:46:34,612][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:46:34,644][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:46:34,658][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 12:46:34,659][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:46:34,659][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 12:46:34,660][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 12:46:34,660][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 12:46:34,660][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 12:46:34,661][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 12:46:34,661][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 12:46:34,661][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-02 12:46:34,661][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-02 12:46:34,661][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 12:46:34,661][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 12:46:34,661][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 12:46:34,661][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 12:46:34,661][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 12:46:34,662][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 12:46:34,662][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-02 12:46:34,662][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-02 12:46:34,662][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 12:46:34,662][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 12:46:34,662][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 12:46:34,662][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 12:46:34,662][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 12:46:34,662][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 12:46:34,662][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-02 12:46:34,662][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-02 12:46:34,663][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 12:46:34,663][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 12:46:34,663][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 12:46:34,663][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 12:46:34,663][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 12:46:34,663][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 12:46:34,663][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 12:46:38,983][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 12:46:38,984][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 12:46:38,984][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=4, freeze_model=True
[2025-05-02 12:46:38,984][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-02 12:46:38,987][src.models.model_factory][INFO] - Model has 116,865 trainable parameters out of 394,238,337 total parameters
[2025-05-02 12:46:38,987][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 116,865 trainable parameters
[2025-05-02 12:46:38,987][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=2, activation=gelu, normalization=layer
[2025-05-02 12:46:38,988][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 128 hidden size
[2025-05-02 12:46:38,988][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 12:46:38,988][__main__][INFO] - Total parameters: 394,238,337
[2025-05-02 12:46:38,989][__main__][INFO] - Trainable parameters: 116,865 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7433Epoch 1/15: [                              ] 2/63 batches, loss: 0.7281Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7221Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7247Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7247Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7175Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7108Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7150Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7095Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7050Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7040Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7039Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7035Epoch 1/15: [======                        ] 14/63 batches, loss: 0.7020Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.7025Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.7013Epoch 1/15: [========                      ] 17/63 batches, loss: 0.7004Epoch 1/15: [========                      ] 18/63 batches, loss: 0.7011Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.7002Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.7005Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.7001Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.7012Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.7004Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6997Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.7006Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6982Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6987Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6988Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6982Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6997Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6997Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6999Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6994Epoch 1/15: [================              ] 34/63 batches, loss: 0.6981Epoch 1/15: [================              ] 35/63 batches, loss: 0.6978Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6978Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6974Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6972Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6969Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6961Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6966Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6962Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6966Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6967Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6960Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6958Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6963Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6953Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6950Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6947Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6939Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6936Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6940Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6943Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6937Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6937Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6936Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6939Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6934Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6933Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6935Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6935Epoch 1/15: [==============================] 63/63 batches, loss: 0.6911
[2025-05-02 12:46:43,223][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6911
[2025-05-02 12:46:43,418][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6976, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6950Epoch 2/15: [                              ] 2/63 batches, loss: 0.6989Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6999Epoch 2/15: [=                             ] 4/63 batches, loss: 0.7038Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6931Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6940Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6881Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6856Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6885Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6893Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6873Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6894Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6911Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6935Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6947Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6954Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6949Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6934Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6934Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6935Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6922Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6903Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6893Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6874Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6887Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6903Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6899Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6900Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6891Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6893Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6892Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6878Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6871Epoch 2/15: [================              ] 34/63 batches, loss: 0.6870Epoch 2/15: [================              ] 35/63 batches, loss: 0.6873Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6871Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6852Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6850Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6846Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6849Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6836Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6837Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6844Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6847Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6845Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6850Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6848Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6857Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6863Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6872Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6870Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6871Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6868Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6861Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6856Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6855Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6855Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6844Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6847Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6841Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6832Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6836Epoch 2/15: [==============================] 63/63 batches, loss: 0.6822
[2025-05-02 12:46:45,758][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6822
[2025-05-02 12:46:45,962][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6955, Metrics: {'accuracy': 0.5, 'f1': 0.08333333333333333, 'precision': 0.25, 'recall': 0.05}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6893Epoch 3/15: [                              ] 2/63 batches, loss: 0.6846Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6875Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6789Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6681Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6653Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6683Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6709Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6686Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6633Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6606Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6607Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6624Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6586Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6611Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6639Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6653Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6661Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6631Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6647Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6618Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6612Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6602Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6592Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6602Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6587Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6609Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6629Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6618Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6589Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6602Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6580Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6573Epoch 3/15: [================              ] 34/63 batches, loss: 0.6588Epoch 3/15: [================              ] 35/63 batches, loss: 0.6611Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6627Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6627Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6640Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6649Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6650Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6664Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6655Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6650Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6649Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6650Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6644Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6634Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6632Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6635Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6642Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6645Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6644Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6642Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6630Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6627Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6614Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6613Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6612Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6610Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6600Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6596Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6585Epoch 3/15: [==============================] 63/63 batches, loss: 0.6608
[2025-05-02 12:46:48,310][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6608
[2025-05-02 12:46:48,524][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6772, Metrics: {'accuracy': 0.6818181818181818, 'f1': 0.6111111111111112, 'precision': 0.6875, 'recall': 0.55}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.5595Epoch 4/15: [                              ] 2/63 batches, loss: 0.5709Epoch 4/15: [=                             ] 3/63 batches, loss: 0.5961Epoch 4/15: [=                             ] 4/63 batches, loss: 0.6066Epoch 4/15: [==                            ] 5/63 batches, loss: 0.6178Epoch 4/15: [==                            ] 6/63 batches, loss: 0.6098Epoch 4/15: [===                           ] 7/63 batches, loss: 0.6172Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6296Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6350Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6385Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6360Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6312Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6343Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6300Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6315Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6346Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6312Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6313Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6309Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6326Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6308Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6342Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6356Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6382Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6385Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6343Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6329Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6314Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6303Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6335Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6331Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6313Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6284Epoch 4/15: [================              ] 34/63 batches, loss: 0.6266Epoch 4/15: [================              ] 35/63 batches, loss: 0.6268Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6254Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6246Epoch 4/15: [==================            ] 38/63 batches, loss: 0.6229Epoch 4/15: [==================            ] 39/63 batches, loss: 0.6237Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6242Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6266Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6258Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6258Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6253Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.6277Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.6264Epoch 4/15: [======================        ] 47/63 batches, loss: 0.6257Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6258Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6262Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6269Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6277Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6286Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6298Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6310Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6309Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6299Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6297Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6307Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6313Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6303Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6299Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6297Epoch 4/15: [==============================] 63/63 batches, loss: 0.6287
[2025-05-02 12:46:50,803][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6287
[2025-05-02 12:46:51,038][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6592, Metrics: {'accuracy': 0.7045454545454546, 'f1': 0.6285714285714286, 'precision': 0.7333333333333333, 'recall': 0.55}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.5814Epoch 5/15: [                              ] 2/63 batches, loss: 0.5835Epoch 5/15: [=                             ] 3/63 batches, loss: 0.5725Epoch 5/15: [=                             ] 4/63 batches, loss: 0.6049Epoch 5/15: [==                            ] 5/63 batches, loss: 0.6033Epoch 5/15: [==                            ] 6/63 batches, loss: 0.6031Epoch 5/15: [===                           ] 7/63 batches, loss: 0.6005Epoch 5/15: [===                           ] 8/63 batches, loss: 0.6021Epoch 5/15: [====                          ] 9/63 batches, loss: 0.5994Epoch 5/15: [====                          ] 10/63 batches, loss: 0.6057Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.6119Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.6122Epoch 5/15: [======                        ] 13/63 batches, loss: 0.6126Epoch 5/15: [======                        ] 14/63 batches, loss: 0.6169Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.6140Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.6125Epoch 5/15: [========                      ] 17/63 batches, loss: 0.6107Epoch 5/15: [========                      ] 18/63 batches, loss: 0.6090Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.6120Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.6087Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.6131Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.6156Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.6182Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.6156Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.6144Epoch 5/15: [============                  ] 26/63 batches, loss: 0.6144Epoch 5/15: [============                  ] 27/63 batches, loss: 0.6140Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.6183Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.6201Epoch 5/15: [==============                ] 30/63 batches, loss: 0.6182Epoch 5/15: [==============                ] 31/63 batches, loss: 0.6184Epoch 5/15: [===============               ] 32/63 batches, loss: 0.6176Epoch 5/15: [===============               ] 33/63 batches, loss: 0.6164Epoch 5/15: [================              ] 34/63 batches, loss: 0.6178Epoch 5/15: [================              ] 35/63 batches, loss: 0.6151Epoch 5/15: [=================             ] 36/63 batches, loss: 0.6158Epoch 5/15: [=================             ] 37/63 batches, loss: 0.6153Epoch 5/15: [==================            ] 38/63 batches, loss: 0.6161Epoch 5/15: [==================            ] 39/63 batches, loss: 0.6164Epoch 5/15: [===================           ] 40/63 batches, loss: 0.6168Epoch 5/15: [===================           ] 41/63 batches, loss: 0.6162Epoch 5/15: [====================          ] 42/63 batches, loss: 0.6151Epoch 5/15: [====================          ] 43/63 batches, loss: 0.6148Epoch 5/15: [====================          ] 44/63 batches, loss: 0.6143Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.6143Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.6158Epoch 5/15: [======================        ] 47/63 batches, loss: 0.6148Epoch 5/15: [======================        ] 48/63 batches, loss: 0.6160Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.6163Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.6161Epoch 5/15: [========================      ] 51/63 batches, loss: 0.6164Epoch 5/15: [========================      ] 52/63 batches, loss: 0.6143Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.6135Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.6131Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.6130Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.6130Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.6112Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.6098Epoch 5/15: [============================  ] 59/63 batches, loss: 0.6087Epoch 5/15: [============================  ] 60/63 batches, loss: 0.6087Epoch 5/15: [============================= ] 61/63 batches, loss: 0.6075Epoch 5/15: [============================= ] 62/63 batches, loss: 0.6068Epoch 5/15: [==============================] 63/63 batches, loss: 0.6103
[2025-05-02 12:46:53,322][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.6103
[2025-05-02 12:46:53,553][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6249, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8636363636363636, 'precision': 0.7916666666666666, 'recall': 0.95}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.5609Epoch 6/15: [                              ] 2/63 batches, loss: 0.6135Epoch 6/15: [=                             ] 3/63 batches, loss: 0.6015Epoch 6/15: [=                             ] 4/63 batches, loss: 0.6027Epoch 6/15: [==                            ] 5/63 batches, loss: 0.6119Epoch 6/15: [==                            ] 6/63 batches, loss: 0.5975Epoch 6/15: [===                           ] 7/63 batches, loss: 0.5977Epoch 6/15: [===                           ] 8/63 batches, loss: 0.5966Epoch 6/15: [====                          ] 9/63 batches, loss: 0.6019Epoch 6/15: [====                          ] 10/63 batches, loss: 0.5962Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.5945Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.5894Epoch 6/15: [======                        ] 13/63 batches, loss: 0.5971Epoch 6/15: [======                        ] 14/63 batches, loss: 0.5935Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.5898Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.5879Epoch 6/15: [========                      ] 17/63 batches, loss: 0.5873Epoch 6/15: [========                      ] 18/63 batches, loss: 0.5924Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.5878Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.5895Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.5919Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.5943Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.5900Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.5883Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.5860Epoch 6/15: [============                  ] 26/63 batches, loss: 0.5857Epoch 6/15: [============                  ] 27/63 batches, loss: 0.5850Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.5856Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.5848Epoch 6/15: [==============                ] 30/63 batches, loss: 0.5838Epoch 6/15: [==============                ] 31/63 batches, loss: 0.5832Epoch 6/15: [===============               ] 32/63 batches, loss: 0.5838Epoch 6/15: [===============               ] 33/63 batches, loss: 0.5838Epoch 6/15: [================              ] 34/63 batches, loss: 0.5849Epoch 6/15: [================              ] 35/63 batches, loss: 0.5842Epoch 6/15: [=================             ] 36/63 batches, loss: 0.5817Epoch 6/15: [=================             ] 37/63 batches, loss: 0.5834Epoch 6/15: [==================            ] 38/63 batches, loss: 0.5840Epoch 6/15: [==================            ] 39/63 batches, loss: 0.5853Epoch 6/15: [===================           ] 40/63 batches, loss: 0.5864Epoch 6/15: [===================           ] 41/63 batches, loss: 0.5856Epoch 6/15: [====================          ] 42/63 batches, loss: 0.5825Epoch 6/15: [====================          ] 43/63 batches, loss: 0.5821Epoch 6/15: [====================          ] 44/63 batches, loss: 0.5799Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.5804Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.5816Epoch 6/15: [======================        ] 47/63 batches, loss: 0.5825Epoch 6/15: [======================        ] 48/63 batches, loss: 0.5829Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.5828Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.5830Epoch 6/15: [========================      ] 51/63 batches, loss: 0.5846Epoch 6/15: [========================      ] 52/63 batches, loss: 0.5838Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.5832Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.5840Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.5820Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.5827Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.5832Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.5850Epoch 6/15: [============================  ] 59/63 batches, loss: 0.5857Epoch 6/15: [============================  ] 60/63 batches, loss: 0.5867Epoch 6/15: [============================= ] 61/63 batches, loss: 0.5876Epoch 6/15: [============================= ] 62/63 batches, loss: 0.5878Epoch 6/15: [==============================] 63/63 batches, loss: 0.5883
[2025-05-02 12:46:55,855][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5883
[2025-05-02 12:46:56,084][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.6103, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.8292682926829268, 'precision': 0.8095238095238095, 'recall': 0.85}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.5924Epoch 7/15: [                              ] 2/63 batches, loss: 0.6076Epoch 7/15: [=                             ] 3/63 batches, loss: 0.5926Epoch 7/15: [=                             ] 4/63 batches, loss: 0.5737Epoch 7/15: [==                            ] 5/63 batches, loss: 0.5805Epoch 7/15: [==                            ] 6/63 batches, loss: 0.5809Epoch 7/15: [===                           ] 7/63 batches, loss: 0.5845Epoch 7/15: [===                           ] 8/63 batches, loss: 0.5737Epoch 7/15: [====                          ] 9/63 batches, loss: 0.5740Epoch 7/15: [====                          ] 10/63 batches, loss: 0.5731Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.5682Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.5700Epoch 7/15: [======                        ] 13/63 batches, loss: 0.5636Epoch 7/15: [======                        ] 14/63 batches, loss: 0.5659Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.5703Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.5713Epoch 7/15: [========                      ] 17/63 batches, loss: 0.5699Epoch 7/15: [========                      ] 18/63 batches, loss: 0.5686Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.5694Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.5702Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.5687Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.5774Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.5770Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.5770Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.5757Epoch 7/15: [============                  ] 26/63 batches, loss: 0.5720Epoch 7/15: [============                  ] 27/63 batches, loss: 0.5727Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.5765Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.5743Epoch 7/15: [==============                ] 30/63 batches, loss: 0.5715Epoch 7/15: [==============                ] 31/63 batches, loss: 0.5707Epoch 7/15: [===============               ] 32/63 batches, loss: 0.5740Epoch 7/15: [===============               ] 33/63 batches, loss: 0.5747Epoch 7/15: [================              ] 34/63 batches, loss: 0.5761Epoch 7/15: [================              ] 35/63 batches, loss: 0.5753Epoch 7/15: [=================             ] 36/63 batches, loss: 0.5765Epoch 7/15: [=================             ] 37/63 batches, loss: 0.5770Epoch 7/15: [==================            ] 38/63 batches, loss: 0.5774Epoch 7/15: [==================            ] 39/63 batches, loss: 0.5753Epoch 7/15: [===================           ] 40/63 batches, loss: 0.5731Epoch 7/15: [===================           ] 41/63 batches, loss: 0.5728Epoch 7/15: [====================          ] 42/63 batches, loss: 0.5736Epoch 7/15: [====================          ] 43/63 batches, loss: 0.5729Epoch 7/15: [====================          ] 44/63 batches, loss: 0.5723Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.5713Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.5712Epoch 7/15: [======================        ] 47/63 batches, loss: 0.5724Epoch 7/15: [======================        ] 48/63 batches, loss: 0.5730Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.5740Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.5745Epoch 7/15: [========================      ] 51/63 batches, loss: 0.5746Epoch 7/15: [========================      ] 52/63 batches, loss: 0.5751Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.5735Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.5733Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.5730Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.5743Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.5746Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.5736Epoch 7/15: [============================  ] 59/63 batches, loss: 0.5748Epoch 7/15: [============================  ] 60/63 batches, loss: 0.5739Epoch 7/15: [============================= ] 61/63 batches, loss: 0.5739Epoch 7/15: [============================= ] 62/63 batches, loss: 0.5734Epoch 7/15: [==============================] 63/63 batches, loss: 0.5738
[2025-05-02 12:46:58,414][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5738
[2025-05-02 12:46:58,630][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.6008, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8837209302325582, 'precision': 0.8260869565217391, 'recall': 0.95}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.6421Epoch 8/15: [                              ] 2/63 batches, loss: 0.5957Epoch 8/15: [=                             ] 3/63 batches, loss: 0.5643Epoch 8/15: [=                             ] 4/63 batches, loss: 0.5624Epoch 8/15: [==                            ] 5/63 batches, loss: 0.5742Epoch 8/15: [==                            ] 6/63 batches, loss: 0.5739Epoch 8/15: [===                           ] 7/63 batches, loss: 0.5737Epoch 8/15: [===                           ] 8/63 batches, loss: 0.5685Epoch 8/15: [====                          ] 9/63 batches, loss: 0.5722Epoch 8/15: [====                          ] 10/63 batches, loss: 0.5742Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.5698Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.5715Epoch 8/15: [======                        ] 13/63 batches, loss: 0.5671Epoch 8/15: [======                        ] 14/63 batches, loss: 0.5676Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.5693Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.5688Epoch 8/15: [========                      ] 17/63 batches, loss: 0.5702Epoch 8/15: [========                      ] 18/63 batches, loss: 0.5666Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.5657Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.5604Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.5606Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.5596Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.5599Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.5590Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.5592Epoch 8/15: [============                  ] 26/63 batches, loss: 0.5572Epoch 8/15: [============                  ] 27/63 batches, loss: 0.5558Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.5551Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.5552Epoch 8/15: [==============                ] 30/63 batches, loss: 0.5561Epoch 8/15: [==============                ] 31/63 batches, loss: 0.5568Epoch 8/15: [===============               ] 32/63 batches, loss: 0.5562Epoch 8/15: [===============               ] 33/63 batches, loss: 0.5543Epoch 8/15: [================              ] 34/63 batches, loss: 0.5551Epoch 8/15: [================              ] 35/63 batches, loss: 0.5523Epoch 8/15: [=================             ] 36/63 batches, loss: 0.5526Epoch 8/15: [=================             ] 37/63 batches, loss: 0.5541Epoch 8/15: [==================            ] 38/63 batches, loss: 0.5556Epoch 8/15: [==================            ] 39/63 batches, loss: 0.5570Epoch 8/15: [===================           ] 40/63 batches, loss: 0.5536Epoch 8/15: [===================           ] 41/63 batches, loss: 0.5539Epoch 8/15: [====================          ] 42/63 batches, loss: 0.5535Epoch 8/15: [====================          ] 43/63 batches, loss: 0.5518Epoch 8/15: [====================          ] 44/63 batches, loss: 0.5530Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.5522Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.5501Epoch 8/15: [======================        ] 47/63 batches, loss: 0.5492Epoch 8/15: [======================        ] 48/63 batches, loss: 0.5499Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.5526Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.5541Epoch 8/15: [========================      ] 51/63 batches, loss: 0.5564Epoch 8/15: [========================      ] 52/63 batches, loss: 0.5561Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.5559Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.5563Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.5588Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.5594Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.5610Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.5605Epoch 8/15: [============================  ] 59/63 batches, loss: 0.5594Epoch 8/15: [============================  ] 60/63 batches, loss: 0.5596Epoch 8/15: [============================= ] 61/63 batches, loss: 0.5597Epoch 8/15: [============================= ] 62/63 batches, loss: 0.5589Epoch 8/15: [==============================] 63/63 batches, loss: 0.5572
[2025-05-02 12:47:01,021][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5572
[2025-05-02 12:47:01,258][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5896, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.4977Epoch 9/15: [                              ] 2/63 batches, loss: 0.5649Epoch 9/15: [=                             ] 3/63 batches, loss: 0.5619Epoch 9/15: [=                             ] 4/63 batches, loss: 0.5546Epoch 9/15: [==                            ] 5/63 batches, loss: 0.5635Epoch 9/15: [==                            ] 6/63 batches, loss: 0.5698Epoch 9/15: [===                           ] 7/63 batches, loss: 0.5734Epoch 9/15: [===                           ] 8/63 batches, loss: 0.5608Epoch 9/15: [====                          ] 9/63 batches, loss: 0.5695Epoch 9/15: [====                          ] 10/63 batches, loss: 0.5655Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.5654Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.5617Epoch 9/15: [======                        ] 13/63 batches, loss: 0.5698Epoch 9/15: [======                        ] 14/63 batches, loss: 0.5700Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.5696Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.5722Epoch 9/15: [========                      ] 17/63 batches, loss: 0.5733Epoch 9/15: [========                      ] 18/63 batches, loss: 0.5713Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.5705Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.5681Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.5677Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.5651Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.5625Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.5624Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.5619Epoch 9/15: [============                  ] 26/63 batches, loss: 0.5606Epoch 9/15: [============                  ] 27/63 batches, loss: 0.5597Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.5561Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.5568Epoch 9/15: [==============                ] 30/63 batches, loss: 0.5563Epoch 9/15: [==============                ] 31/63 batches, loss: 0.5566Epoch 9/15: [===============               ] 32/63 batches, loss: 0.5530Epoch 9/15: [===============               ] 33/63 batches, loss: 0.5547Epoch 9/15: [================              ] 34/63 batches, loss: 0.5543Epoch 9/15: [================              ] 35/63 batches, loss: 0.5549Epoch 9/15: [=================             ] 36/63 batches, loss: 0.5525Epoch 9/15: [=================             ] 37/63 batches, loss: 0.5541Epoch 9/15: [==================            ] 38/63 batches, loss: 0.5548Epoch 9/15: [==================            ] 39/63 batches, loss: 0.5536Epoch 9/15: [===================           ] 40/63 batches, loss: 0.5563Epoch 9/15: [===================           ] 41/63 batches, loss: 0.5581Epoch 9/15: [====================          ] 42/63 batches, loss: 0.5592Epoch 9/15: [====================          ] 43/63 batches, loss: 0.5584Epoch 9/15: [====================          ] 44/63 batches, loss: 0.5579Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.5582Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.5599Epoch 9/15: [======================        ] 47/63 batches, loss: 0.5588Epoch 9/15: [======================        ] 48/63 batches, loss: 0.5594Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.5598Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.5586Epoch 9/15: [========================      ] 51/63 batches, loss: 0.5586Epoch 9/15: [========================      ] 52/63 batches, loss: 0.5568Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.5576Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.5566Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.5578Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.5579Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.5574Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.5585Epoch 9/15: [============================  ] 59/63 batches, loss: 0.5570Epoch 9/15: [============================  ] 60/63 batches, loss: 0.5552Epoch 9/15: [============================= ] 61/63 batches, loss: 0.5550Epoch 9/15: [============================= ] 62/63 batches, loss: 0.5549Epoch 9/15: [==============================] 63/63 batches, loss: 0.5512
[2025-05-02 12:47:03,601][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5512
[2025-05-02 12:47:03,840][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5840, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.5322Epoch 10/15: [                              ] 2/63 batches, loss: 0.5004Epoch 10/15: [=                             ] 3/63 batches, loss: 0.5376Epoch 10/15: [=                             ] 4/63 batches, loss: 0.5294Epoch 10/15: [==                            ] 5/63 batches, loss: 0.5373Epoch 10/15: [==                            ] 6/63 batches, loss: 0.5288Epoch 10/15: [===                           ] 7/63 batches, loss: 0.5261Epoch 10/15: [===                           ] 8/63 batches, loss: 0.5326Epoch 10/15: [====                          ] 9/63 batches, loss: 0.5246Epoch 10/15: [====                          ] 10/63 batches, loss: 0.5253Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.5239Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.5297Epoch 10/15: [======                        ] 13/63 batches, loss: 0.5358Epoch 10/15: [======                        ] 14/63 batches, loss: 0.5403Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.5421Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.5427Epoch 10/15: [========                      ] 17/63 batches, loss: 0.5442Epoch 10/15: [========                      ] 18/63 batches, loss: 0.5531Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.5490Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.5477Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.5511Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.5493Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.5499Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.5525Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.5534Epoch 10/15: [============                  ] 26/63 batches, loss: 0.5509Epoch 10/15: [============                  ] 27/63 batches, loss: 0.5512Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.5488Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.5505Epoch 10/15: [==============                ] 30/63 batches, loss: 0.5501Epoch 10/15: [==============                ] 31/63 batches, loss: 0.5503Epoch 10/15: [===============               ] 32/63 batches, loss: 0.5518Epoch 10/15: [===============               ] 33/63 batches, loss: 0.5499Epoch 10/15: [================              ] 34/63 batches, loss: 0.5501Epoch 10/15: [================              ] 35/63 batches, loss: 0.5492Epoch 10/15: [=================             ] 36/63 batches, loss: 0.5486Epoch 10/15: [=================             ] 37/63 batches, loss: 0.5471Epoch 10/15: [==================            ] 38/63 batches, loss: 0.5472Epoch 10/15: [==================            ] 39/63 batches, loss: 0.5483Epoch 10/15: [===================           ] 40/63 batches, loss: 0.5502Epoch 10/15: [===================           ] 41/63 batches, loss: 0.5465Epoch 10/15: [====================          ] 42/63 batches, loss: 0.5471Epoch 10/15: [====================          ] 43/63 batches, loss: 0.5473Epoch 10/15: [====================          ] 44/63 batches, loss: 0.5492Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.5491Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.5473Epoch 10/15: [======================        ] 47/63 batches, loss: 0.5475Epoch 10/15: [======================        ] 48/63 batches, loss: 0.5491Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.5479Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.5484Epoch 10/15: [========================      ] 51/63 batches, loss: 0.5469Epoch 10/15: [========================      ] 52/63 batches, loss: 0.5466Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.5485Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.5482Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.5474Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.5458Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.5456Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.5466Epoch 10/15: [============================  ] 59/63 batches, loss: 0.5462Epoch 10/15: [============================  ] 60/63 batches, loss: 0.5459Epoch 10/15: [============================= ] 61/63 batches, loss: 0.5461Epoch 10/15: [============================= ] 62/63 batches, loss: 0.5460Epoch 10/15: [==============================] 63/63 batches, loss: 0.5467
[2025-05-02 12:47:06,156][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5467
[2025-05-02 12:47:06,394][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5831, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.5224Epoch 11/15: [                              ] 2/63 batches, loss: 0.5440Epoch 11/15: [=                             ] 3/63 batches, loss: 0.5524Epoch 11/15: [=                             ] 4/63 batches, loss: 0.5575Epoch 11/15: [==                            ] 5/63 batches, loss: 0.5691Epoch 11/15: [==                            ] 6/63 batches, loss: 0.5631Epoch 11/15: [===                           ] 7/63 batches, loss: 0.5680Epoch 11/15: [===                           ] 8/63 batches, loss: 0.5750Epoch 11/15: [====                          ] 9/63 batches, loss: 0.5729Epoch 11/15: [====                          ] 10/63 batches, loss: 0.5628Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.5559Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.5601Epoch 11/15: [======                        ] 13/63 batches, loss: 0.5619Epoch 11/15: [======                        ] 14/63 batches, loss: 0.5651Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.5580Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.5553Epoch 11/15: [========                      ] 17/63 batches, loss: 0.5547Epoch 11/15: [========                      ] 18/63 batches, loss: 0.5524Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.5502Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.5532Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.5560Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.5545Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.5540Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.5548Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.5533Epoch 11/15: [============                  ] 26/63 batches, loss: 0.5519Epoch 11/15: [============                  ] 27/63 batches, loss: 0.5488Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.5507Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.5505Epoch 11/15: [==============                ] 30/63 batches, loss: 0.5493Epoch 11/15: [==============                ] 31/63 batches, loss: 0.5473Epoch 11/15: [===============               ] 32/63 batches, loss: 0.5475Epoch 11/15: [===============               ] 33/63 batches, loss: 0.5441Epoch 11/15: [================              ] 34/63 batches, loss: 0.5425Epoch 11/15: [================              ] 35/63 batches, loss: 0.5434Epoch 11/15: [=================             ] 36/63 batches, loss: 0.5425Epoch 11/15: [=================             ] 37/63 batches, loss: 0.5411Epoch 11/15: [==================            ] 38/63 batches, loss: 0.5424Epoch 11/15: [==================            ] 39/63 batches, loss: 0.5414Epoch 11/15: [===================           ] 40/63 batches, loss: 0.5413Epoch 11/15: [===================           ] 41/63 batches, loss: 0.5401Epoch 11/15: [====================          ] 42/63 batches, loss: 0.5393Epoch 11/15: [====================          ] 43/63 batches, loss: 0.5372Epoch 11/15: [====================          ] 44/63 batches, loss: 0.5376Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.5392Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.5379Epoch 11/15: [======================        ] 47/63 batches, loss: 0.5389Epoch 11/15: [======================        ] 48/63 batches, loss: 0.5380Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.5387Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.5365Epoch 11/15: [========================      ] 51/63 batches, loss: 0.5382Epoch 11/15: [========================      ] 52/63 batches, loss: 0.5381Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.5373Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.5374Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.5369Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.5381Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.5388Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.5397Epoch 11/15: [============================  ] 59/63 batches, loss: 0.5393Epoch 11/15: [============================  ] 60/63 batches, loss: 0.5381Epoch 11/15: [============================= ] 61/63 batches, loss: 0.5384Epoch 11/15: [============================= ] 62/63 batches, loss: 0.5375Epoch 11/15: [==============================] 63/63 batches, loss: 0.5381
[2025-05-02 12:47:08,724][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5381
[2025-05-02 12:47:08,972][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5826, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.4889Epoch 12/15: [                              ] 2/63 batches, loss: 0.5656Epoch 12/15: [=                             ] 3/63 batches, loss: 0.5532Epoch 12/15: [=                             ] 4/63 batches, loss: 0.5641Epoch 12/15: [==                            ] 5/63 batches, loss: 0.5429Epoch 12/15: [==                            ] 6/63 batches, loss: 0.5458Epoch 12/15: [===                           ] 7/63 batches, loss: 0.5500Epoch 12/15: [===                           ] 8/63 batches, loss: 0.5460Epoch 12/15: [====                          ] 9/63 batches, loss: 0.5453Epoch 12/15: [====                          ] 10/63 batches, loss: 0.5322Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.5264Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.5186Epoch 12/15: [======                        ] 13/63 batches, loss: 0.5232Epoch 12/15: [======                        ] 14/63 batches, loss: 0.5187Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.5149Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.5141Epoch 12/15: [========                      ] 17/63 batches, loss: 0.5179Epoch 12/15: [========                      ] 18/63 batches, loss: 0.5177Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.5200Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.5158Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.5191Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.5208Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.5194Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.5184Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.5161Epoch 12/15: [============                  ] 26/63 batches, loss: 0.5180Epoch 12/15: [============                  ] 27/63 batches, loss: 0.5184Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.5184Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.5178Epoch 12/15: [==============                ] 30/63 batches, loss: 0.5185Epoch 12/15: [==============                ] 31/63 batches, loss: 0.5200Epoch 12/15: [===============               ] 32/63 batches, loss: 0.5220Epoch 12/15: [===============               ] 33/63 batches, loss: 0.5231Epoch 12/15: [================              ] 34/63 batches, loss: 0.5237Epoch 12/15: [================              ] 35/63 batches, loss: 0.5266Epoch 12/15: [=================             ] 36/63 batches, loss: 0.5289Epoch 12/15: [=================             ] 37/63 batches, loss: 0.5308Epoch 12/15: [==================            ] 38/63 batches, loss: 0.5320Epoch 12/15: [==================            ] 39/63 batches, loss: 0.5308Epoch 12/15: [===================           ] 40/63 batches, loss: 0.5320Epoch 12/15: [===================           ] 41/63 batches, loss: 0.5304Epoch 12/15: [====================          ] 42/63 batches, loss: 0.5312Epoch 12/15: [====================          ] 43/63 batches, loss: 0.5314Epoch 12/15: [====================          ] 44/63 batches, loss: 0.5322Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.5319Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.5331Epoch 12/15: [======================        ] 47/63 batches, loss: 0.5328Epoch 12/15: [======================        ] 48/63 batches, loss: 0.5336Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.5356Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.5365Epoch 12/15: [========================      ] 51/63 batches, loss: 0.5342Epoch 12/15: [========================      ] 52/63 batches, loss: 0.5346Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.5356Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.5365Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.5381Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.5379Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.5383Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.5396Epoch 12/15: [============================  ] 59/63 batches, loss: 0.5398Epoch 12/15: [============================  ] 60/63 batches, loss: 0.5394Epoch 12/15: [============================= ] 61/63 batches, loss: 0.5398Epoch 12/15: [============================= ] 62/63 batches, loss: 0.5400Epoch 12/15: [==============================] 63/63 batches, loss: 0.5390
[2025-05-02 12:47:11,324][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5390
[2025-05-02 12:47:11,575][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.5801, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.5196Epoch 13/15: [                              ] 2/63 batches, loss: 0.5146Epoch 13/15: [=                             ] 3/63 batches, loss: 0.5302Epoch 13/15: [=                             ] 4/63 batches, loss: 0.5382Epoch 13/15: [==                            ] 5/63 batches, loss: 0.5369Epoch 13/15: [==                            ] 6/63 batches, loss: 0.5516Epoch 13/15: [===                           ] 7/63 batches, loss: 0.5441Epoch 13/15: [===                           ] 8/63 batches, loss: 0.5412Epoch 13/15: [====                          ] 9/63 batches, loss: 0.5410Epoch 13/15: [====                          ] 10/63 batches, loss: 0.5353Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.5367Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.5505Epoch 13/15: [======                        ] 13/63 batches, loss: 0.5498Epoch 13/15: [======                        ] 14/63 batches, loss: 0.5452Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.5397Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.5357Epoch 13/15: [========                      ] 17/63 batches, loss: 0.5300Epoch 13/15: [========                      ] 18/63 batches, loss: 0.5325Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.5333Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.5319Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.5309Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.5315Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.5295Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.5288Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.5273Epoch 13/15: [============                  ] 26/63 batches, loss: 0.5250Epoch 13/15: [============                  ] 27/63 batches, loss: 0.5260Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.5253Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.5277Epoch 13/15: [==============                ] 30/63 batches, loss: 0.5288Epoch 13/15: [==============                ] 31/63 batches, loss: 0.5281Epoch 13/15: [===============               ] 32/63 batches, loss: 0.5318Epoch 13/15: [===============               ] 33/63 batches, loss: 0.5298Epoch 13/15: [================              ] 34/63 batches, loss: 0.5325Epoch 13/15: [================              ] 35/63 batches, loss: 0.5349Epoch 13/15: [=================             ] 36/63 batches, loss: 0.5341Epoch 13/15: [=================             ] 37/63 batches, loss: 0.5328Epoch 13/15: [==================            ] 38/63 batches, loss: 0.5346Epoch 13/15: [==================            ] 39/63 batches, loss: 0.5343Epoch 13/15: [===================           ] 40/63 batches, loss: 0.5333Epoch 13/15: [===================           ] 41/63 batches, loss: 0.5330Epoch 13/15: [====================          ] 42/63 batches, loss: 0.5333Epoch 13/15: [====================          ] 43/63 batches, loss: 0.5312Epoch 13/15: [====================          ] 44/63 batches, loss: 0.5308Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.5322Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.5311Epoch 13/15: [======================        ] 47/63 batches, loss: 0.5323Epoch 13/15: [======================        ] 48/63 batches, loss: 0.5338Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.5332Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.5330Epoch 13/15: [========================      ] 51/63 batches, loss: 0.5325Epoch 13/15: [========================      ] 52/63 batches, loss: 0.5337Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.5343Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.5350Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.5351Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.5351Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.5354Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.5360Epoch 13/15: [============================  ] 59/63 batches, loss: 0.5359Epoch 13/15: [============================  ] 60/63 batches, loss: 0.5359Epoch 13/15: [============================= ] 61/63 batches, loss: 0.5347Epoch 13/15: [============================= ] 62/63 batches, loss: 0.5360Epoch 13/15: [==============================] 63/63 batches, loss: 0.5366
[2025-05-02 12:47:13,948][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5366
[2025-05-02 12:47:14,192][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.5791, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.5207Epoch 14/15: [                              ] 2/63 batches, loss: 0.5359Epoch 14/15: [=                             ] 3/63 batches, loss: 0.5438Epoch 14/15: [=                             ] 4/63 batches, loss: 0.5299Epoch 14/15: [==                            ] 5/63 batches, loss: 0.5358Epoch 14/15: [==                            ] 6/63 batches, loss: 0.5386Epoch 14/15: [===                           ] 7/63 batches, loss: 0.5432Epoch 14/15: [===                           ] 8/63 batches, loss: 0.5471Epoch 14/15: [====                          ] 9/63 batches, loss: 0.5380Epoch 14/15: [====                          ] 10/63 batches, loss: 0.5413Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.5300Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.5277Epoch 14/15: [======                        ] 13/63 batches, loss: 0.5279Epoch 14/15: [======                        ] 14/63 batches, loss: 0.5357Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.5326Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.5293Epoch 14/15: [========                      ] 17/63 batches, loss: 0.5280Epoch 14/15: [========                      ] 18/63 batches, loss: 0.5312Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.5267Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.5282Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.5289Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.5296Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.5312Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.5329Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.5309Epoch 14/15: [============                  ] 26/63 batches, loss: 0.5331Epoch 14/15: [============                  ] 27/63 batches, loss: 0.5316Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.5322Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.5363Epoch 14/15: [==============                ] 30/63 batches, loss: 0.5354Epoch 14/15: [==============                ] 31/63 batches, loss: 0.5328Epoch 14/15: [===============               ] 32/63 batches, loss: 0.5298Epoch 14/15: [===============               ] 33/63 batches, loss: 0.5293Epoch 14/15: [================              ] 34/63 batches, loss: 0.5309Epoch 14/15: [================              ] 35/63 batches, loss: 0.5308Epoch 14/15: [=================             ] 36/63 batches, loss: 0.5308Epoch 14/15: [=================             ] 37/63 batches, loss: 0.5310Epoch 14/15: [==================            ] 38/63 batches, loss: 0.5299Epoch 14/15: [==================            ] 39/63 batches, loss: 0.5276Epoch 14/15: [===================           ] 40/63 batches, loss: 0.5260Epoch 14/15: [===================           ] 41/63 batches, loss: 0.5256Epoch 14/15: [====================          ] 42/63 batches, loss: 0.5245Epoch 14/15: [====================          ] 43/63 batches, loss: 0.5225Epoch 14/15: [====================          ] 44/63 batches, loss: 0.5229Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.5240Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.5231Epoch 14/15: [======================        ] 47/63 batches, loss: 0.5218Epoch 14/15: [======================        ] 48/63 batches, loss: 0.5229Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.5248Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.5250Epoch 14/15: [========================      ] 51/63 batches, loss: 0.5237Epoch 14/15: [========================      ] 52/63 batches, loss: 0.5260Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.5275Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.5285Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.5283Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.5293Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.5295Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.5310Epoch 14/15: [============================  ] 59/63 batches, loss: 0.5302Epoch 14/15: [============================  ] 60/63 batches, loss: 0.5316Epoch 14/15: [============================= ] 61/63 batches, loss: 0.5334Epoch 14/15: [============================= ] 62/63 batches, loss: 0.5337Epoch 14/15: [==============================] 63/63 batches, loss: 0.5302
[2025-05-02 12:47:16,545][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.5302
[2025-05-02 12:47:16,764][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.5785, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.5060Epoch 15/15: [                              ] 2/63 batches, loss: 0.4480Epoch 15/15: [=                             ] 3/63 batches, loss: 0.4871Epoch 15/15: [=                             ] 4/63 batches, loss: 0.5037Epoch 15/15: [==                            ] 5/63 batches, loss: 0.5140Epoch 15/15: [==                            ] 6/63 batches, loss: 0.5190Epoch 15/15: [===                           ] 7/63 batches, loss: 0.5037Epoch 15/15: [===                           ] 8/63 batches, loss: 0.5113Epoch 15/15: [====                          ] 9/63 batches, loss: 0.5117Epoch 15/15: [====                          ] 10/63 batches, loss: 0.5135Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.5118Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.5137Epoch 15/15: [======                        ] 13/63 batches, loss: 0.5135Epoch 15/15: [======                        ] 14/63 batches, loss: 0.5116Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.5114Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.5108Epoch 15/15: [========                      ] 17/63 batches, loss: 0.5121Epoch 15/15: [========                      ] 18/63 batches, loss: 0.5162Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.5191Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.5172Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.5157Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.5167Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.5177Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.5203Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.5198Epoch 15/15: [============                  ] 26/63 batches, loss: 0.5219Epoch 15/15: [============                  ] 27/63 batches, loss: 0.5216Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.5255Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.5234Epoch 15/15: [==============                ] 30/63 batches, loss: 0.5231Epoch 15/15: [==============                ] 31/63 batches, loss: 0.5272Epoch 15/15: [===============               ] 32/63 batches, loss: 0.5282Epoch 15/15: [===============               ] 33/63 batches, loss: 0.5294Epoch 15/15: [================              ] 34/63 batches, loss: 0.5298Epoch 15/15: [================              ] 35/63 batches, loss: 0.5300Epoch 15/15: [=================             ] 36/63 batches, loss: 0.5301Epoch 15/15: [=================             ] 37/63 batches, loss: 0.5276Epoch 15/15: [==================            ] 38/63 batches, loss: 0.5295Epoch 15/15: [==================            ] 39/63 batches, loss: 0.5295Epoch 15/15: [===================           ] 40/63 batches, loss: 0.5271Epoch 15/15: [===================           ] 41/63 batches, loss: 0.5279Epoch 15/15: [====================          ] 42/63 batches, loss: 0.5264Epoch 15/15: [====================          ] 43/63 batches, loss: 0.5258Epoch 15/15: [====================          ] 44/63 batches, loss: 0.5257Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.5244Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.5255Epoch 15/15: [======================        ] 47/63 batches, loss: 0.5257Epoch 15/15: [======================        ] 48/63 batches, loss: 0.5233Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.5220Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.5223Epoch 15/15: [========================      ] 51/63 batches, loss: 0.5234Epoch 15/15: [========================      ] 52/63 batches, loss: 0.5252Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.5259Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.5267Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.5245Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.5238Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.5245Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.5233Epoch 15/15: [============================  ] 59/63 batches, loss: 0.5235Epoch 15/15: [============================  ] 60/63 batches, loss: 0.5250Epoch 15/15: [============================= ] 61/63 batches, loss: 0.5250Epoch 15/15: [============================= ] 62/63 batches, loss: 0.5264Epoch 15/15: [==============================] 63/63 batches, loss: 0.5232
[2025-05-02 12:47:19,181][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.5232
[2025-05-02 12:47:19,421][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.5778, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 12:47:19,811][src.training.lm_trainer][INFO] - Training completed in 39.37 seconds
[2025-05-02 12:47:19,811][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 12:47:22,393][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9889447236180905, 'f1': 0.9888551165146909, 'precision': 0.9959183673469387, 'recall': 0.9818913480885312}
[2025-05-02 12:47:22,393][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 12:47:22,394][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.6363636363636364, 'f1': 0.6111111111111112, 'precision': 0.44, 'recall': 1.0}
[2025-05-02 12:47:24,111][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer4/ar/ar/model.pt
[2025-05-02 12:47:24,113][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▂▁▄▄▇▇█████████
wandb:           best_val_f1 ▁▂▆▆█▇█████████
wandb:         best_val_loss ██▇▆▄▃▂▂▁▁▁▁▁▁▁
wandb:    best_val_precision ▁▃▇▇███████████
wandb:       best_val_recall ▁▁▅▅█▇█████████
wandb:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▁▂▂▃▃▃▃▃▃▃▃▃
wandb:            train_loss ██▇▅▅▄▃▂▂▂▂▂▂▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▂▁▄▄▇▇█████████
wandb:                val_f1 ▁▂▆▆█▇█████████
wandb:              val_loss ██▇▆▄▃▂▂▁▁▁▁▁▁▁
wandb:         val_precision ▁▃▇▇███████████
wandb:            val_recall ▁▁▅▅█▇█████████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.90909
wandb:           best_val_f1 0.90909
wandb:         best_val_loss 0.5778
wandb:    best_val_precision 0.83333
wandb:       best_val_recall 1
wandb:                 epoch 15
wandb:   final_test_accuracy 0.63636
wandb:         final_test_f1 0.61111
wandb:  final_test_precision 0.44
wandb:     final_test_recall 1
wandb:  final_train_accuracy 0.98894
wandb:        final_train_f1 0.98886
wandb: final_train_precision 0.99592
wandb:    final_train_recall 0.98189
wandb:    final_val_accuracy 0.90909
wandb:          final_val_f1 0.90909
wandb:   final_val_precision 0.83333
wandb:      final_val_recall 1
wandb:         learning_rate 0.0001
wandb:            train_loss 0.52317
wandb:            train_time 39.37305
wandb:          val_accuracy 0.90909
wandb:                val_f1 0.90909
wandb:              val_loss 0.5778
wandb:         val_precision 0.83333
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_124630-87oyfjc4
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_124630-87oyfjc4/logs
Experiment probe_layer4_question_type_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer4/ar/ar/results.json for layer 4
Running experiment: probe_layer4_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=4"         "model.probe_hidden_size=96" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer4_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer4/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 12:47:36,387][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer4/ar
experiment_name: probe_layer4_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
  probe_hidden_size: 96
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-02 12:47:36,387][__main__][INFO] - Normalized task: complexity
[2025-05-02 12:47:36,387][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 12:47:36,387][__main__][INFO] - Determined Task Type: regression
[2025-05-02 12:47:36,392][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-02 12:47:36,392][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 12:47:37,825][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 12:47:40,169][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 12:47:40,170][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:47:40,210][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:47:40,234][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:47:40,308][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 12:47:40,315][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:47:40,316][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 12:47:40,317][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:47:40,334][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:47:40,358][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:47:40,368][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 12:47:40,370][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:47:40,370][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 12:47:40,370][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:47:40,388][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:47:40,411][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:47:40,420][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 12:47:40,421][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:47:40,421][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 12:47:40,422][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 12:47:40,422][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 12:47:40,423][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 12:47:40,423][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 12:47:40,423][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 12:47:40,423][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:47:40,423][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-02 12:47:40,423][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 12:47:40,423][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-02 12:47:40,423][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 12:47:40,424][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 12:47:40,424][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 12:47:40,424][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 12:47:40,424][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:47:40,424][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-02 12:47:40,424][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 12:47:40,424][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-02 12:47:40,424][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 12:47:40,424][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 12:47:40,424][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 12:47:40,425][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 12:47:40,425][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:47:40,425][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-02 12:47:40,425][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 12:47:40,425][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-02 12:47:40,425][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 12:47:40,425][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 12:47:40,426][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 12:47:40,426][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-02 12:47:40,426][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 12:47:44,146][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 12:47:44,147][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 12:47:44,147][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=4, freeze_model=True
[2025-05-02 12:47:44,147][src.models.model_factory][INFO] - Using provided probe_hidden_size: 96
[2025-05-02 12:47:44,149][src.models.model_factory][INFO] - Model has 84,961 trainable parameters out of 394,206,433 total parameters
[2025-05-02 12:47:44,149][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 84,961 trainable parameters
[2025-05-02 12:47:44,150][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=96, depth=2, activation=silu, normalization=layer
[2025-05-02 12:47:44,150][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 96 hidden size
[2025-05-02 12:47:44,150][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 12:47:44,151][__main__][INFO] - Total parameters: 394,206,433
[2025-05-02 12:47:44,151][__main__][INFO] - Trainable parameters: 84,961 (0.02%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 4.1902Epoch 1/15: [                              ] 2/63 batches, loss: 4.4389Epoch 1/15: [=                             ] 3/63 batches, loss: 4.4568Epoch 1/15: [=                             ] 4/63 batches, loss: 4.3783Epoch 1/15: [==                            ] 5/63 batches, loss: 4.0997Epoch 1/15: [==                            ] 6/63 batches, loss: 4.0945Epoch 1/15: [===                           ] 7/63 batches, loss: 3.9409Epoch 1/15: [===                           ] 8/63 batches, loss: 3.7696Epoch 1/15: [====                          ] 9/63 batches, loss: 3.7235Epoch 1/15: [====                          ] 10/63 batches, loss: 3.6703Epoch 1/15: [=====                         ] 11/63 batches, loss: 3.6158Epoch 1/15: [=====                         ] 12/63 batches, loss: 3.4930Epoch 1/15: [======                        ] 13/63 batches, loss: 3.4359Epoch 1/15: [======                        ] 14/63 batches, loss: 3.3437Epoch 1/15: [=======                       ] 15/63 batches, loss: 3.2497Epoch 1/15: [=======                       ] 16/63 batches, loss: 3.1540Epoch 1/15: [========                      ] 17/63 batches, loss: 3.1140Epoch 1/15: [========                      ] 18/63 batches, loss: 3.1185Epoch 1/15: [=========                     ] 19/63 batches, loss: 3.0799Epoch 1/15: [=========                     ] 20/63 batches, loss: 3.0297Epoch 1/15: [==========                    ] 21/63 batches, loss: 2.9581Epoch 1/15: [==========                    ] 22/63 batches, loss: 2.9196Epoch 1/15: [==========                    ] 23/63 batches, loss: 2.9619Epoch 1/15: [===========                   ] 24/63 batches, loss: 2.9078Epoch 1/15: [===========                   ] 25/63 batches, loss: 2.8750Epoch 1/15: [============                  ] 26/63 batches, loss: 2.8314Epoch 1/15: [============                  ] 27/63 batches, loss: 2.8051Epoch 1/15: [=============                 ] 28/63 batches, loss: 2.7609Epoch 1/15: [=============                 ] 29/63 batches, loss: 2.7177Epoch 1/15: [==============                ] 30/63 batches, loss: 2.6853Epoch 1/15: [==============                ] 31/63 batches, loss: 2.6410Epoch 1/15: [===============               ] 32/63 batches, loss: 2.5807Epoch 1/15: [===============               ] 33/63 batches, loss: 2.5376Epoch 1/15: [================              ] 34/63 batches, loss: 2.5021Epoch 1/15: [================              ] 35/63 batches, loss: 2.4775Epoch 1/15: [=================             ] 36/63 batches, loss: 2.4686Epoch 1/15: [=================             ] 37/63 batches, loss: 2.4380Epoch 1/15: [==================            ] 38/63 batches, loss: 2.4086Epoch 1/15: [==================            ] 39/63 batches, loss: 2.3745Epoch 1/15: [===================           ] 40/63 batches, loss: 2.3495Epoch 1/15: [===================           ] 41/63 batches, loss: 2.3098Epoch 1/15: [====================          ] 42/63 batches, loss: 2.2769Epoch 1/15: [====================          ] 43/63 batches, loss: 2.2427Epoch 1/15: [====================          ] 44/63 batches, loss: 2.2109Epoch 1/15: [=====================         ] 45/63 batches, loss: 2.1815Epoch 1/15: [=====================         ] 46/63 batches, loss: 2.1673Epoch 1/15: [======================        ] 47/63 batches, loss: 2.1452Epoch 1/15: [======================        ] 48/63 batches, loss: 2.1155Epoch 1/15: [=======================       ] 49/63 batches, loss: 2.0868Epoch 1/15: [=======================       ] 50/63 batches, loss: 2.0674Epoch 1/15: [========================      ] 51/63 batches, loss: 2.0388Epoch 1/15: [========================      ] 52/63 batches, loss: 2.0048Epoch 1/15: [=========================     ] 53/63 batches, loss: 1.9720Epoch 1/15: [=========================     ] 54/63 batches, loss: 1.9515Epoch 1/15: [==========================    ] 55/63 batches, loss: 1.9307Epoch 1/15: [==========================    ] 56/63 batches, loss: 1.9046Epoch 1/15: [===========================   ] 57/63 batches, loss: 1.8934Epoch 1/15: [===========================   ] 58/63 batches, loss: 1.8785Epoch 1/15: [============================  ] 59/63 batches, loss: 1.8600Epoch 1/15: [============================  ] 60/63 batches, loss: 1.8400Epoch 1/15: [============================= ] 61/63 batches, loss: 1.8195Epoch 1/15: [============================= ] 62/63 batches, loss: 1.8013Epoch 1/15: [==============================] 63/63 batches, loss: 1.8098
[2025-05-02 12:47:48,508][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 1.8098
[2025-05-02 12:47:48,698][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.2075, Metrics: {'mse': 0.21418525278568268, 'rmse': 0.462801526343294, 'r2': -2.301307201385498}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.4380Epoch 2/15: [                              ] 2/63 batches, loss: 0.4288Epoch 2/15: [=                             ] 3/63 batches, loss: 0.4069Epoch 2/15: [=                             ] 4/63 batches, loss: 0.5460Epoch 2/15: [==                            ] 5/63 batches, loss: 0.5638Epoch 2/15: [==                            ] 6/63 batches, loss: 0.5850Epoch 2/15: [===                           ] 7/63 batches, loss: 0.5503Epoch 2/15: [===                           ] 8/63 batches, loss: 0.5259Epoch 2/15: [====                          ] 9/63 batches, loss: 0.4956Epoch 2/15: [====                          ] 10/63 batches, loss: 0.4912Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.4632Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.4494Epoch 2/15: [======                        ] 13/63 batches, loss: 0.4572Epoch 2/15: [======                        ] 14/63 batches, loss: 0.4425Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.4565Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.4626Epoch 2/15: [========                      ] 17/63 batches, loss: 0.4614Epoch 2/15: [========                      ] 18/63 batches, loss: 0.4449Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.4454Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.4488Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.4426Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.4466Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.4348Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.4362Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.4348Epoch 2/15: [============                  ] 26/63 batches, loss: 0.4317Epoch 2/15: [============                  ] 27/63 batches, loss: 0.4259Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.4188Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.4199Epoch 2/15: [==============                ] 30/63 batches, loss: 0.4190Epoch 2/15: [==============                ] 31/63 batches, loss: 0.4372Epoch 2/15: [===============               ] 32/63 batches, loss: 0.4314Epoch 2/15: [===============               ] 33/63 batches, loss: 0.4235Epoch 2/15: [================              ] 34/63 batches, loss: 0.4243Epoch 2/15: [================              ] 35/63 batches, loss: 0.4297Epoch 2/15: [=================             ] 36/63 batches, loss: 0.4254Epoch 2/15: [=================             ] 37/63 batches, loss: 0.4186Epoch 2/15: [==================            ] 38/63 batches, loss: 0.4095Epoch 2/15: [==================            ] 39/63 batches, loss: 0.4028Epoch 2/15: [===================           ] 40/63 batches, loss: 0.4091Epoch 2/15: [===================           ] 41/63 batches, loss: 0.4062Epoch 2/15: [====================          ] 42/63 batches, loss: 0.4002Epoch 2/15: [====================          ] 43/63 batches, loss: 0.3993Epoch 2/15: [====================          ] 44/63 batches, loss: 0.4014Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.3974Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.3917Epoch 2/15: [======================        ] 47/63 batches, loss: 0.3889Epoch 2/15: [======================        ] 48/63 batches, loss: 0.3853Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.3802Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.3855Epoch 2/15: [========================      ] 51/63 batches, loss: 0.3800Epoch 2/15: [========================      ] 52/63 batches, loss: 0.3781Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.3754Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.3706Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.3726Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.3721Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.3728Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.3722Epoch 2/15: [============================  ] 59/63 batches, loss: 0.3702Epoch 2/15: [============================  ] 60/63 batches, loss: 0.3677Epoch 2/15: [============================= ] 61/63 batches, loss: 0.3663Epoch 2/15: [============================= ] 62/63 batches, loss: 0.3628Epoch 2/15: [==============================] 63/63 batches, loss: 0.3574
[2025-05-02 12:47:51,041][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.3574
[2025-05-02 12:47:51,250][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.2530, Metrics: {'mse': 0.24768643081188202, 'rmse': 0.49768105329807566, 'r2': -2.817671537399292}
[2025-05-02 12:47:51,251][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.3913Epoch 3/15: [                              ] 2/63 batches, loss: 0.4263Epoch 3/15: [=                             ] 3/63 batches, loss: 0.3877Epoch 3/15: [=                             ] 4/63 batches, loss: 0.3644Epoch 3/15: [==                            ] 5/63 batches, loss: 0.3613Epoch 3/15: [==                            ] 6/63 batches, loss: 0.3537Epoch 3/15: [===                           ] 7/63 batches, loss: 0.3255Epoch 3/15: [===                           ] 8/63 batches, loss: 0.3226Epoch 3/15: [====                          ] 9/63 batches, loss: 0.3314Epoch 3/15: [====                          ] 10/63 batches, loss: 0.3921Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.3663Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.3638Epoch 3/15: [======                        ] 13/63 batches, loss: 0.3594Epoch 3/15: [======                        ] 14/63 batches, loss: 0.3575Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.3409Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.3484Epoch 3/15: [========                      ] 17/63 batches, loss: 0.3419Epoch 3/15: [========                      ] 18/63 batches, loss: 0.3429Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.3545Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.3558Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.3518Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.3496Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.3440Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.3480Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.3422Epoch 3/15: [============                  ] 26/63 batches, loss: 0.3425Epoch 3/15: [============                  ] 27/63 batches, loss: 0.3398Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.3424Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.3351Epoch 3/15: [==============                ] 30/63 batches, loss: 0.3330Epoch 3/15: [==============                ] 31/63 batches, loss: 0.3347Epoch 3/15: [===============               ] 32/63 batches, loss: 0.3358Epoch 3/15: [===============               ] 33/63 batches, loss: 0.3301Epoch 3/15: [================              ] 34/63 batches, loss: 0.3262Epoch 3/15: [================              ] 35/63 batches, loss: 0.3212Epoch 3/15: [=================             ] 36/63 batches, loss: 0.3220Epoch 3/15: [=================             ] 37/63 batches, loss: 0.3181Epoch 3/15: [==================            ] 38/63 batches, loss: 0.3136Epoch 3/15: [==================            ] 39/63 batches, loss: 0.3109Epoch 3/15: [===================           ] 40/63 batches, loss: 0.3096Epoch 3/15: [===================           ] 41/63 batches, loss: 0.3080Epoch 3/15: [====================          ] 42/63 batches, loss: 0.3041Epoch 3/15: [====================          ] 43/63 batches, loss: 0.3027Epoch 3/15: [====================          ] 44/63 batches, loss: 0.3009Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.2981Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.2935Epoch 3/15: [======================        ] 47/63 batches, loss: 0.2919Epoch 3/15: [======================        ] 48/63 batches, loss: 0.2885Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.2899Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.2870Epoch 3/15: [========================      ] 51/63 batches, loss: 0.2860Epoch 3/15: [========================      ] 52/63 batches, loss: 0.2864Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.2866Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.2833Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.2812Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.2793Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.2773Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.2751Epoch 3/15: [============================  ] 59/63 batches, loss: 0.2780Epoch 3/15: [============================  ] 60/63 batches, loss: 0.2784Epoch 3/15: [============================= ] 61/63 batches, loss: 0.2755Epoch 3/15: [============================= ] 62/63 batches, loss: 0.2736Epoch 3/15: [==============================] 63/63 batches, loss: 0.2702
[2025-05-02 12:47:53,162][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.2702
[2025-05-02 12:47:53,376][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.2348, Metrics: {'mse': 0.22986645996570587, 'rmse': 0.47944390700655054, 'r2': -2.543006658554077}
[2025-05-02 12:47:53,377][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.3031Epoch 4/15: [                              ] 2/63 batches, loss: 0.2516Epoch 4/15: [=                             ] 3/63 batches, loss: 0.2455Epoch 4/15: [=                             ] 4/63 batches, loss: 0.2625Epoch 4/15: [==                            ] 5/63 batches, loss: 0.2718Epoch 4/15: [==                            ] 6/63 batches, loss: 0.2825Epoch 4/15: [===                           ] 7/63 batches, loss: 0.2783Epoch 4/15: [===                           ] 8/63 batches, loss: 0.2662Epoch 4/15: [====                          ] 9/63 batches, loss: 0.2674Epoch 4/15: [====                          ] 10/63 batches, loss: 0.2745Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.2726Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.2725Epoch 4/15: [======                        ] 13/63 batches, loss: 0.2659Epoch 4/15: [======                        ] 14/63 batches, loss: 0.2581Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.2629Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.2589Epoch 4/15: [========                      ] 17/63 batches, loss: 0.2520Epoch 4/15: [========                      ] 18/63 batches, loss: 0.2671Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.2579Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.2592Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.2540Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.2503Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.2444Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.2423Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.2401Epoch 4/15: [============                  ] 26/63 batches, loss: 0.2405Epoch 4/15: [============                  ] 27/63 batches, loss: 0.2372Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.2401Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.2515Epoch 4/15: [==============                ] 30/63 batches, loss: 0.2486Epoch 4/15: [==============                ] 31/63 batches, loss: 0.2455Epoch 4/15: [===============               ] 32/63 batches, loss: 0.2472Epoch 4/15: [===============               ] 33/63 batches, loss: 0.2474Epoch 4/15: [================              ] 34/63 batches, loss: 0.2457Epoch 4/15: [================              ] 35/63 batches, loss: 0.2484Epoch 4/15: [=================             ] 36/63 batches, loss: 0.2452Epoch 4/15: [=================             ] 37/63 batches, loss: 0.2453Epoch 4/15: [==================            ] 38/63 batches, loss: 0.2448Epoch 4/15: [==================            ] 39/63 batches, loss: 0.2486Epoch 4/15: [===================           ] 40/63 batches, loss: 0.2458Epoch 4/15: [===================           ] 41/63 batches, loss: 0.2461Epoch 4/15: [====================          ] 42/63 batches, loss: 0.2464Epoch 4/15: [====================          ] 43/63 batches, loss: 0.2442Epoch 4/15: [====================          ] 44/63 batches, loss: 0.2419Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.2388Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.2348Epoch 4/15: [======================        ] 47/63 batches, loss: 0.2330Epoch 4/15: [======================        ] 48/63 batches, loss: 0.2327Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.2336Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.2319Epoch 4/15: [========================      ] 51/63 batches, loss: 0.2334Epoch 4/15: [========================      ] 52/63 batches, loss: 0.2332Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.2304Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.2360Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.2349Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.2329Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.2344Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.2334Epoch 4/15: [============================  ] 59/63 batches, loss: 0.2330Epoch 4/15: [============================  ] 60/63 batches, loss: 0.2315Epoch 4/15: [============================= ] 61/63 batches, loss: 0.2310Epoch 4/15: [============================= ] 62/63 batches, loss: 0.2308Epoch 4/15: [==============================] 63/63 batches, loss: 0.2274
[2025-05-02 12:47:55,312][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.2274
[2025-05-02 12:47:55,524][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.2205, Metrics: {'mse': 0.21558347344398499, 'rmse': 0.4643096740796868, 'r2': -2.3228583335876465}
[2025-05-02 12:47:55,525][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1972Epoch 5/15: [                              ] 2/63 batches, loss: 0.1796Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1619Epoch 5/15: [=                             ] 4/63 batches, loss: 0.1745Epoch 5/15: [==                            ] 5/63 batches, loss: 0.1727Epoch 5/15: [==                            ] 6/63 batches, loss: 0.1835Epoch 5/15: [===                           ] 7/63 batches, loss: 0.1780Epoch 5/15: [===                           ] 8/63 batches, loss: 0.1909Epoch 5/15: [====                          ] 9/63 batches, loss: 0.1863Epoch 5/15: [====                          ] 10/63 batches, loss: 0.1874Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.1977Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.1909Epoch 5/15: [======                        ] 13/63 batches, loss: 0.1911Epoch 5/15: [======                        ] 14/63 batches, loss: 0.1946Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.1934Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.1945Epoch 5/15: [========                      ] 17/63 batches, loss: 0.2009Epoch 5/15: [========                      ] 18/63 batches, loss: 0.2071Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.2094Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.2098Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.2108Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.2098Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.2105Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.2077Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.2064Epoch 5/15: [============                  ] 26/63 batches, loss: 0.2065Epoch 5/15: [============                  ] 27/63 batches, loss: 0.2091Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.2081Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.2059Epoch 5/15: [==============                ] 30/63 batches, loss: 0.2087Epoch 5/15: [==============                ] 31/63 batches, loss: 0.2068Epoch 5/15: [===============               ] 32/63 batches, loss: 0.2026Epoch 5/15: [===============               ] 33/63 batches, loss: 0.2033Epoch 5/15: [================              ] 34/63 batches, loss: 0.2031Epoch 5/15: [================              ] 35/63 batches, loss: 0.2023Epoch 5/15: [=================             ] 36/63 batches, loss: 0.2028Epoch 5/15: [=================             ] 37/63 batches, loss: 0.2034Epoch 5/15: [==================            ] 38/63 batches, loss: 0.1998Epoch 5/15: [==================            ] 39/63 batches, loss: 0.1991Epoch 5/15: [===================           ] 40/63 batches, loss: 0.2009Epoch 5/15: [===================           ] 41/63 batches, loss: 0.2056Epoch 5/15: [====================          ] 42/63 batches, loss: 0.2073Epoch 5/15: [====================          ] 43/63 batches, loss: 0.2094Epoch 5/15: [====================          ] 44/63 batches, loss: 0.2102Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.2109Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.2123Epoch 5/15: [======================        ] 47/63 batches, loss: 0.2122Epoch 5/15: [======================        ] 48/63 batches, loss: 0.2133Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.2124Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.2144Epoch 5/15: [========================      ] 51/63 batches, loss: 0.2159Epoch 5/15: [========================      ] 52/63 batches, loss: 0.2167Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.2186Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.2163Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.2178Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.2198Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.2176Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.2168Epoch 5/15: [============================  ] 59/63 batches, loss: 0.2150Epoch 5/15: [============================  ] 60/63 batches, loss: 0.2125Epoch 5/15: [============================= ] 61/63 batches, loss: 0.2124Epoch 5/15: [============================= ] 62/63 batches, loss: 0.2114Epoch 5/15: [==============================] 63/63 batches, loss: 0.2094
[2025-05-02 12:47:57,444][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.2094
[2025-05-02 12:47:57,653][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1940, Metrics: {'mse': 0.18974319100379944, 'rmse': 0.4355952146245404, 'r2': -1.9245736598968506}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.1645Epoch 6/15: [                              ] 2/63 batches, loss: 0.2202Epoch 6/15: [=                             ] 3/63 batches, loss: 0.1894Epoch 6/15: [=                             ] 4/63 batches, loss: 0.1625Epoch 6/15: [==                            ] 5/63 batches, loss: 0.1438Epoch 6/15: [==                            ] 6/63 batches, loss: 0.1406Epoch 6/15: [===                           ] 7/63 batches, loss: 0.1405Epoch 6/15: [===                           ] 8/63 batches, loss: 0.1345Epoch 6/15: [====                          ] 9/63 batches, loss: 0.1359Epoch 6/15: [====                          ] 10/63 batches, loss: 0.1312Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.1473Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.1472Epoch 6/15: [======                        ] 13/63 batches, loss: 0.1724Epoch 6/15: [======                        ] 14/63 batches, loss: 0.1710Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.1683Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.1673Epoch 6/15: [========                      ] 17/63 batches, loss: 0.1730Epoch 6/15: [========                      ] 18/63 batches, loss: 0.1717Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.1740Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.1722Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.1767Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.1750Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.1737Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.1717Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.1838Epoch 6/15: [============                  ] 26/63 batches, loss: 0.1840Epoch 6/15: [============                  ] 27/63 batches, loss: 0.1847Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.1830Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.1841Epoch 6/15: [==============                ] 30/63 batches, loss: 0.1869Epoch 6/15: [==============                ] 31/63 batches, loss: 0.1866Epoch 6/15: [===============               ] 32/63 batches, loss: 0.1864Epoch 6/15: [===============               ] 33/63 batches, loss: 0.1874Epoch 6/15: [================              ] 34/63 batches, loss: 0.1871Epoch 6/15: [================              ] 35/63 batches, loss: 0.1890Epoch 6/15: [=================             ] 36/63 batches, loss: 0.1892Epoch 6/15: [=================             ] 37/63 batches, loss: 0.1883Epoch 6/15: [==================            ] 38/63 batches, loss: 0.1896Epoch 6/15: [==================            ] 39/63 batches, loss: 0.1909Epoch 6/15: [===================           ] 40/63 batches, loss: 0.1917Epoch 6/15: [===================           ] 41/63 batches, loss: 0.1907Epoch 6/15: [====================          ] 42/63 batches, loss: 0.1914Epoch 6/15: [====================          ] 43/63 batches, loss: 0.1917Epoch 6/15: [====================          ] 44/63 batches, loss: 0.1913Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.1919Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.1914Epoch 6/15: [======================        ] 47/63 batches, loss: 0.1924Epoch 6/15: [======================        ] 48/63 batches, loss: 0.1914Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.1893Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.1894Epoch 6/15: [========================      ] 51/63 batches, loss: 0.1899Epoch 6/15: [========================      ] 52/63 batches, loss: 0.1898Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.1907Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.1911Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.1917Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.1912Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.1912Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.1901Epoch 6/15: [============================  ] 59/63 batches, loss: 0.1901Epoch 6/15: [============================  ] 60/63 batches, loss: 0.1912Epoch 6/15: [============================= ] 61/63 batches, loss: 0.1923Epoch 6/15: [============================= ] 62/63 batches, loss: 0.1919Epoch 6/15: [==============================] 63/63 batches, loss: 0.1913
[2025-05-02 12:48:00,001][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1913
[2025-05-02 12:48:00,229][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1647, Metrics: {'mse': 0.16128410398960114, 'rmse': 0.40160192229320957, 'r2': -1.485924482345581}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.1101Epoch 7/15: [                              ] 2/63 batches, loss: 0.1793Epoch 7/15: [=                             ] 3/63 batches, loss: 0.1995Epoch 7/15: [=                             ] 4/63 batches, loss: 0.2077Epoch 7/15: [==                            ] 5/63 batches, loss: 0.1768Epoch 7/15: [==                            ] 6/63 batches, loss: 0.1759Epoch 7/15: [===                           ] 7/63 batches, loss: 0.2076Epoch 7/15: [===                           ] 8/63 batches, loss: 0.2067Epoch 7/15: [====                          ] 9/63 batches, loss: 0.2038Epoch 7/15: [====                          ] 10/63 batches, loss: 0.1958Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.2020Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.1966Epoch 7/15: [======                        ] 13/63 batches, loss: 0.1903Epoch 7/15: [======                        ] 14/63 batches, loss: 0.1886Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.1963Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.1963Epoch 7/15: [========                      ] 17/63 batches, loss: 0.2005Epoch 7/15: [========                      ] 18/63 batches, loss: 0.2020Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.2028Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.2000Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.2000Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.2007Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.1974Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.1941Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.1894Epoch 7/15: [============                  ] 26/63 batches, loss: 0.1837Epoch 7/15: [============                  ] 27/63 batches, loss: 0.1828Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.1824Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.1797Epoch 7/15: [==============                ] 30/63 batches, loss: 0.1798Epoch 7/15: [==============                ] 31/63 batches, loss: 0.1798Epoch 7/15: [===============               ] 32/63 batches, loss: 0.1794Epoch 7/15: [===============               ] 33/63 batches, loss: 0.1837Epoch 7/15: [================              ] 34/63 batches, loss: 0.1829Epoch 7/15: [================              ] 35/63 batches, loss: 0.1850Epoch 7/15: [=================             ] 36/63 batches, loss: 0.1873Epoch 7/15: [=================             ] 37/63 batches, loss: 0.1855Epoch 7/15: [==================            ] 38/63 batches, loss: 0.1870Epoch 7/15: [==================            ] 39/63 batches, loss: 0.1880Epoch 7/15: [===================           ] 40/63 batches, loss: 0.1863Epoch 7/15: [===================           ] 41/63 batches, loss: 0.1879Epoch 7/15: [====================          ] 42/63 batches, loss: 0.1902Epoch 7/15: [====================          ] 43/63 batches, loss: 0.1895Epoch 7/15: [====================          ] 44/63 batches, loss: 0.1889Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.1887Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.1883Epoch 7/15: [======================        ] 47/63 batches, loss: 0.1899Epoch 7/15: [======================        ] 48/63 batches, loss: 0.1885Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.1880Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.1896Epoch 7/15: [========================      ] 51/63 batches, loss: 0.1881Epoch 7/15: [========================      ] 52/63 batches, loss: 0.1873Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.1879Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.1872Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.1875Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.1897Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.1899Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.1894Epoch 7/15: [============================  ] 59/63 batches, loss: 0.1924Epoch 7/15: [============================  ] 60/63 batches, loss: 0.1937Epoch 7/15: [============================= ] 61/63 batches, loss: 0.1932Epoch 7/15: [============================= ] 62/63 batches, loss: 0.1928Epoch 7/15: [==============================] 63/63 batches, loss: 0.1905
[2025-05-02 12:48:02,529][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1905
[2025-05-02 12:48:02,743][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1580, Metrics: {'mse': 0.15423144400119781, 'rmse': 0.3927231136579534, 'r2': -1.3772196769714355}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.1176Epoch 8/15: [                              ] 2/63 batches, loss: 0.1678Epoch 8/15: [=                             ] 3/63 batches, loss: 0.2013Epoch 8/15: [=                             ] 4/63 batches, loss: 0.1789Epoch 8/15: [==                            ] 5/63 batches, loss: 0.1633Epoch 8/15: [==                            ] 6/63 batches, loss: 0.1590Epoch 8/15: [===                           ] 7/63 batches, loss: 0.1641Epoch 8/15: [===                           ] 8/63 batches, loss: 0.1582Epoch 8/15: [====                          ] 9/63 batches, loss: 0.1592Epoch 8/15: [====                          ] 10/63 batches, loss: 0.1760Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.1684Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.1830Epoch 8/15: [======                        ] 13/63 batches, loss: 0.1858Epoch 8/15: [======                        ] 14/63 batches, loss: 0.1819Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.1774Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.1782Epoch 8/15: [========                      ] 17/63 batches, loss: 0.1754Epoch 8/15: [========                      ] 18/63 batches, loss: 0.1769Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.1812Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.1773Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.1748Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.1728Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.1731Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.1712Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.1675Epoch 8/15: [============                  ] 26/63 batches, loss: 0.1695Epoch 8/15: [============                  ] 27/63 batches, loss: 0.1713Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.1756Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.1752Epoch 8/15: [==============                ] 30/63 batches, loss: 0.1727Epoch 8/15: [==============                ] 31/63 batches, loss: 0.1719Epoch 8/15: [===============               ] 32/63 batches, loss: 0.1706Epoch 8/15: [===============               ] 33/63 batches, loss: 0.1684Epoch 8/15: [================              ] 34/63 batches, loss: 0.1652Epoch 8/15: [================              ] 35/63 batches, loss: 0.1641Epoch 8/15: [=================             ] 36/63 batches, loss: 0.1645Epoch 8/15: [=================             ] 37/63 batches, loss: 0.1657Epoch 8/15: [==================            ] 38/63 batches, loss: 0.1642Epoch 8/15: [==================            ] 39/63 batches, loss: 0.1632Epoch 8/15: [===================           ] 40/63 batches, loss: 0.1629Epoch 8/15: [===================           ] 41/63 batches, loss: 0.1640Epoch 8/15: [====================          ] 42/63 batches, loss: 0.1613Epoch 8/15: [====================          ] 43/63 batches, loss: 0.1620Epoch 8/15: [====================          ] 44/63 batches, loss: 0.1608Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.1596Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.1591Epoch 8/15: [======================        ] 47/63 batches, loss: 0.1595Epoch 8/15: [======================        ] 48/63 batches, loss: 0.1605Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.1595Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.1610Epoch 8/15: [========================      ] 51/63 batches, loss: 0.1604Epoch 8/15: [========================      ] 52/63 batches, loss: 0.1610Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.1592Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.1600Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.1593Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.1580Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.1585Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.1584Epoch 8/15: [============================  ] 59/63 batches, loss: 0.1582Epoch 8/15: [============================  ] 60/63 batches, loss: 0.1608Epoch 8/15: [============================= ] 61/63 batches, loss: 0.1598Epoch 8/15: [============================= ] 62/63 batches, loss: 0.1604Epoch 8/15: [==============================] 63/63 batches, loss: 0.1624
[2025-05-02 12:48:05,104][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.1624
[2025-05-02 12:48:05,341][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1515, Metrics: {'mse': 0.1475682109594345, 'rmse': 0.3841460802343745, 'r2': -1.2745168209075928}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1311Epoch 9/15: [                              ] 2/63 batches, loss: 0.1238Epoch 9/15: [=                             ] 3/63 batches, loss: 0.1165Epoch 9/15: [=                             ] 4/63 batches, loss: 0.1428Epoch 9/15: [==                            ] 5/63 batches, loss: 0.1445Epoch 9/15: [==                            ] 6/63 batches, loss: 0.1358Epoch 9/15: [===                           ] 7/63 batches, loss: 0.1534Epoch 9/15: [===                           ] 8/63 batches, loss: 0.1477Epoch 9/15: [====                          ] 9/63 batches, loss: 0.1544Epoch 9/15: [====                          ] 10/63 batches, loss: 0.1557Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.1664Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.1633Epoch 9/15: [======                        ] 13/63 batches, loss: 0.1633Epoch 9/15: [======                        ] 14/63 batches, loss: 0.1607Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.1582Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.1603Epoch 9/15: [========                      ] 17/63 batches, loss: 0.1571Epoch 9/15: [========                      ] 18/63 batches, loss: 0.1546Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.1580Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.1583Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.1568Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.1539Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.1507Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.1486Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.1445Epoch 9/15: [============                  ] 26/63 batches, loss: 0.1609Epoch 9/15: [============                  ] 27/63 batches, loss: 0.1602Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.1607Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.1592Epoch 9/15: [==============                ] 30/63 batches, loss: 0.1583Epoch 9/15: [==============                ] 31/63 batches, loss: 0.1627Epoch 9/15: [===============               ] 32/63 batches, loss: 0.1641Epoch 9/15: [===============               ] 33/63 batches, loss: 0.1642Epoch 9/15: [================              ] 34/63 batches, loss: 0.1630Epoch 9/15: [================              ] 35/63 batches, loss: 0.1614Epoch 9/15: [=================             ] 36/63 batches, loss: 0.1640Epoch 9/15: [=================             ] 37/63 batches, loss: 0.1611Epoch 9/15: [==================            ] 38/63 batches, loss: 0.1616Epoch 9/15: [==================            ] 39/63 batches, loss: 0.1639Epoch 9/15: [===================           ] 40/63 batches, loss: 0.1621Epoch 9/15: [===================           ] 41/63 batches, loss: 0.1626Epoch 9/15: [====================          ] 42/63 batches, loss: 0.1632Epoch 9/15: [====================          ] 43/63 batches, loss: 0.1611Epoch 9/15: [====================          ] 44/63 batches, loss: 0.1627Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.1621Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.1626Epoch 9/15: [======================        ] 47/63 batches, loss: 0.1624Epoch 9/15: [======================        ] 48/63 batches, loss: 0.1647Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.1632Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.1638Epoch 9/15: [========================      ] 51/63 batches, loss: 0.1628Epoch 9/15: [========================      ] 52/63 batches, loss: 0.1623Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.1606Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.1603Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.1597Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.1594Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.1618Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.1611Epoch 9/15: [============================  ] 59/63 batches, loss: 0.1628Epoch 9/15: [============================  ] 60/63 batches, loss: 0.1659Epoch 9/15: [============================= ] 61/63 batches, loss: 0.1660Epoch 9/15: [============================= ] 62/63 batches, loss: 0.1657Epoch 9/15: [==============================] 63/63 batches, loss: 0.1660
[2025-05-02 12:48:07,653][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.1660
[2025-05-02 12:48:07,877][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.1419, Metrics: {'mse': 0.1380455642938614, 'rmse': 0.37154483483674133, 'r2': -1.1277413368225098}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.1761Epoch 10/15: [                              ] 2/63 batches, loss: 0.2618Epoch 10/15: [=                             ] 3/63 batches, loss: 0.2103Epoch 10/15: [=                             ] 4/63 batches, loss: 0.1725Epoch 10/15: [==                            ] 5/63 batches, loss: 0.1812Epoch 10/15: [==                            ] 6/63 batches, loss: 0.1813Epoch 10/15: [===                           ] 7/63 batches, loss: 0.1756Epoch 10/15: [===                           ] 8/63 batches, loss: 0.1687Epoch 10/15: [====                          ] 9/63 batches, loss: 0.1600Epoch 10/15: [====                          ] 10/63 batches, loss: 0.1679Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.1681Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.1691Epoch 10/15: [======                        ] 13/63 batches, loss: 0.1625Epoch 10/15: [======                        ] 14/63 batches, loss: 0.1675Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.1717Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.1738Epoch 10/15: [========                      ] 17/63 batches, loss: 0.1686Epoch 10/15: [========                      ] 18/63 batches, loss: 0.1617Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.1574Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.1574Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.1593Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.1586Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.1572Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.1620Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.1608Epoch 10/15: [============                  ] 26/63 batches, loss: 0.1592Epoch 10/15: [============                  ] 27/63 batches, loss: 0.1607Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.1609Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.1615Epoch 10/15: [==============                ] 30/63 batches, loss: 0.1595Epoch 10/15: [==============                ] 31/63 batches, loss: 0.1607Epoch 10/15: [===============               ] 32/63 batches, loss: 0.1615Epoch 10/15: [===============               ] 33/63 batches, loss: 0.1609Epoch 10/15: [================              ] 34/63 batches, loss: 0.1603Epoch 10/15: [================              ] 35/63 batches, loss: 0.1609Epoch 10/15: [=================             ] 36/63 batches, loss: 0.1629Epoch 10/15: [=================             ] 37/63 batches, loss: 0.1616Epoch 10/15: [==================            ] 38/63 batches, loss: 0.1600Epoch 10/15: [==================            ] 39/63 batches, loss: 0.1590Epoch 10/15: [===================           ] 40/63 batches, loss: 0.1587Epoch 10/15: [===================           ] 41/63 batches, loss: 0.1574Epoch 10/15: [====================          ] 42/63 batches, loss: 0.1573Epoch 10/15: [====================          ] 43/63 batches, loss: 0.1581Epoch 10/15: [====================          ] 44/63 batches, loss: 0.1575Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.1569Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.1551Epoch 10/15: [======================        ] 47/63 batches, loss: 0.1547Epoch 10/15: [======================        ] 48/63 batches, loss: 0.1550Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.1540Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.1532Epoch 10/15: [========================      ] 51/63 batches, loss: 0.1550Epoch 10/15: [========================      ] 52/63 batches, loss: 0.1539Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.1528Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.1542Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.1542Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.1538Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.1531Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.1528Epoch 10/15: [============================  ] 59/63 batches, loss: 0.1519Epoch 10/15: [============================  ] 60/63 batches, loss: 0.1529Epoch 10/15: [============================= ] 61/63 batches, loss: 0.1535Epoch 10/15: [============================= ] 62/63 batches, loss: 0.1543Epoch 10/15: [==============================] 63/63 batches, loss: 0.1524
[2025-05-02 12:48:10,248][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.1524
[2025-05-02 12:48:10,483][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.1312, Metrics: {'mse': 0.12756535410881042, 'rmse': 0.3571629237600264, 'r2': -0.9662063121795654}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.1793Epoch 11/15: [                              ] 2/63 batches, loss: 0.1754Epoch 11/15: [=                             ] 3/63 batches, loss: 0.1530Epoch 11/15: [=                             ] 4/63 batches, loss: 0.1378Epoch 11/15: [==                            ] 5/63 batches, loss: 0.1343Epoch 11/15: [==                            ] 6/63 batches, loss: 0.1225Epoch 11/15: [===                           ] 7/63 batches, loss: 0.1179Epoch 11/15: [===                           ] 8/63 batches, loss: 0.1191Epoch 11/15: [====                          ] 9/63 batches, loss: 0.1235Epoch 11/15: [====                          ] 10/63 batches, loss: 0.1228Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.1179Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.1250Epoch 11/15: [======                        ] 13/63 batches, loss: 0.1253Epoch 11/15: [======                        ] 14/63 batches, loss: 0.1352Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.1321Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.1344Epoch 11/15: [========                      ] 17/63 batches, loss: 0.1333Epoch 11/15: [========                      ] 18/63 batches, loss: 0.1344Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.1304Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.1322Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.1322Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.1320Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.1313Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.1333Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.1341Epoch 11/15: [============                  ] 26/63 batches, loss: 0.1317Epoch 11/15: [============                  ] 27/63 batches, loss: 0.1345Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.1366Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.1346Epoch 11/15: [==============                ] 30/63 batches, loss: 0.1343Epoch 11/15: [==============                ] 31/63 batches, loss: 0.1363Epoch 11/15: [===============               ] 32/63 batches, loss: 0.1406Epoch 11/15: [===============               ] 33/63 batches, loss: 0.1397Epoch 11/15: [================              ] 34/63 batches, loss: 0.1374Epoch 11/15: [================              ] 35/63 batches, loss: 0.1388Epoch 11/15: [=================             ] 36/63 batches, loss: 0.1396Epoch 11/15: [=================             ] 37/63 batches, loss: 0.1380Epoch 11/15: [==================            ] 38/63 batches, loss: 0.1362Epoch 11/15: [==================            ] 39/63 batches, loss: 0.1381Epoch 11/15: [===================           ] 40/63 batches, loss: 0.1393Epoch 11/15: [===================           ] 41/63 batches, loss: 0.1397Epoch 11/15: [====================          ] 42/63 batches, loss: 0.1399Epoch 11/15: [====================          ] 43/63 batches, loss: 0.1382Epoch 11/15: [====================          ] 44/63 batches, loss: 0.1369Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.1394Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.1385Epoch 11/15: [======================        ] 47/63 batches, loss: 0.1389Epoch 11/15: [======================        ] 48/63 batches, loss: 0.1387Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.1380Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.1385Epoch 11/15: [========================      ] 51/63 batches, loss: 0.1379Epoch 11/15: [========================      ] 52/63 batches, loss: 0.1383Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.1402Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.1394Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.1386Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.1375Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.1358Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.1357Epoch 11/15: [============================  ] 59/63 batches, loss: 0.1356Epoch 11/15: [============================  ] 60/63 batches, loss: 0.1354Epoch 11/15: [============================= ] 61/63 batches, loss: 0.1367Epoch 11/15: [============================= ] 62/63 batches, loss: 0.1359Epoch 11/15: [==============================] 63/63 batches, loss: 0.1365
[2025-05-02 12:48:12,846][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.1365
[2025-05-02 12:48:13,062][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.1281, Metrics: {'mse': 0.1244121789932251, 'rmse': 0.35272110653209443, 'r2': -0.9176054000854492}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.1042Epoch 12/15: [                              ] 2/63 batches, loss: 0.0949Epoch 12/15: [=                             ] 3/63 batches, loss: 0.0973Epoch 12/15: [=                             ] 4/63 batches, loss: 0.1138Epoch 12/15: [==                            ] 5/63 batches, loss: 0.1478Epoch 12/15: [==                            ] 6/63 batches, loss: 0.1420Epoch 12/15: [===                           ] 7/63 batches, loss: 0.1372Epoch 12/15: [===                           ] 8/63 batches, loss: 0.1285Epoch 12/15: [====                          ] 9/63 batches, loss: 0.1472Epoch 12/15: [====                          ] 10/63 batches, loss: 0.1478Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.1435Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.1416Epoch 12/15: [======                        ] 13/63 batches, loss: 0.1444Epoch 12/15: [======                        ] 14/63 batches, loss: 0.1434Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.1480Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.1522Epoch 12/15: [========                      ] 17/63 batches, loss: 0.1511Epoch 12/15: [========                      ] 18/63 batches, loss: 0.1471Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.1446Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.1439Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.1453Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.1452Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.1472Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.1442Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.1417Epoch 12/15: [============                  ] 26/63 batches, loss: 0.1403Epoch 12/15: [============                  ] 27/63 batches, loss: 0.1384Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.1357Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.1340Epoch 12/15: [==============                ] 30/63 batches, loss: 0.1323Epoch 12/15: [==============                ] 31/63 batches, loss: 0.1307Epoch 12/15: [===============               ] 32/63 batches, loss: 0.1297Epoch 12/15: [===============               ] 33/63 batches, loss: 0.1321Epoch 12/15: [================              ] 34/63 batches, loss: 0.1321Epoch 12/15: [================              ] 35/63 batches, loss: 0.1341Epoch 12/15: [=================             ] 36/63 batches, loss: 0.1326Epoch 12/15: [=================             ] 37/63 batches, loss: 0.1330Epoch 12/15: [==================            ] 38/63 batches, loss: 0.1311Epoch 12/15: [==================            ] 39/63 batches, loss: 0.1328Epoch 12/15: [===================           ] 40/63 batches, loss: 0.1313Epoch 12/15: [===================           ] 41/63 batches, loss: 0.1318Epoch 12/15: [====================          ] 42/63 batches, loss: 0.1319Epoch 12/15: [====================          ] 43/63 batches, loss: 0.1322Epoch 12/15: [====================          ] 44/63 batches, loss: 0.1311Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.1334Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.1325Epoch 12/15: [======================        ] 47/63 batches, loss: 0.1332Epoch 12/15: [======================        ] 48/63 batches, loss: 0.1322Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.1324Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.1333Epoch 12/15: [========================      ] 51/63 batches, loss: 0.1333Epoch 12/15: [========================      ] 52/63 batches, loss: 0.1323Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.1321Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.1324Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.1323Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.1324Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.1317Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.1312Epoch 12/15: [============================  ] 59/63 batches, loss: 0.1330Epoch 12/15: [============================  ] 60/63 batches, loss: 0.1323Epoch 12/15: [============================= ] 61/63 batches, loss: 0.1321Epoch 12/15: [============================= ] 62/63 batches, loss: 0.1318Epoch 12/15: [==============================] 63/63 batches, loss: 0.1332
[2025-05-02 12:48:15,428][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.1332
[2025-05-02 12:48:15,651][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.1215, Metrics: {'mse': 0.1177942305803299, 'rmse': 0.3432116410909308, 'r2': -0.8156008720397949}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.0966Epoch 13/15: [                              ] 2/63 batches, loss: 0.1267Epoch 13/15: [=                             ] 3/63 batches, loss: 0.1244Epoch 13/15: [=                             ] 4/63 batches, loss: 0.1348Epoch 13/15: [==                            ] 5/63 batches, loss: 0.1169Epoch 13/15: [==                            ] 6/63 batches, loss: 0.1141Epoch 13/15: [===                           ] 7/63 batches, loss: 0.1064Epoch 13/15: [===                           ] 8/63 batches, loss: 0.1115Epoch 13/15: [====                          ] 9/63 batches, loss: 0.1061Epoch 13/15: [====                          ] 10/63 batches, loss: 0.1038Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.1081Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.1061Epoch 13/15: [======                        ] 13/63 batches, loss: 0.1070Epoch 13/15: [======                        ] 14/63 batches, loss: 0.1169Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.1259Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.1295Epoch 13/15: [========                      ] 17/63 batches, loss: 0.1283Epoch 13/15: [========                      ] 18/63 batches, loss: 0.1278Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.1277Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.1344Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.1335Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.1332Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.1325Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.1317Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.1316Epoch 13/15: [============                  ] 26/63 batches, loss: 0.1316Epoch 13/15: [============                  ] 27/63 batches, loss: 0.1327Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.1329Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.1329Epoch 13/15: [==============                ] 30/63 batches, loss: 0.1322Epoch 13/15: [==============                ] 31/63 batches, loss: 0.1309Epoch 13/15: [===============               ] 32/63 batches, loss: 0.1322Epoch 13/15: [===============               ] 33/63 batches, loss: 0.1335Epoch 13/15: [================              ] 34/63 batches, loss: 0.1329Epoch 13/15: [================              ] 35/63 batches, loss: 0.1319Epoch 13/15: [=================             ] 36/63 batches, loss: 0.1297Epoch 13/15: [=================             ] 37/63 batches, loss: 0.1289Epoch 13/15: [==================            ] 38/63 batches, loss: 0.1286Epoch 13/15: [==================            ] 39/63 batches, loss: 0.1273Epoch 13/15: [===================           ] 40/63 batches, loss: 0.1287Epoch 13/15: [===================           ] 41/63 batches, loss: 0.1288Epoch 13/15: [====================          ] 42/63 batches, loss: 0.1282Epoch 13/15: [====================          ] 43/63 batches, loss: 0.1274Epoch 13/15: [====================          ] 44/63 batches, loss: 0.1260Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.1283Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.1277Epoch 13/15: [======================        ] 47/63 batches, loss: 0.1290Epoch 13/15: [======================        ] 48/63 batches, loss: 0.1321Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.1359Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.1361Epoch 13/15: [========================      ] 51/63 batches, loss: 0.1355Epoch 13/15: [========================      ] 52/63 batches, loss: 0.1350Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.1354Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.1348Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.1333Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.1328Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.1318Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.1310Epoch 13/15: [============================  ] 59/63 batches, loss: 0.1303Epoch 13/15: [============================  ] 60/63 batches, loss: 0.1301Epoch 13/15: [============================= ] 61/63 batches, loss: 0.1316Epoch 13/15: [============================= ] 62/63 batches, loss: 0.1302Epoch 13/15: [==============================] 63/63 batches, loss: 0.1283
[2025-05-02 12:48:17,978][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.1283
[2025-05-02 12:48:18,200][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.1073, Metrics: {'mse': 0.10405903309583664, 'rmse': 0.32258182387703843, 'r2': -0.6038957834243774}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.0920Epoch 14/15: [                              ] 2/63 batches, loss: 0.0797Epoch 14/15: [=                             ] 3/63 batches, loss: 0.0941Epoch 14/15: [=                             ] 4/63 batches, loss: 0.0941Epoch 14/15: [==                            ] 5/63 batches, loss: 0.1126Epoch 14/15: [==                            ] 6/63 batches, loss: 0.1063Epoch 14/15: [===                           ] 7/63 batches, loss: 0.0987Epoch 14/15: [===                           ] 8/63 batches, loss: 0.1041Epoch 14/15: [====                          ] 9/63 batches, loss: 0.1036Epoch 14/15: [====                          ] 10/63 batches, loss: 0.0970Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.1018Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.0977Epoch 14/15: [======                        ] 13/63 batches, loss: 0.0947Epoch 14/15: [======                        ] 14/63 batches, loss: 0.0981Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.0996Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.0986Epoch 14/15: [========                      ] 17/63 batches, loss: 0.1022Epoch 14/15: [========                      ] 18/63 batches, loss: 0.1081Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.1060Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.1084Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.1100Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.1115Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.1129Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.1112Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.1160Epoch 14/15: [============                  ] 26/63 batches, loss: 0.1154Epoch 14/15: [============                  ] 27/63 batches, loss: 0.1163Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.1139Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.1143Epoch 14/15: [==============                ] 30/63 batches, loss: 0.1166Epoch 14/15: [==============                ] 31/63 batches, loss: 0.1183Epoch 14/15: [===============               ] 32/63 batches, loss: 0.1172Epoch 14/15: [===============               ] 33/63 batches, loss: 0.1179Epoch 14/15: [================              ] 34/63 batches, loss: 0.1173Epoch 14/15: [================              ] 35/63 batches, loss: 0.1167Epoch 14/15: [=================             ] 36/63 batches, loss: 0.1170Epoch 14/15: [=================             ] 37/63 batches, loss: 0.1160Epoch 14/15: [==================            ] 38/63 batches, loss: 0.1155Epoch 14/15: [==================            ] 39/63 batches, loss: 0.1193Epoch 14/15: [===================           ] 40/63 batches, loss: 0.1197Epoch 14/15: [===================           ] 41/63 batches, loss: 0.1189Epoch 14/15: [====================          ] 42/63 batches, loss: 0.1195Epoch 14/15: [====================          ] 43/63 batches, loss: 0.1193Epoch 14/15: [====================          ] 44/63 batches, loss: 0.1195Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.1200Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.1202Epoch 14/15: [======================        ] 47/63 batches, loss: 0.1196Epoch 14/15: [======================        ] 48/63 batches, loss: 0.1206Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.1204Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.1203Epoch 14/15: [========================      ] 51/63 batches, loss: 0.1210Epoch 14/15: [========================      ] 52/63 batches, loss: 0.1194Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.1209Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.1216Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.1216Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.1220Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.1230Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.1218Epoch 14/15: [============================  ] 59/63 batches, loss: 0.1223Epoch 14/15: [============================  ] 60/63 batches, loss: 0.1211Epoch 14/15: [============================= ] 61/63 batches, loss: 0.1212Epoch 14/15: [============================= ] 62/63 batches, loss: 0.1204Epoch 14/15: [==============================] 63/63 batches, loss: 0.1193
[2025-05-02 12:48:20,585][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.1193
[2025-05-02 12:48:20,801][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.1035, Metrics: {'mse': 0.10027144104242325, 'rmse': 0.3166566611369848, 'r2': -0.5455163717269897}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.1137Epoch 15/15: [                              ] 2/63 batches, loss: 0.0960Epoch 15/15: [=                             ] 3/63 batches, loss: 0.0765Epoch 15/15: [=                             ] 4/63 batches, loss: 0.0878Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0908Epoch 15/15: [==                            ] 6/63 batches, loss: 0.1182Epoch 15/15: [===                           ] 7/63 batches, loss: 0.1075Epoch 15/15: [===                           ] 8/63 batches, loss: 0.1026Epoch 15/15: [====                          ] 9/63 batches, loss: 0.1093Epoch 15/15: [====                          ] 10/63 batches, loss: 0.1082Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.1087Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.1061Epoch 15/15: [======                        ] 13/63 batches, loss: 0.1098Epoch 15/15: [======                        ] 14/63 batches, loss: 0.1063Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.1038Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.1053Epoch 15/15: [========                      ] 17/63 batches, loss: 0.1074Epoch 15/15: [========                      ] 18/63 batches, loss: 0.1057Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.1056Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.1072Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.1045Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.1040Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.1042Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.1024Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.1014Epoch 15/15: [============                  ] 26/63 batches, loss: 0.1015Epoch 15/15: [============                  ] 27/63 batches, loss: 0.1010Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.1001Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.1025Epoch 15/15: [==============                ] 30/63 batches, loss: 0.1048Epoch 15/15: [==============                ] 31/63 batches, loss: 0.1041Epoch 15/15: [===============               ] 32/63 batches, loss: 0.1037Epoch 15/15: [===============               ] 33/63 batches, loss: 0.1029Epoch 15/15: [================              ] 34/63 batches, loss: 0.1057Epoch 15/15: [================              ] 35/63 batches, loss: 0.1047Epoch 15/15: [=================             ] 36/63 batches, loss: 0.1053Epoch 15/15: [=================             ] 37/63 batches, loss: 0.1063Epoch 15/15: [==================            ] 38/63 batches, loss: 0.1052Epoch 15/15: [==================            ] 39/63 batches, loss: 0.1053Epoch 15/15: [===================           ] 40/63 batches, loss: 0.1052Epoch 15/15: [===================           ] 41/63 batches, loss: 0.1038Epoch 15/15: [====================          ] 42/63 batches, loss: 0.1036Epoch 15/15: [====================          ] 43/63 batches, loss: 0.1038Epoch 15/15: [====================          ] 44/63 batches, loss: 0.1035Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.1018Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.1020Epoch 15/15: [======================        ] 47/63 batches, loss: 0.1017Epoch 15/15: [======================        ] 48/63 batches, loss: 0.1011Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.1007Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.1009Epoch 15/15: [========================      ] 51/63 batches, loss: 0.1018Epoch 15/15: [========================      ] 52/63 batches, loss: 0.1015Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.1005Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.0998Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.0989Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.0979Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.0973Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.0977Epoch 15/15: [============================  ] 59/63 batches, loss: 0.0975Epoch 15/15: [============================  ] 60/63 batches, loss: 0.0993Epoch 15/15: [============================= ] 61/63 batches, loss: 0.0999Epoch 15/15: [============================= ] 62/63 batches, loss: 0.0995Epoch 15/15: [==============================] 63/63 batches, loss: 0.0981
[2025-05-02 12:48:23,206][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0981
[2025-05-02 12:48:23,447][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.1103, Metrics: {'mse': 0.10682888329029083, 'rmse': 0.32684688049649613, 'r2': -0.6465884447097778}
[2025-05-02 12:48:23,448][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-02 12:48:23,448][src.training.lm_trainer][INFO] - Training completed in 37.75 seconds
[2025-05-02 12:48:23,448][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 12:48:26,003][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.0459117628633976, 'rmse': 0.21427030326995292, 'r2': -0.4956256151199341}
[2025-05-02 12:48:26,004][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.10027144104242325, 'rmse': 0.3166566611369848, 'r2': -0.5455163717269897}
[2025-05-02 12:48:26,004][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.12354689836502075, 'rmse': 0.351492387350026, 'r2': -1.1298935413360596}
[2025-05-02 12:48:27,736][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer4/ar/ar/model.pt
[2025-05-02 12:48:27,737][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▅▅▄▄▃▃▂▁▁
wandb:     best_val_mse █▆▅▄▄▃▃▂▂▁▁
wandb:      best_val_r2 ▁▃▄▅▅▆▆▇▇██
wandb:    best_val_rmse █▇▅▅▄▄▃▃▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▃▁▂▃▄▅▅▅▆▆▆▆▇▇
wandb:       train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▆█▇▆▅▄▄▃▃▂▂▂▁▁▁
wandb:          val_mse ▆█▇▆▅▄▄▃▃▂▂▂▁▁▁
wandb:           val_r2 ▃▁▂▃▄▅▅▆▆▇▇▇███
wandb:         val_rmse ▇█▇▇▆▄▄▄▃▃▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.1035
wandb:     best_val_mse 0.10027
wandb:      best_val_r2 -0.54552
wandb:    best_val_rmse 0.31666
wandb:            epoch 15
wandb:   final_test_mse 0.12355
wandb:    final_test_r2 -1.12989
wandb:  final_test_rmse 0.35149
wandb:  final_train_mse 0.04591
wandb:   final_train_r2 -0.49563
wandb: final_train_rmse 0.21427
wandb:    final_val_mse 0.10027
wandb:     final_val_r2 -0.54552
wandb:   final_val_rmse 0.31666
wandb:    learning_rate 2e-05
wandb:       train_loss 0.0981
wandb:       train_time 37.74553
wandb:         val_loss 0.11033
wandb:          val_mse 0.10683
wandb:           val_r2 -0.64659
wandb:         val_rmse 0.32685
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_124736-s32eg2h8
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_124736-s32eg2h8/logs
Experiment probe_layer4_complexity_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer4/ar/ar/results.json for layer 4
=======================
PROBING LAYER 6
=======================
Running experiment: probe_layer6_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=128" "model.probe_depth=2" "model.dropout=0.2" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer6/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 12:48:39,891][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer6/ar
experiment_name: probe_layer6_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 12:48:39,891][__main__][INFO] - Normalized task: question_type
[2025-05-02 12:48:39,892][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 12:48:39,892][__main__][INFO] - Determined Task Type: classification
[2025-05-02 12:48:39,896][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-02 12:48:39,896][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 12:48:41,300][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 12:48:43,507][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 12:48:43,507][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:48:43,572][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:48:43,602][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:48:43,688][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 12:48:43,698][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:48:43,699][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 12:48:43,700][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:48:43,718][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:48:43,747][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:48:43,761][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 12:48:43,762][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:48:43,763][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 12:48:43,764][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:48:43,780][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:48:43,808][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:48:43,831][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 12:48:43,833][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:48:43,834][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 12:48:43,835][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 12:48:43,836][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 12:48:43,836][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 12:48:43,836][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 12:48:43,836][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 12:48:43,836][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-02 12:48:43,837][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-02 12:48:43,837][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 12:48:43,837][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 12:48:43,837][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 12:48:43,837][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 12:48:43,837][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 12:48:43,837][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 12:48:43,837][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-02 12:48:43,837][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-02 12:48:43,837][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 12:48:43,837][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 12:48:43,838][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 12:48:43,838][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 12:48:43,838][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 12:48:43,838][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 12:48:43,838][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-02 12:48:43,838][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-02 12:48:43,838][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 12:48:43,838][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 12:48:43,838][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 12:48:43,838][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 12:48:43,839][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 12:48:43,839][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 12:48:43,840][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 12:48:47,750][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 12:48:47,750][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 12:48:47,751][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-02 12:48:47,751][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-02 12:48:47,753][src.models.model_factory][INFO] - Model has 116,865 trainable parameters out of 394,238,337 total parameters
[2025-05-02 12:48:47,754][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 116,865 trainable parameters
[2025-05-02 12:48:47,754][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=2, activation=gelu, normalization=layer
[2025-05-02 12:48:47,754][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 128 hidden size
[2025-05-02 12:48:47,754][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 12:48:47,755][__main__][INFO] - Total parameters: 394,238,337
[2025-05-02 12:48:47,755][__main__][INFO] - Trainable parameters: 116,865 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7344Epoch 1/15: [                              ] 2/63 batches, loss: 0.7260Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7211Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7208Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7209Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7161Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7094Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7125Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7074Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7027Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7028Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7025Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7020Epoch 1/15: [======                        ] 14/63 batches, loss: 0.7004Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.7002Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.6993Epoch 1/15: [========                      ] 17/63 batches, loss: 0.6988Epoch 1/15: [========                      ] 18/63 batches, loss: 0.6996Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.6983Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.6984Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.6982Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.6994Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.6986Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6982Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.6993Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6961Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6965Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6968Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6963Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6975Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6978Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6979Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6976Epoch 1/15: [================              ] 34/63 batches, loss: 0.6965Epoch 1/15: [================              ] 35/63 batches, loss: 0.6961Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6961Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6957Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6956Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6956Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6948Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6951Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6949Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6955Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6952Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6943Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6939Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6944Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6933Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6930Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6925Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6915Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6913Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6916Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6920Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6917Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6917Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6915Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6918Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6913Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6912Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6914Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6915Epoch 1/15: [==============================] 63/63 batches, loss: 0.6894
[2025-05-02 12:48:52,076][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6894
[2025-05-02 12:48:52,275][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6957, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6966Epoch 2/15: [                              ] 2/63 batches, loss: 0.6895Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6907Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6948Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6860Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6860Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6770Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6746Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6773Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6786Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6768Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6797Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6820Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6846Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6868Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6883Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6875Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6865Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6877Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6881Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6863Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6835Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6818Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6785Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6798Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6825Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6821Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6825Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6811Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6814Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6807Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6791Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6780Epoch 2/15: [================              ] 34/63 batches, loss: 0.6776Epoch 2/15: [================              ] 35/63 batches, loss: 0.6780Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6778Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6750Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6748Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6742Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6747Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6733Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6734Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6743Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6751Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6750Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6755Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6754Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6767Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6778Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6788Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6784Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6787Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6783Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6773Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6771Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6767Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6766Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6752Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6754Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6746Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6738Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6745Epoch 2/15: [==============================] 63/63 batches, loss: 0.6732
[2025-05-02 12:48:54,629][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6732
[2025-05-02 12:48:54,849][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6828, Metrics: {'accuracy': 0.5681818181818182, 'f1': 0.3448275862068966, 'precision': 0.5555555555555556, 'recall': 0.25}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6908Epoch 3/15: [                              ] 2/63 batches, loss: 0.6760Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6783Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6649Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6492Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6489Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6534Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6570Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6552Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6470Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6440Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6439Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6451Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6414Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6446Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6484Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6504Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6506Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6469Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6476Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6457Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6448Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6438Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6424Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6432Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6419Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6445Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6467Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6455Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6431Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6444Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6425Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6420Epoch 3/15: [================              ] 34/63 batches, loss: 0.6438Epoch 3/15: [================              ] 35/63 batches, loss: 0.6459Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6475Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6475Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6487Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6498Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6503Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6515Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6504Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6499Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6493Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6492Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6484Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6476Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6469Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6473Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6475Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6478Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6474Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6471Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6457Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6451Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6433Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6433Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6426Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6420Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6411Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6406Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6394Epoch 3/15: [==============================] 63/63 batches, loss: 0.6416
[2025-05-02 12:48:57,205][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6416
[2025-05-02 12:48:57,411][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6576, Metrics: {'accuracy': 0.75, 'f1': 0.7555555555555555, 'precision': 0.68, 'recall': 0.85}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.5370Epoch 4/15: [                              ] 2/63 batches, loss: 0.5448Epoch 4/15: [=                             ] 3/63 batches, loss: 0.5790Epoch 4/15: [=                             ] 4/63 batches, loss: 0.5894Epoch 4/15: [==                            ] 5/63 batches, loss: 0.6025Epoch 4/15: [==                            ] 6/63 batches, loss: 0.5940Epoch 4/15: [===                           ] 7/63 batches, loss: 0.6023Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6172Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6219Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6245Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6233Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6190Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6212Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6166Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6180Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6215Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6178Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6175Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6160Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6170Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6150Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6165Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6177Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6199Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6200Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6158Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6147Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6123Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6107Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6134Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6132Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6118Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6088Epoch 4/15: [================              ] 34/63 batches, loss: 0.6079Epoch 4/15: [================              ] 35/63 batches, loss: 0.6076Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6068Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6064Epoch 4/15: [==================            ] 38/63 batches, loss: 0.6050Epoch 4/15: [==================            ] 39/63 batches, loss: 0.6055Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6061Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6087Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6083Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6087Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6086Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.6111Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.6100Epoch 4/15: [======================        ] 47/63 batches, loss: 0.6093Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6098Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6099Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6108Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6114Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6122Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6136Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6147Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6146Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6137Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6131Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6143Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6148Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6139Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6139Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6141Epoch 4/15: [==============================] 63/63 batches, loss: 0.6117
[2025-05-02 12:48:59,719][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6117
[2025-05-02 12:48:59,942][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6292, Metrics: {'accuracy': 0.8181818181818182, 'f1': 0.8, 'precision': 0.8, 'recall': 0.8}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.5555Epoch 5/15: [                              ] 2/63 batches, loss: 0.5534Epoch 5/15: [=                             ] 3/63 batches, loss: 0.5477Epoch 5/15: [=                             ] 4/63 batches, loss: 0.5872Epoch 5/15: [==                            ] 5/63 batches, loss: 0.5893Epoch 5/15: [==                            ] 6/63 batches, loss: 0.5834Epoch 5/15: [===                           ] 7/63 batches, loss: 0.5843Epoch 5/15: [===                           ] 8/63 batches, loss: 0.5859Epoch 5/15: [====                          ] 9/63 batches, loss: 0.5834Epoch 5/15: [====                          ] 10/63 batches, loss: 0.5871Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.5942Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.5964Epoch 5/15: [======                        ] 13/63 batches, loss: 0.5932Epoch 5/15: [======                        ] 14/63 batches, loss: 0.5956Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.5917Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.5912Epoch 5/15: [========                      ] 17/63 batches, loss: 0.5894Epoch 5/15: [========                      ] 18/63 batches, loss: 0.5881Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.5895Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.5866Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.5904Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.5930Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.5954Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.5923Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.5917Epoch 5/15: [============                  ] 26/63 batches, loss: 0.5917Epoch 5/15: [============                  ] 27/63 batches, loss: 0.5927Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.5985Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.6004Epoch 5/15: [==============                ] 30/63 batches, loss: 0.5986Epoch 5/15: [==============                ] 31/63 batches, loss: 0.5994Epoch 5/15: [===============               ] 32/63 batches, loss: 0.5979Epoch 5/15: [===============               ] 33/63 batches, loss: 0.5972Epoch 5/15: [================              ] 34/63 batches, loss: 0.5990Epoch 5/15: [================              ] 35/63 batches, loss: 0.5956Epoch 5/15: [=================             ] 36/63 batches, loss: 0.5970Epoch 5/15: [=================             ] 37/63 batches, loss: 0.5967Epoch 5/15: [==================            ] 38/63 batches, loss: 0.5979Epoch 5/15: [==================            ] 39/63 batches, loss: 0.5984Epoch 5/15: [===================           ] 40/63 batches, loss: 0.5992Epoch 5/15: [===================           ] 41/63 batches, loss: 0.5988Epoch 5/15: [====================          ] 42/63 batches, loss: 0.5975Epoch 5/15: [====================          ] 43/63 batches, loss: 0.5971Epoch 5/15: [====================          ] 44/63 batches, loss: 0.5969Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.5972Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.5989Epoch 5/15: [======================        ] 47/63 batches, loss: 0.5975Epoch 5/15: [======================        ] 48/63 batches, loss: 0.5991Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.5994Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.5993Epoch 5/15: [========================      ] 51/63 batches, loss: 0.5996Epoch 5/15: [========================      ] 52/63 batches, loss: 0.5978Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.5968Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.5964Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.5964Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.5966Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.5954Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.5938Epoch 5/15: [============================  ] 59/63 batches, loss: 0.5932Epoch 5/15: [============================  ] 60/63 batches, loss: 0.5930Epoch 5/15: [============================= ] 61/63 batches, loss: 0.5920Epoch 5/15: [============================= ] 62/63 batches, loss: 0.5914Epoch 5/15: [==============================] 63/63 batches, loss: 0.5949
[2025-05-02 12:49:02,247][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5949
[2025-05-02 12:49:02,466][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.6035, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8837209302325582, 'precision': 0.8260869565217391, 'recall': 0.95}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.5202Epoch 6/15: [                              ] 2/63 batches, loss: 0.5803Epoch 6/15: [=                             ] 3/63 batches, loss: 0.5741Epoch 6/15: [=                             ] 4/63 batches, loss: 0.5763Epoch 6/15: [==                            ] 5/63 batches, loss: 0.5835Epoch 6/15: [==                            ] 6/63 batches, loss: 0.5713Epoch 6/15: [===                           ] 7/63 batches, loss: 0.5715Epoch 6/15: [===                           ] 8/63 batches, loss: 0.5719Epoch 6/15: [====                          ] 9/63 batches, loss: 0.5779Epoch 6/15: [====                          ] 10/63 batches, loss: 0.5734Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.5745Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.5700Epoch 6/15: [======                        ] 13/63 batches, loss: 0.5798Epoch 6/15: [======                        ] 14/63 batches, loss: 0.5775Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.5735Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.5721Epoch 6/15: [========                      ] 17/63 batches, loss: 0.5723Epoch 6/15: [========                      ] 18/63 batches, loss: 0.5757Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.5724Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.5734Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.5766Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.5782Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.5741Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.5730Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.5713Epoch 6/15: [============                  ] 26/63 batches, loss: 0.5702Epoch 6/15: [============                  ] 27/63 batches, loss: 0.5708Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.5713Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.5703Epoch 6/15: [==============                ] 30/63 batches, loss: 0.5692Epoch 6/15: [==============                ] 31/63 batches, loss: 0.5690Epoch 6/15: [===============               ] 32/63 batches, loss: 0.5699Epoch 6/15: [===============               ] 33/63 batches, loss: 0.5703Epoch 6/15: [================              ] 34/63 batches, loss: 0.5713Epoch 6/15: [================              ] 35/63 batches, loss: 0.5696Epoch 6/15: [=================             ] 36/63 batches, loss: 0.5679Epoch 6/15: [=================             ] 37/63 batches, loss: 0.5693Epoch 6/15: [==================            ] 38/63 batches, loss: 0.5708Epoch 6/15: [==================            ] 39/63 batches, loss: 0.5729Epoch 6/15: [===================           ] 40/63 batches, loss: 0.5744Epoch 6/15: [===================           ] 41/63 batches, loss: 0.5733Epoch 6/15: [====================          ] 42/63 batches, loss: 0.5698Epoch 6/15: [====================          ] 43/63 batches, loss: 0.5698Epoch 6/15: [====================          ] 44/63 batches, loss: 0.5679Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.5684Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.5701Epoch 6/15: [======================        ] 47/63 batches, loss: 0.5704Epoch 6/15: [======================        ] 48/63 batches, loss: 0.5706Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.5705Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.5711Epoch 6/15: [========================      ] 51/63 batches, loss: 0.5724Epoch 6/15: [========================      ] 52/63 batches, loss: 0.5719Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.5711Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.5720Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.5702Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.5712Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.5716Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.5737Epoch 6/15: [============================  ] 59/63 batches, loss: 0.5744Epoch 6/15: [============================  ] 60/63 batches, loss: 0.5752Epoch 6/15: [============================= ] 61/63 batches, loss: 0.5751Epoch 6/15: [============================= ] 62/63 batches, loss: 0.5752Epoch 6/15: [==============================] 63/63 batches, loss: 0.5751
[2025-05-02 12:49:04,841][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5751
[2025-05-02 12:49:05,048][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5961, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.8292682926829268, 'precision': 0.8095238095238095, 'recall': 0.85}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.5687Epoch 7/15: [                              ] 2/63 batches, loss: 0.5924Epoch 7/15: [=                             ] 3/63 batches, loss: 0.5713Epoch 7/15: [=                             ] 4/63 batches, loss: 0.5532Epoch 7/15: [==                            ] 5/63 batches, loss: 0.5607Epoch 7/15: [==                            ] 6/63 batches, loss: 0.5705Epoch 7/15: [===                           ] 7/63 batches, loss: 0.5697Epoch 7/15: [===                           ] 8/63 batches, loss: 0.5611Epoch 7/15: [====                          ] 9/63 batches, loss: 0.5598Epoch 7/15: [====                          ] 10/63 batches, loss: 0.5603Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.5560Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.5571Epoch 7/15: [======                        ] 13/63 batches, loss: 0.5523Epoch 7/15: [======                        ] 14/63 batches, loss: 0.5547Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.5587Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.5590Epoch 7/15: [========                      ] 17/63 batches, loss: 0.5587Epoch 7/15: [========                      ] 18/63 batches, loss: 0.5571Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.5578Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.5572Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.5539Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.5603Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.5600Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.5593Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.5574Epoch 7/15: [============                  ] 26/63 batches, loss: 0.5539Epoch 7/15: [============                  ] 27/63 batches, loss: 0.5547Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.5586Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.5568Epoch 7/15: [==============                ] 30/63 batches, loss: 0.5540Epoch 7/15: [==============                ] 31/63 batches, loss: 0.5544Epoch 7/15: [===============               ] 32/63 batches, loss: 0.5583Epoch 7/15: [===============               ] 33/63 batches, loss: 0.5580Epoch 7/15: [================              ] 34/63 batches, loss: 0.5599Epoch 7/15: [================              ] 35/63 batches, loss: 0.5583Epoch 7/15: [=================             ] 36/63 batches, loss: 0.5594Epoch 7/15: [=================             ] 37/63 batches, loss: 0.5596Epoch 7/15: [==================            ] 38/63 batches, loss: 0.5594Epoch 7/15: [==================            ] 39/63 batches, loss: 0.5580Epoch 7/15: [===================           ] 40/63 batches, loss: 0.5561Epoch 7/15: [===================           ] 41/63 batches, loss: 0.5559Epoch 7/15: [====================          ] 42/63 batches, loss: 0.5571Epoch 7/15: [====================          ] 43/63 batches, loss: 0.5572Epoch 7/15: [====================          ] 44/63 batches, loss: 0.5569Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.5563Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.5569Epoch 7/15: [======================        ] 47/63 batches, loss: 0.5579Epoch 7/15: [======================        ] 48/63 batches, loss: 0.5586Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.5597Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.5604Epoch 7/15: [========================      ] 51/63 batches, loss: 0.5605Epoch 7/15: [========================      ] 52/63 batches, loss: 0.5617Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.5601Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.5603Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.5601Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.5615Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.5615Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.5602Epoch 7/15: [============================  ] 59/63 batches, loss: 0.5613Epoch 7/15: [============================  ] 60/63 batches, loss: 0.5602Epoch 7/15: [============================= ] 61/63 batches, loss: 0.5606Epoch 7/15: [============================= ] 62/63 batches, loss: 0.5600Epoch 7/15: [==============================] 63/63 batches, loss: 0.5605
[2025-05-02 12:49:07,402][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5605
[2025-05-02 12:49:07,611][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5883, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8571428571428571, 'precision': 0.8181818181818182, 'recall': 0.9}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.6526Epoch 8/15: [                              ] 2/63 batches, loss: 0.5910Epoch 8/15: [=                             ] 3/63 batches, loss: 0.5623Epoch 8/15: [=                             ] 4/63 batches, loss: 0.5550Epoch 8/15: [==                            ] 5/63 batches, loss: 0.5697Epoch 8/15: [==                            ] 6/63 batches, loss: 0.5695Epoch 8/15: [===                           ] 7/63 batches, loss: 0.5665Epoch 8/15: [===                           ] 8/63 batches, loss: 0.5553Epoch 8/15: [====                          ] 9/63 batches, loss: 0.5605Epoch 8/15: [====                          ] 10/63 batches, loss: 0.5646Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.5625Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.5631Epoch 8/15: [======                        ] 13/63 batches, loss: 0.5588Epoch 8/15: [======                        ] 14/63 batches, loss: 0.5597Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.5616Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.5607Epoch 8/15: [========                      ] 17/63 batches, loss: 0.5608Epoch 8/15: [========                      ] 18/63 batches, loss: 0.5589Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.5590Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.5564Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.5571Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.5553Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.5556Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.5549Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.5543Epoch 8/15: [============                  ] 26/63 batches, loss: 0.5530Epoch 8/15: [============                  ] 27/63 batches, loss: 0.5513Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.5514Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.5522Epoch 8/15: [==============                ] 30/63 batches, loss: 0.5534Epoch 8/15: [==============                ] 31/63 batches, loss: 0.5549Epoch 8/15: [===============               ] 32/63 batches, loss: 0.5538Epoch 8/15: [===============               ] 33/63 batches, loss: 0.5522Epoch 8/15: [================              ] 34/63 batches, loss: 0.5526Epoch 8/15: [================              ] 35/63 batches, loss: 0.5501Epoch 8/15: [=================             ] 36/63 batches, loss: 0.5503Epoch 8/15: [=================             ] 37/63 batches, loss: 0.5517Epoch 8/15: [==================            ] 38/63 batches, loss: 0.5523Epoch 8/15: [==================            ] 39/63 batches, loss: 0.5539Epoch 8/15: [===================           ] 40/63 batches, loss: 0.5508Epoch 8/15: [===================           ] 41/63 batches, loss: 0.5508Epoch 8/15: [====================          ] 42/63 batches, loss: 0.5508Epoch 8/15: [====================          ] 43/63 batches, loss: 0.5488Epoch 8/15: [====================          ] 44/63 batches, loss: 0.5499Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.5497Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.5476Epoch 8/15: [======================        ] 47/63 batches, loss: 0.5463Epoch 8/15: [======================        ] 48/63 batches, loss: 0.5467Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.5494Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.5513Epoch 8/15: [========================      ] 51/63 batches, loss: 0.5534Epoch 8/15: [========================      ] 52/63 batches, loss: 0.5530Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.5533Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.5537Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.5569Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.5580Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.5590Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.5584Epoch 8/15: [============================  ] 59/63 batches, loss: 0.5576Epoch 8/15: [============================  ] 60/63 batches, loss: 0.5577Epoch 8/15: [============================= ] 61/63 batches, loss: 0.5581Epoch 8/15: [============================= ] 62/63 batches, loss: 0.5573Epoch 8/15: [==============================] 63/63 batches, loss: 0.5560
[2025-05-02 12:49:09,988][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5560
[2025-05-02 12:49:10,230][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5837, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8837209302325582, 'precision': 0.8260869565217391, 'recall': 0.95}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.4904Epoch 9/15: [                              ] 2/63 batches, loss: 0.5505Epoch 9/15: [=                             ] 3/63 batches, loss: 0.5470Epoch 9/15: [=                             ] 4/63 batches, loss: 0.5446Epoch 9/15: [==                            ] 5/63 batches, loss: 0.5576Epoch 9/15: [==                            ] 6/63 batches, loss: 0.5673Epoch 9/15: [===                           ] 7/63 batches, loss: 0.5736Epoch 9/15: [===                           ] 8/63 batches, loss: 0.5591Epoch 9/15: [====                          ] 9/63 batches, loss: 0.5659Epoch 9/15: [====                          ] 10/63 batches, loss: 0.5581Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.5577Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.5525Epoch 9/15: [======                        ] 13/63 batches, loss: 0.5608Epoch 9/15: [======                        ] 14/63 batches, loss: 0.5599Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.5596Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.5626Epoch 9/15: [========                      ] 17/63 batches, loss: 0.5623Epoch 9/15: [========                      ] 18/63 batches, loss: 0.5607Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.5604Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.5597Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.5588Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.5555Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.5553Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.5554Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.5541Epoch 9/15: [============                  ] 26/63 batches, loss: 0.5531Epoch 9/15: [============                  ] 27/63 batches, loss: 0.5521Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.5492Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.5503Epoch 9/15: [==============                ] 30/63 batches, loss: 0.5498Epoch 9/15: [==============                ] 31/63 batches, loss: 0.5504Epoch 9/15: [===============               ] 32/63 batches, loss: 0.5465Epoch 9/15: [===============               ] 33/63 batches, loss: 0.5477Epoch 9/15: [================              ] 34/63 batches, loss: 0.5474Epoch 9/15: [================              ] 35/63 batches, loss: 0.5480Epoch 9/15: [=================             ] 36/63 batches, loss: 0.5460Epoch 9/15: [=================             ] 37/63 batches, loss: 0.5474Epoch 9/15: [==================            ] 38/63 batches, loss: 0.5477Epoch 9/15: [==================            ] 39/63 batches, loss: 0.5470Epoch 9/15: [===================           ] 40/63 batches, loss: 0.5494Epoch 9/15: [===================           ] 41/63 batches, loss: 0.5512Epoch 9/15: [====================          ] 42/63 batches, loss: 0.5522Epoch 9/15: [====================          ] 43/63 batches, loss: 0.5513Epoch 9/15: [====================          ] 44/63 batches, loss: 0.5510Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.5516Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.5528Epoch 9/15: [======================        ] 47/63 batches, loss: 0.5513Epoch 9/15: [======================        ] 48/63 batches, loss: 0.5518Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.5527Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.5516Epoch 9/15: [========================      ] 51/63 batches, loss: 0.5510Epoch 9/15: [========================      ] 52/63 batches, loss: 0.5496Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.5503Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.5495Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.5506Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.5507Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.5503Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.5511Epoch 9/15: [============================  ] 59/63 batches, loss: 0.5496Epoch 9/15: [============================  ] 60/63 batches, loss: 0.5477Epoch 9/15: [============================= ] 61/63 batches, loss: 0.5469Epoch 9/15: [============================= ] 62/63 batches, loss: 0.5467Epoch 9/15: [==============================] 63/63 batches, loss: 0.5430
[2025-05-02 12:49:12,547][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5430
[2025-05-02 12:49:12,786][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5798, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.5067Epoch 10/15: [                              ] 2/63 batches, loss: 0.4892Epoch 10/15: [=                             ] 3/63 batches, loss: 0.5268Epoch 10/15: [=                             ] 4/63 batches, loss: 0.5292Epoch 10/15: [==                            ] 5/63 batches, loss: 0.5412Epoch 10/15: [==                            ] 6/63 batches, loss: 0.5298Epoch 10/15: [===                           ] 7/63 batches, loss: 0.5222Epoch 10/15: [===                           ] 8/63 batches, loss: 0.5287Epoch 10/15: [====                          ] 9/63 batches, loss: 0.5206Epoch 10/15: [====                          ] 10/63 batches, loss: 0.5216Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.5239Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.5323Epoch 10/15: [======                        ] 13/63 batches, loss: 0.5356Epoch 10/15: [======                        ] 14/63 batches, loss: 0.5393Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.5396Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.5383Epoch 10/15: [========                      ] 17/63 batches, loss: 0.5384Epoch 10/15: [========                      ] 18/63 batches, loss: 0.5469Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.5431Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.5418Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.5465Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.5444Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.5449Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.5467Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.5461Epoch 10/15: [============                  ] 26/63 batches, loss: 0.5446Epoch 10/15: [============                  ] 27/63 batches, loss: 0.5456Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.5436Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.5445Epoch 10/15: [==============                ] 30/63 batches, loss: 0.5447Epoch 10/15: [==============                ] 31/63 batches, loss: 0.5446Epoch 10/15: [===============               ] 32/63 batches, loss: 0.5461Epoch 10/15: [===============               ] 33/63 batches, loss: 0.5445Epoch 10/15: [================              ] 34/63 batches, loss: 0.5451Epoch 10/15: [================              ] 35/63 batches, loss: 0.5444Epoch 10/15: [=================             ] 36/63 batches, loss: 0.5435Epoch 10/15: [=================             ] 37/63 batches, loss: 0.5416Epoch 10/15: [==================            ] 38/63 batches, loss: 0.5416Epoch 10/15: [==================            ] 39/63 batches, loss: 0.5428Epoch 10/15: [===================           ] 40/63 batches, loss: 0.5450Epoch 10/15: [===================           ] 41/63 batches, loss: 0.5414Epoch 10/15: [====================          ] 42/63 batches, loss: 0.5421Epoch 10/15: [====================          ] 43/63 batches, loss: 0.5425Epoch 10/15: [====================          ] 44/63 batches, loss: 0.5436Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.5437Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.5426Epoch 10/15: [======================        ] 47/63 batches, loss: 0.5425Epoch 10/15: [======================        ] 48/63 batches, loss: 0.5442Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.5430Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.5433Epoch 10/15: [========================      ] 51/63 batches, loss: 0.5417Epoch 10/15: [========================      ] 52/63 batches, loss: 0.5415Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.5434Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.5431Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.5427Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.5410Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.5410Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.5416Epoch 10/15: [============================  ] 59/63 batches, loss: 0.5411Epoch 10/15: [============================  ] 60/63 batches, loss: 0.5410Epoch 10/15: [============================= ] 61/63 batches, loss: 0.5413Epoch 10/15: [============================= ] 62/63 batches, loss: 0.5413Epoch 10/15: [==============================] 63/63 batches, loss: 0.5426
[2025-05-02 12:49:15,130][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5426
[2025-05-02 12:49:15,357][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5762, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9047619047619048, 'precision': 0.8636363636363636, 'recall': 0.95}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.5240Epoch 11/15: [                              ] 2/63 batches, loss: 0.5391Epoch 11/15: [=                             ] 3/63 batches, loss: 0.5469Epoch 11/15: [=                             ] 4/63 batches, loss: 0.5576Epoch 11/15: [==                            ] 5/63 batches, loss: 0.5700Epoch 11/15: [==                            ] 6/63 batches, loss: 0.5596Epoch 11/15: [===                           ] 7/63 batches, loss: 0.5609Epoch 11/15: [===                           ] 8/63 batches, loss: 0.5694Epoch 11/15: [====                          ] 9/63 batches, loss: 0.5656Epoch 11/15: [====                          ] 10/63 batches, loss: 0.5573Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.5554Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.5611Epoch 11/15: [======                        ] 13/63 batches, loss: 0.5641Epoch 11/15: [======                        ] 14/63 batches, loss: 0.5679Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.5617Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.5599Epoch 11/15: [========                      ] 17/63 batches, loss: 0.5587Epoch 11/15: [========                      ] 18/63 batches, loss: 0.5569Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.5549Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.5574Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.5589Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.5566Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.5573Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.5588Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.5573Epoch 11/15: [============                  ] 26/63 batches, loss: 0.5554Epoch 11/15: [============                  ] 27/63 batches, loss: 0.5525Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.5542Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.5545Epoch 11/15: [==============                ] 30/63 batches, loss: 0.5526Epoch 11/15: [==============                ] 31/63 batches, loss: 0.5502Epoch 11/15: [===============               ] 32/63 batches, loss: 0.5504Epoch 11/15: [===============               ] 33/63 batches, loss: 0.5469Epoch 11/15: [================              ] 34/63 batches, loss: 0.5447Epoch 11/15: [================              ] 35/63 batches, loss: 0.5456Epoch 11/15: [=================             ] 36/63 batches, loss: 0.5446Epoch 11/15: [=================             ] 37/63 batches, loss: 0.5430Epoch 11/15: [==================            ] 38/63 batches, loss: 0.5440Epoch 11/15: [==================            ] 39/63 batches, loss: 0.5428Epoch 11/15: [===================           ] 40/63 batches, loss: 0.5423Epoch 11/15: [===================           ] 41/63 batches, loss: 0.5407Epoch 11/15: [====================          ] 42/63 batches, loss: 0.5397Epoch 11/15: [====================          ] 43/63 batches, loss: 0.5378Epoch 11/15: [====================          ] 44/63 batches, loss: 0.5381Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.5399Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.5386Epoch 11/15: [======================        ] 47/63 batches, loss: 0.5399Epoch 11/15: [======================        ] 48/63 batches, loss: 0.5390Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.5398Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.5378Epoch 11/15: [========================      ] 51/63 batches, loss: 0.5390Epoch 11/15: [========================      ] 52/63 batches, loss: 0.5392Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.5382Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.5380Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.5380Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.5393Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.5397Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.5404Epoch 11/15: [============================  ] 59/63 batches, loss: 0.5400Epoch 11/15: [============================  ] 60/63 batches, loss: 0.5387Epoch 11/15: [============================= ] 61/63 batches, loss: 0.5387Epoch 11/15: [============================= ] 62/63 batches, loss: 0.5382Epoch 11/15: [==============================] 63/63 batches, loss: 0.5387
[2025-05-02 12:49:17,704][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5387
[2025-05-02 12:49:17,945][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5759, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8780487804878049, 'precision': 0.8571428571428571, 'recall': 0.9}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.5016Epoch 12/15: [                              ] 2/63 batches, loss: 0.5851Epoch 12/15: [=                             ] 3/63 batches, loss: 0.5688Epoch 12/15: [=                             ] 4/63 batches, loss: 0.5764Epoch 12/15: [==                            ] 5/63 batches, loss: 0.5533Epoch 12/15: [==                            ] 6/63 batches, loss: 0.5557Epoch 12/15: [===                           ] 7/63 batches, loss: 0.5572Epoch 12/15: [===                           ] 8/63 batches, loss: 0.5510Epoch 12/15: [====                          ] 9/63 batches, loss: 0.5490Epoch 12/15: [====                          ] 10/63 batches, loss: 0.5357Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.5293Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.5215Epoch 12/15: [======                        ] 13/63 batches, loss: 0.5276Epoch 12/15: [======                        ] 14/63 batches, loss: 0.5218Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.5185Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.5168Epoch 12/15: [========                      ] 17/63 batches, loss: 0.5203Epoch 12/15: [========                      ] 18/63 batches, loss: 0.5198Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.5215Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.5180Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.5193Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.5198Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.5183Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.5171Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.5148Epoch 12/15: [============                  ] 26/63 batches, loss: 0.5152Epoch 12/15: [============                  ] 27/63 batches, loss: 0.5158Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.5162Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.5157Epoch 12/15: [==============                ] 30/63 batches, loss: 0.5165Epoch 12/15: [==============                ] 31/63 batches, loss: 0.5173Epoch 12/15: [===============               ] 32/63 batches, loss: 0.5197Epoch 12/15: [===============               ] 33/63 batches, loss: 0.5217Epoch 12/15: [================              ] 34/63 batches, loss: 0.5221Epoch 12/15: [================              ] 35/63 batches, loss: 0.5249Epoch 12/15: [=================             ] 36/63 batches, loss: 0.5261Epoch 12/15: [=================             ] 37/63 batches, loss: 0.5277Epoch 12/15: [==================            ] 38/63 batches, loss: 0.5279Epoch 12/15: [==================            ] 39/63 batches, loss: 0.5269Epoch 12/15: [===================           ] 40/63 batches, loss: 0.5283Epoch 12/15: [===================           ] 41/63 batches, loss: 0.5269Epoch 12/15: [====================          ] 42/63 batches, loss: 0.5284Epoch 12/15: [====================          ] 43/63 batches, loss: 0.5290Epoch 12/15: [====================          ] 44/63 batches, loss: 0.5296Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.5299Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.5299Epoch 12/15: [======================        ] 47/63 batches, loss: 0.5293Epoch 12/15: [======================        ] 48/63 batches, loss: 0.5302Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.5332Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.5339Epoch 12/15: [========================      ] 51/63 batches, loss: 0.5317Epoch 12/15: [========================      ] 52/63 batches, loss: 0.5322Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.5339Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.5345Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.5358Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.5358Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.5360Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.5368Epoch 12/15: [============================  ] 59/63 batches, loss: 0.5371Epoch 12/15: [============================  ] 60/63 batches, loss: 0.5368Epoch 12/15: [============================= ] 61/63 batches, loss: 0.5366Epoch 12/15: [============================= ] 62/63 batches, loss: 0.5367Epoch 12/15: [==============================] 63/63 batches, loss: 0.5358
[2025-05-02 12:49:20,327][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5358
[2025-05-02 12:49:20,567][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.5742, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.4951Epoch 13/15: [                              ] 2/63 batches, loss: 0.5033Epoch 13/15: [=                             ] 3/63 batches, loss: 0.5133Epoch 13/15: [=                             ] 4/63 batches, loss: 0.5239Epoch 13/15: [==                            ] 5/63 batches, loss: 0.5275Epoch 13/15: [==                            ] 6/63 batches, loss: 0.5441Epoch 13/15: [===                           ] 7/63 batches, loss: 0.5378Epoch 13/15: [===                           ] 8/63 batches, loss: 0.5346Epoch 13/15: [====                          ] 9/63 batches, loss: 0.5362Epoch 13/15: [====                          ] 10/63 batches, loss: 0.5306Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.5292Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.5438Epoch 13/15: [======                        ] 13/63 batches, loss: 0.5430Epoch 13/15: [======                        ] 14/63 batches, loss: 0.5387Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.5331Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.5300Epoch 13/15: [========                      ] 17/63 batches, loss: 0.5235Epoch 13/15: [========                      ] 18/63 batches, loss: 0.5271Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.5291Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.5277Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.5270Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.5272Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.5257Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.5249Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.5235Epoch 13/15: [============                  ] 26/63 batches, loss: 0.5206Epoch 13/15: [============                  ] 27/63 batches, loss: 0.5218Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.5205Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.5233Epoch 13/15: [==============                ] 30/63 batches, loss: 0.5247Epoch 13/15: [==============                ] 31/63 batches, loss: 0.5241Epoch 13/15: [===============               ] 32/63 batches, loss: 0.5273Epoch 13/15: [===============               ] 33/63 batches, loss: 0.5256Epoch 13/15: [================              ] 34/63 batches, loss: 0.5289Epoch 13/15: [================              ] 35/63 batches, loss: 0.5308Epoch 13/15: [=================             ] 36/63 batches, loss: 0.5301Epoch 13/15: [=================             ] 37/63 batches, loss: 0.5292Epoch 13/15: [==================            ] 38/63 batches, loss: 0.5298Epoch 13/15: [==================            ] 39/63 batches, loss: 0.5296Epoch 13/15: [===================           ] 40/63 batches, loss: 0.5284Epoch 13/15: [===================           ] 41/63 batches, loss: 0.5282Epoch 13/15: [====================          ] 42/63 batches, loss: 0.5283Epoch 13/15: [====================          ] 43/63 batches, loss: 0.5264Epoch 13/15: [====================          ] 44/63 batches, loss: 0.5263Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.5276Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.5271Epoch 13/15: [======================        ] 47/63 batches, loss: 0.5276Epoch 13/15: [======================        ] 48/63 batches, loss: 0.5291Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.5285Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.5283Epoch 13/15: [========================      ] 51/63 batches, loss: 0.5275Epoch 13/15: [========================      ] 52/63 batches, loss: 0.5288Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.5287Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.5289Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.5292Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.5296Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.5294Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.5302Epoch 13/15: [============================  ] 59/63 batches, loss: 0.5304Epoch 13/15: [============================  ] 60/63 batches, loss: 0.5305Epoch 13/15: [============================= ] 61/63 batches, loss: 0.5290Epoch 13/15: [============================= ] 62/63 batches, loss: 0.5302Epoch 13/15: [==============================] 63/63 batches, loss: 0.5308
[2025-05-02 12:49:22,937][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5308
[2025-05-02 12:49:23,163][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.5732, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.5196Epoch 14/15: [                              ] 2/63 batches, loss: 0.5481Epoch 14/15: [=                             ] 3/63 batches, loss: 0.5404Epoch 14/15: [=                             ] 4/63 batches, loss: 0.5258Epoch 14/15: [==                            ] 5/63 batches, loss: 0.5276Epoch 14/15: [==                            ] 6/63 batches, loss: 0.5318Epoch 14/15: [===                           ] 7/63 batches, loss: 0.5354Epoch 14/15: [===                           ] 8/63 batches, loss: 0.5356Epoch 14/15: [====                          ] 9/63 batches, loss: 0.5280Epoch 14/15: [====                          ] 10/63 batches, loss: 0.5317Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.5218Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.5202Epoch 14/15: [======                        ] 13/63 batches, loss: 0.5230Epoch 14/15: [======                        ] 14/63 batches, loss: 0.5312Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.5298Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.5265Epoch 14/15: [========                      ] 17/63 batches, loss: 0.5255Epoch 14/15: [========                      ] 18/63 batches, loss: 0.5296Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.5256Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.5272Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.5265Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.5268Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.5288Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.5291Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.5277Epoch 14/15: [============                  ] 26/63 batches, loss: 0.5309Epoch 14/15: [============                  ] 27/63 batches, loss: 0.5286Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.5289Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.5327Epoch 14/15: [==============                ] 30/63 batches, loss: 0.5319Epoch 14/15: [==============                ] 31/63 batches, loss: 0.5306Epoch 14/15: [===============               ] 32/63 batches, loss: 0.5273Epoch 14/15: [===============               ] 33/63 batches, loss: 0.5266Epoch 14/15: [================              ] 34/63 batches, loss: 0.5286Epoch 14/15: [================              ] 35/63 batches, loss: 0.5275Epoch 14/15: [=================             ] 36/63 batches, loss: 0.5272Epoch 14/15: [=================             ] 37/63 batches, loss: 0.5273Epoch 14/15: [==================            ] 38/63 batches, loss: 0.5261Epoch 14/15: [==================            ] 39/63 batches, loss: 0.5238Epoch 14/15: [===================           ] 40/63 batches, loss: 0.5223Epoch 14/15: [===================           ] 41/63 batches, loss: 0.5211Epoch 14/15: [====================          ] 42/63 batches, loss: 0.5201Epoch 14/15: [====================          ] 43/63 batches, loss: 0.5179Epoch 14/15: [====================          ] 44/63 batches, loss: 0.5173Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.5183Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.5172Epoch 14/15: [======================        ] 47/63 batches, loss: 0.5161Epoch 14/15: [======================        ] 48/63 batches, loss: 0.5174Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.5199Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.5201Epoch 14/15: [========================      ] 51/63 batches, loss: 0.5190Epoch 14/15: [========================      ] 52/63 batches, loss: 0.5222Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.5233Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.5242Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.5242Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.5251Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.5256Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.5276Epoch 14/15: [============================  ] 59/63 batches, loss: 0.5265Epoch 14/15: [============================  ] 60/63 batches, loss: 0.5279Epoch 14/15: [============================= ] 61/63 batches, loss: 0.5296Epoch 14/15: [============================= ] 62/63 batches, loss: 0.5300Epoch 14/15: [==============================] 63/63 batches, loss: 0.5266
[2025-05-02 12:49:25,525][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.5266
[2025-05-02 12:49:25,751][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.5701, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.5302Epoch 15/15: [                              ] 2/63 batches, loss: 0.4584Epoch 15/15: [=                             ] 3/63 batches, loss: 0.4958Epoch 15/15: [=                             ] 4/63 batches, loss: 0.5099Epoch 15/15: [==                            ] 5/63 batches, loss: 0.5288Epoch 15/15: [==                            ] 6/63 batches, loss: 0.5297Epoch 15/15: [===                           ] 7/63 batches, loss: 0.5153Epoch 15/15: [===                           ] 8/63 batches, loss: 0.5196Epoch 15/15: [====                          ] 9/63 batches, loss: 0.5195Epoch 15/15: [====                          ] 10/63 batches, loss: 0.5201Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.5179Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.5231Epoch 15/15: [======                        ] 13/63 batches, loss: 0.5224Epoch 15/15: [======                        ] 14/63 batches, loss: 0.5197Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.5199Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.5196Epoch 15/15: [========                      ] 17/63 batches, loss: 0.5204Epoch 15/15: [========                      ] 18/63 batches, loss: 0.5235Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.5257Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.5237Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.5217Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.5214Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.5227Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.5251Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.5244Epoch 15/15: [============                  ] 26/63 batches, loss: 0.5262Epoch 15/15: [============                  ] 27/63 batches, loss: 0.5259Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.5294Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.5280Epoch 15/15: [==============                ] 30/63 batches, loss: 0.5276Epoch 15/15: [==============                ] 31/63 batches, loss: 0.5309Epoch 15/15: [===============               ] 32/63 batches, loss: 0.5317Epoch 15/15: [===============               ] 33/63 batches, loss: 0.5326Epoch 15/15: [================              ] 34/63 batches, loss: 0.5325Epoch 15/15: [================              ] 35/63 batches, loss: 0.5322Epoch 15/15: [=================             ] 36/63 batches, loss: 0.5319Epoch 15/15: [=================             ] 37/63 batches, loss: 0.5294Epoch 15/15: [==================            ] 38/63 batches, loss: 0.5314Epoch 15/15: [==================            ] 39/63 batches, loss: 0.5308Epoch 15/15: [===================           ] 40/63 batches, loss: 0.5280Epoch 15/15: [===================           ] 41/63 batches, loss: 0.5298Epoch 15/15: [====================          ] 42/63 batches, loss: 0.5282Epoch 15/15: [====================          ] 43/63 batches, loss: 0.5274Epoch 15/15: [====================          ] 44/63 batches, loss: 0.5275Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.5257Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.5269Epoch 15/15: [======================        ] 47/63 batches, loss: 0.5272Epoch 15/15: [======================        ] 48/63 batches, loss: 0.5247Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.5240Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.5244Epoch 15/15: [========================      ] 51/63 batches, loss: 0.5251Epoch 15/15: [========================      ] 52/63 batches, loss: 0.5266Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.5274Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.5279Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.5260Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.5253Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.5258Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.5243Epoch 15/15: [============================  ] 59/63 batches, loss: 0.5245Epoch 15/15: [============================  ] 60/63 batches, loss: 0.5256Epoch 15/15: [============================= ] 61/63 batches, loss: 0.5263Epoch 15/15: [============================= ] 62/63 batches, loss: 0.5271Epoch 15/15: [==============================] 63/63 batches, loss: 0.5238
[2025-05-02 12:49:28,165][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.5238
[2025-05-02 12:49:28,406][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.5706, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 12:49:28,407][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
[2025-05-02 12:49:28,407][src.training.lm_trainer][INFO] - Training completed in 39.07 seconds
[2025-05-02 12:49:28,407][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 12:49:30,991][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.985929648241206, 'f1': 0.9857433808553971, 'precision': 0.9979381443298969, 'recall': 0.9738430583501007}
[2025-05-02 12:49:30,991][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
[2025-05-02 12:49:30,992][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.7402597402597403, 'f1': 0.6666666666666666, 'precision': 0.5263157894736842, 'recall': 0.9090909090909091}
[2025-05-02 12:49:32,666][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer6/ar/ar/model.pt
[2025-05-02 12:49:32,668][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▅▆▇▆▇▇██▇███
wandb:           best_val_f1 ▁▄▇▇█▇▇███████
wandb:         best_val_loss █▇▆▄▃▂▂▂▂▁▁▁▁▁
wandb:    best_val_precision ▁▅▆▇██████████
wandb:       best_val_recall ▁▃▇▇█▇▇███▇███
wandb:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▂▂▃▃▃▃▃▃▃▃▃▃
wandb:            train_loss █▇▆▅▄▃▃▂▂▂▂▂▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▅▆▇▆▇▇██▇████
wandb:                val_f1 ▁▄▇▇█▇▇████████
wandb:              val_loss █▇▆▄▃▂▂▂▂▁▁▁▁▁▁
wandb:         val_precision ▁▅▆▇███████████
wandb:            val_recall ▁▃▇▇█▇▇███▇████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.93182
wandb:           best_val_f1 0.93023
wandb:         best_val_loss 0.57012
wandb:    best_val_precision 0.86957
wandb:       best_val_recall 1
wandb:                 epoch 15
wandb:   final_test_accuracy 0.74026
wandb:         final_test_f1 0.66667
wandb:  final_test_precision 0.52632
wandb:     final_test_recall 0.90909
wandb:  final_train_accuracy 0.98593
wandb:        final_train_f1 0.98574
wandb: final_train_precision 0.99794
wandb:    final_train_recall 0.97384
wandb:    final_val_accuracy 0.93182
wandb:          final_val_f1 0.93023
wandb:   final_val_precision 0.86957
wandb:      final_val_recall 1
wandb:         learning_rate 0.0001
wandb:            train_loss 0.52375
wandb:            train_time 39.06568
wandb:          val_accuracy 0.90909
wandb:                val_f1 0.90909
wandb:              val_loss 0.57058
wandb:         val_precision 0.83333
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_124839-lm3dy7pp
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_124839-lm3dy7pp/logs
Experiment probe_layer6_question_type_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer6/ar/ar/results.json for layer 6
Running experiment: probe_layer6_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=96" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer6/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 12:49:43,230][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer6/ar
experiment_name: probe_layer6_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 96
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-02 12:49:43,230][__main__][INFO] - Normalized task: complexity
[2025-05-02 12:49:43,230][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 12:49:43,230][__main__][INFO] - Determined Task Type: regression
[2025-05-02 12:49:43,234][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-02 12:49:43,235][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 12:49:44,649][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 12:49:46,870][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 12:49:46,871][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:49:46,916][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:49:46,944][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:49:47,040][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 12:49:47,047][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:49:47,048][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 12:49:47,049][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:49:47,070][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:49:47,101][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:49:47,114][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 12:49:47,115][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:49:47,115][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 12:49:47,116][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:49:47,146][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:49:47,186][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:49:47,199][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 12:49:47,200][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:49:47,200][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 12:49:47,201][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 12:49:47,202][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 12:49:47,202][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 12:49:47,202][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 12:49:47,202][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 12:49:47,202][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:49:47,202][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-02 12:49:47,203][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 12:49:47,203][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-02 12:49:47,203][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 12:49:47,203][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 12:49:47,203][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 12:49:47,203][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 12:49:47,203][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:49:47,203][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-02 12:49:47,203][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 12:49:47,203][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-02 12:49:47,204][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 12:49:47,204][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 12:49:47,204][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 12:49:47,204][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 12:49:47,204][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:49:47,204][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-02 12:49:47,204][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 12:49:47,204][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-02 12:49:47,204][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 12:49:47,204][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 12:49:47,205][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 12:49:47,205][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-02 12:49:47,205][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 12:49:51,376][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 12:49:51,377][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 12:49:51,377][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-02 12:49:51,377][src.models.model_factory][INFO] - Using provided probe_hidden_size: 96
[2025-05-02 12:49:51,379][src.models.model_factory][INFO] - Model has 84,961 trainable parameters out of 394,206,433 total parameters
[2025-05-02 12:49:51,380][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 84,961 trainable parameters
[2025-05-02 12:49:51,380][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=96, depth=2, activation=silu, normalization=layer
[2025-05-02 12:49:51,380][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 96 hidden size
[2025-05-02 12:49:51,380][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 12:49:51,381][__main__][INFO] - Total parameters: 394,206,433
[2025-05-02 12:49:51,381][__main__][INFO] - Trainable parameters: 84,961 (0.02%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 4.8315Epoch 1/15: [                              ] 2/63 batches, loss: 5.0615Epoch 1/15: [=                             ] 3/63 batches, loss: 5.1503Epoch 1/15: [=                             ] 4/63 batches, loss: 4.9081Epoch 1/15: [==                            ] 5/63 batches, loss: 4.5579Epoch 1/15: [==                            ] 6/63 batches, loss: 4.7088Epoch 1/15: [===                           ] 7/63 batches, loss: 4.5244Epoch 1/15: [===                           ] 8/63 batches, loss: 4.2811Epoch 1/15: [====                          ] 9/63 batches, loss: 4.2137Epoch 1/15: [====                          ] 10/63 batches, loss: 4.0565Epoch 1/15: [=====                         ] 11/63 batches, loss: 4.0370Epoch 1/15: [=====                         ] 12/63 batches, loss: 3.8882Epoch 1/15: [======                        ] 13/63 batches, loss: 3.8149Epoch 1/15: [======                        ] 14/63 batches, loss: 3.7249Epoch 1/15: [=======                       ] 15/63 batches, loss: 3.6207Epoch 1/15: [=======                       ] 16/63 batches, loss: 3.5368Epoch 1/15: [========                      ] 17/63 batches, loss: 3.4855Epoch 1/15: [========                      ] 18/63 batches, loss: 3.4754Epoch 1/15: [=========                     ] 19/63 batches, loss: 3.4315Epoch 1/15: [=========                     ] 20/63 batches, loss: 3.3734Epoch 1/15: [==========                    ] 21/63 batches, loss: 3.2958Epoch 1/15: [==========                    ] 22/63 batches, loss: 3.2673Epoch 1/15: [==========                    ] 23/63 batches, loss: 3.3096Epoch 1/15: [===========                   ] 24/63 batches, loss: 3.2485Epoch 1/15: [===========                   ] 25/63 batches, loss: 3.2093Epoch 1/15: [============                  ] 26/63 batches, loss: 3.1538Epoch 1/15: [============                  ] 27/63 batches, loss: 3.1281Epoch 1/15: [=============                 ] 28/63 batches, loss: 3.0674Epoch 1/15: [=============                 ] 29/63 batches, loss: 3.0182Epoch 1/15: [==============                ] 30/63 batches, loss: 3.0075Epoch 1/15: [==============                ] 31/63 batches, loss: 2.9649Epoch 1/15: [===============               ] 32/63 batches, loss: 2.8973Epoch 1/15: [===============               ] 33/63 batches, loss: 2.8522Epoch 1/15: [================              ] 34/63 batches, loss: 2.8169Epoch 1/15: [================              ] 35/63 batches, loss: 2.7804Epoch 1/15: [=================             ] 36/63 batches, loss: 2.7661Epoch 1/15: [=================             ] 37/63 batches, loss: 2.7332Epoch 1/15: [==================            ] 38/63 batches, loss: 2.7085Epoch 1/15: [==================            ] 39/63 batches, loss: 2.6759Epoch 1/15: [===================           ] 40/63 batches, loss: 2.6460Epoch 1/15: [===================           ] 41/63 batches, loss: 2.6036Epoch 1/15: [====================          ] 42/63 batches, loss: 2.5664Epoch 1/15: [====================          ] 43/63 batches, loss: 2.5293Epoch 1/15: [====================          ] 44/63 batches, loss: 2.4941Epoch 1/15: [=====================         ] 45/63 batches, loss: 2.4531Epoch 1/15: [=====================         ] 46/63 batches, loss: 2.4347Epoch 1/15: [======================        ] 47/63 batches, loss: 2.4097Epoch 1/15: [======================        ] 48/63 batches, loss: 2.3732Epoch 1/15: [=======================       ] 49/63 batches, loss: 2.3447Epoch 1/15: [=======================       ] 50/63 batches, loss: 2.3225Epoch 1/15: [========================      ] 51/63 batches, loss: 2.2901Epoch 1/15: [========================      ] 52/63 batches, loss: 2.2593Epoch 1/15: [=========================     ] 53/63 batches, loss: 2.2206Epoch 1/15: [=========================     ] 54/63 batches, loss: 2.1921Epoch 1/15: [==========================    ] 55/63 batches, loss: 2.1695Epoch 1/15: [==========================    ] 56/63 batches, loss: 2.1364Epoch 1/15: [===========================   ] 57/63 batches, loss: 2.1161Epoch 1/15: [===========================   ] 58/63 batches, loss: 2.1004Epoch 1/15: [============================  ] 59/63 batches, loss: 2.0840Epoch 1/15: [============================  ] 60/63 batches, loss: 2.0671Epoch 1/15: [============================= ] 61/63 batches, loss: 2.0423Epoch 1/15: [============================= ] 62/63 batches, loss: 2.0214Epoch 1/15: [==============================] 63/63 batches, loss: 2.0374
[2025-05-02 12:49:55,686][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 2.0374
[2025-05-02 12:49:55,876][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.2410, Metrics: {'mse': 0.24767690896987915, 'rmse': 0.49767148699707436, 'r2': -2.8175249099731445}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.5775Epoch 2/15: [                              ] 2/63 batches, loss: 0.4782Epoch 2/15: [=                             ] 3/63 batches, loss: 0.4645Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6381Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6789Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6540Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6499Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6185Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6269Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6271Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.5847Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.5672Epoch 2/15: [======                        ] 13/63 batches, loss: 0.5745Epoch 2/15: [======                        ] 14/63 batches, loss: 0.5535Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.5539Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.5539Epoch 2/15: [========                      ] 17/63 batches, loss: 0.5729Epoch 2/15: [========                      ] 18/63 batches, loss: 0.5543Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.5585Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.5575Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.5485Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.5501Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.5318Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.5345Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.5500Epoch 2/15: [============                  ] 26/63 batches, loss: 0.5473Epoch 2/15: [============                  ] 27/63 batches, loss: 0.5363Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.5240Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.5174Epoch 2/15: [==============                ] 30/63 batches, loss: 0.5100Epoch 2/15: [==============                ] 31/63 batches, loss: 0.5127Epoch 2/15: [===============               ] 32/63 batches, loss: 0.5078Epoch 2/15: [===============               ] 33/63 batches, loss: 0.4980Epoch 2/15: [================              ] 34/63 batches, loss: 0.4936Epoch 2/15: [================              ] 35/63 batches, loss: 0.4992Epoch 2/15: [=================             ] 36/63 batches, loss: 0.4958Epoch 2/15: [=================             ] 37/63 batches, loss: 0.4877Epoch 2/15: [==================            ] 38/63 batches, loss: 0.4783Epoch 2/15: [==================            ] 39/63 batches, loss: 0.4724Epoch 2/15: [===================           ] 40/63 batches, loss: 0.4718Epoch 2/15: [===================           ] 41/63 batches, loss: 0.4702Epoch 2/15: [====================          ] 42/63 batches, loss: 0.4646Epoch 2/15: [====================          ] 43/63 batches, loss: 0.4609Epoch 2/15: [====================          ] 44/63 batches, loss: 0.4629Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.4665Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.4591Epoch 2/15: [======================        ] 47/63 batches, loss: 0.4555Epoch 2/15: [======================        ] 48/63 batches, loss: 0.4495Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.4425Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.4476Epoch 2/15: [========================      ] 51/63 batches, loss: 0.4425Epoch 2/15: [========================      ] 52/63 batches, loss: 0.4412Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.4365Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.4319Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.4326Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.4313Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.4315Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.4305Epoch 2/15: [============================  ] 59/63 batches, loss: 0.4284Epoch 2/15: [============================  ] 60/63 batches, loss: 0.4260Epoch 2/15: [============================= ] 61/63 batches, loss: 0.4230Epoch 2/15: [============================= ] 62/63 batches, loss: 0.4180Epoch 2/15: [==============================] 63/63 batches, loss: 0.4126
[2025-05-02 12:49:58,192][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.4126
[2025-05-02 12:49:58,385][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.2611, Metrics: {'mse': 0.25474846363067627, 'rmse': 0.504726127350939, 'r2': -2.9265213012695312}
[2025-05-02 12:49:58,385][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.4131Epoch 3/15: [                              ] 2/63 batches, loss: 0.4353Epoch 3/15: [=                             ] 3/63 batches, loss: 0.3503Epoch 3/15: [=                             ] 4/63 batches, loss: 0.3502Epoch 3/15: [==                            ] 5/63 batches, loss: 0.3289Epoch 3/15: [==                            ] 6/63 batches, loss: 0.3403Epoch 3/15: [===                           ] 7/63 batches, loss: 0.3169Epoch 3/15: [===                           ] 8/63 batches, loss: 0.3261Epoch 3/15: [====                          ] 9/63 batches, loss: 0.3197Epoch 3/15: [====                          ] 10/63 batches, loss: 0.3841Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.3622Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.3530Epoch 3/15: [======                        ] 13/63 batches, loss: 0.3517Epoch 3/15: [======                        ] 14/63 batches, loss: 0.3650Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.3493Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.3534Epoch 3/15: [========                      ] 17/63 batches, loss: 0.3489Epoch 3/15: [========                      ] 18/63 batches, loss: 0.3514Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.3700Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.3660Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.3625Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.3582Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.3609Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.3610Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.3556Epoch 3/15: [============                  ] 26/63 batches, loss: 0.3515Epoch 3/15: [============                  ] 27/63 batches, loss: 0.3471Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.3476Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.3401Epoch 3/15: [==============                ] 30/63 batches, loss: 0.3370Epoch 3/15: [==============                ] 31/63 batches, loss: 0.3378Epoch 3/15: [===============               ] 32/63 batches, loss: 0.3418Epoch 3/15: [===============               ] 33/63 batches, loss: 0.3366Epoch 3/15: [================              ] 34/63 batches, loss: 0.3315Epoch 3/15: [================              ] 35/63 batches, loss: 0.3250Epoch 3/15: [=================             ] 36/63 batches, loss: 0.3284Epoch 3/15: [=================             ] 37/63 batches, loss: 0.3255Epoch 3/15: [==================            ] 38/63 batches, loss: 0.3211Epoch 3/15: [==================            ] 39/63 batches, loss: 0.3186Epoch 3/15: [===================           ] 40/63 batches, loss: 0.3187Epoch 3/15: [===================           ] 41/63 batches, loss: 0.3175Epoch 3/15: [====================          ] 42/63 batches, loss: 0.3137Epoch 3/15: [====================          ] 43/63 batches, loss: 0.3122Epoch 3/15: [====================          ] 44/63 batches, loss: 0.3090Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.3087Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.3040Epoch 3/15: [======================        ] 47/63 batches, loss: 0.3029Epoch 3/15: [======================        ] 48/63 batches, loss: 0.2996Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.3012Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.2987Epoch 3/15: [========================      ] 51/63 batches, loss: 0.2999Epoch 3/15: [========================      ] 52/63 batches, loss: 0.2989Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.2998Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.2966Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.2940Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.2923Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.2905Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.2881Epoch 3/15: [============================  ] 59/63 batches, loss: 0.2908Epoch 3/15: [============================  ] 60/63 batches, loss: 0.2914Epoch 3/15: [============================= ] 61/63 batches, loss: 0.2893Epoch 3/15: [============================= ] 62/63 batches, loss: 0.2871Epoch 3/15: [==============================] 63/63 batches, loss: 0.2840
[2025-05-02 12:50:00,316][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.2840
[2025-05-02 12:50:00,528][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.2628, Metrics: {'mse': 0.25597870349884033, 'rmse': 0.5059433797361522, 'r2': -2.9454829692840576}
[2025-05-02 12:50:00,528][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.2575Epoch 4/15: [                              ] 2/63 batches, loss: 0.2377Epoch 4/15: [=                             ] 3/63 batches, loss: 0.2859Epoch 4/15: [=                             ] 4/63 batches, loss: 0.2870Epoch 4/15: [==                            ] 5/63 batches, loss: 0.2812Epoch 4/15: [==                            ] 6/63 batches, loss: 0.3074Epoch 4/15: [===                           ] 7/63 batches, loss: 0.3104Epoch 4/15: [===                           ] 8/63 batches, loss: 0.3006Epoch 4/15: [====                          ] 9/63 batches, loss: 0.3006Epoch 4/15: [====                          ] 10/63 batches, loss: 0.2998Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.2936Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.2888Epoch 4/15: [======                        ] 13/63 batches, loss: 0.2810Epoch 4/15: [======                        ] 14/63 batches, loss: 0.2758Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.2750Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.2741Epoch 4/15: [========                      ] 17/63 batches, loss: 0.2699Epoch 4/15: [========                      ] 18/63 batches, loss: 0.2953Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.2834Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.2862Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.2799Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.2741Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.2667Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.2645Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.2671Epoch 4/15: [============                  ] 26/63 batches, loss: 0.2662Epoch 4/15: [============                  ] 27/63 batches, loss: 0.2641Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.2712Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.2820Epoch 4/15: [==============                ] 30/63 batches, loss: 0.2762Epoch 4/15: [==============                ] 31/63 batches, loss: 0.2758Epoch 4/15: [===============               ] 32/63 batches, loss: 0.2751Epoch 4/15: [===============               ] 33/63 batches, loss: 0.2709Epoch 4/15: [================              ] 34/63 batches, loss: 0.2658Epoch 4/15: [================              ] 35/63 batches, loss: 0.2700Epoch 4/15: [=================             ] 36/63 batches, loss: 0.2671Epoch 4/15: [=================             ] 37/63 batches, loss: 0.2669Epoch 4/15: [==================            ] 38/63 batches, loss: 0.2670Epoch 4/15: [==================            ] 39/63 batches, loss: 0.2715Epoch 4/15: [===================           ] 40/63 batches, loss: 0.2687Epoch 4/15: [===================           ] 41/63 batches, loss: 0.2696Epoch 4/15: [====================          ] 42/63 batches, loss: 0.2678Epoch 4/15: [====================          ] 43/63 batches, loss: 0.2652Epoch 4/15: [====================          ] 44/63 batches, loss: 0.2630Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.2610Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.2584Epoch 4/15: [======================        ] 47/63 batches, loss: 0.2564Epoch 4/15: [======================        ] 48/63 batches, loss: 0.2549Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.2561Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.2548Epoch 4/15: [========================      ] 51/63 batches, loss: 0.2576Epoch 4/15: [========================      ] 52/63 batches, loss: 0.2575Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.2549Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.2582Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.2583Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.2564Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.2568Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.2559Epoch 4/15: [============================  ] 59/63 batches, loss: 0.2567Epoch 4/15: [============================  ] 60/63 batches, loss: 0.2551Epoch 4/15: [============================= ] 61/63 batches, loss: 0.2539Epoch 4/15: [============================= ] 62/63 batches, loss: 0.2518Epoch 4/15: [==============================] 63/63 batches, loss: 0.2491
[2025-05-02 12:50:02,460][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.2491
[2025-05-02 12:50:02,665][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.2617, Metrics: {'mse': 0.2546095550060272, 'rmse': 0.5045885006676502, 'r2': -2.92438006401062}
[2025-05-02 12:50:02,665][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.2275Epoch 5/15: [                              ] 2/63 batches, loss: 0.2129Epoch 5/15: [=                             ] 3/63 batches, loss: 0.2937Epoch 5/15: [=                             ] 4/63 batches, loss: 0.2777Epoch 5/15: [==                            ] 5/63 batches, loss: 0.2585Epoch 5/15: [==                            ] 6/63 batches, loss: 0.2427Epoch 5/15: [===                           ] 7/63 batches, loss: 0.2399Epoch 5/15: [===                           ] 8/63 batches, loss: 0.2412Epoch 5/15: [====                          ] 9/63 batches, loss: 0.2280Epoch 5/15: [====                          ] 10/63 batches, loss: 0.2239Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.2313Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.2182Epoch 5/15: [======                        ] 13/63 batches, loss: 0.2163Epoch 5/15: [======                        ] 14/63 batches, loss: 0.2329Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.2303Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.2242Epoch 5/15: [========                      ] 17/63 batches, loss: 0.2225Epoch 5/15: [========                      ] 18/63 batches, loss: 0.2304Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.2325Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.2297Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.2307Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.2302Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.2291Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.2264Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.2239Epoch 5/15: [============                  ] 26/63 batches, loss: 0.2223Epoch 5/15: [============                  ] 27/63 batches, loss: 0.2223Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.2208Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.2189Epoch 5/15: [==============                ] 30/63 batches, loss: 0.2177Epoch 5/15: [==============                ] 31/63 batches, loss: 0.2169Epoch 5/15: [===============               ] 32/63 batches, loss: 0.2138Epoch 5/15: [===============               ] 33/63 batches, loss: 0.2129Epoch 5/15: [================              ] 34/63 batches, loss: 0.2124Epoch 5/15: [================              ] 35/63 batches, loss: 0.2107Epoch 5/15: [=================             ] 36/63 batches, loss: 0.2100Epoch 5/15: [=================             ] 37/63 batches, loss: 0.2100Epoch 5/15: [==================            ] 38/63 batches, loss: 0.2075Epoch 5/15: [==================            ] 39/63 batches, loss: 0.2058Epoch 5/15: [===================           ] 40/63 batches, loss: 0.2089Epoch 5/15: [===================           ] 41/63 batches, loss: 0.2100Epoch 5/15: [====================          ] 42/63 batches, loss: 0.2105Epoch 5/15: [====================          ] 43/63 batches, loss: 0.2135Epoch 5/15: [====================          ] 44/63 batches, loss: 0.2166Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.2169Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.2151Epoch 5/15: [======================        ] 47/63 batches, loss: 0.2158Epoch 5/15: [======================        ] 48/63 batches, loss: 0.2148Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.2131Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.2176Epoch 5/15: [========================      ] 51/63 batches, loss: 0.2178Epoch 5/15: [========================      ] 52/63 batches, loss: 0.2187Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.2204Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.2191Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.2204Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.2222Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.2195Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.2225Epoch 5/15: [============================  ] 59/63 batches, loss: 0.2215Epoch 5/15: [============================  ] 60/63 batches, loss: 0.2191Epoch 5/15: [============================= ] 61/63 batches, loss: 0.2206Epoch 5/15: [============================= ] 62/63 batches, loss: 0.2197Epoch 5/15: [==============================] 63/63 batches, loss: 0.2168
[2025-05-02 12:50:04,583][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.2168
[2025-05-02 12:50:04,798][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.2302, Metrics: {'mse': 0.22409510612487793, 'rmse': 0.4733868461679918, 'r2': -2.4540507793426514}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.1595Epoch 6/15: [                              ] 2/63 batches, loss: 0.2203Epoch 6/15: [=                             ] 3/63 batches, loss: 0.1806Epoch 6/15: [=                             ] 4/63 batches, loss: 0.1640Epoch 6/15: [==                            ] 5/63 batches, loss: 0.1504Epoch 6/15: [==                            ] 6/63 batches, loss: 0.1454Epoch 6/15: [===                           ] 7/63 batches, loss: 0.1537Epoch 6/15: [===                           ] 8/63 batches, loss: 0.1490Epoch 6/15: [====                          ] 9/63 batches, loss: 0.1504Epoch 6/15: [====                          ] 10/63 batches, loss: 0.1470Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.1814Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.1811Epoch 6/15: [======                        ] 13/63 batches, loss: 0.2046Epoch 6/15: [======                        ] 14/63 batches, loss: 0.1970Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.1949Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.1892Epoch 6/15: [========                      ] 17/63 batches, loss: 0.1903Epoch 6/15: [========                      ] 18/63 batches, loss: 0.1975Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.1988Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.1931Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.1926Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.1925Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.1916Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.1906Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.1985Epoch 6/15: [============                  ] 26/63 batches, loss: 0.1972Epoch 6/15: [============                  ] 27/63 batches, loss: 0.1989Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.2008Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.2008Epoch 6/15: [==============                ] 30/63 batches, loss: 0.2017Epoch 6/15: [==============                ] 31/63 batches, loss: 0.2023Epoch 6/15: [===============               ] 32/63 batches, loss: 0.2015Epoch 6/15: [===============               ] 33/63 batches, loss: 0.2005Epoch 6/15: [================              ] 34/63 batches, loss: 0.2010Epoch 6/15: [================              ] 35/63 batches, loss: 0.2015Epoch 6/15: [=================             ] 36/63 batches, loss: 0.2029Epoch 6/15: [=================             ] 37/63 batches, loss: 0.2011Epoch 6/15: [==================            ] 38/63 batches, loss: 0.2008Epoch 6/15: [==================            ] 39/63 batches, loss: 0.2038Epoch 6/15: [===================           ] 40/63 batches, loss: 0.2050Epoch 6/15: [===================           ] 41/63 batches, loss: 0.2037Epoch 6/15: [====================          ] 42/63 batches, loss: 0.2070Epoch 6/15: [====================          ] 43/63 batches, loss: 0.2073Epoch 6/15: [====================          ] 44/63 batches, loss: 0.2066Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.2051Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.2051Epoch 6/15: [======================        ] 47/63 batches, loss: 0.2071Epoch 6/15: [======================        ] 48/63 batches, loss: 0.2049Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.2022Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.2013Epoch 6/15: [========================      ] 51/63 batches, loss: 0.2011Epoch 6/15: [========================      ] 52/63 batches, loss: 0.2003Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.1983Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.1974Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.1993Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.2017Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.2029Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.2017Epoch 6/15: [============================  ] 59/63 batches, loss: 0.2024Epoch 6/15: [============================  ] 60/63 batches, loss: 0.2031Epoch 6/15: [============================= ] 61/63 batches, loss: 0.2034Epoch 6/15: [============================= ] 62/63 batches, loss: 0.2017Epoch 6/15: [==============================] 63/63 batches, loss: 0.2016
[2025-05-02 12:50:07,140][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.2016
[2025-05-02 12:50:07,355][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.2019, Metrics: {'mse': 0.19661937654018402, 'rmse': 0.44341783516248423, 'r2': -2.0305585861206055}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.1728Epoch 7/15: [                              ] 2/63 batches, loss: 0.1870Epoch 7/15: [=                             ] 3/63 batches, loss: 0.2392Epoch 7/15: [=                             ] 4/63 batches, loss: 0.2262Epoch 7/15: [==                            ] 5/63 batches, loss: 0.2025Epoch 7/15: [==                            ] 6/63 batches, loss: 0.1921Epoch 7/15: [===                           ] 7/63 batches, loss: 0.2144Epoch 7/15: [===                           ] 8/63 batches, loss: 0.2140Epoch 7/15: [====                          ] 9/63 batches, loss: 0.2090Epoch 7/15: [====                          ] 10/63 batches, loss: 0.1984Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.2027Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.2004Epoch 7/15: [======                        ] 13/63 batches, loss: 0.1932Epoch 7/15: [======                        ] 14/63 batches, loss: 0.1965Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.2039Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.2010Epoch 7/15: [========                      ] 17/63 batches, loss: 0.2180Epoch 7/15: [========                      ] 18/63 batches, loss: 0.2333Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.2367Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.2302Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.2252Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.2227Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.2175Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.2134Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.2080Epoch 7/15: [============                  ] 26/63 batches, loss: 0.2012Epoch 7/15: [============                  ] 27/63 batches, loss: 0.2015Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.2037Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.2004Epoch 7/15: [==============                ] 30/63 batches, loss: 0.2003Epoch 7/15: [==============                ] 31/63 batches, loss: 0.2027Epoch 7/15: [===============               ] 32/63 batches, loss: 0.2038Epoch 7/15: [===============               ] 33/63 batches, loss: 0.2109Epoch 7/15: [================              ] 34/63 batches, loss: 0.2094Epoch 7/15: [================              ] 35/63 batches, loss: 0.2094Epoch 7/15: [=================             ] 36/63 batches, loss: 0.2096Epoch 7/15: [=================             ] 37/63 batches, loss: 0.2070Epoch 7/15: [==================            ] 38/63 batches, loss: 0.2074Epoch 7/15: [==================            ] 39/63 batches, loss: 0.2077Epoch 7/15: [===================           ] 40/63 batches, loss: 0.2049Epoch 7/15: [===================           ] 41/63 batches, loss: 0.2054Epoch 7/15: [====================          ] 42/63 batches, loss: 0.2071Epoch 7/15: [====================          ] 43/63 batches, loss: 0.2070Epoch 7/15: [====================          ] 44/63 batches, loss: 0.2067Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.2059Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.2032Epoch 7/15: [======================        ] 47/63 batches, loss: 0.2030Epoch 7/15: [======================        ] 48/63 batches, loss: 0.2014Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.2005Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.2023Epoch 7/15: [========================      ] 51/63 batches, loss: 0.2022Epoch 7/15: [========================      ] 52/63 batches, loss: 0.2018Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.2015Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.2008Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.2011Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.2025Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.2024Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.2019Epoch 7/15: [============================  ] 59/63 batches, loss: 0.2042Epoch 7/15: [============================  ] 60/63 batches, loss: 0.2046Epoch 7/15: [============================= ] 61/63 batches, loss: 0.2048Epoch 7/15: [============================= ] 62/63 batches, loss: 0.2035Epoch 7/15: [==============================] 63/63 batches, loss: 0.2009
[2025-05-02 12:50:09,821][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.2009
[2025-05-02 12:50:10,046][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1892, Metrics: {'mse': 0.18398374319076538, 'rmse': 0.4289332619309971, 'r2': -1.835801362991333}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0768Epoch 8/15: [                              ] 2/63 batches, loss: 0.1730Epoch 8/15: [=                             ] 3/63 batches, loss: 0.1864Epoch 8/15: [=                             ] 4/63 batches, loss: 0.1590Epoch 8/15: [==                            ] 5/63 batches, loss: 0.1604Epoch 8/15: [==                            ] 6/63 batches, loss: 0.1552Epoch 8/15: [===                           ] 7/63 batches, loss: 0.1721Epoch 8/15: [===                           ] 8/63 batches, loss: 0.1720Epoch 8/15: [====                          ] 9/63 batches, loss: 0.1720Epoch 8/15: [====                          ] 10/63 batches, loss: 0.1832Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.1764Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.1960Epoch 8/15: [======                        ] 13/63 batches, loss: 0.2018Epoch 8/15: [======                        ] 14/63 batches, loss: 0.1959Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.1957Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.1910Epoch 8/15: [========                      ] 17/63 batches, loss: 0.1896Epoch 8/15: [========                      ] 18/63 batches, loss: 0.1900Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.1923Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.1858Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.1932Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.1906Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.1921Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.1912Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.1869Epoch 8/15: [============                  ] 26/63 batches, loss: 0.1885Epoch 8/15: [============                  ] 27/63 batches, loss: 0.1887Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.1937Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.1917Epoch 8/15: [==============                ] 30/63 batches, loss: 0.1896Epoch 8/15: [==============                ] 31/63 batches, loss: 0.1909Epoch 8/15: [===============               ] 32/63 batches, loss: 0.1896Epoch 8/15: [===============               ] 33/63 batches, loss: 0.1894Epoch 8/15: [================              ] 34/63 batches, loss: 0.1877Epoch 8/15: [================              ] 35/63 batches, loss: 0.1846Epoch 8/15: [=================             ] 36/63 batches, loss: 0.1829Epoch 8/15: [=================             ] 37/63 batches, loss: 0.1845Epoch 8/15: [==================            ] 38/63 batches, loss: 0.1812Epoch 8/15: [==================            ] 39/63 batches, loss: 0.1802Epoch 8/15: [===================           ] 40/63 batches, loss: 0.1788Epoch 8/15: [===================           ] 41/63 batches, loss: 0.1801Epoch 8/15: [====================          ] 42/63 batches, loss: 0.1782Epoch 8/15: [====================          ] 43/63 batches, loss: 0.1766Epoch 8/15: [====================          ] 44/63 batches, loss: 0.1746Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.1727Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.1752Epoch 8/15: [======================        ] 47/63 batches, loss: 0.1744Epoch 8/15: [======================        ] 48/63 batches, loss: 0.1739Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.1734Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.1758Epoch 8/15: [========================      ] 51/63 batches, loss: 0.1754Epoch 8/15: [========================      ] 52/63 batches, loss: 0.1741Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.1726Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.1724Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.1714Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.1702Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.1711Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.1715Epoch 8/15: [============================  ] 59/63 batches, loss: 0.1711Epoch 8/15: [============================  ] 60/63 batches, loss: 0.1717Epoch 8/15: [============================= ] 61/63 batches, loss: 0.1702Epoch 8/15: [============================= ] 62/63 batches, loss: 0.1716Epoch 8/15: [==============================] 63/63 batches, loss: 0.1723
[2025-05-02 12:50:12,458][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.1723
[2025-05-02 12:50:12,700][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1871, Metrics: {'mse': 0.1818285435438156, 'rmse': 0.4264135827384203, 'r2': -1.8025825023651123}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1164Epoch 9/15: [                              ] 2/63 batches, loss: 0.1412Epoch 9/15: [=                             ] 3/63 batches, loss: 0.1368Epoch 9/15: [=                             ] 4/63 batches, loss: 0.1626Epoch 9/15: [==                            ] 5/63 batches, loss: 0.1913Epoch 9/15: [==                            ] 6/63 batches, loss: 0.1756Epoch 9/15: [===                           ] 7/63 batches, loss: 0.1945Epoch 9/15: [===                           ] 8/63 batches, loss: 0.1871Epoch 9/15: [====                          ] 9/63 batches, loss: 0.1943Epoch 9/15: [====                          ] 10/63 batches, loss: 0.1949Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.1865Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.1842Epoch 9/15: [======                        ] 13/63 batches, loss: 0.1790Epoch 9/15: [======                        ] 14/63 batches, loss: 0.1738Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.1700Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.1696Epoch 9/15: [========                      ] 17/63 batches, loss: 0.1648Epoch 9/15: [========                      ] 18/63 batches, loss: 0.1689Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.1727Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.1770Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.1752Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.1796Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.1778Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.1765Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.1718Epoch 9/15: [============                  ] 26/63 batches, loss: 0.1821Epoch 9/15: [============                  ] 27/63 batches, loss: 0.1813Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.1848Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.1828Epoch 9/15: [==============                ] 30/63 batches, loss: 0.1824Epoch 9/15: [==============                ] 31/63 batches, loss: 0.1824Epoch 9/15: [===============               ] 32/63 batches, loss: 0.1808Epoch 9/15: [===============               ] 33/63 batches, loss: 0.1799Epoch 9/15: [================              ] 34/63 batches, loss: 0.1807Epoch 9/15: [================              ] 35/63 batches, loss: 0.1776Epoch 9/15: [=================             ] 36/63 batches, loss: 0.1872Epoch 9/15: [=================             ] 37/63 batches, loss: 0.1840Epoch 9/15: [==================            ] 38/63 batches, loss: 0.1854Epoch 9/15: [==================            ] 39/63 batches, loss: 0.1880Epoch 9/15: [===================           ] 40/63 batches, loss: 0.1858Epoch 9/15: [===================           ] 41/63 batches, loss: 0.1850Epoch 9/15: [====================          ] 42/63 batches, loss: 0.1842Epoch 9/15: [====================          ] 43/63 batches, loss: 0.1826Epoch 9/15: [====================          ] 44/63 batches, loss: 0.1835Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.1828Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.1828Epoch 9/15: [======================        ] 47/63 batches, loss: 0.1815Epoch 9/15: [======================        ] 48/63 batches, loss: 0.1833Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.1813Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.1817Epoch 9/15: [========================      ] 51/63 batches, loss: 0.1800Epoch 9/15: [========================      ] 52/63 batches, loss: 0.1784Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.1775Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.1786Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.1769Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.1758Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.1781Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.1770Epoch 9/15: [============================  ] 59/63 batches, loss: 0.1799Epoch 9/15: [============================  ] 60/63 batches, loss: 0.1813Epoch 9/15: [============================= ] 61/63 batches, loss: 0.1811Epoch 9/15: [============================= ] 62/63 batches, loss: 0.1813Epoch 9/15: [==============================] 63/63 batches, loss: 0.1824
[2025-05-02 12:50:15,004][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.1824
[2025-05-02 12:50:15,244][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.1784, Metrics: {'mse': 0.1732395440340042, 'rmse': 0.41622054734720176, 'r2': -1.6701974868774414}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.1931Epoch 10/15: [                              ] 2/63 batches, loss: 0.2918Epoch 10/15: [=                             ] 3/63 batches, loss: 0.2177Epoch 10/15: [=                             ] 4/63 batches, loss: 0.1952Epoch 10/15: [==                            ] 5/63 batches, loss: 0.1990Epoch 10/15: [==                            ] 6/63 batches, loss: 0.1846Epoch 10/15: [===                           ] 7/63 batches, loss: 0.1831Epoch 10/15: [===                           ] 8/63 batches, loss: 0.1727Epoch 10/15: [====                          ] 9/63 batches, loss: 0.1641Epoch 10/15: [====                          ] 10/63 batches, loss: 0.1685Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.1709Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.1764Epoch 10/15: [======                        ] 13/63 batches, loss: 0.1730Epoch 10/15: [======                        ] 14/63 batches, loss: 0.1862Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.1865Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.1846Epoch 10/15: [========                      ] 17/63 batches, loss: 0.1784Epoch 10/15: [========                      ] 18/63 batches, loss: 0.1765Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.1738Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.1719Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.1771Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.1732Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.1709Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.1728Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.1693Epoch 10/15: [============                  ] 26/63 batches, loss: 0.1705Epoch 10/15: [============                  ] 27/63 batches, loss: 0.1701Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.1712Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.1697Epoch 10/15: [==============                ] 30/63 batches, loss: 0.1674Epoch 10/15: [==============                ] 31/63 batches, loss: 0.1682Epoch 10/15: [===============               ] 32/63 batches, loss: 0.1691Epoch 10/15: [===============               ] 33/63 batches, loss: 0.1679Epoch 10/15: [================              ] 34/63 batches, loss: 0.1682Epoch 10/15: [================              ] 35/63 batches, loss: 0.1730Epoch 10/15: [=================             ] 36/63 batches, loss: 0.1744Epoch 10/15: [=================             ] 37/63 batches, loss: 0.1720Epoch 10/15: [==================            ] 38/63 batches, loss: 0.1688Epoch 10/15: [==================            ] 39/63 batches, loss: 0.1699Epoch 10/15: [===================           ] 40/63 batches, loss: 0.1705Epoch 10/15: [===================           ] 41/63 batches, loss: 0.1690Epoch 10/15: [====================          ] 42/63 batches, loss: 0.1676Epoch 10/15: [====================          ] 43/63 batches, loss: 0.1678Epoch 10/15: [====================          ] 44/63 batches, loss: 0.1675Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.1680Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.1668Epoch 10/15: [======================        ] 47/63 batches, loss: 0.1663Epoch 10/15: [======================        ] 48/63 batches, loss: 0.1677Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.1671Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.1662Epoch 10/15: [========================      ] 51/63 batches, loss: 0.1671Epoch 10/15: [========================      ] 52/63 batches, loss: 0.1652Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.1651Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.1663Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.1668Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.1660Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.1646Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.1643Epoch 10/15: [============================  ] 59/63 batches, loss: 0.1645Epoch 10/15: [============================  ] 60/63 batches, loss: 0.1648Epoch 10/15: [============================= ] 61/63 batches, loss: 0.1644Epoch 10/15: [============================= ] 62/63 batches, loss: 0.1653Epoch 10/15: [==============================] 63/63 batches, loss: 0.1698
[2025-05-02 12:50:17,562][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.1698
[2025-05-02 12:50:17,808][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.1676, Metrics: {'mse': 0.16268745064735413, 'rmse': 0.40334532431572095, 'r2': -1.5075547695159912}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.2083Epoch 11/15: [                              ] 2/63 batches, loss: 0.1946Epoch 11/15: [=                             ] 3/63 batches, loss: 0.1617Epoch 11/15: [=                             ] 4/63 batches, loss: 0.1404Epoch 11/15: [==                            ] 5/63 batches, loss: 0.1317Epoch 11/15: [==                            ] 6/63 batches, loss: 0.1277Epoch 11/15: [===                           ] 7/63 batches, loss: 0.1234Epoch 11/15: [===                           ] 8/63 batches, loss: 0.1325Epoch 11/15: [====                          ] 9/63 batches, loss: 0.1360Epoch 11/15: [====                          ] 10/63 batches, loss: 0.1348Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.1304Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.1390Epoch 11/15: [======                        ] 13/63 batches, loss: 0.1413Epoch 11/15: [======                        ] 14/63 batches, loss: 0.1537Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.1472Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.1537Epoch 11/15: [========                      ] 17/63 batches, loss: 0.1505Epoch 11/15: [========                      ] 18/63 batches, loss: 0.1479Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.1433Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.1427Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.1418Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.1406Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.1403Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.1423Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.1416Epoch 11/15: [============                  ] 26/63 batches, loss: 0.1384Epoch 11/15: [============                  ] 27/63 batches, loss: 0.1404Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.1415Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.1410Epoch 11/15: [==============                ] 30/63 batches, loss: 0.1417Epoch 11/15: [==============                ] 31/63 batches, loss: 0.1443Epoch 11/15: [===============               ] 32/63 batches, loss: 0.1480Epoch 11/15: [===============               ] 33/63 batches, loss: 0.1485Epoch 11/15: [================              ] 34/63 batches, loss: 0.1472Epoch 11/15: [================              ] 35/63 batches, loss: 0.1502Epoch 11/15: [=================             ] 36/63 batches, loss: 0.1523Epoch 11/15: [=================             ] 37/63 batches, loss: 0.1527Epoch 11/15: [==================            ] 38/63 batches, loss: 0.1504Epoch 11/15: [==================            ] 39/63 batches, loss: 0.1527Epoch 11/15: [===================           ] 40/63 batches, loss: 0.1536Epoch 11/15: [===================           ] 41/63 batches, loss: 0.1540Epoch 11/15: [====================          ] 42/63 batches, loss: 0.1534Epoch 11/15: [====================          ] 43/63 batches, loss: 0.1518Epoch 11/15: [====================          ] 44/63 batches, loss: 0.1507Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.1521Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.1516Epoch 11/15: [======================        ] 47/63 batches, loss: 0.1514Epoch 11/15: [======================        ] 48/63 batches, loss: 0.1525Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.1513Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.1520Epoch 11/15: [========================      ] 51/63 batches, loss: 0.1517Epoch 11/15: [========================      ] 52/63 batches, loss: 0.1520Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.1529Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.1524Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.1513Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.1498Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.1481Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.1491Epoch 11/15: [============================  ] 59/63 batches, loss: 0.1491Epoch 11/15: [============================  ] 60/63 batches, loss: 0.1490Epoch 11/15: [============================= ] 61/63 batches, loss: 0.1495Epoch 11/15: [============================= ] 62/63 batches, loss: 0.1482Epoch 11/15: [==============================] 63/63 batches, loss: 0.1482
[2025-05-02 12:50:20,141][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.1482
[2025-05-02 12:50:20,392][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.1603, Metrics: {'mse': 0.15552043914794922, 'rmse': 0.39436079818859937, 'r2': -1.3970870971679688}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.0835Epoch 12/15: [                              ] 2/63 batches, loss: 0.0812Epoch 12/15: [=                             ] 3/63 batches, loss: 0.0885Epoch 12/15: [=                             ] 4/63 batches, loss: 0.1054Epoch 12/15: [==                            ] 5/63 batches, loss: 0.1483Epoch 12/15: [==                            ] 6/63 batches, loss: 0.1406Epoch 12/15: [===                           ] 7/63 batches, loss: 0.1636Epoch 12/15: [===                           ] 8/63 batches, loss: 0.1536Epoch 12/15: [====                          ] 9/63 batches, loss: 0.1778Epoch 12/15: [====                          ] 10/63 batches, loss: 0.1765Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.1794Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.1757Epoch 12/15: [======                        ] 13/63 batches, loss: 0.1746Epoch 12/15: [======                        ] 14/63 batches, loss: 0.1728Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.1751Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.1735Epoch 12/15: [========                      ] 17/63 batches, loss: 0.1705Epoch 12/15: [========                      ] 18/63 batches, loss: 0.1653Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.1623Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.1592Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.1604Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.1618Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.1623Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.1599Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.1559Epoch 12/15: [============                  ] 26/63 batches, loss: 0.1560Epoch 12/15: [============                  ] 27/63 batches, loss: 0.1557Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.1520Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.1495Epoch 12/15: [==============                ] 30/63 batches, loss: 0.1544Epoch 12/15: [==============                ] 31/63 batches, loss: 0.1525Epoch 12/15: [===============               ] 32/63 batches, loss: 0.1523Epoch 12/15: [===============               ] 33/63 batches, loss: 0.1531Epoch 12/15: [================              ] 34/63 batches, loss: 0.1534Epoch 12/15: [================              ] 35/63 batches, loss: 0.1544Epoch 12/15: [=================             ] 36/63 batches, loss: 0.1530Epoch 12/15: [=================             ] 37/63 batches, loss: 0.1535Epoch 12/15: [==================            ] 38/63 batches, loss: 0.1507Epoch 12/15: [==================            ] 39/63 batches, loss: 0.1514Epoch 12/15: [===================           ] 40/63 batches, loss: 0.1497Epoch 12/15: [===================           ] 41/63 batches, loss: 0.1511Epoch 12/15: [====================          ] 42/63 batches, loss: 0.1503Epoch 12/15: [====================          ] 43/63 batches, loss: 0.1497Epoch 12/15: [====================          ] 44/63 batches, loss: 0.1492Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.1502Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.1489Epoch 12/15: [======================        ] 47/63 batches, loss: 0.1501Epoch 12/15: [======================        ] 48/63 batches, loss: 0.1490Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.1486Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.1499Epoch 12/15: [========================      ] 51/63 batches, loss: 0.1508Epoch 12/15: [========================      ] 52/63 batches, loss: 0.1497Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.1492Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.1478Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.1470Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.1471Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.1464Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.1507Epoch 12/15: [============================  ] 59/63 batches, loss: 0.1534Epoch 12/15: [============================  ] 60/63 batches, loss: 0.1530Epoch 12/15: [============================= ] 61/63 batches, loss: 0.1526Epoch 12/15: [============================= ] 62/63 batches, loss: 0.1518Epoch 12/15: [==============================] 63/63 batches, loss: 0.1519
[2025-05-02 12:50:22,738][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.1519
[2025-05-02 12:50:22,989][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.1552, Metrics: {'mse': 0.1506108194589615, 'rmse': 0.3880860979975468, 'r2': -1.321413516998291}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.0584Epoch 13/15: [                              ] 2/63 batches, loss: 0.0682Epoch 13/15: [=                             ] 3/63 batches, loss: 0.0951Epoch 13/15: [=                             ] 4/63 batches, loss: 0.0963Epoch 13/15: [==                            ] 5/63 batches, loss: 0.0927Epoch 13/15: [==                            ] 6/63 batches, loss: 0.0941Epoch 13/15: [===                           ] 7/63 batches, loss: 0.0934Epoch 13/15: [===                           ] 8/63 batches, loss: 0.1016Epoch 13/15: [====                          ] 9/63 batches, loss: 0.1063Epoch 13/15: [====                          ] 10/63 batches, loss: 0.1047Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.1073Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.1043Epoch 13/15: [======                        ] 13/63 batches, loss: 0.1071Epoch 13/15: [======                        ] 14/63 batches, loss: 0.1102Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.1147Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.1183Epoch 13/15: [========                      ] 17/63 batches, loss: 0.1188Epoch 13/15: [========                      ] 18/63 batches, loss: 0.1189Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.1187Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.1191Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.1204Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.1231Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.1228Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.1232Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.1267Epoch 13/15: [============                  ] 26/63 batches, loss: 0.1268Epoch 13/15: [============                  ] 27/63 batches, loss: 0.1336Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.1339Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.1342Epoch 13/15: [==============                ] 30/63 batches, loss: 0.1343Epoch 13/15: [==============                ] 31/63 batches, loss: 0.1330Epoch 13/15: [===============               ] 32/63 batches, loss: 0.1346Epoch 13/15: [===============               ] 33/63 batches, loss: 0.1334Epoch 13/15: [================              ] 34/63 batches, loss: 0.1318Epoch 13/15: [================              ] 35/63 batches, loss: 0.1329Epoch 13/15: [=================             ] 36/63 batches, loss: 0.1307Epoch 13/15: [=================             ] 37/63 batches, loss: 0.1297Epoch 13/15: [==================            ] 38/63 batches, loss: 0.1289Epoch 13/15: [==================            ] 39/63 batches, loss: 0.1278Epoch 13/15: [===================           ] 40/63 batches, loss: 0.1310Epoch 13/15: [===================           ] 41/63 batches, loss: 0.1304Epoch 13/15: [====================          ] 42/63 batches, loss: 0.1321Epoch 13/15: [====================          ] 43/63 batches, loss: 0.1311Epoch 13/15: [====================          ] 44/63 batches, loss: 0.1292Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.1326Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.1313Epoch 13/15: [======================        ] 47/63 batches, loss: 0.1323Epoch 13/15: [======================        ] 48/63 batches, loss: 0.1331Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.1339Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.1328Epoch 13/15: [========================      ] 51/63 batches, loss: 0.1329Epoch 13/15: [========================      ] 52/63 batches, loss: 0.1334Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.1339Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.1328Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.1315Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.1312Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.1303Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.1294Epoch 13/15: [============================  ] 59/63 batches, loss: 0.1296Epoch 13/15: [============================  ] 60/63 batches, loss: 0.1299Epoch 13/15: [============================= ] 61/63 batches, loss: 0.1309Epoch 13/15: [============================= ] 62/63 batches, loss: 0.1302Epoch 13/15: [==============================] 63/63 batches, loss: 0.1285
[2025-05-02 12:50:25,341][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.1285
[2025-05-02 12:50:25,591][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.1410, Metrics: {'mse': 0.1368524581193924, 'rmse': 0.36993574863669554, 'r2': -1.109351634979248}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.0576Epoch 14/15: [                              ] 2/63 batches, loss: 0.0888Epoch 14/15: [=                             ] 3/63 batches, loss: 0.1020Epoch 14/15: [=                             ] 4/63 batches, loss: 0.1074Epoch 14/15: [==                            ] 5/63 batches, loss: 0.1445Epoch 14/15: [==                            ] 6/63 batches, loss: 0.1379Epoch 14/15: [===                           ] 7/63 batches, loss: 0.1266Epoch 14/15: [===                           ] 8/63 batches, loss: 0.1296Epoch 14/15: [====                          ] 9/63 batches, loss: 0.1283Epoch 14/15: [====                          ] 10/63 batches, loss: 0.1265Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.1260Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.1288Epoch 14/15: [======                        ] 13/63 batches, loss: 0.1307Epoch 14/15: [======                        ] 14/63 batches, loss: 0.1290Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.1323Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.1304Epoch 14/15: [========                      ] 17/63 batches, loss: 0.1324Epoch 14/15: [========                      ] 18/63 batches, loss: 0.1357Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.1316Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.1352Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.1335Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.1334Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.1332Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.1314Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.1368Epoch 14/15: [============                  ] 26/63 batches, loss: 0.1367Epoch 14/15: [============                  ] 27/63 batches, loss: 0.1347Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.1321Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.1325Epoch 14/15: [==============                ] 30/63 batches, loss: 0.1330Epoch 14/15: [==============                ] 31/63 batches, loss: 0.1354Epoch 14/15: [===============               ] 32/63 batches, loss: 0.1335Epoch 14/15: [===============               ] 33/63 batches, loss: 0.1353Epoch 14/15: [================              ] 34/63 batches, loss: 0.1329Epoch 14/15: [================              ] 35/63 batches, loss: 0.1310Epoch 14/15: [=================             ] 36/63 batches, loss: 0.1314Epoch 14/15: [=================             ] 37/63 batches, loss: 0.1301Epoch 14/15: [==================            ] 38/63 batches, loss: 0.1312Epoch 14/15: [==================            ] 39/63 batches, loss: 0.1363Epoch 14/15: [===================           ] 40/63 batches, loss: 0.1375Epoch 14/15: [===================           ] 41/63 batches, loss: 0.1355Epoch 14/15: [====================          ] 42/63 batches, loss: 0.1369Epoch 14/15: [====================          ] 43/63 batches, loss: 0.1353Epoch 14/15: [====================          ] 44/63 batches, loss: 0.1363Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.1369Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.1366Epoch 14/15: [======================        ] 47/63 batches, loss: 0.1353Epoch 14/15: [======================        ] 48/63 batches, loss: 0.1376Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.1375Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.1386Epoch 14/15: [========================      ] 51/63 batches, loss: 0.1384Epoch 14/15: [========================      ] 52/63 batches, loss: 0.1371Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.1387Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.1387Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.1382Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.1385Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.1394Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.1385Epoch 14/15: [============================  ] 59/63 batches, loss: 0.1390Epoch 14/15: [============================  ] 60/63 batches, loss: 0.1376Epoch 14/15: [============================= ] 61/63 batches, loss: 0.1373Epoch 14/15: [============================= ] 62/63 batches, loss: 0.1361Epoch 14/15: [==============================] 63/63 batches, loss: 0.1395
[2025-05-02 12:50:27,951][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.1395
[2025-05-02 12:50:28,188][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.1384, Metrics: {'mse': 0.13419489562511444, 'rmse': 0.3663262147664489, 'r2': -1.068389654159546}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.1239Epoch 15/15: [                              ] 2/63 batches, loss: 0.1065Epoch 15/15: [=                             ] 3/63 batches, loss: 0.0926Epoch 15/15: [=                             ] 4/63 batches, loss: 0.0955Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0996Epoch 15/15: [==                            ] 6/63 batches, loss: 0.1112Epoch 15/15: [===                           ] 7/63 batches, loss: 0.1066Epoch 15/15: [===                           ] 8/63 batches, loss: 0.1040Epoch 15/15: [====                          ] 9/63 batches, loss: 0.1028Epoch 15/15: [====                          ] 10/63 batches, loss: 0.1068Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.1089Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.1099Epoch 15/15: [======                        ] 13/63 batches, loss: 0.1083Epoch 15/15: [======                        ] 14/63 batches, loss: 0.1037Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.1059Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.1065Epoch 15/15: [========                      ] 17/63 batches, loss: 0.1076Epoch 15/15: [========                      ] 18/63 batches, loss: 0.1086Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.1066Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.1104Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.1078Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.1062Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.1083Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.1059Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.1034Epoch 15/15: [============                  ] 26/63 batches, loss: 0.1042Epoch 15/15: [============                  ] 27/63 batches, loss: 0.1041Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.1039Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.1063Epoch 15/15: [==============                ] 30/63 batches, loss: 0.1066Epoch 15/15: [==============                ] 31/63 batches, loss: 0.1075Epoch 15/15: [===============               ] 32/63 batches, loss: 0.1060Epoch 15/15: [===============               ] 33/63 batches, loss: 0.1055Epoch 15/15: [================              ] 34/63 batches, loss: 0.1073Epoch 15/15: [================              ] 35/63 batches, loss: 0.1087Epoch 15/15: [=================             ] 36/63 batches, loss: 0.1111Epoch 15/15: [=================             ] 37/63 batches, loss: 0.1125Epoch 15/15: [==================            ] 38/63 batches, loss: 0.1127Epoch 15/15: [==================            ] 39/63 batches, loss: 0.1123Epoch 15/15: [===================           ] 40/63 batches, loss: 0.1109Epoch 15/15: [===================           ] 41/63 batches, loss: 0.1095Epoch 15/15: [====================          ] 42/63 batches, loss: 0.1104Epoch 15/15: [====================          ] 43/63 batches, loss: 0.1107Epoch 15/15: [====================          ] 44/63 batches, loss: 0.1109Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.1099Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.1099Epoch 15/15: [======================        ] 47/63 batches, loss: 0.1092Epoch 15/15: [======================        ] 48/63 batches, loss: 0.1078Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.1079Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.1084Epoch 15/15: [========================      ] 51/63 batches, loss: 0.1088Epoch 15/15: [========================      ] 52/63 batches, loss: 0.1089Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.1089Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.1080Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.1075Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.1072Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.1074Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.1083Epoch 15/15: [============================  ] 59/63 batches, loss: 0.1082Epoch 15/15: [============================  ] 60/63 batches, loss: 0.1095Epoch 15/15: [============================= ] 61/63 batches, loss: 0.1115Epoch 15/15: [============================= ] 62/63 batches, loss: 0.1109Epoch 15/15: [==============================] 63/63 batches, loss: 0.1103
[2025-05-02 12:50:30,633][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.1103
[2025-05-02 12:50:30,891][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.1422, Metrics: {'mse': 0.13778649270534515, 'rmse': 0.37119603002368595, 'r2': -1.1237480640411377}
[2025-05-02 12:50:30,892][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-02 12:50:30,892][src.training.lm_trainer][INFO] - Training completed in 37.93 seconds
[2025-05-02 12:50:30,892][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 12:50:33,478][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.07120104134082794, 'rmse': 0.266835232570266, 'r2': -1.3194515705108643}
[2025-05-02 12:50:33,478][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.13419489562511444, 'rmse': 0.3663262147664489, 'r2': -1.068389654159546}
[2025-05-02 12:50:33,478][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.14635783433914185, 'rmse': 0.3825674245661042, 'r2': -1.523144006729126}
[2025-05-02 12:50:35,165][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer6/ar/ar/model.pt
[2025-05-02 12:50:35,166][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▅▄▄▄▃▂▂▁▁
wandb:     best_val_mse █▇▅▄▄▃▃▂▂▁▁
wandb:      best_val_r2 ▁▂▄▅▅▆▆▇▇██
wandb:    best_val_rmse █▇▅▄▄▄▃▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▁▁▃▄▄▄▅▅▅▆▆▆
wandb:       train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇███▆▅▄▄▃▃▂▂▁▁▁
wandb:          val_mse ████▆▅▄▄▃▃▂▂▁▁▁
wandb:           val_r2 ▁▁▁▁▃▄▅▅▆▆▇▇███
wandb:         val_rmse ████▆▅▄▄▄▃▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.13837
wandb:     best_val_mse 0.13419
wandb:      best_val_r2 -1.06839
wandb:    best_val_rmse 0.36633
wandb:            epoch 15
wandb:   final_test_mse 0.14636
wandb:    final_test_r2 -1.52314
wandb:  final_test_rmse 0.38257
wandb:  final_train_mse 0.0712
wandb:   final_train_r2 -1.31945
wandb: final_train_rmse 0.26684
wandb:    final_val_mse 0.13419
wandb:     final_val_r2 -1.06839
wandb:   final_val_rmse 0.36633
wandb:    learning_rate 2e-05
wandb:       train_loss 0.11029
wandb:       train_time 37.9285
wandb:         val_loss 0.14217
wandb:          val_mse 0.13779
wandb:           val_r2 -1.12375
wandb:         val_rmse 0.3712
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_124943-lgb3dpv1
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_124943-lgb3dpv1/logs
Experiment probe_layer6_complexity_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer6/ar/ar/results.json for layer 6
=======================
PROBING LAYER 9
=======================
Running experiment: probe_layer9_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=9"         "model.probe_hidden_size=128" "model.probe_depth=2" "model.dropout=0.2" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer9_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer9/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 12:50:47,835][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer9/ar
experiment_name: probe_layer9_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 12:50:47,835][__main__][INFO] - Normalized task: question_type
[2025-05-02 12:50:47,835][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 12:50:47,835][__main__][INFO] - Determined Task Type: classification
[2025-05-02 12:50:47,839][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-02 12:50:47,840][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 12:50:49,447][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 12:50:51,889][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 12:50:51,890][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:50:52,031][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:50:52,080][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:50:52,216][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 12:50:52,224][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:50:52,225][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 12:50:52,227][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:50:52,265][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:50:52,311][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:50:52,328][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 12:50:52,334][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:50:52,335][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 12:50:52,337][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:50:52,364][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:50:52,400][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:50:52,417][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 12:50:52,419][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:50:52,419][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 12:50:52,421][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 12:50:52,422][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 12:50:52,422][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 12:50:52,422][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 12:50:52,422][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 12:50:52,422][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-02 12:50:52,422][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-02 12:50:52,422][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 12:50:52,422][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 12:50:52,423][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 12:50:52,423][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 12:50:52,423][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 12:50:52,423][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 12:50:52,423][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-02 12:50:52,423][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-02 12:50:52,423][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 12:50:52,423][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 12:50:52,423][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 12:50:52,423][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 12:50:52,424][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 12:50:52,424][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 12:50:52,424][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-02 12:50:52,424][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-02 12:50:52,424][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 12:50:52,424][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 12:50:52,424][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 12:50:52,424][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 12:50:52,425][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 12:50:52,425][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 12:50:52,425][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 12:50:57,214][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 12:50:57,215][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 12:50:57,215][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=9, freeze_model=True
[2025-05-02 12:50:57,215][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-02 12:50:57,218][src.models.model_factory][INFO] - Model has 116,865 trainable parameters out of 394,238,337 total parameters
[2025-05-02 12:50:57,218][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 116,865 trainable parameters
[2025-05-02 12:50:57,219][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=2, activation=gelu, normalization=layer
[2025-05-02 12:50:57,219][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 128 hidden size
[2025-05-02 12:50:57,219][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 12:50:57,220][__main__][INFO] - Total parameters: 394,238,337
[2025-05-02 12:50:57,220][__main__][INFO] - Trainable parameters: 116,865 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7567Epoch 1/15: [                              ] 2/63 batches, loss: 0.7300Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7228Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7240Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7245Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7142Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7061Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7104Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7052Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7009Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7015Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7014Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7011Epoch 1/15: [======                        ] 14/63 batches, loss: 0.6991Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.6991Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.6981Epoch 1/15: [========                      ] 17/63 batches, loss: 0.6976Epoch 1/15: [========                      ] 18/63 batches, loss: 0.6983Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.6970Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.6966Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.6966Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.6975Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.6965Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6963Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.6975Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6942Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6944Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6949Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6946Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6962Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6963Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6963Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6960Epoch 1/15: [================              ] 34/63 batches, loss: 0.6951Epoch 1/15: [================              ] 35/63 batches, loss: 0.6945Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6943Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6940Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6937Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6939Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6932Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6937Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6934Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6939Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6937Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6929Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6928Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6933Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6922Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6919Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6915Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6908Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6905Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6909Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6914Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6911Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6910Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6908Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6910Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6906Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6904Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6906Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6904Epoch 1/15: [==============================] 63/63 batches, loss: 0.6884
[2025-05-02 12:51:01,804][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6884
[2025-05-02 12:51:01,990][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6953, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6939Epoch 2/15: [                              ] 2/63 batches, loss: 0.6893Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6930Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6966Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6861Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6862Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6767Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6739Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6774Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6780Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6753Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6786Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6832Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6865Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6888Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6901Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6899Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6887Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6892Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6897Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6878Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6858Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6848Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6820Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6828Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6850Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6849Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6854Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6841Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6841Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6837Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6820Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6811Epoch 2/15: [================              ] 34/63 batches, loss: 0.6807Epoch 2/15: [================              ] 35/63 batches, loss: 0.6810Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6810Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6786Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6782Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6776Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6779Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6767Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6768Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6776Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6779Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6776Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6780Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6782Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6790Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6801Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6809Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6803Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6808Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6805Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6796Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6796Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6793Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6791Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6777Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6779Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6773Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6765Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6770Epoch 2/15: [==============================] 63/63 batches, loss: 0.6763
[2025-05-02 12:51:04,323][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6763
[2025-05-02 12:51:04,529][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6793, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.23076923076923078, 'precision': 0.5, 'recall': 0.15}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6999Epoch 3/15: [                              ] 2/63 batches, loss: 0.6844Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6806Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6703Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6569Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6571Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6599Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6605Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6574Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6515Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6478Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6487Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6493Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6450Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6474Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6510Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6532Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6528Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6492Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6504Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6487Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6483Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6470Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6455Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6465Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6454Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6478Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6499Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6491Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6464Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6477Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6453Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6458Epoch 3/15: [================              ] 34/63 batches, loss: 0.6475Epoch 3/15: [================              ] 35/63 batches, loss: 0.6487Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6498Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6498Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6511Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6519Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6526Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6535Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6522Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6517Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6510Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6510Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6498Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6490Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6483Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6484Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6489Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6492Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6490Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6487Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6473Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6468Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6450Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6447Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6440Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6433Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6423Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6412Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6400Epoch 3/15: [==============================] 63/63 batches, loss: 0.6421
[2025-05-02 12:51:06,900][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6421
[2025-05-02 12:51:07,128][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6378, Metrics: {'accuracy': 0.8409090909090909, 'f1': 0.8292682926829268, 'precision': 0.8095238095238095, 'recall': 0.85}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.5217Epoch 4/15: [                              ] 2/63 batches, loss: 0.5409Epoch 4/15: [=                             ] 3/63 batches, loss: 0.5681Epoch 4/15: [=                             ] 4/63 batches, loss: 0.5780Epoch 4/15: [==                            ] 5/63 batches, loss: 0.5922Epoch 4/15: [==                            ] 6/63 batches, loss: 0.5845Epoch 4/15: [===                           ] 7/63 batches, loss: 0.5928Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6072Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6136Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6163Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6162Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6122Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6139Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6075Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6079Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6111Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6085Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6077Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6060Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6070Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6053Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6061Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6076Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6096Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6087Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6040Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6027Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6001Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.5987Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6015Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6020Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6005Epoch 4/15: [===============               ] 33/63 batches, loss: 0.5974Epoch 4/15: [================              ] 34/63 batches, loss: 0.5963Epoch 4/15: [================              ] 35/63 batches, loss: 0.5966Epoch 4/15: [=================             ] 36/63 batches, loss: 0.5962Epoch 4/15: [=================             ] 37/63 batches, loss: 0.5954Epoch 4/15: [==================            ] 38/63 batches, loss: 0.5940Epoch 4/15: [==================            ] 39/63 batches, loss: 0.5945Epoch 4/15: [===================           ] 40/63 batches, loss: 0.5947Epoch 4/15: [===================           ] 41/63 batches, loss: 0.5976Epoch 4/15: [====================          ] 42/63 batches, loss: 0.5970Epoch 4/15: [====================          ] 43/63 batches, loss: 0.5973Epoch 4/15: [====================          ] 44/63 batches, loss: 0.5970Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.5997Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.5983Epoch 4/15: [======================        ] 47/63 batches, loss: 0.5976Epoch 4/15: [======================        ] 48/63 batches, loss: 0.5985Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.5989Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.5996Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6002Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6009Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6021Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6036Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6034Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6022Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6015Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6025Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6029Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6021Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6023Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6027Epoch 4/15: [==============================] 63/63 batches, loss: 0.5996
[2025-05-02 12:51:09,538][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.5996
[2025-05-02 12:51:09,756][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6128, Metrics: {'accuracy': 0.8181818181818182, 'f1': 0.8, 'precision': 0.8, 'recall': 0.8}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.5349Epoch 5/15: [                              ] 2/63 batches, loss: 0.5377Epoch 5/15: [=                             ] 3/63 batches, loss: 0.5334Epoch 5/15: [=                             ] 4/63 batches, loss: 0.5738Epoch 5/15: [==                            ] 5/63 batches, loss: 0.5822Epoch 5/15: [==                            ] 6/63 batches, loss: 0.5734Epoch 5/15: [===                           ] 7/63 batches, loss: 0.5729Epoch 5/15: [===                           ] 8/63 batches, loss: 0.5748Epoch 5/15: [====                          ] 9/63 batches, loss: 0.5709Epoch 5/15: [====                          ] 10/63 batches, loss: 0.5747Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.5815Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.5848Epoch 5/15: [======                        ] 13/63 batches, loss: 0.5828Epoch 5/15: [======                        ] 14/63 batches, loss: 0.5846Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.5812Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.5805Epoch 5/15: [========                      ] 17/63 batches, loss: 0.5786Epoch 5/15: [========                      ] 18/63 batches, loss: 0.5774Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.5791Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.5760Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.5796Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.5807Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.5839Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.5811Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.5815Epoch 5/15: [============                  ] 26/63 batches, loss: 0.5819Epoch 5/15: [============                  ] 27/63 batches, loss: 0.5838Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.5900Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.5913Epoch 5/15: [==============                ] 30/63 batches, loss: 0.5902Epoch 5/15: [==============                ] 31/63 batches, loss: 0.5908Epoch 5/15: [===============               ] 32/63 batches, loss: 0.5899Epoch 5/15: [===============               ] 33/63 batches, loss: 0.5889Epoch 5/15: [================              ] 34/63 batches, loss: 0.5908Epoch 5/15: [================              ] 35/63 batches, loss: 0.5874Epoch 5/15: [=================             ] 36/63 batches, loss: 0.5881Epoch 5/15: [=================             ] 37/63 batches, loss: 0.5876Epoch 5/15: [==================            ] 38/63 batches, loss: 0.5893Epoch 5/15: [==================            ] 39/63 batches, loss: 0.5891Epoch 5/15: [===================           ] 40/63 batches, loss: 0.5883Epoch 5/15: [===================           ] 41/63 batches, loss: 0.5873Epoch 5/15: [====================          ] 42/63 batches, loss: 0.5862Epoch 5/15: [====================          ] 43/63 batches, loss: 0.5854Epoch 5/15: [====================          ] 44/63 batches, loss: 0.5852Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.5860Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.5869Epoch 5/15: [======================        ] 47/63 batches, loss: 0.5852Epoch 5/15: [======================        ] 48/63 batches, loss: 0.5859Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.5865Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.5864Epoch 5/15: [========================      ] 51/63 batches, loss: 0.5866Epoch 5/15: [========================      ] 52/63 batches, loss: 0.5840Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.5832Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.5829Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.5828Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.5834Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.5819Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.5805Epoch 5/15: [============================  ] 59/63 batches, loss: 0.5802Epoch 5/15: [============================  ] 60/63 batches, loss: 0.5801Epoch 5/15: [============================= ] 61/63 batches, loss: 0.5788Epoch 5/15: [============================= ] 62/63 batches, loss: 0.5782Epoch 5/15: [==============================] 63/63 batches, loss: 0.5819
[2025-05-02 12:51:12,049][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5819
[2025-05-02 12:51:12,268][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5921, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8837209302325582, 'precision': 0.8260869565217391, 'recall': 0.95}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.5045Epoch 6/15: [                              ] 2/63 batches, loss: 0.5640Epoch 6/15: [=                             ] 3/63 batches, loss: 0.5612Epoch 6/15: [=                             ] 4/63 batches, loss: 0.5604Epoch 6/15: [==                            ] 5/63 batches, loss: 0.5671Epoch 6/15: [==                            ] 6/63 batches, loss: 0.5506Epoch 6/15: [===                           ] 7/63 batches, loss: 0.5513Epoch 6/15: [===                           ] 8/63 batches, loss: 0.5534Epoch 6/15: [====                          ] 9/63 batches, loss: 0.5592Epoch 6/15: [====                          ] 10/63 batches, loss: 0.5540Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.5546Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.5503Epoch 6/15: [======                        ] 13/63 batches, loss: 0.5594Epoch 6/15: [======                        ] 14/63 batches, loss: 0.5586Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.5537Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.5532Epoch 6/15: [========                      ] 17/63 batches, loss: 0.5536Epoch 6/15: [========                      ] 18/63 batches, loss: 0.5569Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.5529Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.5543Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.5566Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.5589Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.5548Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.5534Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.5512Epoch 6/15: [============                  ] 26/63 batches, loss: 0.5511Epoch 6/15: [============                  ] 27/63 batches, loss: 0.5518Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.5525Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.5517Epoch 6/15: [==============                ] 30/63 batches, loss: 0.5516Epoch 6/15: [==============                ] 31/63 batches, loss: 0.5516Epoch 6/15: [===============               ] 32/63 batches, loss: 0.5532Epoch 6/15: [===============               ] 33/63 batches, loss: 0.5538Epoch 6/15: [================              ] 34/63 batches, loss: 0.5557Epoch 6/15: [================              ] 35/63 batches, loss: 0.5541Epoch 6/15: [=================             ] 36/63 batches, loss: 0.5520Epoch 6/15: [=================             ] 37/63 batches, loss: 0.5536Epoch 6/15: [==================            ] 38/63 batches, loss: 0.5551Epoch 6/15: [==================            ] 39/63 batches, loss: 0.5566Epoch 6/15: [===================           ] 40/63 batches, loss: 0.5579Epoch 6/15: [===================           ] 41/63 batches, loss: 0.5568Epoch 6/15: [====================          ] 42/63 batches, loss: 0.5532Epoch 6/15: [====================          ] 43/63 batches, loss: 0.5533Epoch 6/15: [====================          ] 44/63 batches, loss: 0.5517Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.5521Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.5539Epoch 6/15: [======================        ] 47/63 batches, loss: 0.5543Epoch 6/15: [======================        ] 48/63 batches, loss: 0.5545Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.5544Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.5543Epoch 6/15: [========================      ] 51/63 batches, loss: 0.5561Epoch 6/15: [========================      ] 52/63 batches, loss: 0.5554Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.5546Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.5555Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.5541Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.5551Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.5555Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.5577Epoch 6/15: [============================  ] 59/63 batches, loss: 0.5580Epoch 6/15: [============================  ] 60/63 batches, loss: 0.5586Epoch 6/15: [============================= ] 61/63 batches, loss: 0.5587Epoch 6/15: [============================= ] 62/63 batches, loss: 0.5583Epoch 6/15: [==============================] 63/63 batches, loss: 0.5585
[2025-05-02 12:51:14,555][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5585
[2025-05-02 12:51:14,775][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5856, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8571428571428571, 'precision': 0.8181818181818182, 'recall': 0.9}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.5483Epoch 7/15: [                              ] 2/63 batches, loss: 0.5819Epoch 7/15: [=                             ] 3/63 batches, loss: 0.5622Epoch 7/15: [=                             ] 4/63 batches, loss: 0.5414Epoch 7/15: [==                            ] 5/63 batches, loss: 0.5525Epoch 7/15: [==                            ] 6/63 batches, loss: 0.5626Epoch 7/15: [===                           ] 7/63 batches, loss: 0.5601Epoch 7/15: [===                           ] 8/63 batches, loss: 0.5532Epoch 7/15: [====                          ] 9/63 batches, loss: 0.5533Epoch 7/15: [====                          ] 10/63 batches, loss: 0.5531Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.5478Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.5483Epoch 7/15: [======                        ] 13/63 batches, loss: 0.5420Epoch 7/15: [======                        ] 14/63 batches, loss: 0.5455Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.5479Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.5480Epoch 7/15: [========                      ] 17/63 batches, loss: 0.5491Epoch 7/15: [========                      ] 18/63 batches, loss: 0.5477Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.5479Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.5474Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.5444Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.5491Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.5488Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.5488Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.5469Epoch 7/15: [============                  ] 26/63 batches, loss: 0.5432Epoch 7/15: [============                  ] 27/63 batches, loss: 0.5436Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.5474Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.5459Epoch 7/15: [==============                ] 30/63 batches, loss: 0.5431Epoch 7/15: [==============                ] 31/63 batches, loss: 0.5429Epoch 7/15: [===============               ] 32/63 batches, loss: 0.5458Epoch 7/15: [===============               ] 33/63 batches, loss: 0.5462Epoch 7/15: [================              ] 34/63 batches, loss: 0.5478Epoch 7/15: [================              ] 35/63 batches, loss: 0.5466Epoch 7/15: [=================             ] 36/63 batches, loss: 0.5482Epoch 7/15: [=================             ] 37/63 batches, loss: 0.5479Epoch 7/15: [==================            ] 38/63 batches, loss: 0.5479Epoch 7/15: [==================            ] 39/63 batches, loss: 0.5466Epoch 7/15: [===================           ] 40/63 batches, loss: 0.5448Epoch 7/15: [===================           ] 41/63 batches, loss: 0.5448Epoch 7/15: [====================          ] 42/63 batches, loss: 0.5460Epoch 7/15: [====================          ] 43/63 batches, loss: 0.5465Epoch 7/15: [====================          ] 44/63 batches, loss: 0.5457Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.5448Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.5452Epoch 7/15: [======================        ] 47/63 batches, loss: 0.5460Epoch 7/15: [======================        ] 48/63 batches, loss: 0.5469Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.5480Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.5488Epoch 7/15: [========================      ] 51/63 batches, loss: 0.5492Epoch 7/15: [========================      ] 52/63 batches, loss: 0.5504Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.5487Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.5490Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.5483Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.5503Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.5507Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.5491Epoch 7/15: [============================  ] 59/63 batches, loss: 0.5504Epoch 7/15: [============================  ] 60/63 batches, loss: 0.5491Epoch 7/15: [============================= ] 61/63 batches, loss: 0.5494Epoch 7/15: [============================= ] 62/63 batches, loss: 0.5489Epoch 7/15: [==============================] 63/63 batches, loss: 0.5493
[2025-05-02 12:51:17,089][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5493
[2025-05-02 12:51:17,292][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5806, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8780487804878049, 'precision': 0.8571428571428571, 'recall': 0.9}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.6491Epoch 8/15: [                              ] 2/63 batches, loss: 0.5808Epoch 8/15: [=                             ] 3/63 batches, loss: 0.5573Epoch 8/15: [=                             ] 4/63 batches, loss: 0.5415Epoch 8/15: [==                            ] 5/63 batches, loss: 0.5532Epoch 8/15: [==                            ] 6/63 batches, loss: 0.5536Epoch 8/15: [===                           ] 7/63 batches, loss: 0.5488Epoch 8/15: [===                           ] 8/63 batches, loss: 0.5388Epoch 8/15: [====                          ] 9/63 batches, loss: 0.5451Epoch 8/15: [====                          ] 10/63 batches, loss: 0.5493Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.5462Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.5480Epoch 8/15: [======                        ] 13/63 batches, loss: 0.5434Epoch 8/15: [======                        ] 14/63 batches, loss: 0.5437Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.5471Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.5462Epoch 8/15: [========                      ] 17/63 batches, loss: 0.5465Epoch 8/15: [========                      ] 18/63 batches, loss: 0.5438Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.5424Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.5393Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.5414Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.5393Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.5393Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.5388Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.5386Epoch 8/15: [============                  ] 26/63 batches, loss: 0.5388Epoch 8/15: [============                  ] 27/63 batches, loss: 0.5383Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.5385Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.5377Epoch 8/15: [==============                ] 30/63 batches, loss: 0.5388Epoch 8/15: [==============                ] 31/63 batches, loss: 0.5389Epoch 8/15: [===============               ] 32/63 batches, loss: 0.5380Epoch 8/15: [===============               ] 33/63 batches, loss: 0.5369Epoch 8/15: [================              ] 34/63 batches, loss: 0.5374Epoch 8/15: [================              ] 35/63 batches, loss: 0.5348Epoch 8/15: [=================             ] 36/63 batches, loss: 0.5350Epoch 8/15: [=================             ] 37/63 batches, loss: 0.5361Epoch 8/15: [==================            ] 38/63 batches, loss: 0.5362Epoch 8/15: [==================            ] 39/63 batches, loss: 0.5379Epoch 8/15: [===================           ] 40/63 batches, loss: 0.5358Epoch 8/15: [===================           ] 41/63 batches, loss: 0.5361Epoch 8/15: [====================          ] 42/63 batches, loss: 0.5357Epoch 8/15: [====================          ] 43/63 batches, loss: 0.5330Epoch 8/15: [====================          ] 44/63 batches, loss: 0.5340Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.5333Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.5307Epoch 8/15: [======================        ] 47/63 batches, loss: 0.5298Epoch 8/15: [======================        ] 48/63 batches, loss: 0.5304Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.5338Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.5352Epoch 8/15: [========================      ] 51/63 batches, loss: 0.5378Epoch 8/15: [========================      ] 52/63 batches, loss: 0.5374Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.5375Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.5381Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.5409Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.5419Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.5429Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.5428Epoch 8/15: [============================  ] 59/63 batches, loss: 0.5423Epoch 8/15: [============================  ] 60/63 batches, loss: 0.5427Epoch 8/15: [============================= ] 61/63 batches, loss: 0.5421Epoch 8/15: [============================= ] 62/63 batches, loss: 0.5416Epoch 8/15: [==============================] 63/63 batches, loss: 0.5404
[2025-05-02 12:51:19,664][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5404
[2025-05-02 12:51:19,887][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5771, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.4860Epoch 9/15: [                              ] 2/63 batches, loss: 0.5467Epoch 9/15: [=                             ] 3/63 batches, loss: 0.5377Epoch 9/15: [=                             ] 4/63 batches, loss: 0.5334Epoch 9/15: [==                            ] 5/63 batches, loss: 0.5396Epoch 9/15: [==                            ] 6/63 batches, loss: 0.5464Epoch 9/15: [===                           ] 7/63 batches, loss: 0.5530Epoch 9/15: [===                           ] 8/63 batches, loss: 0.5422Epoch 9/15: [====                          ] 9/63 batches, loss: 0.5491Epoch 9/15: [====                          ] 10/63 batches, loss: 0.5438Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.5416Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.5364Epoch 9/15: [======                        ] 13/63 batches, loss: 0.5453Epoch 9/15: [======                        ] 14/63 batches, loss: 0.5466Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.5456Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.5484Epoch 9/15: [========                      ] 17/63 batches, loss: 0.5486Epoch 9/15: [========                      ] 18/63 batches, loss: 0.5472Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.5473Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.5459Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.5444Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.5420Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.5406Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.5408Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.5399Epoch 9/15: [============                  ] 26/63 batches, loss: 0.5389Epoch 9/15: [============                  ] 27/63 batches, loss: 0.5381Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.5351Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.5354Epoch 9/15: [==============                ] 30/63 batches, loss: 0.5356Epoch 9/15: [==============                ] 31/63 batches, loss: 0.5367Epoch 9/15: [===============               ] 32/63 batches, loss: 0.5335Epoch 9/15: [===============               ] 33/63 batches, loss: 0.5344Epoch 9/15: [================              ] 34/63 batches, loss: 0.5339Epoch 9/15: [================              ] 35/63 batches, loss: 0.5343Epoch 9/15: [=================             ] 36/63 batches, loss: 0.5323Epoch 9/15: [=================             ] 37/63 batches, loss: 0.5342Epoch 9/15: [==================            ] 38/63 batches, loss: 0.5344Epoch 9/15: [==================            ] 39/63 batches, loss: 0.5345Epoch 9/15: [===================           ] 40/63 batches, loss: 0.5369Epoch 9/15: [===================           ] 41/63 batches, loss: 0.5384Epoch 9/15: [====================          ] 42/63 batches, loss: 0.5400Epoch 9/15: [====================          ] 43/63 batches, loss: 0.5394Epoch 9/15: [====================          ] 44/63 batches, loss: 0.5397Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.5403Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.5418Epoch 9/15: [======================        ] 47/63 batches, loss: 0.5402Epoch 9/15: [======================        ] 48/63 batches, loss: 0.5409Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.5422Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.5410Epoch 9/15: [========================      ] 51/63 batches, loss: 0.5407Epoch 9/15: [========================      ] 52/63 batches, loss: 0.5397Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.5405Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.5395Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.5408Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.5410Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.5403Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.5408Epoch 9/15: [============================  ] 59/63 batches, loss: 0.5395Epoch 9/15: [============================  ] 60/63 batches, loss: 0.5379Epoch 9/15: [============================= ] 61/63 batches, loss: 0.5372Epoch 9/15: [============================= ] 62/63 batches, loss: 0.5377Epoch 9/15: [==============================] 63/63 batches, loss: 0.5341
[2025-05-02 12:51:22,201][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5341
[2025-05-02 12:51:22,422][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5765, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.5200Epoch 10/15: [                              ] 2/63 batches, loss: 0.4943Epoch 10/15: [=                             ] 3/63 batches, loss: 0.5241Epoch 10/15: [=                             ] 4/63 batches, loss: 0.5259Epoch 10/15: [==                            ] 5/63 batches, loss: 0.5366Epoch 10/15: [==                            ] 6/63 batches, loss: 0.5242Epoch 10/15: [===                           ] 7/63 batches, loss: 0.5196Epoch 10/15: [===                           ] 8/63 batches, loss: 0.5259Epoch 10/15: [====                          ] 9/63 batches, loss: 0.5175Epoch 10/15: [====                          ] 10/63 batches, loss: 0.5192Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.5183Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.5256Epoch 10/15: [======                        ] 13/63 batches, loss: 0.5284Epoch 10/15: [======                        ] 14/63 batches, loss: 0.5329Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.5331Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.5327Epoch 10/15: [========                      ] 17/63 batches, loss: 0.5336Epoch 10/15: [========                      ] 18/63 batches, loss: 0.5425Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.5387Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.5369Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.5409Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.5388Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.5397Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.5402Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.5396Epoch 10/15: [============                  ] 26/63 batches, loss: 0.5377Epoch 10/15: [============                  ] 27/63 batches, loss: 0.5383Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.5365Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.5377Epoch 10/15: [==============                ] 30/63 batches, loss: 0.5374Epoch 10/15: [==============                ] 31/63 batches, loss: 0.5378Epoch 10/15: [===============               ] 32/63 batches, loss: 0.5398Epoch 10/15: [===============               ] 33/63 batches, loss: 0.5384Epoch 10/15: [================              ] 34/63 batches, loss: 0.5386Epoch 10/15: [================              ] 35/63 batches, loss: 0.5378Epoch 10/15: [=================             ] 36/63 batches, loss: 0.5370Epoch 10/15: [=================             ] 37/63 batches, loss: 0.5348Epoch 10/15: [==================            ] 38/63 batches, loss: 0.5347Epoch 10/15: [==================            ] 39/63 batches, loss: 0.5364Epoch 10/15: [===================           ] 40/63 batches, loss: 0.5386Epoch 10/15: [===================           ] 41/63 batches, loss: 0.5353Epoch 10/15: [====================          ] 42/63 batches, loss: 0.5359Epoch 10/15: [====================          ] 43/63 batches, loss: 0.5348Epoch 10/15: [====================          ] 44/63 batches, loss: 0.5354Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.5348Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.5336Epoch 10/15: [======================        ] 47/63 batches, loss: 0.5339Epoch 10/15: [======================        ] 48/63 batches, loss: 0.5354Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.5343Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.5352Epoch 10/15: [========================      ] 51/63 batches, loss: 0.5338Epoch 10/15: [========================      ] 52/63 batches, loss: 0.5336Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.5351Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.5349Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.5342Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.5328Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.5327Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.5337Epoch 10/15: [============================  ] 59/63 batches, loss: 0.5333Epoch 10/15: [============================  ] 60/63 batches, loss: 0.5334Epoch 10/15: [============================= ] 61/63 batches, loss: 0.5334Epoch 10/15: [============================= ] 62/63 batches, loss: 0.5333Epoch 10/15: [==============================] 63/63 batches, loss: 0.5340
[2025-05-02 12:51:24,728][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5340
[2025-05-02 12:51:24,954][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5732, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.5340Epoch 11/15: [                              ] 2/63 batches, loss: 0.5443Epoch 11/15: [=                             ] 3/63 batches, loss: 0.5478Epoch 11/15: [=                             ] 4/63 batches, loss: 0.5551Epoch 11/15: [==                            ] 5/63 batches, loss: 0.5689Epoch 11/15: [==                            ] 6/63 batches, loss: 0.5588Epoch 11/15: [===                           ] 7/63 batches, loss: 0.5587Epoch 11/15: [===                           ] 8/63 batches, loss: 0.5670Epoch 11/15: [====                          ] 9/63 batches, loss: 0.5666Epoch 11/15: [====                          ] 10/63 batches, loss: 0.5569Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.5519Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.5550Epoch 11/15: [======                        ] 13/63 batches, loss: 0.5568Epoch 11/15: [======                        ] 14/63 batches, loss: 0.5597Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.5538Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.5512Epoch 11/15: [========                      ] 17/63 batches, loss: 0.5495Epoch 11/15: [========                      ] 18/63 batches, loss: 0.5484Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.5473Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.5506Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.5524Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.5503Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.5502Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.5520Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.5507Epoch 11/15: [============                  ] 26/63 batches, loss: 0.5487Epoch 11/15: [============                  ] 27/63 batches, loss: 0.5454Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.5474Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.5472Epoch 11/15: [==============                ] 30/63 batches, loss: 0.5458Epoch 11/15: [==============                ] 31/63 batches, loss: 0.5443Epoch 11/15: [===============               ] 32/63 batches, loss: 0.5439Epoch 11/15: [===============               ] 33/63 batches, loss: 0.5406Epoch 11/15: [================              ] 34/63 batches, loss: 0.5379Epoch 11/15: [================              ] 35/63 batches, loss: 0.5389Epoch 11/15: [=================             ] 36/63 batches, loss: 0.5383Epoch 11/15: [=================             ] 37/63 batches, loss: 0.5365Epoch 11/15: [==================            ] 38/63 batches, loss: 0.5377Epoch 11/15: [==================            ] 39/63 batches, loss: 0.5368Epoch 11/15: [===================           ] 40/63 batches, loss: 0.5362Epoch 11/15: [===================           ] 41/63 batches, loss: 0.5347Epoch 11/15: [====================          ] 42/63 batches, loss: 0.5339Epoch 11/15: [====================          ] 43/63 batches, loss: 0.5319Epoch 11/15: [====================          ] 44/63 batches, loss: 0.5321Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.5335Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.5321Epoch 11/15: [======================        ] 47/63 batches, loss: 0.5331Epoch 11/15: [======================        ] 48/63 batches, loss: 0.5324Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.5332Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.5310Epoch 11/15: [========================      ] 51/63 batches, loss: 0.5322Epoch 11/15: [========================      ] 52/63 batches, loss: 0.5322Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.5313Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.5310Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.5304Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.5318Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.5323Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.5328Epoch 11/15: [============================  ] 59/63 batches, loss: 0.5325Epoch 11/15: [============================  ] 60/63 batches, loss: 0.5309Epoch 11/15: [============================= ] 61/63 batches, loss: 0.5308Epoch 11/15: [============================= ] 62/63 batches, loss: 0.5299Epoch 11/15: [==============================] 63/63 batches, loss: 0.5305
[2025-05-02 12:51:27,298][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5305
[2025-05-02 12:51:27,523][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5721, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.4700Epoch 12/15: [                              ] 2/63 batches, loss: 0.5695Epoch 12/15: [=                             ] 3/63 batches, loss: 0.5525Epoch 12/15: [=                             ] 4/63 batches, loss: 0.5626Epoch 12/15: [==                            ] 5/63 batches, loss: 0.5371Epoch 12/15: [==                            ] 6/63 batches, loss: 0.5402Epoch 12/15: [===                           ] 7/63 batches, loss: 0.5460Epoch 12/15: [===                           ] 8/63 batches, loss: 0.5423Epoch 12/15: [====                          ] 9/63 batches, loss: 0.5408Epoch 12/15: [====                          ] 10/63 batches, loss: 0.5278Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.5222Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.5135Epoch 12/15: [======                        ] 13/63 batches, loss: 0.5205Epoch 12/15: [======                        ] 14/63 batches, loss: 0.5144Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.5113Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.5097Epoch 12/15: [========                      ] 17/63 batches, loss: 0.5136Epoch 12/15: [========                      ] 18/63 batches, loss: 0.5133Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.5151Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.5115Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.5135Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.5131Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.5117Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.5106Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.5086Epoch 12/15: [============                  ] 26/63 batches, loss: 0.5087Epoch 12/15: [============                  ] 27/63 batches, loss: 0.5090Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.5092Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.5089Epoch 12/15: [==============                ] 30/63 batches, loss: 0.5104Epoch 12/15: [==============                ] 31/63 batches, loss: 0.5114Epoch 12/15: [===============               ] 32/63 batches, loss: 0.5133Epoch 12/15: [===============               ] 33/63 batches, loss: 0.5143Epoch 12/15: [================              ] 34/63 batches, loss: 0.5155Epoch 12/15: [================              ] 35/63 batches, loss: 0.5168Epoch 12/15: [=================             ] 36/63 batches, loss: 0.5184Epoch 12/15: [=================             ] 37/63 batches, loss: 0.5204Epoch 12/15: [==================            ] 38/63 batches, loss: 0.5204Epoch 12/15: [==================            ] 39/63 batches, loss: 0.5194Epoch 12/15: [===================           ] 40/63 batches, loss: 0.5208Epoch 12/15: [===================           ] 41/63 batches, loss: 0.5198Epoch 12/15: [====================          ] 42/63 batches, loss: 0.5212Epoch 12/15: [====================          ] 43/63 batches, loss: 0.5214Epoch 12/15: [====================          ] 44/63 batches, loss: 0.5223Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.5227Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.5230Epoch 12/15: [======================        ] 47/63 batches, loss: 0.5226Epoch 12/15: [======================        ] 48/63 batches, loss: 0.5237Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.5260Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.5269Epoch 12/15: [========================      ] 51/63 batches, loss: 0.5245Epoch 12/15: [========================      ] 52/63 batches, loss: 0.5251Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.5267Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.5276Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.5283Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.5282Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.5287Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.5296Epoch 12/15: [============================  ] 59/63 batches, loss: 0.5301Epoch 12/15: [============================  ] 60/63 batches, loss: 0.5297Epoch 12/15: [============================= ] 61/63 batches, loss: 0.5294Epoch 12/15: [============================= ] 62/63 batches, loss: 0.5293Epoch 12/15: [==============================] 63/63 batches, loss: 0.5292
[2025-05-02 12:51:29,838][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5292
[2025-05-02 12:51:30,070][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.5707, Metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.5095Epoch 13/15: [                              ] 2/63 batches, loss: 0.5116Epoch 13/15: [=                             ] 3/63 batches, loss: 0.5156Epoch 13/15: [=                             ] 4/63 batches, loss: 0.5249Epoch 13/15: [==                            ] 5/63 batches, loss: 0.5312Epoch 13/15: [==                            ] 6/63 batches, loss: 0.5425Epoch 13/15: [===                           ] 7/63 batches, loss: 0.5355Epoch 13/15: [===                           ] 8/63 batches, loss: 0.5318Epoch 13/15: [====                          ] 9/63 batches, loss: 0.5346Epoch 13/15: [====                          ] 10/63 batches, loss: 0.5295Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.5280Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.5383Epoch 13/15: [======                        ] 13/63 batches, loss: 0.5385Epoch 13/15: [======                        ] 14/63 batches, loss: 0.5345Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.5284Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.5260Epoch 13/15: [========                      ] 17/63 batches, loss: 0.5204Epoch 13/15: [========                      ] 18/63 batches, loss: 0.5233Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.5232Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.5214Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.5208Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.5213Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.5213Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.5208Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.5197Epoch 13/15: [============                  ] 26/63 batches, loss: 0.5168Epoch 13/15: [============                  ] 27/63 batches, loss: 0.5182Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.5169Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.5187Epoch 13/15: [==============                ] 30/63 batches, loss: 0.5202Epoch 13/15: [==============                ] 31/63 batches, loss: 0.5198Epoch 13/15: [===============               ] 32/63 batches, loss: 0.5213Epoch 13/15: [===============               ] 33/63 batches, loss: 0.5188Epoch 13/15: [================              ] 34/63 batches, loss: 0.5220Epoch 13/15: [================              ] 35/63 batches, loss: 0.5239Epoch 13/15: [=================             ] 36/63 batches, loss: 0.5234Epoch 13/15: [=================             ] 37/63 batches, loss: 0.5224Epoch 13/15: [==================            ] 38/63 batches, loss: 0.5229Epoch 13/15: [==================            ] 39/63 batches, loss: 0.5230Epoch 13/15: [===================           ] 40/63 batches, loss: 0.5222Epoch 13/15: [===================           ] 41/63 batches, loss: 0.5220Epoch 13/15: [====================          ] 42/63 batches, loss: 0.5230Epoch 13/15: [====================          ] 43/63 batches, loss: 0.5211Epoch 13/15: [====================          ] 44/63 batches, loss: 0.5209Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.5225Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.5220Epoch 13/15: [======================        ] 47/63 batches, loss: 0.5230Epoch 13/15: [======================        ] 48/63 batches, loss: 0.5246Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.5239Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.5240Epoch 13/15: [========================      ] 51/63 batches, loss: 0.5232Epoch 13/15: [========================      ] 52/63 batches, loss: 0.5242Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.5243Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.5245Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.5246Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.5254Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.5253Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.5258Epoch 13/15: [============================  ] 59/63 batches, loss: 0.5262Epoch 13/15: [============================  ] 60/63 batches, loss: 0.5263Epoch 13/15: [============================= ] 61/63 batches, loss: 0.5248Epoch 13/15: [============================= ] 62/63 batches, loss: 0.5262Epoch 13/15: [==============================] 63/63 batches, loss: 0.5269
[2025-05-02 12:51:32,391][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5269
[2025-05-02 12:51:32,621][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.5740, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 12:51:32,621][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.5079Epoch 14/15: [                              ] 2/63 batches, loss: 0.5365Epoch 14/15: [=                             ] 3/63 batches, loss: 0.5336Epoch 14/15: [=                             ] 4/63 batches, loss: 0.5206Epoch 14/15: [==                            ] 5/63 batches, loss: 0.5186Epoch 14/15: [==                            ] 6/63 batches, loss: 0.5246Epoch 14/15: [===                           ] 7/63 batches, loss: 0.5300Epoch 14/15: [===                           ] 8/63 batches, loss: 0.5299Epoch 14/15: [====                          ] 9/63 batches, loss: 0.5232Epoch 14/15: [====                          ] 10/63 batches, loss: 0.5234Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.5137Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.5102Epoch 14/15: [======                        ] 13/63 batches, loss: 0.5124Epoch 14/15: [======                        ] 14/63 batches, loss: 0.5206Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.5180Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.5151Epoch 14/15: [========                      ] 17/63 batches, loss: 0.5144Epoch 14/15: [========                      ] 18/63 batches, loss: 0.5178Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.5126Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.5150Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.5149Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.5150Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.5167Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.5172Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.5154Epoch 14/15: [============                  ] 26/63 batches, loss: 0.5189Epoch 14/15: [============                  ] 27/63 batches, loss: 0.5169Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.5168Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.5209Epoch 14/15: [==============                ] 30/63 batches, loss: 0.5204Epoch 14/15: [==============                ] 31/63 batches, loss: 0.5182Epoch 14/15: [===============               ] 32/63 batches, loss: 0.5149Epoch 14/15: [===============               ] 33/63 batches, loss: 0.5150Epoch 14/15: [================              ] 34/63 batches, loss: 0.5168Epoch 14/15: [================              ] 35/63 batches, loss: 0.5160Epoch 14/15: [=================             ] 36/63 batches, loss: 0.5159Epoch 14/15: [=================             ] 37/63 batches, loss: 0.5162Epoch 14/15: [==================            ] 38/63 batches, loss: 0.5147Epoch 14/15: [==================            ] 39/63 batches, loss: 0.5127Epoch 14/15: [===================           ] 40/63 batches, loss: 0.5119Epoch 14/15: [===================           ] 41/63 batches, loss: 0.5112Epoch 14/15: [====================          ] 42/63 batches, loss: 0.5110Epoch 14/15: [====================          ] 43/63 batches, loss: 0.5093Epoch 14/15: [====================          ] 44/63 batches, loss: 0.5091Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.5103Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.5094Epoch 14/15: [======================        ] 47/63 batches, loss: 0.5084Epoch 14/15: [======================        ] 48/63 batches, loss: 0.5099Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.5124Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.5126Epoch 14/15: [========================      ] 51/63 batches, loss: 0.5119Epoch 14/15: [========================      ] 52/63 batches, loss: 0.5144Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.5157Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.5167Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.5170Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.5177Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.5185Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.5203Epoch 14/15: [============================  ] 59/63 batches, loss: 0.5193Epoch 14/15: [============================  ] 60/63 batches, loss: 0.5210Epoch 14/15: [============================= ] 61/63 batches, loss: 0.5224Epoch 14/15: [============================= ] 62/63 batches, loss: 0.5227Epoch 14/15: [==============================] 63/63 batches, loss: 0.5194
[2025-05-02 12:51:34,580][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.5194
[2025-05-02 12:51:34,791][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.5726, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 12:51:34,791][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.5233Epoch 15/15: [                              ] 2/63 batches, loss: 0.4552Epoch 15/15: [=                             ] 3/63 batches, loss: 0.4867Epoch 15/15: [=                             ] 4/63 batches, loss: 0.5028Epoch 15/15: [==                            ] 5/63 batches, loss: 0.5183Epoch 15/15: [==                            ] 6/63 batches, loss: 0.5265Epoch 15/15: [===                           ] 7/63 batches, loss: 0.5112Epoch 15/15: [===                           ] 8/63 batches, loss: 0.5138Epoch 15/15: [====                          ] 9/63 batches, loss: 0.5132Epoch 15/15: [====                          ] 10/63 batches, loss: 0.5128Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.5112Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.5141Epoch 15/15: [======                        ] 13/63 batches, loss: 0.5125Epoch 15/15: [======                        ] 14/63 batches, loss: 0.5103Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.5114Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.5095Epoch 15/15: [========                      ] 17/63 batches, loss: 0.5108Epoch 15/15: [========                      ] 18/63 batches, loss: 0.5146Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.5168Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.5149Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.5133Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.5132Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.5141Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.5170Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.5166Epoch 15/15: [============                  ] 26/63 batches, loss: 0.5190Epoch 15/15: [============                  ] 27/63 batches, loss: 0.5187Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.5223Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.5201Epoch 15/15: [==============                ] 30/63 batches, loss: 0.5197Epoch 15/15: [==============                ] 31/63 batches, loss: 0.5240Epoch 15/15: [===============               ] 32/63 batches, loss: 0.5250Epoch 15/15: [===============               ] 33/63 batches, loss: 0.5262Epoch 15/15: [================              ] 34/63 batches, loss: 0.5258Epoch 15/15: [================              ] 35/63 batches, loss: 0.5257Epoch 15/15: [=================             ] 36/63 batches, loss: 0.5254Epoch 15/15: [=================             ] 37/63 batches, loss: 0.5229Epoch 15/15: [==================            ] 38/63 batches, loss: 0.5249Epoch 15/15: [==================            ] 39/63 batches, loss: 0.5250Epoch 15/15: [===================           ] 40/63 batches, loss: 0.5223Epoch 15/15: [===================           ] 41/63 batches, loss: 0.5232Epoch 15/15: [====================          ] 42/63 batches, loss: 0.5217Epoch 15/15: [====================          ] 43/63 batches, loss: 0.5211Epoch 15/15: [====================          ] 44/63 batches, loss: 0.5207Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.5189Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.5200Epoch 15/15: [======================        ] 47/63 batches, loss: 0.5211Epoch 15/15: [======================        ] 48/63 batches, loss: 0.5184Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.5172Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.5174Epoch 15/15: [========================      ] 51/63 batches, loss: 0.5185Epoch 15/15: [========================      ] 52/63 batches, loss: 0.5196Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.5204Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.5211Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.5191Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.5189Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.5199Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.5187Epoch 15/15: [============================  ] 59/63 batches, loss: 0.5190Epoch 15/15: [============================  ] 60/63 batches, loss: 0.5199Epoch 15/15: [============================= ] 61/63 batches, loss: 0.5205Epoch 15/15: [============================= ] 62/63 batches, loss: 0.5216Epoch 15/15: [==============================] 63/63 batches, loss: 0.5183
[2025-05-02 12:51:36,733][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.5183
[2025-05-02 12:51:36,957][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.5712, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 12:51:36,958][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-02 12:51:36,958][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 15
[2025-05-02 12:51:36,958][src.training.lm_trainer][INFO] - Training completed in 38.11 seconds
[2025-05-02 12:51:36,958][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 12:51:39,411][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9959798994974874, 'f1': 0.9959677419354839, 'precision': 0.997979797979798, 'recall': 0.993963782696177}
[2025-05-02 12:51:39,412][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9318181818181818, 'f1': 0.9302325581395349, 'precision': 0.8695652173913043, 'recall': 1.0}
[2025-05-02 12:51:39,412][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.6753246753246753, 'f1': 0.6376811594202898, 'precision': 0.46808510638297873, 'recall': 1.0}
[2025-05-02 12:51:41,125][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer9/ar/ar/model.pt
[2025-05-02 12:51:41,127][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▁▆▆▇▇▇█████
wandb:           best_val_f1 ▁▃▇▇█▇██████
wandb:         best_val_loss █▇▅▃▂▂▂▁▁▁▁▁
wandb:    best_val_precision ▁▅█▇████████
wandb:       best_val_recall ▁▂▇▇█▇▇█████
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▂▃▃▃▃▃▃▃▃▃▃▃
wandb:            train_loss ██▆▄▄▃▂▂▂▂▂▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▁▆▆▇▇▇████████
wandb:                val_f1 ▁▃▇▇█▇█████████
wandb:              val_loss █▇▅▃▂▂▂▁▁▁▁▁▁▁▁
wandb:         val_precision ▁▅█▇███████████
wandb:            val_recall ▁▂▇▇█▇▇████████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.93182
wandb:           best_val_f1 0.93023
wandb:         best_val_loss 0.5707
wandb:    best_val_precision 0.86957
wandb:       best_val_recall 1
wandb:      early_stop_epoch 15
wandb:                 epoch 15
wandb:   final_test_accuracy 0.67532
wandb:         final_test_f1 0.63768
wandb:  final_test_precision 0.46809
wandb:     final_test_recall 1
wandb:  final_train_accuracy 0.99598
wandb:        final_train_f1 0.99597
wandb: final_train_precision 0.99798
wandb:    final_train_recall 0.99396
wandb:    final_val_accuracy 0.93182
wandb:          final_val_f1 0.93023
wandb:   final_val_precision 0.86957
wandb:      final_val_recall 1
wandb:         learning_rate 0.0001
wandb:            train_loss 0.51833
wandb:            train_time 38.10754
wandb:          val_accuracy 0.90909
wandb:                val_f1 0.90909
wandb:              val_loss 0.57116
wandb:         val_precision 0.83333
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125047-imrgdnv7
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125047-imrgdnv7/logs
Experiment probe_layer9_question_type_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer9/ar/ar/results.json for layer 9
Running experiment: probe_layer9_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=9"         "model.probe_hidden_size=96" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer9_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer9/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 12:51:51,859][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer9/ar
experiment_name: probe_layer9_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
  probe_hidden_size: 96
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-02 12:51:51,859][__main__][INFO] - Normalized task: complexity
[2025-05-02 12:51:51,859][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 12:51:51,859][__main__][INFO] - Determined Task Type: regression
[2025-05-02 12:51:51,868][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-02 12:51:51,868][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 12:51:53,189][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 12:51:55,419][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 12:51:55,420][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:51:55,469][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:51:55,498][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:51:55,586][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 12:51:55,593][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:51:55,594][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 12:51:55,595][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:51:55,613][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:51:55,641][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:51:55,655][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 12:51:55,656][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:51:55,656][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 12:51:55,657][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:51:55,673][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:51:55,701][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:51:55,715][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 12:51:55,716][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:51:55,716][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 12:51:55,717][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 12:51:55,718][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 12:51:55,718][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 12:51:55,719][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 12:51:55,719][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 12:51:55,719][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:51:55,719][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-02 12:51:55,719][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 12:51:55,719][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-02 12:51:55,719][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 12:51:55,719][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 12:51:55,720][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 12:51:55,720][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 12:51:55,720][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:51:55,720][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-02 12:51:55,720][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 12:51:55,720][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-02 12:51:55,720][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 12:51:55,720][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 12:51:55,720][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 12:51:55,720][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 12:51:55,721][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:51:55,721][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-02 12:51:55,721][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 12:51:55,721][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-02 12:51:55,721][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 12:51:55,721][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 12:51:55,721][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 12:51:55,722][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-02 12:51:55,722][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 12:51:59,588][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 12:51:59,589][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 12:51:59,589][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=9, freeze_model=True
[2025-05-02 12:51:59,589][src.models.model_factory][INFO] - Using provided probe_hidden_size: 96
[2025-05-02 12:51:59,591][src.models.model_factory][INFO] - Model has 84,961 trainable parameters out of 394,206,433 total parameters
[2025-05-02 12:51:59,592][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 84,961 trainable parameters
[2025-05-02 12:51:59,592][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=96, depth=2, activation=silu, normalization=layer
[2025-05-02 12:51:59,592][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 96 hidden size
[2025-05-02 12:51:59,592][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 12:51:59,593][__main__][INFO] - Total parameters: 394,206,433
[2025-05-02 12:51:59,593][__main__][INFO] - Trainable parameters: 84,961 (0.02%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 4.7432Epoch 1/15: [                              ] 2/63 batches, loss: 5.1508Epoch 1/15: [=                             ] 3/63 batches, loss: 5.0019Epoch 1/15: [=                             ] 4/63 batches, loss: 4.8501Epoch 1/15: [==                            ] 5/63 batches, loss: 4.4515Epoch 1/15: [==                            ] 6/63 batches, loss: 4.5320Epoch 1/15: [===                           ] 7/63 batches, loss: 4.3820Epoch 1/15: [===                           ] 8/63 batches, loss: 4.1863Epoch 1/15: [====                          ] 9/63 batches, loss: 4.0965Epoch 1/15: [====                          ] 10/63 batches, loss: 4.0146Epoch 1/15: [=====                         ] 11/63 batches, loss: 3.9721Epoch 1/15: [=====                         ] 12/63 batches, loss: 3.8409Epoch 1/15: [======                        ] 13/63 batches, loss: 3.7883Epoch 1/15: [======                        ] 14/63 batches, loss: 3.7431Epoch 1/15: [=======                       ] 15/63 batches, loss: 3.6692Epoch 1/15: [=======                       ] 16/63 batches, loss: 3.5940Epoch 1/15: [========                      ] 17/63 batches, loss: 3.5449Epoch 1/15: [========                      ] 18/63 batches, loss: 3.5456Epoch 1/15: [=========                     ] 19/63 batches, loss: 3.4990Epoch 1/15: [=========                     ] 20/63 batches, loss: 3.4387Epoch 1/15: [==========                    ] 21/63 batches, loss: 3.3787Epoch 1/15: [==========                    ] 22/63 batches, loss: 3.3514Epoch 1/15: [==========                    ] 23/63 batches, loss: 3.3207Epoch 1/15: [===========                   ] 24/63 batches, loss: 3.2777Epoch 1/15: [===========                   ] 25/63 batches, loss: 3.2433Epoch 1/15: [============                  ] 26/63 batches, loss: 3.2165Epoch 1/15: [============                  ] 27/63 batches, loss: 3.1821Epoch 1/15: [=============                 ] 28/63 batches, loss: 3.1267Epoch 1/15: [=============                 ] 29/63 batches, loss: 3.0785Epoch 1/15: [==============                ] 30/63 batches, loss: 3.0680Epoch 1/15: [==============                ] 31/63 batches, loss: 3.0387Epoch 1/15: [===============               ] 32/63 batches, loss: 2.9747Epoch 1/15: [===============               ] 33/63 batches, loss: 2.9184Epoch 1/15: [================              ] 34/63 batches, loss: 2.8872Epoch 1/15: [================              ] 35/63 batches, loss: 2.8493Epoch 1/15: [=================             ] 36/63 batches, loss: 2.8440Epoch 1/15: [=================             ] 37/63 batches, loss: 2.8153Epoch 1/15: [==================            ] 38/63 batches, loss: 2.8000Epoch 1/15: [==================            ] 39/63 batches, loss: 2.7656Epoch 1/15: [===================           ] 40/63 batches, loss: 2.7401Epoch 1/15: [===================           ] 41/63 batches, loss: 2.7010Epoch 1/15: [====================          ] 42/63 batches, loss: 2.6675Epoch 1/15: [====================          ] 43/63 batches, loss: 2.6324Epoch 1/15: [====================          ] 44/63 batches, loss: 2.5891Epoch 1/15: [=====================         ] 45/63 batches, loss: 2.5486Epoch 1/15: [=====================         ] 46/63 batches, loss: 2.5263Epoch 1/15: [======================        ] 47/63 batches, loss: 2.5020Epoch 1/15: [======================        ] 48/63 batches, loss: 2.4702Epoch 1/15: [=======================       ] 49/63 batches, loss: 2.4428Epoch 1/15: [=======================       ] 50/63 batches, loss: 2.4239Epoch 1/15: [========================      ] 51/63 batches, loss: 2.3955Epoch 1/15: [========================      ] 52/63 batches, loss: 2.3610Epoch 1/15: [=========================     ] 53/63 batches, loss: 2.3221Epoch 1/15: [=========================     ] 54/63 batches, loss: 2.2965Epoch 1/15: [==========================    ] 55/63 batches, loss: 2.2750Epoch 1/15: [==========================    ] 56/63 batches, loss: 2.2408Epoch 1/15: [===========================   ] 57/63 batches, loss: 2.2204Epoch 1/15: [===========================   ] 58/63 batches, loss: 2.2055Epoch 1/15: [============================  ] 59/63 batches, loss: 2.1871Epoch 1/15: [============================  ] 60/63 batches, loss: 2.1629Epoch 1/15: [============================= ] 61/63 batches, loss: 2.1358Epoch 1/15: [============================= ] 62/63 batches, loss: 2.1144Epoch 1/15: [==============================] 63/63 batches, loss: 2.1144
[2025-05-02 12:52:03,791][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 2.1144
[2025-05-02 12:52:03,976][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.4255, Metrics: {'mse': 0.4363723695278168, 'rmse': 0.660584869284649, 'r2': -5.725949287414551}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6889Epoch 2/15: [                              ] 2/63 batches, loss: 0.6015Epoch 2/15: [=                             ] 3/63 batches, loss: 0.5373Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6368Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6952Epoch 2/15: [==                            ] 6/63 batches, loss: 0.7175Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6783Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6511Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6236Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6190Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6162Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6058Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6201Epoch 2/15: [======                        ] 14/63 batches, loss: 0.5993Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6000Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.5949Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6002Epoch 2/15: [========                      ] 18/63 batches, loss: 0.5868Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.5893Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.5996Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.5971Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.5934Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.5747Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.5705Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.5686Epoch 2/15: [============                  ] 26/63 batches, loss: 0.5604Epoch 2/15: [============                  ] 27/63 batches, loss: 0.5452Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.5352Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.5343Epoch 2/15: [==============                ] 30/63 batches, loss: 0.5292Epoch 2/15: [==============                ] 31/63 batches, loss: 0.5276Epoch 2/15: [===============               ] 32/63 batches, loss: 0.5242Epoch 2/15: [===============               ] 33/63 batches, loss: 0.5173Epoch 2/15: [================              ] 34/63 batches, loss: 0.5159Epoch 2/15: [================              ] 35/63 batches, loss: 0.5260Epoch 2/15: [=================             ] 36/63 batches, loss: 0.5230Epoch 2/15: [=================             ] 37/63 batches, loss: 0.5170Epoch 2/15: [==================            ] 38/63 batches, loss: 0.5067Epoch 2/15: [==================            ] 39/63 batches, loss: 0.5029Epoch 2/15: [===================           ] 40/63 batches, loss: 0.5020Epoch 2/15: [===================           ] 41/63 batches, loss: 0.5057Epoch 2/15: [====================          ] 42/63 batches, loss: 0.5006Epoch 2/15: [====================          ] 43/63 batches, loss: 0.4937Epoch 2/15: [====================          ] 44/63 batches, loss: 0.4942Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.4917Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.4857Epoch 2/15: [======================        ] 47/63 batches, loss: 0.4795Epoch 2/15: [======================        ] 48/63 batches, loss: 0.4718Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.4667Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.4677Epoch 2/15: [========================      ] 51/63 batches, loss: 0.4630Epoch 2/15: [========================      ] 52/63 batches, loss: 0.4578Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.4541Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.4494Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.4549Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.4516Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.4481Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.4466Epoch 2/15: [============================  ] 59/63 batches, loss: 0.4462Epoch 2/15: [============================  ] 60/63 batches, loss: 0.4429Epoch 2/15: [============================= ] 61/63 batches, loss: 0.4403Epoch 2/15: [============================= ] 62/63 batches, loss: 0.4379Epoch 2/15: [==============================] 63/63 batches, loss: 0.4348
[2025-05-02 12:52:06,311][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.4348
[2025-05-02 12:52:06,517][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.2051, Metrics: {'mse': 0.20759530365467072, 'rmse': 0.4556262762996343, 'r2': -2.1997339725494385}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.3987Epoch 3/15: [                              ] 2/63 batches, loss: 0.4731Epoch 3/15: [=                             ] 3/63 batches, loss: 0.3606Epoch 3/15: [=                             ] 4/63 batches, loss: 0.3478Epoch 3/15: [==                            ] 5/63 batches, loss: 0.3706Epoch 3/15: [==                            ] 6/63 batches, loss: 0.3576Epoch 3/15: [===                           ] 7/63 batches, loss: 0.3237Epoch 3/15: [===                           ] 8/63 batches, loss: 0.3502Epoch 3/15: [====                          ] 9/63 batches, loss: 0.3383Epoch 3/15: [====                          ] 10/63 batches, loss: 0.3841Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.3654Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.3554Epoch 3/15: [======                        ] 13/63 batches, loss: 0.3564Epoch 3/15: [======                        ] 14/63 batches, loss: 0.3566Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.3407Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.3366Epoch 3/15: [========                      ] 17/63 batches, loss: 0.3309Epoch 3/15: [========                      ] 18/63 batches, loss: 0.3248Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.3299Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.3260Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.3253Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.3226Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.3220Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.3263Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.3223Epoch 3/15: [============                  ] 26/63 batches, loss: 0.3190Epoch 3/15: [============                  ] 27/63 batches, loss: 0.3185Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.3193Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.3133Epoch 3/15: [==============                ] 30/63 batches, loss: 0.3107Epoch 3/15: [==============                ] 31/63 batches, loss: 0.3122Epoch 3/15: [===============               ] 32/63 batches, loss: 0.3122Epoch 3/15: [===============               ] 33/63 batches, loss: 0.3099Epoch 3/15: [================              ] 34/63 batches, loss: 0.3061Epoch 3/15: [================              ] 35/63 batches, loss: 0.3015Epoch 3/15: [=================             ] 36/63 batches, loss: 0.3023Epoch 3/15: [=================             ] 37/63 batches, loss: 0.3009Epoch 3/15: [==================            ] 38/63 batches, loss: 0.3000Epoch 3/15: [==================            ] 39/63 batches, loss: 0.2996Epoch 3/15: [===================           ] 40/63 batches, loss: 0.3028Epoch 3/15: [===================           ] 41/63 batches, loss: 0.3029Epoch 3/15: [====================          ] 42/63 batches, loss: 0.3008Epoch 3/15: [====================          ] 43/63 batches, loss: 0.2975Epoch 3/15: [====================          ] 44/63 batches, loss: 0.2934Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.2930Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.2890Epoch 3/15: [======================        ] 47/63 batches, loss: 0.2866Epoch 3/15: [======================        ] 48/63 batches, loss: 0.2844Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.2834Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.2815Epoch 3/15: [========================      ] 51/63 batches, loss: 0.2801Epoch 3/15: [========================      ] 52/63 batches, loss: 0.2805Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.2793Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.2776Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.2755Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.2763Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.2746Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.2735Epoch 3/15: [============================  ] 59/63 batches, loss: 0.2745Epoch 3/15: [============================  ] 60/63 batches, loss: 0.2746Epoch 3/15: [============================= ] 61/63 batches, loss: 0.2729Epoch 3/15: [============================= ] 62/63 batches, loss: 0.2714Epoch 3/15: [==============================] 63/63 batches, loss: 0.2692
[2025-05-02 12:52:08,856][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.2692
[2025-05-02 12:52:09,080][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.2151, Metrics: {'mse': 0.2162361741065979, 'rmse': 0.4650120150131585, 'r2': -2.332918643951416}
[2025-05-02 12:52:09,081][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.1534Epoch 4/15: [                              ] 2/63 batches, loss: 0.1913Epoch 4/15: [=                             ] 3/63 batches, loss: 0.1698Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1914Epoch 4/15: [==                            ] 5/63 batches, loss: 0.2052Epoch 4/15: [==                            ] 6/63 batches, loss: 0.2379Epoch 4/15: [===                           ] 7/63 batches, loss: 0.2475Epoch 4/15: [===                           ] 8/63 batches, loss: 0.2584Epoch 4/15: [====                          ] 9/63 batches, loss: 0.2841Epoch 4/15: [====                          ] 10/63 batches, loss: 0.2846Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.2870Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.2752Epoch 4/15: [======                        ] 13/63 batches, loss: 0.2707Epoch 4/15: [======                        ] 14/63 batches, loss: 0.2666Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.2637Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.2572Epoch 4/15: [========                      ] 17/63 batches, loss: 0.2534Epoch 4/15: [========                      ] 18/63 batches, loss: 0.2671Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.2569Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.2597Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.2529Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.2503Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.2486Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.2493Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.2468Epoch 4/15: [============                  ] 26/63 batches, loss: 0.2473Epoch 4/15: [============                  ] 27/63 batches, loss: 0.2443Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.2524Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.2532Epoch 4/15: [==============                ] 30/63 batches, loss: 0.2485Epoch 4/15: [==============                ] 31/63 batches, loss: 0.2456Epoch 4/15: [===============               ] 32/63 batches, loss: 0.2470Epoch 4/15: [===============               ] 33/63 batches, loss: 0.2445Epoch 4/15: [================              ] 34/63 batches, loss: 0.2415Epoch 4/15: [================              ] 35/63 batches, loss: 0.2411Epoch 4/15: [=================             ] 36/63 batches, loss: 0.2403Epoch 4/15: [=================             ] 37/63 batches, loss: 0.2420Epoch 4/15: [==================            ] 38/63 batches, loss: 0.2426Epoch 4/15: [==================            ] 39/63 batches, loss: 0.2455Epoch 4/15: [===================           ] 40/63 batches, loss: 0.2439Epoch 4/15: [===================           ] 41/63 batches, loss: 0.2457Epoch 4/15: [====================          ] 42/63 batches, loss: 0.2474Epoch 4/15: [====================          ] 43/63 batches, loss: 0.2450Epoch 4/15: [====================          ] 44/63 batches, loss: 0.2428Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.2419Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.2414Epoch 4/15: [======================        ] 47/63 batches, loss: 0.2394Epoch 4/15: [======================        ] 48/63 batches, loss: 0.2378Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.2368Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.2348Epoch 4/15: [========================      ] 51/63 batches, loss: 0.2377Epoch 4/15: [========================      ] 52/63 batches, loss: 0.2383Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.2353Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.2355Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.2358Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.2349Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.2333Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.2333Epoch 4/15: [============================  ] 59/63 batches, loss: 0.2330Epoch 4/15: [============================  ] 60/63 batches, loss: 0.2324Epoch 4/15: [============================= ] 61/63 batches, loss: 0.2306Epoch 4/15: [============================= ] 62/63 batches, loss: 0.2290Epoch 4/15: [==============================] 63/63 batches, loss: 0.2264
[2025-05-02 12:52:11,019][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.2264
[2025-05-02 12:52:11,240][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.2099, Metrics: {'mse': 0.21053925156593323, 'rmse': 0.4588455639601774, 'r2': -2.245110034942627}
[2025-05-02 12:52:11,240][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/4
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1550Epoch 5/15: [                              ] 2/63 batches, loss: 0.1646Epoch 5/15: [=                             ] 3/63 batches, loss: 0.2091Epoch 5/15: [=                             ] 4/63 batches, loss: 0.1950Epoch 5/15: [==                            ] 5/63 batches, loss: 0.2086Epoch 5/15: [==                            ] 6/63 batches, loss: 0.2084Epoch 5/15: [===                           ] 7/63 batches, loss: 0.2038Epoch 5/15: [===                           ] 8/63 batches, loss: 0.2201Epoch 5/15: [====                          ] 9/63 batches, loss: 0.2052Epoch 5/15: [====                          ] 10/63 batches, loss: 0.2016Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.2086Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.1998Epoch 5/15: [======                        ] 13/63 batches, loss: 0.1999Epoch 5/15: [======                        ] 14/63 batches, loss: 0.2129Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.2073Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.2013Epoch 5/15: [========                      ] 17/63 batches, loss: 0.2034Epoch 5/15: [========                      ] 18/63 batches, loss: 0.2034Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.2071Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.2014Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.2023Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.2012Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.2003Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.2037Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.2016Epoch 5/15: [============                  ] 26/63 batches, loss: 0.1990Epoch 5/15: [============                  ] 27/63 batches, loss: 0.2009Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.2011Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.2027Epoch 5/15: [==============                ] 30/63 batches, loss: 0.2036Epoch 5/15: [==============                ] 31/63 batches, loss: 0.2053Epoch 5/15: [===============               ] 32/63 batches, loss: 0.2033Epoch 5/15: [===============               ] 33/63 batches, loss: 0.2031Epoch 5/15: [================              ] 34/63 batches, loss: 0.2013Epoch 5/15: [================              ] 35/63 batches, loss: 0.1994Epoch 5/15: [=================             ] 36/63 batches, loss: 0.2003Epoch 5/15: [=================             ] 37/63 batches, loss: 0.1979Epoch 5/15: [==================            ] 38/63 batches, loss: 0.1975Epoch 5/15: [==================            ] 39/63 batches, loss: 0.1964Epoch 5/15: [===================           ] 40/63 batches, loss: 0.1972Epoch 5/15: [===================           ] 41/63 batches, loss: 0.1975Epoch 5/15: [====================          ] 42/63 batches, loss: 0.1994Epoch 5/15: [====================          ] 43/63 batches, loss: 0.2027Epoch 5/15: [====================          ] 44/63 batches, loss: 0.2038Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.2045Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.2032Epoch 5/15: [======================        ] 47/63 batches, loss: 0.2056Epoch 5/15: [======================        ] 48/63 batches, loss: 0.2044Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.2038Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.2034Epoch 5/15: [========================      ] 51/63 batches, loss: 0.2034Epoch 5/15: [========================      ] 52/63 batches, loss: 0.2033Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.2072Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.2079Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.2093Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.2110Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.2105Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.2138Epoch 5/15: [============================  ] 59/63 batches, loss: 0.2136Epoch 5/15: [============================  ] 60/63 batches, loss: 0.2119Epoch 5/15: [============================= ] 61/63 batches, loss: 0.2119Epoch 5/15: [============================= ] 62/63 batches, loss: 0.2111Epoch 5/15: [==============================] 63/63 batches, loss: 0.2079
[2025-05-02 12:52:13,166][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.2079
[2025-05-02 12:52:13,378][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1953, Metrics: {'mse': 0.19579850137233734, 'rmse': 0.44249124440189475, 'r2': -2.0179061889648438}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.1594Epoch 6/15: [                              ] 2/63 batches, loss: 0.1655Epoch 6/15: [=                             ] 3/63 batches, loss: 0.1629Epoch 6/15: [=                             ] 4/63 batches, loss: 0.1572Epoch 6/15: [==                            ] 5/63 batches, loss: 0.1486Epoch 6/15: [==                            ] 6/63 batches, loss: 0.1406Epoch 6/15: [===                           ] 7/63 batches, loss: 0.1455Epoch 6/15: [===                           ] 8/63 batches, loss: 0.1433Epoch 6/15: [====                          ] 9/63 batches, loss: 0.1634Epoch 6/15: [====                          ] 10/63 batches, loss: 0.1579Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.1768Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.1700Epoch 6/15: [======                        ] 13/63 batches, loss: 0.1937Epoch 6/15: [======                        ] 14/63 batches, loss: 0.1851Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.1838Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.1854Epoch 6/15: [========                      ] 17/63 batches, loss: 0.1850Epoch 6/15: [========                      ] 18/63 batches, loss: 0.1815Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.1786Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.1745Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.1760Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.1811Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.1808Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.1789Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.1820Epoch 6/15: [============                  ] 26/63 batches, loss: 0.1833Epoch 6/15: [============                  ] 27/63 batches, loss: 0.1875Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.1894Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.1878Epoch 6/15: [==============                ] 30/63 batches, loss: 0.1882Epoch 6/15: [==============                ] 31/63 batches, loss: 0.1890Epoch 6/15: [===============               ] 32/63 batches, loss: 0.1880Epoch 6/15: [===============               ] 33/63 batches, loss: 0.1887Epoch 6/15: [================              ] 34/63 batches, loss: 0.1902Epoch 6/15: [================              ] 35/63 batches, loss: 0.1909Epoch 6/15: [=================             ] 36/63 batches, loss: 0.1920Epoch 6/15: [=================             ] 37/63 batches, loss: 0.1912Epoch 6/15: [==================            ] 38/63 batches, loss: 0.1926Epoch 6/15: [==================            ] 39/63 batches, loss: 0.1929Epoch 6/15: [===================           ] 40/63 batches, loss: 0.1947Epoch 6/15: [===================           ] 41/63 batches, loss: 0.1944Epoch 6/15: [====================          ] 42/63 batches, loss: 0.1947Epoch 6/15: [====================          ] 43/63 batches, loss: 0.1940Epoch 6/15: [====================          ] 44/63 batches, loss: 0.1924Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.1907Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.1897Epoch 6/15: [======================        ] 47/63 batches, loss: 0.1942Epoch 6/15: [======================        ] 48/63 batches, loss: 0.1948Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.1921Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.1916Epoch 6/15: [========================      ] 51/63 batches, loss: 0.1908Epoch 6/15: [========================      ] 52/63 batches, loss: 0.1901Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.1890Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.1876Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.1888Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.1891Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.1904Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.1892Epoch 6/15: [============================  ] 59/63 batches, loss: 0.1894Epoch 6/15: [============================  ] 60/63 batches, loss: 0.1896Epoch 6/15: [============================= ] 61/63 batches, loss: 0.1890Epoch 6/15: [============================= ] 62/63 batches, loss: 0.1875Epoch 6/15: [==============================] 63/63 batches, loss: 0.1860
[2025-05-02 12:52:15,643][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1860
[2025-05-02 12:52:15,860][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1769, Metrics: {'mse': 0.1773969978094101, 'rmse': 0.421185229809178, 'r2': -1.7342777252197266}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.1262Epoch 7/15: [                              ] 2/63 batches, loss: 0.1294Epoch 7/15: [=                             ] 3/63 batches, loss: 0.1613Epoch 7/15: [=                             ] 4/63 batches, loss: 0.1873Epoch 7/15: [==                            ] 5/63 batches, loss: 0.1731Epoch 7/15: [==                            ] 6/63 batches, loss: 0.1788Epoch 7/15: [===                           ] 7/63 batches, loss: 0.2164Epoch 7/15: [===                           ] 8/63 batches, loss: 0.2069Epoch 7/15: [====                          ] 9/63 batches, loss: 0.2011Epoch 7/15: [====                          ] 10/63 batches, loss: 0.1895Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.1965Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.1957Epoch 7/15: [======                        ] 13/63 batches, loss: 0.1888Epoch 7/15: [======                        ] 14/63 batches, loss: 0.1900Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.1914Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.1855Epoch 7/15: [========                      ] 17/63 batches, loss: 0.1937Epoch 7/15: [========                      ] 18/63 batches, loss: 0.1953Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.1974Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.1968Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.1936Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.1874Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.1833Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.1810Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.1761Epoch 7/15: [============                  ] 26/63 batches, loss: 0.1732Epoch 7/15: [============                  ] 27/63 batches, loss: 0.1753Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.1761Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.1757Epoch 7/15: [==============                ] 30/63 batches, loss: 0.1750Epoch 7/15: [==============                ] 31/63 batches, loss: 0.1761Epoch 7/15: [===============               ] 32/63 batches, loss: 0.1760Epoch 7/15: [===============               ] 33/63 batches, loss: 0.1819Epoch 7/15: [================              ] 34/63 batches, loss: 0.1805Epoch 7/15: [================              ] 35/63 batches, loss: 0.1819Epoch 7/15: [=================             ] 36/63 batches, loss: 0.1830Epoch 7/15: [=================             ] 37/63 batches, loss: 0.1812Epoch 7/15: [==================            ] 38/63 batches, loss: 0.1811Epoch 7/15: [==================            ] 39/63 batches, loss: 0.1819Epoch 7/15: [===================           ] 40/63 batches, loss: 0.1801Epoch 7/15: [===================           ] 41/63 batches, loss: 0.1798Epoch 7/15: [====================          ] 42/63 batches, loss: 0.1813Epoch 7/15: [====================          ] 43/63 batches, loss: 0.1808Epoch 7/15: [====================          ] 44/63 batches, loss: 0.1802Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.1839Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.1828Epoch 7/15: [======================        ] 47/63 batches, loss: 0.1830Epoch 7/15: [======================        ] 48/63 batches, loss: 0.1809Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.1803Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.1815Epoch 7/15: [========================      ] 51/63 batches, loss: 0.1819Epoch 7/15: [========================      ] 52/63 batches, loss: 0.1818Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.1812Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.1802Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.1792Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.1812Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.1826Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.1824Epoch 7/15: [============================  ] 59/63 batches, loss: 0.1838Epoch 7/15: [============================  ] 60/63 batches, loss: 0.1832Epoch 7/15: [============================= ] 61/63 batches, loss: 0.1845Epoch 7/15: [============================= ] 62/63 batches, loss: 0.1829Epoch 7/15: [==============================] 63/63 batches, loss: 0.1807
[2025-05-02 12:52:18,161][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1807
[2025-05-02 12:52:18,354][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1656, Metrics: {'mse': 0.16596093773841858, 'rmse': 0.4073830356536936, 'r2': -1.5580101013183594}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0800Epoch 8/15: [                              ] 2/63 batches, loss: 0.1436Epoch 8/15: [=                             ] 3/63 batches, loss: 0.1506Epoch 8/15: [=                             ] 4/63 batches, loss: 0.1294Epoch 8/15: [==                            ] 5/63 batches, loss: 0.1336Epoch 8/15: [==                            ] 6/63 batches, loss: 0.1457Epoch 8/15: [===                           ] 7/63 batches, loss: 0.1555Epoch 8/15: [===                           ] 8/63 batches, loss: 0.1569Epoch 8/15: [====                          ] 9/63 batches, loss: 0.1659Epoch 8/15: [====                          ] 10/63 batches, loss: 0.1770Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.1697Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.1710Epoch 8/15: [======                        ] 13/63 batches, loss: 0.1758Epoch 8/15: [======                        ] 14/63 batches, loss: 0.1709Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.1691Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.1686Epoch 8/15: [========                      ] 17/63 batches, loss: 0.1694Epoch 8/15: [========                      ] 18/63 batches, loss: 0.1688Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.1670Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.1635Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.1689Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.1696Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.1696Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.1682Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.1652Epoch 8/15: [============                  ] 26/63 batches, loss: 0.1663Epoch 8/15: [============                  ] 27/63 batches, loss: 0.1652Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.1657Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.1641Epoch 8/15: [==============                ] 30/63 batches, loss: 0.1635Epoch 8/15: [==============                ] 31/63 batches, loss: 0.1649Epoch 8/15: [===============               ] 32/63 batches, loss: 0.1637Epoch 8/15: [===============               ] 33/63 batches, loss: 0.1643Epoch 8/15: [================              ] 34/63 batches, loss: 0.1652Epoch 8/15: [================              ] 35/63 batches, loss: 0.1621Epoch 8/15: [=================             ] 36/63 batches, loss: 0.1619Epoch 8/15: [=================             ] 37/63 batches, loss: 0.1623Epoch 8/15: [==================            ] 38/63 batches, loss: 0.1598Epoch 8/15: [==================            ] 39/63 batches, loss: 0.1607Epoch 8/15: [===================           ] 40/63 batches, loss: 0.1604Epoch 8/15: [===================           ] 41/63 batches, loss: 0.1619Epoch 8/15: [====================          ] 42/63 batches, loss: 0.1618Epoch 8/15: [====================          ] 43/63 batches, loss: 0.1619Epoch 8/15: [====================          ] 44/63 batches, loss: 0.1604Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.1587Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.1600Epoch 8/15: [======================        ] 47/63 batches, loss: 0.1596Epoch 8/15: [======================        ] 48/63 batches, loss: 0.1593Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.1583Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.1591Epoch 8/15: [========================      ] 51/63 batches, loss: 0.1591Epoch 8/15: [========================      ] 52/63 batches, loss: 0.1601Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.1585Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.1591Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.1583Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.1577Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.1592Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.1631Epoch 8/15: [============================  ] 59/63 batches, loss: 0.1620Epoch 8/15: [============================  ] 60/63 batches, loss: 0.1631Epoch 8/15: [============================= ] 61/63 batches, loss: 0.1623Epoch 8/15: [============================= ] 62/63 batches, loss: 0.1625Epoch 8/15: [==============================] 63/63 batches, loss: 0.1625
[2025-05-02 12:52:20,699][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.1625
[2025-05-02 12:52:20,909][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1675, Metrics: {'mse': 0.16747432947158813, 'rmse': 0.40923627585001326, 'r2': -1.581336259841919}
[2025-05-02 12:52:20,910][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1387Epoch 9/15: [                              ] 2/63 batches, loss: 0.1305Epoch 9/15: [=                             ] 3/63 batches, loss: 0.1318Epoch 9/15: [=                             ] 4/63 batches, loss: 0.1509Epoch 9/15: [==                            ] 5/63 batches, loss: 0.1482Epoch 9/15: [==                            ] 6/63 batches, loss: 0.1399Epoch 9/15: [===                           ] 7/63 batches, loss: 0.1597Epoch 9/15: [===                           ] 8/63 batches, loss: 0.1613Epoch 9/15: [====                          ] 9/63 batches, loss: 0.1645Epoch 9/15: [====                          ] 10/63 batches, loss: 0.1609Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.1589Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.1581Epoch 9/15: [======                        ] 13/63 batches, loss: 0.1547Epoch 9/15: [======                        ] 14/63 batches, loss: 0.1521Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.1495Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.1495Epoch 9/15: [========                      ] 17/63 batches, loss: 0.1481Epoch 9/15: [========                      ] 18/63 batches, loss: 0.1497Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.1491Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.1481Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.1473Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.1501Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.1510Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.1520Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.1520Epoch 9/15: [============                  ] 26/63 batches, loss: 0.1535Epoch 9/15: [============                  ] 27/63 batches, loss: 0.1528Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.1563Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.1568Epoch 9/15: [==============                ] 30/63 batches, loss: 0.1580Epoch 9/15: [==============                ] 31/63 batches, loss: 0.1662Epoch 9/15: [===============               ] 32/63 batches, loss: 0.1637Epoch 9/15: [===============               ] 33/63 batches, loss: 0.1614Epoch 9/15: [================              ] 34/63 batches, loss: 0.1620Epoch 9/15: [================              ] 35/63 batches, loss: 0.1605Epoch 9/15: [=================             ] 36/63 batches, loss: 0.1632Epoch 9/15: [=================             ] 37/63 batches, loss: 0.1603Epoch 9/15: [==================            ] 38/63 batches, loss: 0.1614Epoch 9/15: [==================            ] 39/63 batches, loss: 0.1636Epoch 9/15: [===================           ] 40/63 batches, loss: 0.1613Epoch 9/15: [===================           ] 41/63 batches, loss: 0.1608Epoch 9/15: [====================          ] 42/63 batches, loss: 0.1621Epoch 9/15: [====================          ] 43/63 batches, loss: 0.1612Epoch 9/15: [====================          ] 44/63 batches, loss: 0.1625Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.1625Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.1640Epoch 9/15: [======================        ] 47/63 batches, loss: 0.1640Epoch 9/15: [======================        ] 48/63 batches, loss: 0.1640Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.1627Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.1640Epoch 9/15: [========================      ] 51/63 batches, loss: 0.1630Epoch 9/15: [========================      ] 52/63 batches, loss: 0.1612Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.1599Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.1597Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.1585Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.1576Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.1592Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.1589Epoch 9/15: [============================  ] 59/63 batches, loss: 0.1596Epoch 9/15: [============================  ] 60/63 batches, loss: 0.1599Epoch 9/15: [============================= ] 61/63 batches, loss: 0.1603Epoch 9/15: [============================= ] 62/63 batches, loss: 0.1599Epoch 9/15: [==============================] 63/63 batches, loss: 0.1589
[2025-05-02 12:52:22,850][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.1589
[2025-05-02 12:52:23,062][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.1627, Metrics: {'mse': 0.16242775321006775, 'rmse': 0.4030232663383936, 'r2': -1.503551959991455}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.1446Epoch 10/15: [                              ] 2/63 batches, loss: 0.1588Epoch 10/15: [=                             ] 3/63 batches, loss: 0.1350Epoch 10/15: [=                             ] 4/63 batches, loss: 0.1273Epoch 10/15: [==                            ] 5/63 batches, loss: 0.1397Epoch 10/15: [==                            ] 6/63 batches, loss: 0.1359Epoch 10/15: [===                           ] 7/63 batches, loss: 0.1363Epoch 10/15: [===                           ] 8/63 batches, loss: 0.1363Epoch 10/15: [====                          ] 9/63 batches, loss: 0.1302Epoch 10/15: [====                          ] 10/63 batches, loss: 0.1371Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.1393Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.1447Epoch 10/15: [======                        ] 13/63 batches, loss: 0.1462Epoch 10/15: [======                        ] 14/63 batches, loss: 0.1486Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.1457Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.1460Epoch 10/15: [========                      ] 17/63 batches, loss: 0.1395Epoch 10/15: [========                      ] 18/63 batches, loss: 0.1382Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.1396Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.1396Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.1417Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.1402Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.1392Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.1427Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.1429Epoch 10/15: [============                  ] 26/63 batches, loss: 0.1452Epoch 10/15: [============                  ] 27/63 batches, loss: 0.1469Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.1467Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.1461Epoch 10/15: [==============                ] 30/63 batches, loss: 0.1447Epoch 10/15: [==============                ] 31/63 batches, loss: 0.1438Epoch 10/15: [===============               ] 32/63 batches, loss: 0.1449Epoch 10/15: [===============               ] 33/63 batches, loss: 0.1465Epoch 10/15: [================              ] 34/63 batches, loss: 0.1477Epoch 10/15: [================              ] 35/63 batches, loss: 0.1496Epoch 10/15: [=================             ] 36/63 batches, loss: 0.1515Epoch 10/15: [=================             ] 37/63 batches, loss: 0.1516Epoch 10/15: [==================            ] 38/63 batches, loss: 0.1490Epoch 10/15: [==================            ] 39/63 batches, loss: 0.1504Epoch 10/15: [===================           ] 40/63 batches, loss: 0.1503Epoch 10/15: [===================           ] 41/63 batches, loss: 0.1492Epoch 10/15: [====================          ] 42/63 batches, loss: 0.1480Epoch 10/15: [====================          ] 43/63 batches, loss: 0.1483Epoch 10/15: [====================          ] 44/63 batches, loss: 0.1478Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.1494Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.1493Epoch 10/15: [======================        ] 47/63 batches, loss: 0.1500Epoch 10/15: [======================        ] 48/63 batches, loss: 0.1512Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.1511Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.1509Epoch 10/15: [========================      ] 51/63 batches, loss: 0.1521Epoch 10/15: [========================      ] 52/63 batches, loss: 0.1521Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.1529Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.1559Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.1559Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.1558Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.1547Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.1539Epoch 10/15: [============================  ] 59/63 batches, loss: 0.1544Epoch 10/15: [============================  ] 60/63 batches, loss: 0.1547Epoch 10/15: [============================= ] 61/63 batches, loss: 0.1563Epoch 10/15: [============================= ] 62/63 batches, loss: 0.1556Epoch 10/15: [==============================] 63/63 batches, loss: 0.1548
[2025-05-02 12:52:25,365][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.1548
[2025-05-02 12:52:25,590][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.1448, Metrics: {'mse': 0.14459051191806793, 'rmse': 0.38025059095032043, 'r2': -1.2286205291748047}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.2217Epoch 11/15: [                              ] 2/63 batches, loss: 0.2127Epoch 11/15: [=                             ] 3/63 batches, loss: 0.1832Epoch 11/15: [=                             ] 4/63 batches, loss: 0.1623Epoch 11/15: [==                            ] 5/63 batches, loss: 0.1546Epoch 11/15: [==                            ] 6/63 batches, loss: 0.1462Epoch 11/15: [===                           ] 7/63 batches, loss: 0.1408Epoch 11/15: [===                           ] 8/63 batches, loss: 0.1429Epoch 11/15: [====                          ] 9/63 batches, loss: 0.1533Epoch 11/15: [====                          ] 10/63 batches, loss: 0.1525Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.1479Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.1489Epoch 11/15: [======                        ] 13/63 batches, loss: 0.1608Epoch 11/15: [======                        ] 14/63 batches, loss: 0.1645Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.1594Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.1575Epoch 11/15: [========                      ] 17/63 batches, loss: 0.1575Epoch 11/15: [========                      ] 18/63 batches, loss: 0.1578Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.1562Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.1582Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.1548Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.1545Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.1549Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.1543Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.1519Epoch 11/15: [============                  ] 26/63 batches, loss: 0.1488Epoch 11/15: [============                  ] 27/63 batches, loss: 0.1505Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.1497Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.1494Epoch 11/15: [==============                ] 30/63 batches, loss: 0.1494Epoch 11/15: [==============                ] 31/63 batches, loss: 0.1516Epoch 11/15: [===============               ] 32/63 batches, loss: 0.1542Epoch 11/15: [===============               ] 33/63 batches, loss: 0.1554Epoch 11/15: [================              ] 34/63 batches, loss: 0.1538Epoch 11/15: [================              ] 35/63 batches, loss: 0.1548Epoch 11/15: [=================             ] 36/63 batches, loss: 0.1548Epoch 11/15: [=================             ] 37/63 batches, loss: 0.1550Epoch 11/15: [==================            ] 38/63 batches, loss: 0.1547Epoch 11/15: [==================            ] 39/63 batches, loss: 0.1567Epoch 11/15: [===================           ] 40/63 batches, loss: 0.1580Epoch 11/15: [===================           ] 41/63 batches, loss: 0.1600Epoch 11/15: [====================          ] 42/63 batches, loss: 0.1600Epoch 11/15: [====================          ] 43/63 batches, loss: 0.1591Epoch 11/15: [====================          ] 44/63 batches, loss: 0.1576Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.1571Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.1564Epoch 11/15: [======================        ] 47/63 batches, loss: 0.1567Epoch 11/15: [======================        ] 48/63 batches, loss: 0.1560Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.1551Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.1559Epoch 11/15: [========================      ] 51/63 batches, loss: 0.1549Epoch 11/15: [========================      ] 52/63 batches, loss: 0.1540Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.1550Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.1535Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.1523Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.1515Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.1506Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.1538Epoch 11/15: [============================  ] 59/63 batches, loss: 0.1535Epoch 11/15: [============================  ] 60/63 batches, loss: 0.1525Epoch 11/15: [============================= ] 61/63 batches, loss: 0.1534Epoch 11/15: [============================= ] 62/63 batches, loss: 0.1523Epoch 11/15: [==============================] 63/63 batches, loss: 0.1526
[2025-05-02 12:52:27,911][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.1526
[2025-05-02 12:52:28,120][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.1454, Metrics: {'mse': 0.14498217403888702, 'rmse': 0.3807652479401016, 'r2': -1.2346575260162354}
[2025-05-02 12:52:28,121][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.0837Epoch 12/15: [                              ] 2/63 batches, loss: 0.1217Epoch 12/15: [=                             ] 3/63 batches, loss: 0.1162Epoch 12/15: [=                             ] 4/63 batches, loss: 0.1233Epoch 12/15: [==                            ] 5/63 batches, loss: 0.1372Epoch 12/15: [==                            ] 6/63 batches, loss: 0.1353Epoch 12/15: [===                           ] 7/63 batches, loss: 0.1411Epoch 12/15: [===                           ] 8/63 batches, loss: 0.1368Epoch 12/15: [====                          ] 9/63 batches, loss: 0.1654Epoch 12/15: [====                          ] 10/63 batches, loss: 0.1656Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.1615Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.1645Epoch 12/15: [======                        ] 13/63 batches, loss: 0.1572Epoch 12/15: [======                        ] 14/63 batches, loss: 0.1567Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.1567Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.1565Epoch 12/15: [========                      ] 17/63 batches, loss: 0.1538Epoch 12/15: [========                      ] 18/63 batches, loss: 0.1494Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.1469Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.1427Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.1448Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.1465Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.1469Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.1462Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.1442Epoch 12/15: [============                  ] 26/63 batches, loss: 0.1425Epoch 12/15: [============                  ] 27/63 batches, loss: 0.1394Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.1363Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.1362Epoch 12/15: [==============                ] 30/63 batches, loss: 0.1360Epoch 12/15: [==============                ] 31/63 batches, loss: 0.1333Epoch 12/15: [===============               ] 32/63 batches, loss: 0.1345Epoch 12/15: [===============               ] 33/63 batches, loss: 0.1365Epoch 12/15: [================              ] 34/63 batches, loss: 0.1362Epoch 12/15: [================              ] 35/63 batches, loss: 0.1361Epoch 12/15: [=================             ] 36/63 batches, loss: 0.1346Epoch 12/15: [=================             ] 37/63 batches, loss: 0.1351Epoch 12/15: [==================            ] 38/63 batches, loss: 0.1331Epoch 12/15: [==================            ] 39/63 batches, loss: 0.1335Epoch 12/15: [===================           ] 40/63 batches, loss: 0.1334Epoch 12/15: [===================           ] 41/63 batches, loss: 0.1336Epoch 12/15: [====================          ] 42/63 batches, loss: 0.1331Epoch 12/15: [====================          ] 43/63 batches, loss: 0.1331Epoch 12/15: [====================          ] 44/63 batches, loss: 0.1327Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.1334Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.1321Epoch 12/15: [======================        ] 47/63 batches, loss: 0.1320Epoch 12/15: [======================        ] 48/63 batches, loss: 0.1309Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.1307Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.1315Epoch 12/15: [========================      ] 51/63 batches, loss: 0.1321Epoch 12/15: [========================      ] 52/63 batches, loss: 0.1315Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.1312Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.1306Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.1295Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.1306Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.1313Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.1316Epoch 12/15: [============================  ] 59/63 batches, loss: 0.1340Epoch 12/15: [============================  ] 60/63 batches, loss: 0.1335Epoch 12/15: [============================= ] 61/63 batches, loss: 0.1335Epoch 12/15: [============================= ] 62/63 batches, loss: 0.1326Epoch 12/15: [==============================] 63/63 batches, loss: 0.1342
[2025-05-02 12:52:30,068][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.1342
[2025-05-02 12:52:30,287][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.1403, Metrics: {'mse': 0.13980251550674438, 'rmse': 0.37390174579258706, 'r2': -1.1548218727111816}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.0625Epoch 13/15: [                              ] 2/63 batches, loss: 0.0690Epoch 13/15: [=                             ] 3/63 batches, loss: 0.0773Epoch 13/15: [=                             ] 4/63 batches, loss: 0.0827Epoch 13/15: [==                            ] 5/63 batches, loss: 0.0794Epoch 13/15: [==                            ] 6/63 batches, loss: 0.0858Epoch 13/15: [===                           ] 7/63 batches, loss: 0.0893Epoch 13/15: [===                           ] 8/63 batches, loss: 0.0934Epoch 13/15: [====                          ] 9/63 batches, loss: 0.0952Epoch 13/15: [====                          ] 10/63 batches, loss: 0.0928Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.0995Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.1021Epoch 13/15: [======                        ] 13/63 batches, loss: 0.1025Epoch 13/15: [======                        ] 14/63 batches, loss: 0.1105Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.1142Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.1192Epoch 13/15: [========                      ] 17/63 batches, loss: 0.1227Epoch 13/15: [========                      ] 18/63 batches, loss: 0.1226Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.1244Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.1261Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.1315Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.1309Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.1294Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.1282Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.1299Epoch 13/15: [============                  ] 26/63 batches, loss: 0.1295Epoch 13/15: [============                  ] 27/63 batches, loss: 0.1328Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.1321Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.1347Epoch 13/15: [==============                ] 30/63 batches, loss: 0.1336Epoch 13/15: [==============                ] 31/63 batches, loss: 0.1316Epoch 13/15: [===============               ] 32/63 batches, loss: 0.1323Epoch 13/15: [===============               ] 33/63 batches, loss: 0.1325Epoch 13/15: [================              ] 34/63 batches, loss: 0.1322Epoch 13/15: [================              ] 35/63 batches, loss: 0.1313Epoch 13/15: [=================             ] 36/63 batches, loss: 0.1290Epoch 13/15: [=================             ] 37/63 batches, loss: 0.1285Epoch 13/15: [==================            ] 38/63 batches, loss: 0.1280Epoch 13/15: [==================            ] 39/63 batches, loss: 0.1275Epoch 13/15: [===================           ] 40/63 batches, loss: 0.1291Epoch 13/15: [===================           ] 41/63 batches, loss: 0.1287Epoch 13/15: [====================          ] 42/63 batches, loss: 0.1295Epoch 13/15: [====================          ] 43/63 batches, loss: 0.1292Epoch 13/15: [====================          ] 44/63 batches, loss: 0.1278Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.1290Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.1287Epoch 13/15: [======================        ] 47/63 batches, loss: 0.1300Epoch 13/15: [======================        ] 48/63 batches, loss: 0.1304Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.1305Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.1295Epoch 13/15: [========================      ] 51/63 batches, loss: 0.1292Epoch 13/15: [========================      ] 52/63 batches, loss: 0.1313Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.1321Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.1316Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.1311Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.1314Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.1310Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.1306Epoch 13/15: [============================  ] 59/63 batches, loss: 0.1315Epoch 13/15: [============================  ] 60/63 batches, loss: 0.1312Epoch 13/15: [============================= ] 61/63 batches, loss: 0.1319Epoch 13/15: [============================= ] 62/63 batches, loss: 0.1313Epoch 13/15: [==============================] 63/63 batches, loss: 0.1292
[2025-05-02 12:52:32,622][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.1292
[2025-05-02 12:52:32,839][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.1258, Metrics: {'mse': 0.12534892559051514, 'rmse': 0.3540465020170587, 'r2': -0.9320437908172607}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.1613Epoch 14/15: [                              ] 2/63 batches, loss: 0.1671Epoch 14/15: [=                             ] 3/63 batches, loss: 0.1371Epoch 14/15: [=                             ] 4/63 batches, loss: 0.1437Epoch 14/15: [==                            ] 5/63 batches, loss: 0.1620Epoch 14/15: [==                            ] 6/63 batches, loss: 0.1522Epoch 14/15: [===                           ] 7/63 batches, loss: 0.1445Epoch 14/15: [===                           ] 8/63 batches, loss: 0.1421Epoch 14/15: [====                          ] 9/63 batches, loss: 0.1386Epoch 14/15: [====                          ] 10/63 batches, loss: 0.1328Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.1281Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.1294Epoch 14/15: [======                        ] 13/63 batches, loss: 0.1309Epoch 14/15: [======                        ] 14/63 batches, loss: 0.1312Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.1354Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.1366Epoch 14/15: [========                      ] 17/63 batches, loss: 0.1353Epoch 14/15: [========                      ] 18/63 batches, loss: 0.1355Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.1322Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.1414Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.1393Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.1388Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.1394Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.1363Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.1386Epoch 14/15: [============                  ] 26/63 batches, loss: 0.1377Epoch 14/15: [============                  ] 27/63 batches, loss: 0.1343Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.1336Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.1334Epoch 14/15: [==============                ] 30/63 batches, loss: 0.1331Epoch 14/15: [==============                ] 31/63 batches, loss: 0.1331Epoch 14/15: [===============               ] 32/63 batches, loss: 0.1332Epoch 14/15: [===============               ] 33/63 batches, loss: 0.1354Epoch 14/15: [================              ] 34/63 batches, loss: 0.1327Epoch 14/15: [================              ] 35/63 batches, loss: 0.1318Epoch 14/15: [=================             ] 36/63 batches, loss: 0.1308Epoch 14/15: [=================             ] 37/63 batches, loss: 0.1299Epoch 14/15: [==================            ] 38/63 batches, loss: 0.1293Epoch 14/15: [==================            ] 39/63 batches, loss: 0.1322Epoch 14/15: [===================           ] 40/63 batches, loss: 0.1338Epoch 14/15: [===================           ] 41/63 batches, loss: 0.1330Epoch 14/15: [====================          ] 42/63 batches, loss: 0.1342Epoch 14/15: [====================          ] 43/63 batches, loss: 0.1335Epoch 14/15: [====================          ] 44/63 batches, loss: 0.1331Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.1345Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.1350Epoch 14/15: [======================        ] 47/63 batches, loss: 0.1344Epoch 14/15: [======================        ] 48/63 batches, loss: 0.1352Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.1362Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.1357Epoch 14/15: [========================      ] 51/63 batches, loss: 0.1351Epoch 14/15: [========================      ] 52/63 batches, loss: 0.1340Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.1352Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.1346Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.1346Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.1341Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.1354Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.1348Epoch 14/15: [============================  ] 59/63 batches, loss: 0.1349Epoch 14/15: [============================  ] 60/63 batches, loss: 0.1335Epoch 14/15: [============================= ] 61/63 batches, loss: 0.1340Epoch 14/15: [============================= ] 62/63 batches, loss: 0.1328Epoch 14/15: [==============================] 63/63 batches, loss: 0.1341
[2025-05-02 12:52:35,174][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.1341
[2025-05-02 12:52:35,372][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.1204, Metrics: {'mse': 0.1198895275592804, 'rmse': 0.3462506715650966, 'r2': -0.8478963375091553}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.1019Epoch 15/15: [                              ] 2/63 batches, loss: 0.1009Epoch 15/15: [=                             ] 3/63 batches, loss: 0.0912Epoch 15/15: [=                             ] 4/63 batches, loss: 0.0883Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0978Epoch 15/15: [==                            ] 6/63 batches, loss: 0.1106Epoch 15/15: [===                           ] 7/63 batches, loss: 0.1062Epoch 15/15: [===                           ] 8/63 batches, loss: 0.1267Epoch 15/15: [====                          ] 9/63 batches, loss: 0.1330Epoch 15/15: [====                          ] 10/63 batches, loss: 0.1282Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.1250Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.1238Epoch 15/15: [======                        ] 13/63 batches, loss: 0.1272Epoch 15/15: [======                        ] 14/63 batches, loss: 0.1246Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.1201Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.1197Epoch 15/15: [========                      ] 17/63 batches, loss: 0.1194Epoch 15/15: [========                      ] 18/63 batches, loss: 0.1208Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.1204Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.1254Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.1234Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.1214Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.1207Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.1172Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.1151Epoch 15/15: [============                  ] 26/63 batches, loss: 0.1154Epoch 15/15: [============                  ] 27/63 batches, loss: 0.1153Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.1135Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.1143Epoch 15/15: [==============                ] 30/63 batches, loss: 0.1158Epoch 15/15: [==============                ] 31/63 batches, loss: 0.1164Epoch 15/15: [===============               ] 32/63 batches, loss: 0.1151Epoch 15/15: [===============               ] 33/63 batches, loss: 0.1141Epoch 15/15: [================              ] 34/63 batches, loss: 0.1145Epoch 15/15: [================              ] 35/63 batches, loss: 0.1146Epoch 15/15: [=================             ] 36/63 batches, loss: 0.1143Epoch 15/15: [=================             ] 37/63 batches, loss: 0.1155Epoch 15/15: [==================            ] 38/63 batches, loss: 0.1136Epoch 15/15: [==================            ] 39/63 batches, loss: 0.1134Epoch 15/15: [===================           ] 40/63 batches, loss: 0.1128Epoch 15/15: [===================           ] 41/63 batches, loss: 0.1122Epoch 15/15: [====================          ] 42/63 batches, loss: 0.1124Epoch 15/15: [====================          ] 43/63 batches, loss: 0.1123Epoch 15/15: [====================          ] 44/63 batches, loss: 0.1125Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.1119Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.1117Epoch 15/15: [======================        ] 47/63 batches, loss: 0.1127Epoch 15/15: [======================        ] 48/63 batches, loss: 0.1112Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.1110Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.1112Epoch 15/15: [========================      ] 51/63 batches, loss: 0.1113Epoch 15/15: [========================      ] 52/63 batches, loss: 0.1106Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.1095Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.1082Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.1081Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.1077Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.1078Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.1091Epoch 15/15: [============================  ] 59/63 batches, loss: 0.1079Epoch 15/15: [============================  ] 60/63 batches, loss: 0.1086Epoch 15/15: [============================= ] 61/63 batches, loss: 0.1098Epoch 15/15: [============================= ] 62/63 batches, loss: 0.1094Epoch 15/15: [==============================] 63/63 batches, loss: 0.1089
[2025-05-02 12:52:37,757][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.1089
[2025-05-02 12:52:37,985][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.1290, Metrics: {'mse': 0.12818843126296997, 'rmse': 0.35803412024969067, 'r2': -0.975810170173645}
[2025-05-02 12:52:37,986][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-02 12:52:37,986][src.training.lm_trainer][INFO] - Training completed in 37.02 seconds
[2025-05-02 12:52:37,986][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 12:52:40,490][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.05424650385975838, 'rmse': 0.23290878871300322, 'r2': -0.7671389579772949}
[2025-05-02 12:52:40,491][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.1198895275592804, 'rmse': 0.3462506715650966, 'r2': -0.8478963375091553}
[2025-05-02 12:52:40,491][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.10579752177000046, 'rmse': 0.32526530981646423, 'r2': -0.8239021301269531}
[2025-05-02 12:52:42,161][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer9/ar/ar/model.pt
[2025-05-02 12:52:42,162][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▃▂▂▂▂▁▁▁
wandb:     best_val_mse █▃▃▂▂▂▂▁▁▁
wandb:      best_val_r2 ▁▆▆▇▇▇▇███
wandb:    best_val_rmse █▃▃▃▂▂▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▆▆▆▇▇▇▇▇▇▇▇▇▇
wandb:       train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▃▃▃▂▂▂▂▂▂▁▁▁▁
wandb:          val_mse █▃▃▃▃▂▂▂▂▂▂▁▁▁▁
wandb:           val_r2 ▁▆▆▆▆▇▇▇▇▇▇████
wandb:         val_rmse █▃▄▄▃▃▂▂▂▂▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.12045
wandb:     best_val_mse 0.11989
wandb:      best_val_r2 -0.8479
wandb:    best_val_rmse 0.34625
wandb:            epoch 15
wandb:   final_test_mse 0.1058
wandb:    final_test_r2 -0.8239
wandb:  final_test_rmse 0.32527
wandb:  final_train_mse 0.05425
wandb:   final_train_r2 -0.76714
wandb: final_train_rmse 0.23291
wandb:    final_val_mse 0.11989
wandb:     final_val_r2 -0.8479
wandb:   final_val_rmse 0.34625
wandb:    learning_rate 2e-05
wandb:       train_loss 0.10893
wandb:       train_time 37.01717
wandb:         val_loss 0.12897
wandb:          val_mse 0.12819
wandb:           val_r2 -0.97581
wandb:         val_rmse 0.35803
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125151-nrxmn5u3
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125151-nrxmn5u3/logs
Experiment probe_layer9_complexity_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer9/ar/ar/results.json for layer 9
=======================
PROBING LAYER 11
=======================
Running experiment: probe_layer11_question_type_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=question_type"         "experiment.tasks=question_type"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=11"         "model.probe_hidden_size=128" "model.probe_depth=2" "model.dropout=0.2" "model.activation=gelu" "model.normalization=layer" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=classification"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=1e-4" "training.patience=3" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer11_question_type_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/question_type/layer11/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 12:52:53,156][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/question_type/layer11/ar
experiment_name: probe_layer11_question_type_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.2
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
  probe_hidden_size: 128
  probe_depth: 2
  activation: gelu
  normalization: layer
  weight_init: xavier
  output_standardization: false
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: classification
  batch_size: 16
  num_epochs: 15
  lr: 0.0001
  weight_decay: 0.01
  patience: 3
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: question_type
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false

[2025-05-02 12:52:53,156][__main__][INFO] - Normalized task: question_type
[2025-05-02 12:52:53,156][__main__][INFO] - Using explicit task_type from config: classification
[2025-05-02 12:52:53,156][__main__][INFO] - Determined Task Type: classification
[2025-05-02 12:52:53,160][__main__][INFO] - Running LM experiment for task 'question_type' (type: classification) on languages: ['ar']
[2025-05-02 12:52:53,161][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 12:52:54,992][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'question_type', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 12:52:57,230][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 12:52:57,231][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:52:57,271][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:52:57,295][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:52:57,378][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 12:52:57,386][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:52:57,386][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 12:52:57,387][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:52:57,406][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:52:57,440][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:52:57,454][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 12:52:57,455][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:52:57,455][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 12:52:57,456][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:52:57,471][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:52:57,498][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:52:57,511][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 12:52:57,512][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:52:57,512][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 12:52:57,513][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 12:52:57,514][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 12:52:57,514][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 12:52:57,514][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 12:52:57,514][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 12:52:57,514][src.data.datasets][INFO] -   Label 0: 498 examples (50.1%)
[2025-05-02 12:52:57,514][src.data.datasets][INFO] -   Label 1: 497 examples (49.9%)
[2025-05-02 12:52:57,514][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 12:52:57,514][src.data.datasets][INFO] - Sample label: 1
[2025-05-02 12:52:57,515][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 12:52:57,515][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 12:52:57,515][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 12:52:57,515][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 12:52:57,515][src.data.datasets][INFO] -   Label 0: 24 examples (54.5%)
[2025-05-02 12:52:57,515][src.data.datasets][INFO] -   Label 1: 20 examples (45.5%)
[2025-05-02 12:52:57,515][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 12:52:57,515][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 12:52:57,515][src.data.datasets][INFO] - Task 'question_type' is classification: True
[2025-05-02 12:52:57,515][src.data.datasets][INFO] - Getting feature name for task: 'question_type', submetric: 'None'
[2025-05-02 12:52:57,515][src.data.datasets][INFO] - Selected feature name: 'question_type' for task: 'question_type'
[2025-05-02 12:52:57,516][src.data.datasets][INFO] - Label statistics for question_type (feature: question_type):
[2025-05-02 12:52:57,516][src.data.datasets][INFO] -   Label 0: 55 examples (71.4%)
[2025-05-02 12:52:57,516][src.data.datasets][INFO] -   Label 1: 22 examples (28.6%)
[2025-05-02 12:52:57,516][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 12:52:57,516][src.data.datasets][INFO] - Sample label: 0
[2025-05-02 12:52:57,516][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 12:52:57,516][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 12:52:57,516][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 12:52:57,517][__main__][INFO] - Using model type: lm_probe for question_type
[2025-05-02 12:52:57,517][src.models.model_factory][INFO] - Creating lm_probe model for classification task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 12:53:01,338][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 12:53:01,339][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 12:53:01,339][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=11, freeze_model=True
[2025-05-02 12:53:01,339][src.models.model_factory][INFO] - Using provided probe_hidden_size: 128
[2025-05-02 12:53:01,342][src.models.model_factory][INFO] - Model has 116,865 trainable parameters out of 394,238,337 total parameters
[2025-05-02 12:53:01,342][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 116,865 trainable parameters
[2025-05-02 12:53:01,342][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=128, depth=2, activation=gelu, normalization=layer
[2025-05-02 12:53:01,342][src.models.model_factory][INFO] - Created specialized classification probe with 2 layers, 128 hidden size
[2025-05-02 12:53:01,343][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 12:53:01,343][__main__][INFO] - Total parameters: 394,238,337
[2025-05-02 12:53:01,343][__main__][INFO] - Trainable parameters: 116,865 (0.03%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 0.7614Epoch 1/15: [                              ] 2/63 batches, loss: 0.7288Epoch 1/15: [=                             ] 3/63 batches, loss: 0.7227Epoch 1/15: [=                             ] 4/63 batches, loss: 0.7239Epoch 1/15: [==                            ] 5/63 batches, loss: 0.7250Epoch 1/15: [==                            ] 6/63 batches, loss: 0.7147Epoch 1/15: [===                           ] 7/63 batches, loss: 0.7054Epoch 1/15: [===                           ] 8/63 batches, loss: 0.7106Epoch 1/15: [====                          ] 9/63 batches, loss: 0.7060Epoch 1/15: [====                          ] 10/63 batches, loss: 0.7024Epoch 1/15: [=====                         ] 11/63 batches, loss: 0.7035Epoch 1/15: [=====                         ] 12/63 batches, loss: 0.7028Epoch 1/15: [======                        ] 13/63 batches, loss: 0.7022Epoch 1/15: [======                        ] 14/63 batches, loss: 0.7001Epoch 1/15: [=======                       ] 15/63 batches, loss: 0.7001Epoch 1/15: [=======                       ] 16/63 batches, loss: 0.6992Epoch 1/15: [========                      ] 17/63 batches, loss: 0.6986Epoch 1/15: [========                      ] 18/63 batches, loss: 0.6994Epoch 1/15: [=========                     ] 19/63 batches, loss: 0.6980Epoch 1/15: [=========                     ] 20/63 batches, loss: 0.6976Epoch 1/15: [==========                    ] 21/63 batches, loss: 0.6973Epoch 1/15: [==========                    ] 22/63 batches, loss: 0.6979Epoch 1/15: [==========                    ] 23/63 batches, loss: 0.6973Epoch 1/15: [===========                   ] 24/63 batches, loss: 0.6975Epoch 1/15: [===========                   ] 25/63 batches, loss: 0.6985Epoch 1/15: [============                  ] 26/63 batches, loss: 0.6956Epoch 1/15: [============                  ] 27/63 batches, loss: 0.6956Epoch 1/15: [=============                 ] 28/63 batches, loss: 0.6961Epoch 1/15: [=============                 ] 29/63 batches, loss: 0.6953Epoch 1/15: [==============                ] 30/63 batches, loss: 0.6969Epoch 1/15: [==============                ] 31/63 batches, loss: 0.6971Epoch 1/15: [===============               ] 32/63 batches, loss: 0.6970Epoch 1/15: [===============               ] 33/63 batches, loss: 0.6965Epoch 1/15: [================              ] 34/63 batches, loss: 0.6954Epoch 1/15: [================              ] 35/63 batches, loss: 0.6945Epoch 1/15: [=================             ] 36/63 batches, loss: 0.6943Epoch 1/15: [=================             ] 37/63 batches, loss: 0.6939Epoch 1/15: [==================            ] 38/63 batches, loss: 0.6934Epoch 1/15: [==================            ] 39/63 batches, loss: 0.6933Epoch 1/15: [===================           ] 40/63 batches, loss: 0.6926Epoch 1/15: [===================           ] 41/63 batches, loss: 0.6929Epoch 1/15: [====================          ] 42/63 batches, loss: 0.6926Epoch 1/15: [====================          ] 43/63 batches, loss: 0.6930Epoch 1/15: [====================          ] 44/63 batches, loss: 0.6928Epoch 1/15: [=====================         ] 45/63 batches, loss: 0.6918Epoch 1/15: [=====================         ] 46/63 batches, loss: 0.6917Epoch 1/15: [======================        ] 47/63 batches, loss: 0.6921Epoch 1/15: [======================        ] 48/63 batches, loss: 0.6909Epoch 1/15: [=======================       ] 49/63 batches, loss: 0.6908Epoch 1/15: [=======================       ] 50/63 batches, loss: 0.6902Epoch 1/15: [========================      ] 51/63 batches, loss: 0.6894Epoch 1/15: [========================      ] 52/63 batches, loss: 0.6890Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.6894Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.6898Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.6894Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.6894Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.6893Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.6893Epoch 1/15: [============================  ] 59/63 batches, loss: 0.6888Epoch 1/15: [============================  ] 60/63 batches, loss: 0.6886Epoch 1/15: [============================= ] 61/63 batches, loss: 0.6888Epoch 1/15: [============================= ] 62/63 batches, loss: 0.6886Epoch 1/15: [==============================] 63/63 batches, loss: 0.6859
[2025-05-02 12:53:05,442][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.6859
[2025-05-02 12:53:05,626][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.6953, Metrics: {'accuracy': 0.5454545454545454, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.6951Epoch 2/15: [                              ] 2/63 batches, loss: 0.6886Epoch 2/15: [=                             ] 3/63 batches, loss: 0.6866Epoch 2/15: [=                             ] 4/63 batches, loss: 0.6889Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6801Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6801Epoch 2/15: [===                           ] 7/63 batches, loss: 0.6709Epoch 2/15: [===                           ] 8/63 batches, loss: 0.6668Epoch 2/15: [====                          ] 9/63 batches, loss: 0.6689Epoch 2/15: [====                          ] 10/63 batches, loss: 0.6708Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.6670Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.6703Epoch 2/15: [======                        ] 13/63 batches, loss: 0.6733Epoch 2/15: [======                        ] 14/63 batches, loss: 0.6770Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.6790Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.6807Epoch 2/15: [========                      ] 17/63 batches, loss: 0.6812Epoch 2/15: [========                      ] 18/63 batches, loss: 0.6796Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.6797Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.6807Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.6792Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.6770Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.6754Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.6729Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.6752Epoch 2/15: [============                  ] 26/63 batches, loss: 0.6784Epoch 2/15: [============                  ] 27/63 batches, loss: 0.6784Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.6785Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.6768Epoch 2/15: [==============                ] 30/63 batches, loss: 0.6774Epoch 2/15: [==============                ] 31/63 batches, loss: 0.6767Epoch 2/15: [===============               ] 32/63 batches, loss: 0.6754Epoch 2/15: [===============               ] 33/63 batches, loss: 0.6743Epoch 2/15: [================              ] 34/63 batches, loss: 0.6739Epoch 2/15: [================              ] 35/63 batches, loss: 0.6744Epoch 2/15: [=================             ] 36/63 batches, loss: 0.6749Epoch 2/15: [=================             ] 37/63 batches, loss: 0.6725Epoch 2/15: [==================            ] 38/63 batches, loss: 0.6720Epoch 2/15: [==================            ] 39/63 batches, loss: 0.6713Epoch 2/15: [===================           ] 40/63 batches, loss: 0.6715Epoch 2/15: [===================           ] 41/63 batches, loss: 0.6705Epoch 2/15: [====================          ] 42/63 batches, loss: 0.6705Epoch 2/15: [====================          ] 43/63 batches, loss: 0.6713Epoch 2/15: [====================          ] 44/63 batches, loss: 0.6718Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.6715Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.6717Epoch 2/15: [======================        ] 47/63 batches, loss: 0.6716Epoch 2/15: [======================        ] 48/63 batches, loss: 0.6725Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.6737Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.6749Epoch 2/15: [========================      ] 51/63 batches, loss: 0.6743Epoch 2/15: [========================      ] 52/63 batches, loss: 0.6749Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.6746Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.6734Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.6735Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.6732Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.6729Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.6714Epoch 2/15: [============================  ] 59/63 batches, loss: 0.6718Epoch 2/15: [============================  ] 60/63 batches, loss: 0.6711Epoch 2/15: [============================= ] 61/63 batches, loss: 0.6703Epoch 2/15: [============================= ] 62/63 batches, loss: 0.6706Epoch 2/15: [==============================] 63/63 batches, loss: 0.6703
[2025-05-02 12:53:07,949][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.6703
[2025-05-02 12:53:08,160][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.6756, Metrics: {'accuracy': 0.6136363636363636, 'f1': 0.45161290322580644, 'precision': 0.6363636363636364, 'recall': 0.35}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.6977Epoch 3/15: [                              ] 2/63 batches, loss: 0.6838Epoch 3/15: [=                             ] 3/63 batches, loss: 0.6784Epoch 3/15: [=                             ] 4/63 batches, loss: 0.6644Epoch 3/15: [==                            ] 5/63 batches, loss: 0.6469Epoch 3/15: [==                            ] 6/63 batches, loss: 0.6453Epoch 3/15: [===                           ] 7/63 batches, loss: 0.6485Epoch 3/15: [===                           ] 8/63 batches, loss: 0.6535Epoch 3/15: [====                          ] 9/63 batches, loss: 0.6517Epoch 3/15: [====                          ] 10/63 batches, loss: 0.6447Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.6408Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.6420Epoch 3/15: [======                        ] 13/63 batches, loss: 0.6433Epoch 3/15: [======                        ] 14/63 batches, loss: 0.6399Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.6417Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.6439Epoch 3/15: [========                      ] 17/63 batches, loss: 0.6474Epoch 3/15: [========                      ] 18/63 batches, loss: 0.6470Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.6428Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.6437Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.6423Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.6425Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.6417Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.6400Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.6418Epoch 3/15: [============                  ] 26/63 batches, loss: 0.6412Epoch 3/15: [============                  ] 27/63 batches, loss: 0.6437Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.6454Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.6449Epoch 3/15: [==============                ] 30/63 batches, loss: 0.6426Epoch 3/15: [==============                ] 31/63 batches, loss: 0.6440Epoch 3/15: [===============               ] 32/63 batches, loss: 0.6418Epoch 3/15: [===============               ] 33/63 batches, loss: 0.6426Epoch 3/15: [================              ] 34/63 batches, loss: 0.6443Epoch 3/15: [================              ] 35/63 batches, loss: 0.6456Epoch 3/15: [=================             ] 36/63 batches, loss: 0.6465Epoch 3/15: [=================             ] 37/63 batches, loss: 0.6462Epoch 3/15: [==================            ] 38/63 batches, loss: 0.6474Epoch 3/15: [==================            ] 39/63 batches, loss: 0.6476Epoch 3/15: [===================           ] 40/63 batches, loss: 0.6487Epoch 3/15: [===================           ] 41/63 batches, loss: 0.6494Epoch 3/15: [====================          ] 42/63 batches, loss: 0.6477Epoch 3/15: [====================          ] 43/63 batches, loss: 0.6471Epoch 3/15: [====================          ] 44/63 batches, loss: 0.6461Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.6461Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.6451Epoch 3/15: [======================        ] 47/63 batches, loss: 0.6441Epoch 3/15: [======================        ] 48/63 batches, loss: 0.6432Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.6433Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.6438Epoch 3/15: [========================      ] 51/63 batches, loss: 0.6440Epoch 3/15: [========================      ] 52/63 batches, loss: 0.6439Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.6436Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.6420Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.6417Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.6402Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.6401Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.6394Epoch 3/15: [============================  ] 59/63 batches, loss: 0.6390Epoch 3/15: [============================  ] 60/63 batches, loss: 0.6377Epoch 3/15: [============================= ] 61/63 batches, loss: 0.6367Epoch 3/15: [============================= ] 62/63 batches, loss: 0.6358Epoch 3/15: [==============================] 63/63 batches, loss: 0.6380
[2025-05-02 12:53:10,488][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.6380
[2025-05-02 12:53:10,701][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.6423, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8571428571428571, 'precision': 0.8181818181818182, 'recall': 0.9}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.5248Epoch 4/15: [                              ] 2/63 batches, loss: 0.5418Epoch 4/15: [=                             ] 3/63 batches, loss: 0.5614Epoch 4/15: [=                             ] 4/63 batches, loss: 0.5722Epoch 4/15: [==                            ] 5/63 batches, loss: 0.5836Epoch 4/15: [==                            ] 6/63 batches, loss: 0.5795Epoch 4/15: [===                           ] 7/63 batches, loss: 0.5916Epoch 4/15: [===                           ] 8/63 batches, loss: 0.6092Epoch 4/15: [====                          ] 9/63 batches, loss: 0.6181Epoch 4/15: [====                          ] 10/63 batches, loss: 0.6220Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.6227Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.6187Epoch 4/15: [======                        ] 13/63 batches, loss: 0.6208Epoch 4/15: [======                        ] 14/63 batches, loss: 0.6147Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.6154Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.6177Epoch 4/15: [========                      ] 17/63 batches, loss: 0.6157Epoch 4/15: [========                      ] 18/63 batches, loss: 0.6144Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.6124Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.6131Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.6124Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.6126Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.6141Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.6160Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.6143Epoch 4/15: [============                  ] 26/63 batches, loss: 0.6104Epoch 4/15: [============                  ] 27/63 batches, loss: 0.6092Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.6060Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.6044Epoch 4/15: [==============                ] 30/63 batches, loss: 0.6070Epoch 4/15: [==============                ] 31/63 batches, loss: 0.6072Epoch 4/15: [===============               ] 32/63 batches, loss: 0.6059Epoch 4/15: [===============               ] 33/63 batches, loss: 0.6036Epoch 4/15: [================              ] 34/63 batches, loss: 0.6019Epoch 4/15: [================              ] 35/63 batches, loss: 0.6021Epoch 4/15: [=================             ] 36/63 batches, loss: 0.6016Epoch 4/15: [=================             ] 37/63 batches, loss: 0.6007Epoch 4/15: [==================            ] 38/63 batches, loss: 0.5991Epoch 4/15: [==================            ] 39/63 batches, loss: 0.5994Epoch 4/15: [===================           ] 40/63 batches, loss: 0.6001Epoch 4/15: [===================           ] 41/63 batches, loss: 0.6034Epoch 4/15: [====================          ] 42/63 batches, loss: 0.6027Epoch 4/15: [====================          ] 43/63 batches, loss: 0.6029Epoch 4/15: [====================          ] 44/63 batches, loss: 0.6030Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.6066Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.6058Epoch 4/15: [======================        ] 47/63 batches, loss: 0.6051Epoch 4/15: [======================        ] 48/63 batches, loss: 0.6059Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.6065Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.6070Epoch 4/15: [========================      ] 51/63 batches, loss: 0.6079Epoch 4/15: [========================      ] 52/63 batches, loss: 0.6084Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.6092Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.6105Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.6105Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.6092Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.6089Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.6101Epoch 4/15: [============================  ] 59/63 batches, loss: 0.6104Epoch 4/15: [============================  ] 60/63 batches, loss: 0.6096Epoch 4/15: [============================= ] 61/63 batches, loss: 0.6097Epoch 4/15: [============================= ] 62/63 batches, loss: 0.6102Epoch 4/15: [==============================] 63/63 batches, loss: 0.6077
[2025-05-02 12:53:12,982][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.6077
[2025-05-02 12:53:13,191][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.6212, Metrics: {'accuracy': 0.8181818181818182, 'f1': 0.8, 'precision': 0.8, 'recall': 0.8}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.5585Epoch 5/15: [                              ] 2/63 batches, loss: 0.5499Epoch 5/15: [=                             ] 3/63 batches, loss: 0.5425Epoch 5/15: [=                             ] 4/63 batches, loss: 0.5812Epoch 5/15: [==                            ] 5/63 batches, loss: 0.5932Epoch 5/15: [==                            ] 6/63 batches, loss: 0.5864Epoch 5/15: [===                           ] 7/63 batches, loss: 0.5847Epoch 5/15: [===                           ] 8/63 batches, loss: 0.5871Epoch 5/15: [====                          ] 9/63 batches, loss: 0.5826Epoch 5/15: [====                          ] 10/63 batches, loss: 0.5868Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.5908Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.5922Epoch 5/15: [======                        ] 13/63 batches, loss: 0.5904Epoch 5/15: [======                        ] 14/63 batches, loss: 0.5923Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.5894Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.5889Epoch 5/15: [========                      ] 17/63 batches, loss: 0.5884Epoch 5/15: [========                      ] 18/63 batches, loss: 0.5867Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.5890Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.5866Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.5908Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.5928Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.5963Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.5934Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.5937Epoch 5/15: [============                  ] 26/63 batches, loss: 0.5939Epoch 5/15: [============                  ] 27/63 batches, loss: 0.5950Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.6021Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.6034Epoch 5/15: [==============                ] 30/63 batches, loss: 0.6017Epoch 5/15: [==============                ] 31/63 batches, loss: 0.6022Epoch 5/15: [===============               ] 32/63 batches, loss: 0.6013Epoch 5/15: [===============               ] 33/63 batches, loss: 0.5999Epoch 5/15: [================              ] 34/63 batches, loss: 0.6017Epoch 5/15: [================              ] 35/63 batches, loss: 0.5982Epoch 5/15: [=================             ] 36/63 batches, loss: 0.5986Epoch 5/15: [=================             ] 37/63 batches, loss: 0.5983Epoch 5/15: [==================            ] 38/63 batches, loss: 0.6002Epoch 5/15: [==================            ] 39/63 batches, loss: 0.6002Epoch 5/15: [===================           ] 40/63 batches, loss: 0.5998Epoch 5/15: [===================           ] 41/63 batches, loss: 0.5984Epoch 5/15: [====================          ] 42/63 batches, loss: 0.5971Epoch 5/15: [====================          ] 43/63 batches, loss: 0.5962Epoch 5/15: [====================          ] 44/63 batches, loss: 0.5959Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.5969Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.5978Epoch 5/15: [======================        ] 47/63 batches, loss: 0.5956Epoch 5/15: [======================        ] 48/63 batches, loss: 0.5968Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.5972Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.5972Epoch 5/15: [========================      ] 51/63 batches, loss: 0.5975Epoch 5/15: [========================      ] 52/63 batches, loss: 0.5952Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.5942Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.5941Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.5940Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.5946Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.5932Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.5918Epoch 5/15: [============================  ] 59/63 batches, loss: 0.5918Epoch 5/15: [============================  ] 60/63 batches, loss: 0.5915Epoch 5/15: [============================= ] 61/63 batches, loss: 0.5902Epoch 5/15: [============================= ] 62/63 batches, loss: 0.5896Epoch 5/15: [==============================] 63/63 batches, loss: 0.5918
[2025-05-02 12:53:15,460][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.5918
[2025-05-02 12:53:15,677][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.5988, Metrics: {'accuracy': 0.8863636363636364, 'f1': 0.8837209302325582, 'precision': 0.8260869565217391, 'recall': 0.95}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.5340Epoch 6/15: [                              ] 2/63 batches, loss: 0.5791Epoch 6/15: [=                             ] 3/63 batches, loss: 0.5791Epoch 6/15: [=                             ] 4/63 batches, loss: 0.5819Epoch 6/15: [==                            ] 5/63 batches, loss: 0.5873Epoch 6/15: [==                            ] 6/63 batches, loss: 0.5692Epoch 6/15: [===                           ] 7/63 batches, loss: 0.5689Epoch 6/15: [===                           ] 8/63 batches, loss: 0.5696Epoch 6/15: [====                          ] 9/63 batches, loss: 0.5762Epoch 6/15: [====                          ] 10/63 batches, loss: 0.5718Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.5703Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.5638Epoch 6/15: [======                        ] 13/63 batches, loss: 0.5707Epoch 6/15: [======                        ] 14/63 batches, loss: 0.5685Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.5661Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.5667Epoch 6/15: [========                      ] 17/63 batches, loss: 0.5664Epoch 6/15: [========                      ] 18/63 batches, loss: 0.5691Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.5662Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.5672Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.5695Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.5725Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.5678Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.5662Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.5632Epoch 6/15: [============                  ] 26/63 batches, loss: 0.5622Epoch 6/15: [============                  ] 27/63 batches, loss: 0.5628Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.5637Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.5632Epoch 6/15: [==============                ] 30/63 batches, loss: 0.5618Epoch 6/15: [==============                ] 31/63 batches, loss: 0.5616Epoch 6/15: [===============               ] 32/63 batches, loss: 0.5637Epoch 6/15: [===============               ] 33/63 batches, loss: 0.5639Epoch 6/15: [================              ] 34/63 batches, loss: 0.5656Epoch 6/15: [================              ] 35/63 batches, loss: 0.5639Epoch 6/15: [=================             ] 36/63 batches, loss: 0.5615Epoch 6/15: [=================             ] 37/63 batches, loss: 0.5632Epoch 6/15: [==================            ] 38/63 batches, loss: 0.5646Epoch 6/15: [==================            ] 39/63 batches, loss: 0.5665Epoch 6/15: [===================           ] 40/63 batches, loss: 0.5683Epoch 6/15: [===================           ] 41/63 batches, loss: 0.5673Epoch 6/15: [====================          ] 42/63 batches, loss: 0.5637Epoch 6/15: [====================          ] 43/63 batches, loss: 0.5635Epoch 6/15: [====================          ] 44/63 batches, loss: 0.5623Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.5629Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.5647Epoch 6/15: [======================        ] 47/63 batches, loss: 0.5649Epoch 6/15: [======================        ] 48/63 batches, loss: 0.5656Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.5656Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.5658Epoch 6/15: [========================      ] 51/63 batches, loss: 0.5673Epoch 6/15: [========================      ] 52/63 batches, loss: 0.5666Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.5657Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.5666Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.5651Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.5659Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.5668Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.5692Epoch 6/15: [============================  ] 59/63 batches, loss: 0.5700Epoch 6/15: [============================  ] 60/63 batches, loss: 0.5711Epoch 6/15: [============================= ] 61/63 batches, loss: 0.5712Epoch 6/15: [============================= ] 62/63 batches, loss: 0.5709Epoch 6/15: [==============================] 63/63 batches, loss: 0.5709
[2025-05-02 12:53:17,974][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.5709
[2025-05-02 12:53:18,193][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.5932, Metrics: {'accuracy': 0.8636363636363636, 'f1': 0.8571428571428571, 'precision': 0.8181818181818182, 'recall': 0.9}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.5412Epoch 7/15: [                              ] 2/63 batches, loss: 0.5912Epoch 7/15: [=                             ] 3/63 batches, loss: 0.5683Epoch 7/15: [=                             ] 4/63 batches, loss: 0.5453Epoch 7/15: [==                            ] 5/63 batches, loss: 0.5558Epoch 7/15: [==                            ] 6/63 batches, loss: 0.5728Epoch 7/15: [===                           ] 7/63 batches, loss: 0.5714Epoch 7/15: [===                           ] 8/63 batches, loss: 0.5653Epoch 7/15: [====                          ] 9/63 batches, loss: 0.5636Epoch 7/15: [====                          ] 10/63 batches, loss: 0.5633Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.5606Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.5594Epoch 7/15: [======                        ] 13/63 batches, loss: 0.5528Epoch 7/15: [======                        ] 14/63 batches, loss: 0.5553Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.5577Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.5585Epoch 7/15: [========                      ] 17/63 batches, loss: 0.5589Epoch 7/15: [========                      ] 18/63 batches, loss: 0.5587Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.5590Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.5573Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.5534Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.5590Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.5606Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.5600Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.5584Epoch 7/15: [============                  ] 26/63 batches, loss: 0.5551Epoch 7/15: [============                  ] 27/63 batches, loss: 0.5553Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.5595Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.5579Epoch 7/15: [==============                ] 30/63 batches, loss: 0.5552Epoch 7/15: [==============                ] 31/63 batches, loss: 0.5549Epoch 7/15: [===============               ] 32/63 batches, loss: 0.5571Epoch 7/15: [===============               ] 33/63 batches, loss: 0.5571Epoch 7/15: [================              ] 34/63 batches, loss: 0.5581Epoch 7/15: [================              ] 35/63 batches, loss: 0.5574Epoch 7/15: [=================             ] 36/63 batches, loss: 0.5584Epoch 7/15: [=================             ] 37/63 batches, loss: 0.5584Epoch 7/15: [==================            ] 38/63 batches, loss: 0.5583Epoch 7/15: [==================            ] 39/63 batches, loss: 0.5571Epoch 7/15: [===================           ] 40/63 batches, loss: 0.5556Epoch 7/15: [===================           ] 41/63 batches, loss: 0.5554Epoch 7/15: [====================          ] 42/63 batches, loss: 0.5567Epoch 7/15: [====================          ] 43/63 batches, loss: 0.5566Epoch 7/15: [====================          ] 44/63 batches, loss: 0.5556Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.5548Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.5549Epoch 7/15: [======================        ] 47/63 batches, loss: 0.5554Epoch 7/15: [======================        ] 48/63 batches, loss: 0.5560Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.5571Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.5576Epoch 7/15: [========================      ] 51/63 batches, loss: 0.5575Epoch 7/15: [========================      ] 52/63 batches, loss: 0.5590Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.5576Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.5579Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.5573Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.5594Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.5596Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.5580Epoch 7/15: [============================  ] 59/63 batches, loss: 0.5593Epoch 7/15: [============================  ] 60/63 batches, loss: 0.5580Epoch 7/15: [============================= ] 61/63 batches, loss: 0.5582Epoch 7/15: [============================= ] 62/63 batches, loss: 0.5580Epoch 7/15: [==============================] 63/63 batches, loss: 0.5592
[2025-05-02 12:53:20,481][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.5592
[2025-05-02 12:53:20,675][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.5864, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.6689Epoch 8/15: [                              ] 2/63 batches, loss: 0.6002Epoch 8/15: [=                             ] 3/63 batches, loss: 0.5704Epoch 8/15: [=                             ] 4/63 batches, loss: 0.5604Epoch 8/15: [==                            ] 5/63 batches, loss: 0.5712Epoch 8/15: [==                            ] 6/63 batches, loss: 0.5693Epoch 8/15: [===                           ] 7/63 batches, loss: 0.5652Epoch 8/15: [===                           ] 8/63 batches, loss: 0.5554Epoch 8/15: [====                          ] 9/63 batches, loss: 0.5599Epoch 8/15: [====                          ] 10/63 batches, loss: 0.5636Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.5597Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.5607Epoch 8/15: [======                        ] 13/63 batches, loss: 0.5559Epoch 8/15: [======                        ] 14/63 batches, loss: 0.5561Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.5607Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.5593Epoch 8/15: [========                      ] 17/63 batches, loss: 0.5609Epoch 8/15: [========                      ] 18/63 batches, loss: 0.5583Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.5564Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.5519Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.5533Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.5505Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.5505Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.5497Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.5495Epoch 8/15: [============                  ] 26/63 batches, loss: 0.5499Epoch 8/15: [============                  ] 27/63 batches, loss: 0.5486Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.5483Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.5482Epoch 8/15: [==============                ] 30/63 batches, loss: 0.5481Epoch 8/15: [==============                ] 31/63 batches, loss: 0.5488Epoch 8/15: [===============               ] 32/63 batches, loss: 0.5477Epoch 8/15: [===============               ] 33/63 batches, loss: 0.5464Epoch 8/15: [================              ] 34/63 batches, loss: 0.5472Epoch 8/15: [================              ] 35/63 batches, loss: 0.5447Epoch 8/15: [=================             ] 36/63 batches, loss: 0.5454Epoch 8/15: [=================             ] 37/63 batches, loss: 0.5467Epoch 8/15: [==================            ] 38/63 batches, loss: 0.5472Epoch 8/15: [==================            ] 39/63 batches, loss: 0.5495Epoch 8/15: [===================           ] 40/63 batches, loss: 0.5472Epoch 8/15: [===================           ] 41/63 batches, loss: 0.5481Epoch 8/15: [====================          ] 42/63 batches, loss: 0.5478Epoch 8/15: [====================          ] 43/63 batches, loss: 0.5450Epoch 8/15: [====================          ] 44/63 batches, loss: 0.5461Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.5452Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.5425Epoch 8/15: [======================        ] 47/63 batches, loss: 0.5413Epoch 8/15: [======================        ] 48/63 batches, loss: 0.5418Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.5451Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.5461Epoch 8/15: [========================      ] 51/63 batches, loss: 0.5484Epoch 8/15: [========================      ] 52/63 batches, loss: 0.5482Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.5484Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.5488Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.5509Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.5517Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.5527Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.5528Epoch 8/15: [============================  ] 59/63 batches, loss: 0.5522Epoch 8/15: [============================  ] 60/63 batches, loss: 0.5525Epoch 8/15: [============================= ] 61/63 batches, loss: 0.5520Epoch 8/15: [============================= ] 62/63 batches, loss: 0.5513Epoch 8/15: [==============================] 63/63 batches, loss: 0.5498
[2025-05-02 12:53:23,042][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.5498
[2025-05-02 12:53:23,255][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.5835, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.4953Epoch 9/15: [                              ] 2/63 batches, loss: 0.5488Epoch 9/15: [=                             ] 3/63 batches, loss: 0.5404Epoch 9/15: [=                             ] 4/63 batches, loss: 0.5386Epoch 9/15: [==                            ] 5/63 batches, loss: 0.5512Epoch 9/15: [==                            ] 6/63 batches, loss: 0.5560Epoch 9/15: [===                           ] 7/63 batches, loss: 0.5626Epoch 9/15: [===                           ] 8/63 batches, loss: 0.5500Epoch 9/15: [====                          ] 9/63 batches, loss: 0.5571Epoch 9/15: [====                          ] 10/63 batches, loss: 0.5538Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.5540Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.5521Epoch 9/15: [======                        ] 13/63 batches, loss: 0.5600Epoch 9/15: [======                        ] 14/63 batches, loss: 0.5601Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.5580Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.5613Epoch 9/15: [========                      ] 17/63 batches, loss: 0.5612Epoch 9/15: [========                      ] 18/63 batches, loss: 0.5608Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.5595Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.5581Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.5558Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.5536Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.5511Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.5506Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.5494Epoch 9/15: [============                  ] 26/63 batches, loss: 0.5488Epoch 9/15: [============                  ] 27/63 batches, loss: 0.5480Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.5451Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.5454Epoch 9/15: [==============                ] 30/63 batches, loss: 0.5451Epoch 9/15: [==============                ] 31/63 batches, loss: 0.5462Epoch 9/15: [===============               ] 32/63 batches, loss: 0.5421Epoch 9/15: [===============               ] 33/63 batches, loss: 0.5433Epoch 9/15: [================              ] 34/63 batches, loss: 0.5426Epoch 9/15: [================              ] 35/63 batches, loss: 0.5426Epoch 9/15: [=================             ] 36/63 batches, loss: 0.5414Epoch 9/15: [=================             ] 37/63 batches, loss: 0.5426Epoch 9/15: [==================            ] 38/63 batches, loss: 0.5425Epoch 9/15: [==================            ] 39/63 batches, loss: 0.5422Epoch 9/15: [===================           ] 40/63 batches, loss: 0.5448Epoch 9/15: [===================           ] 41/63 batches, loss: 0.5463Epoch 9/15: [====================          ] 42/63 batches, loss: 0.5478Epoch 9/15: [====================          ] 43/63 batches, loss: 0.5471Epoch 9/15: [====================          ] 44/63 batches, loss: 0.5470Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.5483Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.5496Epoch 9/15: [======================        ] 47/63 batches, loss: 0.5483Epoch 9/15: [======================        ] 48/63 batches, loss: 0.5492Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.5502Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.5495Epoch 9/15: [========================      ] 51/63 batches, loss: 0.5493Epoch 9/15: [========================      ] 52/63 batches, loss: 0.5482Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.5489Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.5478Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.5489Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.5493Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.5483Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.5486Epoch 9/15: [============================  ] 59/63 batches, loss: 0.5474Epoch 9/15: [============================  ] 60/63 batches, loss: 0.5456Epoch 9/15: [============================= ] 61/63 batches, loss: 0.5454Epoch 9/15: [============================= ] 62/63 batches, loss: 0.5453Epoch 9/15: [==============================] 63/63 batches, loss: 0.5417
[2025-05-02 12:53:25,574][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.5417
[2025-05-02 12:53:25,789][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.5806, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.5399Epoch 10/15: [                              ] 2/63 batches, loss: 0.5066Epoch 10/15: [=                             ] 3/63 batches, loss: 0.5329Epoch 10/15: [=                             ] 4/63 batches, loss: 0.5281Epoch 10/15: [==                            ] 5/63 batches, loss: 0.5315Epoch 10/15: [==                            ] 6/63 batches, loss: 0.5210Epoch 10/15: [===                           ] 7/63 batches, loss: 0.5167Epoch 10/15: [===                           ] 8/63 batches, loss: 0.5275Epoch 10/15: [====                          ] 9/63 batches, loss: 0.5183Epoch 10/15: [====                          ] 10/63 batches, loss: 0.5212Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.5216Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.5281Epoch 10/15: [======                        ] 13/63 batches, loss: 0.5314Epoch 10/15: [======                        ] 14/63 batches, loss: 0.5364Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.5363Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.5363Epoch 10/15: [========                      ] 17/63 batches, loss: 0.5378Epoch 10/15: [========                      ] 18/63 batches, loss: 0.5463Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.5428Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.5407Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.5453Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.5430Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.5438Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.5445Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.5451Epoch 10/15: [============                  ] 26/63 batches, loss: 0.5430Epoch 10/15: [============                  ] 27/63 batches, loss: 0.5439Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.5416Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.5435Epoch 10/15: [==============                ] 30/63 batches, loss: 0.5433Epoch 10/15: [==============                ] 31/63 batches, loss: 0.5434Epoch 10/15: [===============               ] 32/63 batches, loss: 0.5453Epoch 10/15: [===============               ] 33/63 batches, loss: 0.5438Epoch 10/15: [================              ] 34/63 batches, loss: 0.5445Epoch 10/15: [================              ] 35/63 batches, loss: 0.5436Epoch 10/15: [=================             ] 36/63 batches, loss: 0.5429Epoch 10/15: [=================             ] 37/63 batches, loss: 0.5404Epoch 10/15: [==================            ] 38/63 batches, loss: 0.5404Epoch 10/15: [==================            ] 39/63 batches, loss: 0.5417Epoch 10/15: [===================           ] 40/63 batches, loss: 0.5442Epoch 10/15: [===================           ] 41/63 batches, loss: 0.5411Epoch 10/15: [====================          ] 42/63 batches, loss: 0.5415Epoch 10/15: [====================          ] 43/63 batches, loss: 0.5404Epoch 10/15: [====================          ] 44/63 batches, loss: 0.5409Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.5405Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.5394Epoch 10/15: [======================        ] 47/63 batches, loss: 0.5401Epoch 10/15: [======================        ] 48/63 batches, loss: 0.5410Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.5400Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.5409Epoch 10/15: [========================      ] 51/63 batches, loss: 0.5397Epoch 10/15: [========================      ] 52/63 batches, loss: 0.5394Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.5409Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.5406Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.5400Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.5384Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.5383Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.5389Epoch 10/15: [============================  ] 59/63 batches, loss: 0.5384Epoch 10/15: [============================  ] 60/63 batches, loss: 0.5387Epoch 10/15: [============================= ] 61/63 batches, loss: 0.5386Epoch 10/15: [============================= ] 62/63 batches, loss: 0.5387Epoch 10/15: [==============================] 63/63 batches, loss: 0.5392
[2025-05-02 12:53:28,123][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.5392
[2025-05-02 12:53:28,354][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.5801, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.5396Epoch 11/15: [                              ] 2/63 batches, loss: 0.5490Epoch 11/15: [=                             ] 3/63 batches, loss: 0.5556Epoch 11/15: [=                             ] 4/63 batches, loss: 0.5666Epoch 11/15: [==                            ] 5/63 batches, loss: 0.5784Epoch 11/15: [==                            ] 6/63 batches, loss: 0.5678Epoch 11/15: [===                           ] 7/63 batches, loss: 0.5701Epoch 11/15: [===                           ] 8/63 batches, loss: 0.5776Epoch 11/15: [====                          ] 9/63 batches, loss: 0.5774Epoch 11/15: [====                          ] 10/63 batches, loss: 0.5670Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.5591Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.5650Epoch 11/15: [======                        ] 13/63 batches, loss: 0.5660Epoch 11/15: [======                        ] 14/63 batches, loss: 0.5695Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.5643Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.5606Epoch 11/15: [========                      ] 17/63 batches, loss: 0.5597Epoch 11/15: [========                      ] 18/63 batches, loss: 0.5579Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.5566Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.5603Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.5619Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.5603Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.5594Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.5615Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.5604Epoch 11/15: [============                  ] 26/63 batches, loss: 0.5582Epoch 11/15: [============                  ] 27/63 batches, loss: 0.5557Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.5590Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.5582Epoch 11/15: [==============                ] 30/63 batches, loss: 0.5569Epoch 11/15: [==============                ] 31/63 batches, loss: 0.5549Epoch 11/15: [===============               ] 32/63 batches, loss: 0.5545Epoch 11/15: [===============               ] 33/63 batches, loss: 0.5511Epoch 11/15: [================              ] 34/63 batches, loss: 0.5480Epoch 11/15: [================              ] 35/63 batches, loss: 0.5487Epoch 11/15: [=================             ] 36/63 batches, loss: 0.5478Epoch 11/15: [=================             ] 37/63 batches, loss: 0.5464Epoch 11/15: [==================            ] 38/63 batches, loss: 0.5470Epoch 11/15: [==================            ] 39/63 batches, loss: 0.5457Epoch 11/15: [===================           ] 40/63 batches, loss: 0.5457Epoch 11/15: [===================           ] 41/63 batches, loss: 0.5445Epoch 11/15: [====================          ] 42/63 batches, loss: 0.5440Epoch 11/15: [====================          ] 43/63 batches, loss: 0.5417Epoch 11/15: [====================          ] 44/63 batches, loss: 0.5418Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.5428Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.5417Epoch 11/15: [======================        ] 47/63 batches, loss: 0.5428Epoch 11/15: [======================        ] 48/63 batches, loss: 0.5420Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.5424Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.5404Epoch 11/15: [========================      ] 51/63 batches, loss: 0.5413Epoch 11/15: [========================      ] 52/63 batches, loss: 0.5411Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.5401Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.5399Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.5397Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.5414Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.5420Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.5425Epoch 11/15: [============================  ] 59/63 batches, loss: 0.5421Epoch 11/15: [============================  ] 60/63 batches, loss: 0.5405Epoch 11/15: [============================= ] 61/63 batches, loss: 0.5401Epoch 11/15: [============================= ] 62/63 batches, loss: 0.5389Epoch 11/15: [==============================] 63/63 batches, loss: 0.5394
[2025-05-02 12:53:30,682][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.5394
[2025-05-02 12:53:30,901][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.5796, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.4705Epoch 12/15: [                              ] 2/63 batches, loss: 0.5588Epoch 12/15: [=                             ] 3/63 batches, loss: 0.5540Epoch 12/15: [=                             ] 4/63 batches, loss: 0.5645Epoch 12/15: [==                            ] 5/63 batches, loss: 0.5404Epoch 12/15: [==                            ] 6/63 batches, loss: 0.5441Epoch 12/15: [===                           ] 7/63 batches, loss: 0.5497Epoch 12/15: [===                           ] 8/63 batches, loss: 0.5477Epoch 12/15: [====                          ] 9/63 batches, loss: 0.5462Epoch 12/15: [====                          ] 10/63 batches, loss: 0.5343Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.5287Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.5209Epoch 12/15: [======                        ] 13/63 batches, loss: 0.5262Epoch 12/15: [======                        ] 14/63 batches, loss: 0.5203Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.5150Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.5136Epoch 12/15: [========                      ] 17/63 batches, loss: 0.5186Epoch 12/15: [========                      ] 18/63 batches, loss: 0.5179Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.5199Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.5169Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.5189Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.5185Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.5172Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.5161Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.5144Epoch 12/15: [============                  ] 26/63 batches, loss: 0.5150Epoch 12/15: [============                  ] 27/63 batches, loss: 0.5148Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.5165Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.5157Epoch 12/15: [==============                ] 30/63 batches, loss: 0.5173Epoch 12/15: [==============                ] 31/63 batches, loss: 0.5186Epoch 12/15: [===============               ] 32/63 batches, loss: 0.5202Epoch 12/15: [===============               ] 33/63 batches, loss: 0.5210Epoch 12/15: [================              ] 34/63 batches, loss: 0.5227Epoch 12/15: [================              ] 35/63 batches, loss: 0.5242Epoch 12/15: [=================             ] 36/63 batches, loss: 0.5258Epoch 12/15: [=================             ] 37/63 batches, loss: 0.5279Epoch 12/15: [==================            ] 38/63 batches, loss: 0.5279Epoch 12/15: [==================            ] 39/63 batches, loss: 0.5268Epoch 12/15: [===================           ] 40/63 batches, loss: 0.5282Epoch 12/15: [===================           ] 41/63 batches, loss: 0.5277Epoch 12/15: [====================          ] 42/63 batches, loss: 0.5293Epoch 12/15: [====================          ] 43/63 batches, loss: 0.5299Epoch 12/15: [====================          ] 44/63 batches, loss: 0.5311Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.5316Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.5320Epoch 12/15: [======================        ] 47/63 batches, loss: 0.5313Epoch 12/15: [======================        ] 48/63 batches, loss: 0.5319Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.5345Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.5350Epoch 12/15: [========================      ] 51/63 batches, loss: 0.5326Epoch 12/15: [========================      ] 52/63 batches, loss: 0.5332Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.5348Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.5356Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.5360Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.5357Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.5362Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.5373Epoch 12/15: [============================  ] 59/63 batches, loss: 0.5376Epoch 12/15: [============================  ] 60/63 batches, loss: 0.5373Epoch 12/15: [============================= ] 61/63 batches, loss: 0.5373Epoch 12/15: [============================= ] 62/63 batches, loss: 0.5373Epoch 12/15: [==============================] 63/63 batches, loss: 0.5359
[2025-05-02 12:53:33,257][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.5359
[2025-05-02 12:53:33,476][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.5748, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.5259Epoch 13/15: [                              ] 2/63 batches, loss: 0.5217Epoch 13/15: [=                             ] 3/63 batches, loss: 0.5329Epoch 13/15: [=                             ] 4/63 batches, loss: 0.5412Epoch 13/15: [==                            ] 5/63 batches, loss: 0.5463Epoch 13/15: [==                            ] 6/63 batches, loss: 0.5551Epoch 13/15: [===                           ] 7/63 batches, loss: 0.5469Epoch 13/15: [===                           ] 8/63 batches, loss: 0.5416Epoch 13/15: [====                          ] 9/63 batches, loss: 0.5443Epoch 13/15: [====                          ] 10/63 batches, loss: 0.5390Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.5376Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.5479Epoch 13/15: [======                        ] 13/63 batches, loss: 0.5466Epoch 13/15: [======                        ] 14/63 batches, loss: 0.5421Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.5357Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.5326Epoch 13/15: [========                      ] 17/63 batches, loss: 0.5268Epoch 13/15: [========                      ] 18/63 batches, loss: 0.5290Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.5301Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.5298Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.5295Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.5295Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.5290Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.5283Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.5268Epoch 13/15: [============                  ] 26/63 batches, loss: 0.5244Epoch 13/15: [============                  ] 27/63 batches, loss: 0.5255Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.5246Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.5261Epoch 13/15: [==============                ] 30/63 batches, loss: 0.5282Epoch 13/15: [==============                ] 31/63 batches, loss: 0.5278Epoch 13/15: [===============               ] 32/63 batches, loss: 0.5293Epoch 13/15: [===============               ] 33/63 batches, loss: 0.5269Epoch 13/15: [================              ] 34/63 batches, loss: 0.5303Epoch 13/15: [================              ] 35/63 batches, loss: 0.5320Epoch 13/15: [=================             ] 36/63 batches, loss: 0.5313Epoch 13/15: [=================             ] 37/63 batches, loss: 0.5305Epoch 13/15: [==================            ] 38/63 batches, loss: 0.5314Epoch 13/15: [==================            ] 39/63 batches, loss: 0.5321Epoch 13/15: [===================           ] 40/63 batches, loss: 0.5313Epoch 13/15: [===================           ] 41/63 batches, loss: 0.5309Epoch 13/15: [====================          ] 42/63 batches, loss: 0.5311Epoch 13/15: [====================          ] 43/63 batches, loss: 0.5294Epoch 13/15: [====================          ] 44/63 batches, loss: 0.5291Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.5310Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.5305Epoch 13/15: [======================        ] 47/63 batches, loss: 0.5317Epoch 13/15: [======================        ] 48/63 batches, loss: 0.5332Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.5326Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.5327Epoch 13/15: [========================      ] 51/63 batches, loss: 0.5320Epoch 13/15: [========================      ] 52/63 batches, loss: 0.5327Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.5327Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.5329Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.5330Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.5332Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.5331Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.5333Epoch 13/15: [============================  ] 59/63 batches, loss: 0.5334Epoch 13/15: [============================  ] 60/63 batches, loss: 0.5341Epoch 13/15: [============================= ] 61/63 batches, loss: 0.5325Epoch 13/15: [============================= ] 62/63 batches, loss: 0.5337Epoch 13/15: [==============================] 63/63 batches, loss: 0.5343
[2025-05-02 12:53:35,834][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.5343
[2025-05-02 12:53:36,066][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.5784, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 12:53:36,067][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/3
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.5122Epoch 14/15: [                              ] 2/63 batches, loss: 0.5264Epoch 14/15: [=                             ] 3/63 batches, loss: 0.5276Epoch 14/15: [=                             ] 4/63 batches, loss: 0.5164Epoch 14/15: [==                            ] 5/63 batches, loss: 0.5156Epoch 14/15: [==                            ] 6/63 batches, loss: 0.5219Epoch 14/15: [===                           ] 7/63 batches, loss: 0.5308Epoch 14/15: [===                           ] 8/63 batches, loss: 0.5315Epoch 14/15: [====                          ] 9/63 batches, loss: 0.5245Epoch 14/15: [====                          ] 10/63 batches, loss: 0.5262Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.5164Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.5131Epoch 14/15: [======                        ] 13/63 batches, loss: 0.5170Epoch 14/15: [======                        ] 14/63 batches, loss: 0.5272Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.5245Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.5226Epoch 14/15: [========                      ] 17/63 batches, loss: 0.5216Epoch 14/15: [========                      ] 18/63 batches, loss: 0.5247Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.5197Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.5216Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.5216Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.5221Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.5237Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.5243Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.5229Epoch 14/15: [============                  ] 26/63 batches, loss: 0.5262Epoch 14/15: [============                  ] 27/63 batches, loss: 0.5239Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.5238Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.5283Epoch 14/15: [==============                ] 30/63 batches, loss: 0.5277Epoch 14/15: [==============                ] 31/63 batches, loss: 0.5263Epoch 14/15: [===============               ] 32/63 batches, loss: 0.5227Epoch 14/15: [===============               ] 33/63 batches, loss: 0.5230Epoch 14/15: [================              ] 34/63 batches, loss: 0.5248Epoch 14/15: [================              ] 35/63 batches, loss: 0.5236Epoch 14/15: [=================             ] 36/63 batches, loss: 0.5232Epoch 14/15: [=================             ] 37/63 batches, loss: 0.5234Epoch 14/15: [==================            ] 38/63 batches, loss: 0.5220Epoch 14/15: [==================            ] 39/63 batches, loss: 0.5200Epoch 14/15: [===================           ] 40/63 batches, loss: 0.5187Epoch 14/15: [===================           ] 41/63 batches, loss: 0.5178Epoch 14/15: [====================          ] 42/63 batches, loss: 0.5171Epoch 14/15: [====================          ] 43/63 batches, loss: 0.5153Epoch 14/15: [====================          ] 44/63 batches, loss: 0.5158Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.5173Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.5163Epoch 14/15: [======================        ] 47/63 batches, loss: 0.5152Epoch 14/15: [======================        ] 48/63 batches, loss: 0.5170Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.5187Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.5189Epoch 14/15: [========================      ] 51/63 batches, loss: 0.5179Epoch 14/15: [========================      ] 52/63 batches, loss: 0.5203Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.5216Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.5226Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.5230Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.5241Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.5244Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.5264Epoch 14/15: [============================  ] 59/63 batches, loss: 0.5255Epoch 14/15: [============================  ] 60/63 batches, loss: 0.5269Epoch 14/15: [============================= ] 61/63 batches, loss: 0.5283Epoch 14/15: [============================= ] 62/63 batches, loss: 0.5286Epoch 14/15: [==============================] 63/63 batches, loss: 0.5252
[2025-05-02 12:53:38,021][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.5252
[2025-05-02 12:53:38,223][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.5781, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 12:53:38,224][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 2/3
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.5224Epoch 15/15: [                              ] 2/63 batches, loss: 0.4542Epoch 15/15: [=                             ] 3/63 batches, loss: 0.4901Epoch 15/15: [=                             ] 4/63 batches, loss: 0.5065Epoch 15/15: [==                            ] 5/63 batches, loss: 0.5180Epoch 15/15: [==                            ] 6/63 batches, loss: 0.5236Epoch 15/15: [===                           ] 7/63 batches, loss: 0.5114Epoch 15/15: [===                           ] 8/63 batches, loss: 0.5143Epoch 15/15: [====                          ] 9/63 batches, loss: 0.5133Epoch 15/15: [====                          ] 10/63 batches, loss: 0.5130Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.5115Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.5154Epoch 15/15: [======                        ] 13/63 batches, loss: 0.5139Epoch 15/15: [======                        ] 14/63 batches, loss: 0.5133Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.5154Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.5139Epoch 15/15: [========                      ] 17/63 batches, loss: 0.5151Epoch 15/15: [========                      ] 18/63 batches, loss: 0.5185Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.5213Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.5194Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.5178Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.5183Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.5188Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.5217Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.5216Epoch 15/15: [============                  ] 26/63 batches, loss: 0.5249Epoch 15/15: [============                  ] 27/63 batches, loss: 0.5244Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.5276Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.5256Epoch 15/15: [==============                ] 30/63 batches, loss: 0.5251Epoch 15/15: [==============                ] 31/63 batches, loss: 0.5293Epoch 15/15: [===============               ] 32/63 batches, loss: 0.5300Epoch 15/15: [===============               ] 33/63 batches, loss: 0.5320Epoch 15/15: [================              ] 34/63 batches, loss: 0.5313Epoch 15/15: [================              ] 35/63 batches, loss: 0.5313Epoch 15/15: [=================             ] 36/63 batches, loss: 0.5312Epoch 15/15: [=================             ] 37/63 batches, loss: 0.5292Epoch 15/15: [==================            ] 38/63 batches, loss: 0.5313Epoch 15/15: [==================            ] 39/63 batches, loss: 0.5310Epoch 15/15: [===================           ] 40/63 batches, loss: 0.5287Epoch 15/15: [===================           ] 41/63 batches, loss: 0.5303Epoch 15/15: [====================          ] 42/63 batches, loss: 0.5287Epoch 15/15: [====================          ] 43/63 batches, loss: 0.5278Epoch 15/15: [====================          ] 44/63 batches, loss: 0.5274Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.5258Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.5269Epoch 15/15: [======================        ] 47/63 batches, loss: 0.5275Epoch 15/15: [======================        ] 48/63 batches, loss: 0.5247Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.5237Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.5242Epoch 15/15: [========================      ] 51/63 batches, loss: 0.5252Epoch 15/15: [========================      ] 52/63 batches, loss: 0.5267Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.5279Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.5299Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.5275Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.5273Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.5281Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.5268Epoch 15/15: [============================  ] 59/63 batches, loss: 0.5269Epoch 15/15: [============================  ] 60/63 batches, loss: 0.5278Epoch 15/15: [============================= ] 61/63 batches, loss: 0.5283Epoch 15/15: [============================= ] 62/63 batches, loss: 0.5294Epoch 15/15: [==============================] 63/63 batches, loss: 0.5270
[2025-05-02 12:53:40,155][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.5270
[2025-05-02 12:53:40,368][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.5800, Metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 12:53:40,369][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 3/3
[2025-05-02 12:53:40,369][src.training.lm_trainer][INFO] - Early stopping triggered at epoch 15
[2025-05-02 12:53:40,369][src.training.lm_trainer][INFO] - Training completed in 37.60 seconds
[2025-05-02 12:53:40,369][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 12:53:42,857][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'accuracy': 0.9919597989949749, 'f1': 0.9919354838709677, 'precision': 0.9939393939393939, 'recall': 0.9899396378269618}
[2025-05-02 12:53:42,858][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'accuracy': 0.9090909090909091, 'f1': 0.9090909090909091, 'precision': 0.8333333333333334, 'recall': 1.0}
[2025-05-02 12:53:42,858][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'accuracy': 0.6623376623376623, 'f1': 0.6176470588235294, 'precision': 0.45652173913043476, 'recall': 0.9545454545454546}
[2025-05-02 12:53:44,502][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/question_type/layer11/ar/ar/model.pt
[2025-05-02 12:53:44,503][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:     best_val_accuracy ▁▂▇▆█▇██████
wandb:           best_val_f1 ▁▄█▇████████
wandb:         best_val_loss █▇▅▄▂▂▂▂▁▁▁▁
wandb:    best_val_precision ▁▆██████████
wandb:       best_val_recall ▁▃▇▇█▇██████
wandb:      early_stop_epoch ▁
wandb:                 epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_accuracy ▁
wandb:         final_test_f1 ▁
wandb:  final_test_precision ▁
wandb:     final_test_recall ▁
wandb:  final_train_accuracy ▁
wandb:        final_train_f1 ▁
wandb: final_train_precision ▁
wandb:    final_train_recall ▁
wandb:    final_val_accuracy ▁
wandb:          final_val_f1 ▁
wandb:   final_val_precision ▁
wandb:      final_val_recall ▁
wandb:         learning_rate █▁▁▂▂▃▃▃▃▃▃▃▃▃▃
wandb:            train_loss █▇▆▅▄▃▂▂▂▂▂▁▁▁▁
wandb:            train_time ▁
wandb:          val_accuracy ▁▂▇▆█▇█████████
wandb:                val_f1 ▁▄█▇███████████
wandb:              val_loss █▇▅▄▂▂▂▂▁▁▁▁▁▁▁
wandb:         val_precision ▁▆█████████████
wandb:            val_recall ▁▃▇▇█▇█████████
wandb: 
wandb: Run summary:
wandb:     best_val_accuracy 0.90909
wandb:           best_val_f1 0.90909
wandb:         best_val_loss 0.57479
wandb:    best_val_precision 0.83333
wandb:       best_val_recall 1
wandb:      early_stop_epoch 15
wandb:                 epoch 15
wandb:   final_test_accuracy 0.66234
wandb:         final_test_f1 0.61765
wandb:  final_test_precision 0.45652
wandb:     final_test_recall 0.95455
wandb:  final_train_accuracy 0.99196
wandb:        final_train_f1 0.99194
wandb: final_train_precision 0.99394
wandb:    final_train_recall 0.98994
wandb:    final_val_accuracy 0.90909
wandb:          final_val_f1 0.90909
wandb:   final_val_precision 0.83333
wandb:      final_val_recall 1
wandb:         learning_rate 0.0001
wandb:            train_loss 0.52698
wandb:            train_time 37.59792
wandb:          val_accuracy 0.90909
wandb:                val_f1 0.90909
wandb:              val_loss 0.57999
wandb:         val_precision 0.83333
wandb:            val_recall 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125253-5vkqu1iw
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125253-5vkqu1iw/logs
Experiment probe_layer11_question_type_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/question_type/layer11/ar/ar/results.json for layer 11
Running experiment: probe_layer11_complexity_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=complexity"         "experiment.tasks=complexity"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=11"         "model.probe_hidden_size=96" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer11_complexity_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/complexity/layer11/ar"         "wandb.mode=offline"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 12:53:55,380][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/complexity/layer11/ar
experiment_name: probe_layer11_complexity_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
  probe_hidden_size: 96
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: complexity
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression
  feature: lang_norm_complexity_score
  training:
    patience: 5
    scheduler_patience: 4
    scheduler_factor: 0.8
    dropout: 0.1

[2025-05-02 12:53:55,380][__main__][INFO] - Normalized task: complexity
[2025-05-02 12:53:55,380][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 12:53:55,380][__main__][INFO] - Determined Task Type: regression
[2025-05-02 12:53:55,385][__main__][INFO] - Running LM experiment for task 'complexity' (type: regression) on languages: ['ar']
[2025-05-02 12:53:55,385][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 12:53:56,636][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'complexity', submetric: 'None'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 12:53:59,008][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 12:53:59,008][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:53:59,048][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:53:59,073][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:53:59,155][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 12:53:59,162][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:53:59,162][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 12:53:59,163][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:53:59,181][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:53:59,209][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:53:59,223][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 12:53:59,224][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:53:59,225][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 12:53:59,225][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:53:59,242][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:53:59,266][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:53:59,280][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 12:53:59,281][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:53:59,281][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 12:53:59,282][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 12:53:59,283][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 12:53:59,283][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 12:53:59,283][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 12:53:59,283][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 12:53:59,283][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:53:59,283][src.data.datasets][INFO] -   Mean: 0.4236, Std: 0.1752
[2025-05-02 12:53:59,284][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 12:53:59,284][src.data.datasets][INFO] - Sample label: 0.41602465510368347
[2025-05-02 12:53:59,284][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 12:53:59,284][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 12:53:59,284][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 12:53:59,284][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 12:53:59,284][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:53:59,284][src.data.datasets][INFO] -   Mean: 0.3847, Std: 0.2547
[2025-05-02 12:53:59,284][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 12:53:59,284][src.data.datasets][INFO] - Sample label: 0.09095905721187592
[2025-05-02 12:53:59,285][src.data.datasets][INFO] - Task 'complexity' is classification: False
[2025-05-02 12:53:59,285][src.data.datasets][INFO] - Getting feature name for task: 'complexity', submetric: 'None'
[2025-05-02 12:53:59,285][src.data.datasets][INFO] - Selected feature name: 'lang_norm_complexity_score' for task: 'complexity'
[2025-05-02 12:53:59,285][src.data.datasets][INFO] - Label statistics for complexity (feature: lang_norm_complexity_score):
[2025-05-02 12:53:59,285][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:53:59,285][src.data.datasets][INFO] -   Mean: 0.4157, Std: 0.2408
[2025-05-02 12:53:59,285][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 12:53:59,285][src.data.datasets][INFO] - Sample label: 0.5635213255882263
[2025-05-02 12:53:59,285][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 12:53:59,285][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 12:53:59,286][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 12:53:59,286][__main__][INFO] - Using model type: lm_probe for complexity
[2025-05-02 12:53:59,286][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 12:54:02,981][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 12:54:02,982][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 12:54:02,982][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=11, freeze_model=True
[2025-05-02 12:54:02,982][src.models.model_factory][INFO] - Using provided probe_hidden_size: 96
[2025-05-02 12:54:02,985][src.models.model_factory][INFO] - Model has 84,961 trainable parameters out of 394,206,433 total parameters
[2025-05-02 12:54:02,985][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 84,961 trainable parameters
[2025-05-02 12:54:02,985][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=96, depth=2, activation=silu, normalization=layer
[2025-05-02 12:54:02,985][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 96 hidden size
[2025-05-02 12:54:02,986][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 12:54:02,986][__main__][INFO] - Total parameters: 394,206,433
[2025-05-02 12:54:02,986][__main__][INFO] - Trainable parameters: 84,961 (0.02%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 3.4643Epoch 1/15: [                              ] 2/63 batches, loss: 4.2074Epoch 1/15: [=                             ] 3/63 batches, loss: 4.1531Epoch 1/15: [=                             ] 4/63 batches, loss: 4.1566Epoch 1/15: [==                            ] 5/63 batches, loss: 3.8865Epoch 1/15: [==                            ] 6/63 batches, loss: 3.9713Epoch 1/15: [===                           ] 7/63 batches, loss: 3.8842Epoch 1/15: [===                           ] 8/63 batches, loss: 3.7213Epoch 1/15: [====                          ] 9/63 batches, loss: 3.6750Epoch 1/15: [====                          ] 10/63 batches, loss: 3.5500Epoch 1/15: [=====                         ] 11/63 batches, loss: 3.5231Epoch 1/15: [=====                         ] 12/63 batches, loss: 3.4076Epoch 1/15: [======                        ] 13/63 batches, loss: 3.3753Epoch 1/15: [======                        ] 14/63 batches, loss: 3.3284Epoch 1/15: [=======                       ] 15/63 batches, loss: 3.2739Epoch 1/15: [=======                       ] 16/63 batches, loss: 3.2184Epoch 1/15: [========                      ] 17/63 batches, loss: 3.1836Epoch 1/15: [========                      ] 18/63 batches, loss: 3.1925Epoch 1/15: [=========                     ] 19/63 batches, loss: 3.1518Epoch 1/15: [=========                     ] 20/63 batches, loss: 3.1061Epoch 1/15: [==========                    ] 21/63 batches, loss: 3.0344Epoch 1/15: [==========                    ] 22/63 batches, loss: 2.9995Epoch 1/15: [==========                    ] 23/63 batches, loss: 2.9885Epoch 1/15: [===========                   ] 24/63 batches, loss: 2.9530Epoch 1/15: [===========                   ] 25/63 batches, loss: 2.9282Epoch 1/15: [============                  ] 26/63 batches, loss: 2.9056Epoch 1/15: [============                  ] 27/63 batches, loss: 2.8884Epoch 1/15: [=============                 ] 28/63 batches, loss: 2.8376Epoch 1/15: [=============                 ] 29/63 batches, loss: 2.8035Epoch 1/15: [==============                ] 30/63 batches, loss: 2.7723Epoch 1/15: [==============                ] 31/63 batches, loss: 2.7424Epoch 1/15: [===============               ] 32/63 batches, loss: 2.6803Epoch 1/15: [===============               ] 33/63 batches, loss: 2.6310Epoch 1/15: [================              ] 34/63 batches, loss: 2.6098Epoch 1/15: [================              ] 35/63 batches, loss: 2.5768Epoch 1/15: [=================             ] 36/63 batches, loss: 2.5523Epoch 1/15: [=================             ] 37/63 batches, loss: 2.5195Epoch 1/15: [==================            ] 38/63 batches, loss: 2.5051Epoch 1/15: [==================            ] 39/63 batches, loss: 2.4761Epoch 1/15: [===================           ] 40/63 batches, loss: 2.4550Epoch 1/15: [===================           ] 41/63 batches, loss: 2.4190Epoch 1/15: [====================          ] 42/63 batches, loss: 2.3907Epoch 1/15: [====================          ] 43/63 batches, loss: 2.3592Epoch 1/15: [====================          ] 44/63 batches, loss: 2.3207Epoch 1/15: [=====================         ] 45/63 batches, loss: 2.2857Epoch 1/15: [=====================         ] 46/63 batches, loss: 2.2678Epoch 1/15: [======================        ] 47/63 batches, loss: 2.2488Epoch 1/15: [======================        ] 48/63 batches, loss: 2.2181Epoch 1/15: [=======================       ] 49/63 batches, loss: 2.1976Epoch 1/15: [=======================       ] 50/63 batches, loss: 2.1745Epoch 1/15: [========================      ] 51/63 batches, loss: 2.1488Epoch 1/15: [========================      ] 52/63 batches, loss: 2.1167Epoch 1/15: [=========================     ] 53/63 batches, loss: 2.0816Epoch 1/15: [=========================     ] 54/63 batches, loss: 2.0579Epoch 1/15: [==========================    ] 55/63 batches, loss: 2.0327Epoch 1/15: [==========================    ] 56/63 batches, loss: 2.0009Epoch 1/15: [===========================   ] 57/63 batches, loss: 1.9838Epoch 1/15: [===========================   ] 58/63 batches, loss: 1.9690Epoch 1/15: [============================  ] 59/63 batches, loss: 1.9584Epoch 1/15: [============================  ] 60/63 batches, loss: 1.9391Epoch 1/15: [============================= ] 61/63 batches, loss: 1.9163Epoch 1/15: [============================= ] 62/63 batches, loss: 1.8974Epoch 1/15: [==============================] 63/63 batches, loss: 1.8923
[2025-05-02 12:54:07,152][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 1.8923
[2025-05-02 12:54:07,343][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.2965, Metrics: {'mse': 0.3046724498271942, 'rmse': 0.5519714212051148, 'r2': -3.6960158348083496}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.5679Epoch 2/15: [                              ] 2/63 batches, loss: 0.4688Epoch 2/15: [=                             ] 3/63 batches, loss: 0.4249Epoch 2/15: [=                             ] 4/63 batches, loss: 0.5018Epoch 2/15: [==                            ] 5/63 batches, loss: 0.6047Epoch 2/15: [==                            ] 6/63 batches, loss: 0.6328Epoch 2/15: [===                           ] 7/63 batches, loss: 0.5850Epoch 2/15: [===                           ] 8/63 batches, loss: 0.5542Epoch 2/15: [====                          ] 9/63 batches, loss: 0.5093Epoch 2/15: [====                          ] 10/63 batches, loss: 0.5110Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.4992Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.4916Epoch 2/15: [======                        ] 13/63 batches, loss: 0.4919Epoch 2/15: [======                        ] 14/63 batches, loss: 0.4792Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.4831Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.4862Epoch 2/15: [========                      ] 17/63 batches, loss: 0.4914Epoch 2/15: [========                      ] 18/63 batches, loss: 0.4917Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.4869Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.4962Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.4951Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.4854Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.4703Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.4666Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.4598Epoch 2/15: [============                  ] 26/63 batches, loss: 0.4561Epoch 2/15: [============                  ] 27/63 batches, loss: 0.4434Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.4326Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.4292Epoch 2/15: [==============                ] 30/63 batches, loss: 0.4223Epoch 2/15: [==============                ] 31/63 batches, loss: 0.4218Epoch 2/15: [===============               ] 32/63 batches, loss: 0.4185Epoch 2/15: [===============               ] 33/63 batches, loss: 0.4152Epoch 2/15: [================              ] 34/63 batches, loss: 0.4129Epoch 2/15: [================              ] 35/63 batches, loss: 0.4184Epoch 2/15: [=================             ] 36/63 batches, loss: 0.4160Epoch 2/15: [=================             ] 37/63 batches, loss: 0.4097Epoch 2/15: [==================            ] 38/63 batches, loss: 0.4012Epoch 2/15: [==================            ] 39/63 batches, loss: 0.3979Epoch 2/15: [===================           ] 40/63 batches, loss: 0.3963Epoch 2/15: [===================           ] 41/63 batches, loss: 0.3948Epoch 2/15: [====================          ] 42/63 batches, loss: 0.3914Epoch 2/15: [====================          ] 43/63 batches, loss: 0.3885Epoch 2/15: [====================          ] 44/63 batches, loss: 0.3888Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.3848Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.3831Epoch 2/15: [======================        ] 47/63 batches, loss: 0.3790Epoch 2/15: [======================        ] 48/63 batches, loss: 0.3741Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.3695Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.3749Epoch 2/15: [========================      ] 51/63 batches, loss: 0.3722Epoch 2/15: [========================      ] 52/63 batches, loss: 0.3680Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.3652Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.3613Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.3655Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.3639Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.3611Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.3598Epoch 2/15: [============================  ] 59/63 batches, loss: 0.3655Epoch 2/15: [============================  ] 60/63 batches, loss: 0.3628Epoch 2/15: [============================= ] 61/63 batches, loss: 0.3619Epoch 2/15: [============================= ] 62/63 batches, loss: 0.3606Epoch 2/15: [==============================] 63/63 batches, loss: 0.3590
[2025-05-02 12:54:09,669][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.3590
[2025-05-02 12:54:09,857][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.2124, Metrics: {'mse': 0.2152758091688156, 'rmse': 0.4639782421286753, 'r2': -2.3181161880493164}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.3469Epoch 3/15: [                              ] 2/63 batches, loss: 0.3405Epoch 3/15: [=                             ] 3/63 batches, loss: 0.2703Epoch 3/15: [=                             ] 4/63 batches, loss: 0.2601Epoch 3/15: [==                            ] 5/63 batches, loss: 0.3029Epoch 3/15: [==                            ] 6/63 batches, loss: 0.2929Epoch 3/15: [===                           ] 7/63 batches, loss: 0.2652Epoch 3/15: [===                           ] 8/63 batches, loss: 0.2857Epoch 3/15: [====                          ] 9/63 batches, loss: 0.2852Epoch 3/15: [====                          ] 10/63 batches, loss: 0.3035Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.2950Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.2893Epoch 3/15: [======                        ] 13/63 batches, loss: 0.2929Epoch 3/15: [======                        ] 14/63 batches, loss: 0.3020Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.2963Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.2986Epoch 3/15: [========                      ] 17/63 batches, loss: 0.2965Epoch 3/15: [========                      ] 18/63 batches, loss: 0.2892Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.2927Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.2951Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.2918Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.2870Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.2852Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.2871Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.2881Epoch 3/15: [============                  ] 26/63 batches, loss: 0.2851Epoch 3/15: [============                  ] 27/63 batches, loss: 0.2836Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.2817Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.2758Epoch 3/15: [==============                ] 30/63 batches, loss: 0.2750Epoch 3/15: [==============                ] 31/63 batches, loss: 0.2818Epoch 3/15: [===============               ] 32/63 batches, loss: 0.2806Epoch 3/15: [===============               ] 33/63 batches, loss: 0.2796Epoch 3/15: [================              ] 34/63 batches, loss: 0.2757Epoch 3/15: [================              ] 35/63 batches, loss: 0.2717Epoch 3/15: [=================             ] 36/63 batches, loss: 0.2689Epoch 3/15: [=================             ] 37/63 batches, loss: 0.2687Epoch 3/15: [==================            ] 38/63 batches, loss: 0.2687Epoch 3/15: [==================            ] 39/63 batches, loss: 0.2677Epoch 3/15: [===================           ] 40/63 batches, loss: 0.2688Epoch 3/15: [===================           ] 41/63 batches, loss: 0.2693Epoch 3/15: [====================          ] 42/63 batches, loss: 0.2688Epoch 3/15: [====================          ] 43/63 batches, loss: 0.2675Epoch 3/15: [====================          ] 44/63 batches, loss: 0.2657Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.2658Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.2638Epoch 3/15: [======================        ] 47/63 batches, loss: 0.2627Epoch 3/15: [======================        ] 48/63 batches, loss: 0.2607Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.2604Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.2586Epoch 3/15: [========================      ] 51/63 batches, loss: 0.2590Epoch 3/15: [========================      ] 52/63 batches, loss: 0.2583Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.2574Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.2551Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.2522Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.2528Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.2508Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.2514Epoch 3/15: [============================  ] 59/63 batches, loss: 0.2507Epoch 3/15: [============================  ] 60/63 batches, loss: 0.2499Epoch 3/15: [============================= ] 61/63 batches, loss: 0.2491Epoch 3/15: [============================= ] 62/63 batches, loss: 0.2487Epoch 3/15: [==============================] 63/63 batches, loss: 0.2466
[2025-05-02 12:54:12,174][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.2466
[2025-05-02 12:54:12,377][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.2180, Metrics: {'mse': 0.22033563256263733, 'rmse': 0.46939922514064436, 'r2': -2.3961050510406494}
[2025-05-02 12:54:12,378][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.1206Epoch 4/15: [                              ] 2/63 batches, loss: 0.1242Epoch 4/15: [=                             ] 3/63 batches, loss: 0.1405Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1701Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1747Epoch 4/15: [==                            ] 6/63 batches, loss: 0.2150Epoch 4/15: [===                           ] 7/63 batches, loss: 0.2290Epoch 4/15: [===                           ] 8/63 batches, loss: 0.2584Epoch 4/15: [====                          ] 9/63 batches, loss: 0.2750Epoch 4/15: [====                          ] 10/63 batches, loss: 0.2925Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.2841Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.2766Epoch 4/15: [======                        ] 13/63 batches, loss: 0.2689Epoch 4/15: [======                        ] 14/63 batches, loss: 0.2699Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.2672Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.2615Epoch 4/15: [========                      ] 17/63 batches, loss: 0.2532Epoch 4/15: [========                      ] 18/63 batches, loss: 0.2626Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.2568Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.2591Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.2529Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.2473Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.2471Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.2443Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.2441Epoch 4/15: [============                  ] 26/63 batches, loss: 0.2426Epoch 4/15: [============                  ] 27/63 batches, loss: 0.2403Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.2441Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.2413Epoch 4/15: [==============                ] 30/63 batches, loss: 0.2376Epoch 4/15: [==============                ] 31/63 batches, loss: 0.2345Epoch 4/15: [===============               ] 32/63 batches, loss: 0.2349Epoch 4/15: [===============               ] 33/63 batches, loss: 0.2340Epoch 4/15: [================              ] 34/63 batches, loss: 0.2296Epoch 4/15: [================              ] 35/63 batches, loss: 0.2283Epoch 4/15: [=================             ] 36/63 batches, loss: 0.2260Epoch 4/15: [=================             ] 37/63 batches, loss: 0.2297Epoch 4/15: [==================            ] 38/63 batches, loss: 0.2272Epoch 4/15: [==================            ] 39/63 batches, loss: 0.2331Epoch 4/15: [===================           ] 40/63 batches, loss: 0.2311Epoch 4/15: [===================           ] 41/63 batches, loss: 0.2304Epoch 4/15: [====================          ] 42/63 batches, loss: 0.2312Epoch 4/15: [====================          ] 43/63 batches, loss: 0.2277Epoch 4/15: [====================          ] 44/63 batches, loss: 0.2268Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.2283Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.2272Epoch 4/15: [======================        ] 47/63 batches, loss: 0.2254Epoch 4/15: [======================        ] 48/63 batches, loss: 0.2236Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.2241Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.2229Epoch 4/15: [========================      ] 51/63 batches, loss: 0.2239Epoch 4/15: [========================      ] 52/63 batches, loss: 0.2243Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.2219Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.2219Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.2213Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.2210Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.2190Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.2178Epoch 4/15: [============================  ] 59/63 batches, loss: 0.2168Epoch 4/15: [============================  ] 60/63 batches, loss: 0.2170Epoch 4/15: [============================= ] 61/63 batches, loss: 0.2158Epoch 4/15: [============================= ] 62/63 batches, loss: 0.2145Epoch 4/15: [==============================] 63/63 batches, loss: 0.2119
[2025-05-02 12:54:14,300][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.2119
[2025-05-02 12:54:14,506][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.2108, Metrics: {'mse': 0.21297067403793335, 'rmse': 0.46148745815886844, 'r2': -2.2825863361358643}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1302Epoch 5/15: [                              ] 2/63 batches, loss: 0.1390Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1398Epoch 5/15: [=                             ] 4/63 batches, loss: 0.1378Epoch 5/15: [==                            ] 5/63 batches, loss: 0.1599Epoch 5/15: [==                            ] 6/63 batches, loss: 0.1599Epoch 5/15: [===                           ] 7/63 batches, loss: 0.1549Epoch 5/15: [===                           ] 8/63 batches, loss: 0.1520Epoch 5/15: [====                          ] 9/63 batches, loss: 0.1504Epoch 5/15: [====                          ] 10/63 batches, loss: 0.1463Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.1707Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.1660Epoch 5/15: [======                        ] 13/63 batches, loss: 0.1708Epoch 5/15: [======                        ] 14/63 batches, loss: 0.1794Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.1743Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.1710Epoch 5/15: [========                      ] 17/63 batches, loss: 0.1736Epoch 5/15: [========                      ] 18/63 batches, loss: 0.1710Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.1742Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.1704Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.1767Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.1765Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.1769Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.1819Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.1800Epoch 5/15: [============                  ] 26/63 batches, loss: 0.1812Epoch 5/15: [============                  ] 27/63 batches, loss: 0.1834Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.1834Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.1816Epoch 5/15: [==============                ] 30/63 batches, loss: 0.1816Epoch 5/15: [==============                ] 31/63 batches, loss: 0.1831Epoch 5/15: [===============               ] 32/63 batches, loss: 0.1819Epoch 5/15: [===============               ] 33/63 batches, loss: 0.1816Epoch 5/15: [================              ] 34/63 batches, loss: 0.1794Epoch 5/15: [================              ] 35/63 batches, loss: 0.1792Epoch 5/15: [=================             ] 36/63 batches, loss: 0.1790Epoch 5/15: [=================             ] 37/63 batches, loss: 0.1779Epoch 5/15: [==================            ] 38/63 batches, loss: 0.1787Epoch 5/15: [==================            ] 39/63 batches, loss: 0.1786Epoch 5/15: [===================           ] 40/63 batches, loss: 0.1804Epoch 5/15: [===================           ] 41/63 batches, loss: 0.1827Epoch 5/15: [====================          ] 42/63 batches, loss: 0.1855Epoch 5/15: [====================          ] 43/63 batches, loss: 0.1881Epoch 5/15: [====================          ] 44/63 batches, loss: 0.1883Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.1886Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.1886Epoch 5/15: [======================        ] 47/63 batches, loss: 0.1880Epoch 5/15: [======================        ] 48/63 batches, loss: 0.1872Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.1874Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.1879Epoch 5/15: [========================      ] 51/63 batches, loss: 0.1897Epoch 5/15: [========================      ] 52/63 batches, loss: 0.1896Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.1928Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.1927Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.1927Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.1941Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.1945Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.1978Epoch 5/15: [============================  ] 59/63 batches, loss: 0.1984Epoch 5/15: [============================  ] 60/63 batches, loss: 0.1969Epoch 5/15: [============================= ] 61/63 batches, loss: 0.1966Epoch 5/15: [============================= ] 62/63 batches, loss: 0.1962Epoch 5/15: [==============================] 63/63 batches, loss: 0.1939
[2025-05-02 12:54:16,796][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1939
[2025-05-02 12:54:16,996][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1989, Metrics: {'mse': 0.20091336965560913, 'rmse': 0.44823361058226, 'r2': -2.096743106842041}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.2887Epoch 6/15: [                              ] 2/63 batches, loss: 0.2200Epoch 6/15: [=                             ] 3/63 batches, loss: 0.1989Epoch 6/15: [=                             ] 4/63 batches, loss: 0.1850Epoch 6/15: [==                            ] 5/63 batches, loss: 0.1635Epoch 6/15: [==                            ] 6/63 batches, loss: 0.1618Epoch 6/15: [===                           ] 7/63 batches, loss: 0.1632Epoch 6/15: [===                           ] 8/63 batches, loss: 0.1611Epoch 6/15: [====                          ] 9/63 batches, loss: 0.1685Epoch 6/15: [====                          ] 10/63 batches, loss: 0.1633Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.1823Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.1739Epoch 6/15: [======                        ] 13/63 batches, loss: 0.1870Epoch 6/15: [======                        ] 14/63 batches, loss: 0.1788Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.1788Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.1802Epoch 6/15: [========                      ] 17/63 batches, loss: 0.1841Epoch 6/15: [========                      ] 18/63 batches, loss: 0.1838Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.1832Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.1807Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.1843Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.1842Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.1809Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.1812Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.1805Epoch 6/15: [============                  ] 26/63 batches, loss: 0.1820Epoch 6/15: [============                  ] 27/63 batches, loss: 0.1918Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.1955Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.1969Epoch 6/15: [==============                ] 30/63 batches, loss: 0.2016Epoch 6/15: [==============                ] 31/63 batches, loss: 0.2026Epoch 6/15: [===============               ] 32/63 batches, loss: 0.2018Epoch 6/15: [===============               ] 33/63 batches, loss: 0.2008Epoch 6/15: [================              ] 34/63 batches, loss: 0.2012Epoch 6/15: [================              ] 35/63 batches, loss: 0.2010Epoch 6/15: [=================             ] 36/63 batches, loss: 0.2045Epoch 6/15: [=================             ] 37/63 batches, loss: 0.2015Epoch 6/15: [==================            ] 38/63 batches, loss: 0.2049Epoch 6/15: [==================            ] 39/63 batches, loss: 0.2038Epoch 6/15: [===================           ] 40/63 batches, loss: 0.2050Epoch 6/15: [===================           ] 41/63 batches, loss: 0.2036Epoch 6/15: [====================          ] 42/63 batches, loss: 0.2035Epoch 6/15: [====================          ] 43/63 batches, loss: 0.2037Epoch 6/15: [====================          ] 44/63 batches, loss: 0.2014Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.2008Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.1985Epoch 6/15: [======================        ] 47/63 batches, loss: 0.2018Epoch 6/15: [======================        ] 48/63 batches, loss: 0.2033Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.2002Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.1986Epoch 6/15: [========================      ] 51/63 batches, loss: 0.2021Epoch 6/15: [========================      ] 52/63 batches, loss: 0.2011Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.2012Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.2000Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.2020Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.2052Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.2069Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.2062Epoch 6/15: [============================  ] 59/63 batches, loss: 0.2072Epoch 6/15: [============================  ] 60/63 batches, loss: 0.2086Epoch 6/15: [============================= ] 61/63 batches, loss: 0.2069Epoch 6/15: [============================= ] 62/63 batches, loss: 0.2059Epoch 6/15: [==============================] 63/63 batches, loss: 0.2042
[2025-05-02 12:54:19,273][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.2042
[2025-05-02 12:54:19,490][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1775, Metrics: {'mse': 0.17947602272033691, 'rmse': 0.4236461055177268, 'r2': -1.766322374343872}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.1067Epoch 7/15: [                              ] 2/63 batches, loss: 0.1163Epoch 7/15: [=                             ] 3/63 batches, loss: 0.1487Epoch 7/15: [=                             ] 4/63 batches, loss: 0.1804Epoch 7/15: [==                            ] 5/63 batches, loss: 0.1657Epoch 7/15: [==                            ] 6/63 batches, loss: 0.1677Epoch 7/15: [===                           ] 7/63 batches, loss: 0.1948Epoch 7/15: [===                           ] 8/63 batches, loss: 0.1970Epoch 7/15: [====                          ] 9/63 batches, loss: 0.1963Epoch 7/15: [====                          ] 10/63 batches, loss: 0.1883Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.1859Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.1845Epoch 7/15: [======                        ] 13/63 batches, loss: 0.1802Epoch 7/15: [======                        ] 14/63 batches, loss: 0.1840Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.1822Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.1788Epoch 7/15: [========                      ] 17/63 batches, loss: 0.1784Epoch 7/15: [========                      ] 18/63 batches, loss: 0.1832Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.1857Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.1917Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.1893Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.1847Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.1792Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.1785Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.1751Epoch 7/15: [============                  ] 26/63 batches, loss: 0.1716Epoch 7/15: [============                  ] 27/63 batches, loss: 0.1715Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.1736Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.1727Epoch 7/15: [==============                ] 30/63 batches, loss: 0.1716Epoch 7/15: [==============                ] 31/63 batches, loss: 0.1731Epoch 7/15: [===============               ] 32/63 batches, loss: 0.1753Epoch 7/15: [===============               ] 33/63 batches, loss: 0.1784Epoch 7/15: [================              ] 34/63 batches, loss: 0.1764Epoch 7/15: [================              ] 35/63 batches, loss: 0.1787Epoch 7/15: [=================             ] 36/63 batches, loss: 0.1803Epoch 7/15: [=================             ] 37/63 batches, loss: 0.1783Epoch 7/15: [==================            ] 38/63 batches, loss: 0.1781Epoch 7/15: [==================            ] 39/63 batches, loss: 0.1806Epoch 7/15: [===================           ] 40/63 batches, loss: 0.1800Epoch 7/15: [===================           ] 41/63 batches, loss: 0.1815Epoch 7/15: [====================          ] 42/63 batches, loss: 0.1844Epoch 7/15: [====================          ] 43/63 batches, loss: 0.1844Epoch 7/15: [====================          ] 44/63 batches, loss: 0.1855Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.1884Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.1866Epoch 7/15: [======================        ] 47/63 batches, loss: 0.1867Epoch 7/15: [======================        ] 48/63 batches, loss: 0.1855Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.1864Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.1868Epoch 7/15: [========================      ] 51/63 batches, loss: 0.1862Epoch 7/15: [========================      ] 52/63 batches, loss: 0.1861Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.1856Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.1843Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.1884Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.1895Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.1916Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.1924Epoch 7/15: [============================  ] 59/63 batches, loss: 0.1936Epoch 7/15: [============================  ] 60/63 batches, loss: 0.1938Epoch 7/15: [============================= ] 61/63 batches, loss: 0.1944Epoch 7/15: [============================= ] 62/63 batches, loss: 0.1927Epoch 7/15: [==============================] 63/63 batches, loss: 0.1912
[2025-05-02 12:54:21,800][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1912
[2025-05-02 12:54:21,996][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1735, Metrics: {'mse': 0.17510882019996643, 'rmse': 0.4184600580700223, 'r2': -1.6990094184875488}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0515Epoch 8/15: [                              ] 2/63 batches, loss: 0.1937Epoch 8/15: [=                             ] 3/63 batches, loss: 0.1960Epoch 8/15: [=                             ] 4/63 batches, loss: 0.1675Epoch 8/15: [==                            ] 5/63 batches, loss: 0.1700Epoch 8/15: [==                            ] 6/63 batches, loss: 0.1718Epoch 8/15: [===                           ] 7/63 batches, loss: 0.1853Epoch 8/15: [===                           ] 8/63 batches, loss: 0.1897Epoch 8/15: [====                          ] 9/63 batches, loss: 0.1992Epoch 8/15: [====                          ] 10/63 batches, loss: 0.2051Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.1973Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.1992Epoch 8/15: [======                        ] 13/63 batches, loss: 0.1989Epoch 8/15: [======                        ] 14/63 batches, loss: 0.1911Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.1888Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.1887Epoch 8/15: [========                      ] 17/63 batches, loss: 0.1898Epoch 8/15: [========                      ] 18/63 batches, loss: 0.1883Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.1863Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.1866Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.1891Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.1879Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.1897Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.1891Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.1856Epoch 8/15: [============                  ] 26/63 batches, loss: 0.1867Epoch 8/15: [============                  ] 27/63 batches, loss: 0.1868Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.1883Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.1854Epoch 8/15: [==============                ] 30/63 batches, loss: 0.1872Epoch 8/15: [==============                ] 31/63 batches, loss: 0.1897Epoch 8/15: [===============               ] 32/63 batches, loss: 0.1886Epoch 8/15: [===============               ] 33/63 batches, loss: 0.1874Epoch 8/15: [================              ] 34/63 batches, loss: 0.1922Epoch 8/15: [================              ] 35/63 batches, loss: 0.1883Epoch 8/15: [=================             ] 36/63 batches, loss: 0.1883Epoch 8/15: [=================             ] 37/63 batches, loss: 0.1896Epoch 8/15: [==================            ] 38/63 batches, loss: 0.1868Epoch 8/15: [==================            ] 39/63 batches, loss: 0.1858Epoch 8/15: [===================           ] 40/63 batches, loss: 0.1848Epoch 8/15: [===================           ] 41/63 batches, loss: 0.1862Epoch 8/15: [====================          ] 42/63 batches, loss: 0.1852Epoch 8/15: [====================          ] 43/63 batches, loss: 0.1865Epoch 8/15: [====================          ] 44/63 batches, loss: 0.1836Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.1815Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.1814Epoch 8/15: [======================        ] 47/63 batches, loss: 0.1799Epoch 8/15: [======================        ] 48/63 batches, loss: 0.1796Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.1784Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.1789Epoch 8/15: [========================      ] 51/63 batches, loss: 0.1793Epoch 8/15: [========================      ] 52/63 batches, loss: 0.1810Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.1793Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.1793Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.1779Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.1763Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.1769Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.1771Epoch 8/15: [============================  ] 59/63 batches, loss: 0.1764Epoch 8/15: [============================  ] 60/63 batches, loss: 0.1777Epoch 8/15: [============================= ] 61/63 batches, loss: 0.1766Epoch 8/15: [============================= ] 62/63 batches, loss: 0.1768Epoch 8/15: [==============================] 63/63 batches, loss: 0.1784
[2025-05-02 12:54:24,357][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.1784
[2025-05-02 12:54:24,569][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1700, Metrics: {'mse': 0.17137980461120605, 'rmse': 0.41398043988962335, 'r2': -1.6415326595306396}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1297Epoch 9/15: [                              ] 2/63 batches, loss: 0.1659Epoch 9/15: [=                             ] 3/63 batches, loss: 0.1300Epoch 9/15: [=                             ] 4/63 batches, loss: 0.1447Epoch 9/15: [==                            ] 5/63 batches, loss: 0.1486Epoch 9/15: [==                            ] 6/63 batches, loss: 0.1349Epoch 9/15: [===                           ] 7/63 batches, loss: 0.1595Epoch 9/15: [===                           ] 8/63 batches, loss: 0.1574Epoch 9/15: [====                          ] 9/63 batches, loss: 0.1600Epoch 9/15: [====                          ] 10/63 batches, loss: 0.1623Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.1558Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.1609Epoch 9/15: [======                        ] 13/63 batches, loss: 0.1593Epoch 9/15: [======                        ] 14/63 batches, loss: 0.1582Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.1568Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.1605Epoch 9/15: [========                      ] 17/63 batches, loss: 0.1586Epoch 9/15: [========                      ] 18/63 batches, loss: 0.1599Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.1593Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.1591Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.1583Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.1571Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.1583Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.1573Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.1544Epoch 9/15: [============                  ] 26/63 batches, loss: 0.1558Epoch 9/15: [============                  ] 27/63 batches, loss: 0.1536Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.1569Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.1572Epoch 9/15: [==============                ] 30/63 batches, loss: 0.1557Epoch 9/15: [==============                ] 31/63 batches, loss: 0.1654Epoch 9/15: [===============               ] 32/63 batches, loss: 0.1647Epoch 9/15: [===============               ] 33/63 batches, loss: 0.1625Epoch 9/15: [================              ] 34/63 batches, loss: 0.1624Epoch 9/15: [================              ] 35/63 batches, loss: 0.1599Epoch 9/15: [=================             ] 36/63 batches, loss: 0.1617Epoch 9/15: [=================             ] 37/63 batches, loss: 0.1593Epoch 9/15: [==================            ] 38/63 batches, loss: 0.1600Epoch 9/15: [==================            ] 39/63 batches, loss: 0.1605Epoch 9/15: [===================           ] 40/63 batches, loss: 0.1591Epoch 9/15: [===================           ] 41/63 batches, loss: 0.1580Epoch 9/15: [====================          ] 42/63 batches, loss: 0.1603Epoch 9/15: [====================          ] 43/63 batches, loss: 0.1602Epoch 9/15: [====================          ] 44/63 batches, loss: 0.1598Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.1596Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.1598Epoch 9/15: [======================        ] 47/63 batches, loss: 0.1596Epoch 9/15: [======================        ] 48/63 batches, loss: 0.1606Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.1596Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.1614Epoch 9/15: [========================      ] 51/63 batches, loss: 0.1605Epoch 9/15: [========================      ] 52/63 batches, loss: 0.1601Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.1585Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.1590Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.1591Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.1584Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.1604Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.1615Epoch 9/15: [============================  ] 59/63 batches, loss: 0.1635Epoch 9/15: [============================  ] 60/63 batches, loss: 0.1650Epoch 9/15: [============================= ] 61/63 batches, loss: 0.1652Epoch 9/15: [============================= ] 62/63 batches, loss: 0.1648Epoch 9/15: [==============================] 63/63 batches, loss: 0.1645
[2025-05-02 12:54:26,881][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.1645
[2025-05-02 12:54:27,100][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.1628, Metrics: {'mse': 0.16393378376960754, 'rmse': 0.4048873717092292, 'r2': -1.5267648696899414}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.1521Epoch 10/15: [                              ] 2/63 batches, loss: 0.1380Epoch 10/15: [=                             ] 3/63 batches, loss: 0.1249Epoch 10/15: [=                             ] 4/63 batches, loss: 0.1273Epoch 10/15: [==                            ] 5/63 batches, loss: 0.1284Epoch 10/15: [==                            ] 6/63 batches, loss: 0.1285Epoch 10/15: [===                           ] 7/63 batches, loss: 0.1408Epoch 10/15: [===                           ] 8/63 batches, loss: 0.1426Epoch 10/15: [====                          ] 9/63 batches, loss: 0.1396Epoch 10/15: [====                          ] 10/63 batches, loss: 0.1450Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.1450Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.1460Epoch 10/15: [======                        ] 13/63 batches, loss: 0.1452Epoch 10/15: [======                        ] 14/63 batches, loss: 0.1487Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.1477Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.1477Epoch 10/15: [========                      ] 17/63 batches, loss: 0.1419Epoch 10/15: [========                      ] 18/63 batches, loss: 0.1401Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.1398Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.1392Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.1422Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.1405Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.1403Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.1463Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.1469Epoch 10/15: [============                  ] 26/63 batches, loss: 0.1484Epoch 10/15: [============                  ] 27/63 batches, loss: 0.1479Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.1460Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.1494Epoch 10/15: [==============                ] 30/63 batches, loss: 0.1506Epoch 10/15: [==============                ] 31/63 batches, loss: 0.1501Epoch 10/15: [===============               ] 32/63 batches, loss: 0.1546Epoch 10/15: [===============               ] 33/63 batches, loss: 0.1571Epoch 10/15: [================              ] 34/63 batches, loss: 0.1577Epoch 10/15: [================              ] 35/63 batches, loss: 0.1587Epoch 10/15: [=================             ] 36/63 batches, loss: 0.1623Epoch 10/15: [=================             ] 37/63 batches, loss: 0.1636Epoch 10/15: [==================            ] 38/63 batches, loss: 0.1607Epoch 10/15: [==================            ] 39/63 batches, loss: 0.1611Epoch 10/15: [===================           ] 40/63 batches, loss: 0.1601Epoch 10/15: [===================           ] 41/63 batches, loss: 0.1581Epoch 10/15: [====================          ] 42/63 batches, loss: 0.1584Epoch 10/15: [====================          ] 43/63 batches, loss: 0.1580Epoch 10/15: [====================          ] 44/63 batches, loss: 0.1581Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.1596Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.1593Epoch 10/15: [======================        ] 47/63 batches, loss: 0.1612Epoch 10/15: [======================        ] 48/63 batches, loss: 0.1617Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.1623Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.1613Epoch 10/15: [========================      ] 51/63 batches, loss: 0.1618Epoch 10/15: [========================      ] 52/63 batches, loss: 0.1621Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.1623Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.1637Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.1643Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.1647Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.1641Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.1632Epoch 10/15: [============================  ] 59/63 batches, loss: 0.1641Epoch 10/15: [============================  ] 60/63 batches, loss: 0.1641Epoch 10/15: [============================= ] 61/63 batches, loss: 0.1648Epoch 10/15: [============================= ] 62/63 batches, loss: 0.1643Epoch 10/15: [==============================] 63/63 batches, loss: 0.1626
[2025-05-02 12:54:29,445][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.1626
[2025-05-02 12:54:29,658][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.1475, Metrics: {'mse': 0.1485442966222763, 'rmse': 0.38541444786395374, 'r2': -1.2895617485046387}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.1962Epoch 11/15: [                              ] 2/63 batches, loss: 0.2434Epoch 11/15: [=                             ] 3/63 batches, loss: 0.2192Epoch 11/15: [=                             ] 4/63 batches, loss: 0.2113Epoch 11/15: [==                            ] 5/63 batches, loss: 0.1843Epoch 11/15: [==                            ] 6/63 batches, loss: 0.1672Epoch 11/15: [===                           ] 7/63 batches, loss: 0.1601Epoch 11/15: [===                           ] 8/63 batches, loss: 0.1581Epoch 11/15: [====                          ] 9/63 batches, loss: 0.1643Epoch 11/15: [====                          ] 10/63 batches, loss: 0.1655Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.1635Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.1668Epoch 11/15: [======                        ] 13/63 batches, loss: 0.1763Epoch 11/15: [======                        ] 14/63 batches, loss: 0.1764Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.1723Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.1683Epoch 11/15: [========                      ] 17/63 batches, loss: 0.1653Epoch 11/15: [========                      ] 18/63 batches, loss: 0.1646Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.1623Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.1618Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.1611Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.1626Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.1642Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.1632Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.1626Epoch 11/15: [============                  ] 26/63 batches, loss: 0.1630Epoch 11/15: [============                  ] 27/63 batches, loss: 0.1647Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.1641Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.1619Epoch 11/15: [==============                ] 30/63 batches, loss: 0.1604Epoch 11/15: [==============                ] 31/63 batches, loss: 0.1628Epoch 11/15: [===============               ] 32/63 batches, loss: 0.1683Epoch 11/15: [===============               ] 33/63 batches, loss: 0.1692Epoch 11/15: [================              ] 34/63 batches, loss: 0.1668Epoch 11/15: [================              ] 35/63 batches, loss: 0.1669Epoch 11/15: [=================             ] 36/63 batches, loss: 0.1687Epoch 11/15: [=================             ] 37/63 batches, loss: 0.1694Epoch 11/15: [==================            ] 38/63 batches, loss: 0.1691Epoch 11/15: [==================            ] 39/63 batches, loss: 0.1688Epoch 11/15: [===================           ] 40/63 batches, loss: 0.1708Epoch 11/15: [===================           ] 41/63 batches, loss: 0.1702Epoch 11/15: [====================          ] 42/63 batches, loss: 0.1714Epoch 11/15: [====================          ] 43/63 batches, loss: 0.1708Epoch 11/15: [====================          ] 44/63 batches, loss: 0.1691Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.1688Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.1699Epoch 11/15: [======================        ] 47/63 batches, loss: 0.1702Epoch 11/15: [======================        ] 48/63 batches, loss: 0.1688Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.1678Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.1683Epoch 11/15: [========================      ] 51/63 batches, loss: 0.1668Epoch 11/15: [========================      ] 52/63 batches, loss: 0.1653Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.1653Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.1635Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.1623Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.1613Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.1602Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.1615Epoch 11/15: [============================  ] 59/63 batches, loss: 0.1614Epoch 11/15: [============================  ] 60/63 batches, loss: 0.1602Epoch 11/15: [============================= ] 61/63 batches, loss: 0.1597Epoch 11/15: [============================= ] 62/63 batches, loss: 0.1588Epoch 11/15: [==============================] 63/63 batches, loss: 0.1607
[2025-05-02 12:54:32,021][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.1607
[2025-05-02 12:54:32,234][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.1486, Metrics: {'mse': 0.14943687617778778, 'rmse': 0.38657066129982987, 'r2': -1.3033192157745361}
[2025-05-02 12:54:32,234][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.0805Epoch 12/15: [                              ] 2/63 batches, loss: 0.1183Epoch 12/15: [=                             ] 3/63 batches, loss: 0.1109Epoch 12/15: [=                             ] 4/63 batches, loss: 0.1181Epoch 12/15: [==                            ] 5/63 batches, loss: 0.1597Epoch 12/15: [==                            ] 6/63 batches, loss: 0.1533Epoch 12/15: [===                           ] 7/63 batches, loss: 0.1522Epoch 12/15: [===                           ] 8/63 batches, loss: 0.1505Epoch 12/15: [====                          ] 9/63 batches, loss: 0.1521Epoch 12/15: [====                          ] 10/63 batches, loss: 0.1531Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.1479Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.1484Epoch 12/15: [======                        ] 13/63 batches, loss: 0.1464Epoch 12/15: [======                        ] 14/63 batches, loss: 0.1519Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.1545Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.1583Epoch 12/15: [========                      ] 17/63 batches, loss: 0.1563Epoch 12/15: [========                      ] 18/63 batches, loss: 0.1552Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.1542Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.1528Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.1560Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.1552Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.1562Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.1565Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.1554Epoch 12/15: [============                  ] 26/63 batches, loss: 0.1525Epoch 12/15: [============                  ] 27/63 batches, loss: 0.1517Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.1489Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.1477Epoch 12/15: [==============                ] 30/63 batches, loss: 0.1492Epoch 12/15: [==============                ] 31/63 batches, loss: 0.1470Epoch 12/15: [===============               ] 32/63 batches, loss: 0.1476Epoch 12/15: [===============               ] 33/63 batches, loss: 0.1532Epoch 12/15: [================              ] 34/63 batches, loss: 0.1527Epoch 12/15: [================              ] 35/63 batches, loss: 0.1517Epoch 12/15: [=================             ] 36/63 batches, loss: 0.1497Epoch 12/15: [=================             ] 37/63 batches, loss: 0.1488Epoch 12/15: [==================            ] 38/63 batches, loss: 0.1471Epoch 12/15: [==================            ] 39/63 batches, loss: 0.1485Epoch 12/15: [===================           ] 40/63 batches, loss: 0.1470Epoch 12/15: [===================           ] 41/63 batches, loss: 0.1478Epoch 12/15: [====================          ] 42/63 batches, loss: 0.1476Epoch 12/15: [====================          ] 43/63 batches, loss: 0.1468Epoch 12/15: [====================          ] 44/63 batches, loss: 0.1470Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.1483Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.1478Epoch 12/15: [======================        ] 47/63 batches, loss: 0.1479Epoch 12/15: [======================        ] 48/63 batches, loss: 0.1464Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.1452Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.1459Epoch 12/15: [========================      ] 51/63 batches, loss: 0.1457Epoch 12/15: [========================      ] 52/63 batches, loss: 0.1453Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.1453Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.1470Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.1471Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.1479Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.1477Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.1466Epoch 12/15: [============================  ] 59/63 batches, loss: 0.1494Epoch 12/15: [============================  ] 60/63 batches, loss: 0.1485Epoch 12/15: [============================= ] 61/63 batches, loss: 0.1475Epoch 12/15: [============================= ] 62/63 batches, loss: 0.1474Epoch 12/15: [==============================] 63/63 batches, loss: 0.1484
[2025-05-02 12:54:34,179][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.1484
[2025-05-02 12:54:34,393][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.1403, Metrics: {'mse': 0.1410408616065979, 'rmse': 0.37555407281322073, 'r2': -1.1739089488983154}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.0890Epoch 13/15: [                              ] 2/63 batches, loss: 0.0942Epoch 13/15: [=                             ] 3/63 batches, loss: 0.0899Epoch 13/15: [=                             ] 4/63 batches, loss: 0.0980Epoch 13/15: [==                            ] 5/63 batches, loss: 0.0985Epoch 13/15: [==                            ] 6/63 batches, loss: 0.1012Epoch 13/15: [===                           ] 7/63 batches, loss: 0.1057Epoch 13/15: [===                           ] 8/63 batches, loss: 0.1088Epoch 13/15: [====                          ] 9/63 batches, loss: 0.1085Epoch 13/15: [====                          ] 10/63 batches, loss: 0.1080Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.1074Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.1095Epoch 13/15: [======                        ] 13/63 batches, loss: 0.1122Epoch 13/15: [======                        ] 14/63 batches, loss: 0.1210Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.1221Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.1331Epoch 13/15: [========                      ] 17/63 batches, loss: 0.1338Epoch 13/15: [========                      ] 18/63 batches, loss: 0.1306Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.1276Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.1292Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.1309Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.1325Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.1332Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.1310Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.1319Epoch 13/15: [============                  ] 26/63 batches, loss: 0.1338Epoch 13/15: [============                  ] 27/63 batches, loss: 0.1390Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.1393Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.1431Epoch 13/15: [==============                ] 30/63 batches, loss: 0.1421Epoch 13/15: [==============                ] 31/63 batches, loss: 0.1403Epoch 13/15: [===============               ] 32/63 batches, loss: 0.1410Epoch 13/15: [===============               ] 33/63 batches, loss: 0.1400Epoch 13/15: [================              ] 34/63 batches, loss: 0.1387Epoch 13/15: [================              ] 35/63 batches, loss: 0.1394Epoch 13/15: [=================             ] 36/63 batches, loss: 0.1375Epoch 13/15: [=================             ] 37/63 batches, loss: 0.1372Epoch 13/15: [==================            ] 38/63 batches, loss: 0.1361Epoch 13/15: [==================            ] 39/63 batches, loss: 0.1365Epoch 13/15: [===================           ] 40/63 batches, loss: 0.1395Epoch 13/15: [===================           ] 41/63 batches, loss: 0.1389Epoch 13/15: [====================          ] 42/63 batches, loss: 0.1398Epoch 13/15: [====================          ] 43/63 batches, loss: 0.1383Epoch 13/15: [====================          ] 44/63 batches, loss: 0.1373Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.1399Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.1398Epoch 13/15: [======================        ] 47/63 batches, loss: 0.1404Epoch 13/15: [======================        ] 48/63 batches, loss: 0.1408Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.1399Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.1394Epoch 13/15: [========================      ] 51/63 batches, loss: 0.1387Epoch 13/15: [========================      ] 52/63 batches, loss: 0.1400Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.1414Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.1412Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.1412Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.1412Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.1406Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.1396Epoch 13/15: [============================  ] 59/63 batches, loss: 0.1400Epoch 13/15: [============================  ] 60/63 batches, loss: 0.1400Epoch 13/15: [============================= ] 61/63 batches, loss: 0.1397Epoch 13/15: [============================= ] 62/63 batches, loss: 0.1393Epoch 13/15: [==============================] 63/63 batches, loss: 0.1371
[2025-05-02 12:54:36,746][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.1371
[2025-05-02 12:54:36,969][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.1305, Metrics: {'mse': 0.13111881911754608, 'rmse': 0.3621033265761944, 'r2': -1.0209770202636719}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.0597Epoch 14/15: [                              ] 2/63 batches, loss: 0.1046Epoch 14/15: [=                             ] 3/63 batches, loss: 0.0921Epoch 14/15: [=                             ] 4/63 batches, loss: 0.1107Epoch 14/15: [==                            ] 5/63 batches, loss: 0.1388Epoch 14/15: [==                            ] 6/63 batches, loss: 0.1416Epoch 14/15: [===                           ] 7/63 batches, loss: 0.1299Epoch 14/15: [===                           ] 8/63 batches, loss: 0.1221Epoch 14/15: [====                          ] 9/63 batches, loss: 0.1269Epoch 14/15: [====                          ] 10/63 batches, loss: 0.1288Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.1262Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.1298Epoch 14/15: [======                        ] 13/63 batches, loss: 0.1283Epoch 14/15: [======                        ] 14/63 batches, loss: 0.1302Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.1329Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.1346Epoch 14/15: [========                      ] 17/63 batches, loss: 0.1343Epoch 14/15: [========                      ] 18/63 batches, loss: 0.1351Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.1310Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.1374Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.1442Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.1423Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.1424Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.1388Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.1430Epoch 14/15: [============                  ] 26/63 batches, loss: 0.1415Epoch 14/15: [============                  ] 27/63 batches, loss: 0.1398Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.1390Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.1388Epoch 14/15: [==============                ] 30/63 batches, loss: 0.1381Epoch 14/15: [==============                ] 31/63 batches, loss: 0.1362Epoch 14/15: [===============               ] 32/63 batches, loss: 0.1354Epoch 14/15: [===============               ] 33/63 batches, loss: 0.1373Epoch 14/15: [================              ] 34/63 batches, loss: 0.1347Epoch 14/15: [================              ] 35/63 batches, loss: 0.1331Epoch 14/15: [=================             ] 36/63 batches, loss: 0.1316Epoch 14/15: [=================             ] 37/63 batches, loss: 0.1322Epoch 14/15: [==================            ] 38/63 batches, loss: 0.1314Epoch 14/15: [==================            ] 39/63 batches, loss: 0.1340Epoch 14/15: [===================           ] 40/63 batches, loss: 0.1344Epoch 14/15: [===================           ] 41/63 batches, loss: 0.1336Epoch 14/15: [====================          ] 42/63 batches, loss: 0.1332Epoch 14/15: [====================          ] 43/63 batches, loss: 0.1323Epoch 14/15: [====================          ] 44/63 batches, loss: 0.1322Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.1328Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.1330Epoch 14/15: [======================        ] 47/63 batches, loss: 0.1329Epoch 14/15: [======================        ] 48/63 batches, loss: 0.1324Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.1313Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.1318Epoch 14/15: [========================      ] 51/63 batches, loss: 0.1314Epoch 14/15: [========================      ] 52/63 batches, loss: 0.1309Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.1306Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.1306Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.1310Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.1311Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.1316Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.1315Epoch 14/15: [============================  ] 59/63 batches, loss: 0.1315Epoch 14/15: [============================  ] 60/63 batches, loss: 0.1302Epoch 14/15: [============================= ] 61/63 batches, loss: 0.1302Epoch 14/15: [============================= ] 62/63 batches, loss: 0.1291Epoch 14/15: [==============================] 63/63 batches, loss: 0.1293
[2025-05-02 12:54:39,365][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.1293
[2025-05-02 12:54:39,572][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.1232, Metrics: {'mse': 0.12370805442333221, 'rmse': 0.3517215580872634, 'r2': -0.9067525863647461}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.1266Epoch 15/15: [                              ] 2/63 batches, loss: 0.1014Epoch 15/15: [=                             ] 3/63 batches, loss: 0.0938Epoch 15/15: [=                             ] 4/63 batches, loss: 0.0941Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0981Epoch 15/15: [==                            ] 6/63 batches, loss: 0.1160Epoch 15/15: [===                           ] 7/63 batches, loss: 0.1136Epoch 15/15: [===                           ] 8/63 batches, loss: 0.1108Epoch 15/15: [====                          ] 9/63 batches, loss: 0.1177Epoch 15/15: [====                          ] 10/63 batches, loss: 0.1140Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.1185Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.1162Epoch 15/15: [======                        ] 13/63 batches, loss: 0.1172Epoch 15/15: [======                        ] 14/63 batches, loss: 0.1166Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.1123Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.1122Epoch 15/15: [========                      ] 17/63 batches, loss: 0.1128Epoch 15/15: [========                      ] 18/63 batches, loss: 0.1140Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.1112Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.1130Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.1126Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.1127Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.1126Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.1098Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.1091Epoch 15/15: [============                  ] 26/63 batches, loss: 0.1102Epoch 15/15: [============                  ] 27/63 batches, loss: 0.1107Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.1091Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.1098Epoch 15/15: [==============                ] 30/63 batches, loss: 0.1096Epoch 15/15: [==============                ] 31/63 batches, loss: 0.1108Epoch 15/15: [===============               ] 32/63 batches, loss: 0.1101Epoch 15/15: [===============               ] 33/63 batches, loss: 0.1097Epoch 15/15: [================              ] 34/63 batches, loss: 0.1131Epoch 15/15: [================              ] 35/63 batches, loss: 0.1120Epoch 15/15: [=================             ] 36/63 batches, loss: 0.1120Epoch 15/15: [=================             ] 37/63 batches, loss: 0.1139Epoch 15/15: [==================            ] 38/63 batches, loss: 0.1132Epoch 15/15: [==================            ] 39/63 batches, loss: 0.1127Epoch 15/15: [===================           ] 40/63 batches, loss: 0.1122Epoch 15/15: [===================           ] 41/63 batches, loss: 0.1110Epoch 15/15: [====================          ] 42/63 batches, loss: 0.1113Epoch 15/15: [====================          ] 43/63 batches, loss: 0.1115Epoch 15/15: [====================          ] 44/63 batches, loss: 0.1125Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.1124Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.1120Epoch 15/15: [======================        ] 47/63 batches, loss: 0.1128Epoch 15/15: [======================        ] 48/63 batches, loss: 0.1120Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.1121Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.1135Epoch 15/15: [========================      ] 51/63 batches, loss: 0.1138Epoch 15/15: [========================      ] 52/63 batches, loss: 0.1136Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.1126Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.1112Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.1117Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.1117Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.1116Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.1144Epoch 15/15: [============================  ] 59/63 batches, loss: 0.1132Epoch 15/15: [============================  ] 60/63 batches, loss: 0.1139Epoch 15/15: [============================= ] 61/63 batches, loss: 0.1140Epoch 15/15: [============================= ] 62/63 batches, loss: 0.1140Epoch 15/15: [==============================] 63/63 batches, loss: 0.1143
[2025-05-02 12:54:41,980][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.1143
[2025-05-02 12:54:42,198][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.1317, Metrics: {'mse': 0.13215674459934235, 'rmse': 0.3635336911475226, 'r2': -1.0369749069213867}
[2025-05-02 12:54:42,199][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-02 12:54:42,199][src.training.lm_trainer][INFO] - Training completed in 37.72 seconds
[2025-05-02 12:54:42,199][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 12:54:44,755][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.05703485384583473, 'rmse': 0.23881970991908255, 'r2': -0.8579723834991455}
[2025-05-02 12:54:44,755][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.12370805442333221, 'rmse': 0.3517215580872634, 'r2': -0.9067525863647461}
[2025-05-02 12:54:44,755][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.09976081550121307, 'rmse': 0.31584935570808603, 'r2': -0.7198320627212524}
[2025-05-02 12:54:46,440][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/complexity/layer11/ar/ar/model.pt
[2025-05-02 12:54:46,442][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▅▅▄▃▃▃▃▂▂▁▁
wandb:     best_val_mse █▅▄▄▃▃▃▃▂▂▁▁
wandb:      best_val_r2 ▁▄▅▅▆▆▆▆▇▇██
wandb:    best_val_rmse █▅▅▄▄▃▃▃▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▄▄▅▅▆▆▆▆▆▆▇▇
wandb:       train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▅▅▅▄▃▃▃▃▂▂▂▁▁▁
wandb:          val_mse █▅▅▄▄▃▃▃▃▂▂▂▁▁▁
wandb:           val_r2 ▁▄▄▅▅▆▆▆▆▇▇▇███
wandb:         val_rmse █▅▅▅▄▄▃▃▃▂▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.12319
wandb:     best_val_mse 0.12371
wandb:      best_val_r2 -0.90675
wandb:    best_val_rmse 0.35172
wandb:            epoch 15
wandb:   final_test_mse 0.09976
wandb:    final_test_r2 -0.71983
wandb:  final_test_rmse 0.31585
wandb:  final_train_mse 0.05703
wandb:   final_train_r2 -0.85797
wandb: final_train_rmse 0.23882
wandb:    final_val_mse 0.12371
wandb:     final_val_r2 -0.90675
wandb:   final_val_rmse 0.35172
wandb:    learning_rate 2e-05
wandb:       train_loss 0.11426
wandb:       train_time 37.72455
wandb:         val_loss 0.13173
wandb:          val_mse 0.13216
wandb:           val_r2 -1.03697
wandb:         val_rmse 0.36353
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125355-rzy0h7sy
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125355-rzy0h7sy/logs
Experiment probe_layer11_complexity_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/complexity/layer11/ar/ar/results.json for layer 11
Running control probing experiments...
=======================
PROBING LAYER 1 (CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 4 (CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 6 (CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 9 (CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 11 (CONTROL EXPERIMENTS)
=======================
Running submetric probing experiments...
=======================
PROBING LAYER 1 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer1_avg_links_len_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=1"         "model.probe_hidden_size=96" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer1_avg_links_len_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer1/ar"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 12:54:57,973][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer1/ar
experiment_name: probe_layer1_avg_links_len_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 1
  num_outputs: 1
  probe_hidden_size: 96
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-02 12:54:57,973][__main__][INFO] - Normalized task: single_submetric
[2025-05-02 12:54:57,973][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 12:54:57,973][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 12:54:57,973][__main__][INFO] - Determined Task Type: regression
[2025-05-02 12:54:57,978][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ar']
[2025-05-02 12:54:57,978][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 12:54:57,978][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 12:54:59,423][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 12:55:01,658][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 12:55:01,658][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:55:01,702][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:55:01,723][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:55:01,792][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 12:55:01,800][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:55:01,801][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 12:55:01,802][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:55:01,819][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:55:01,841][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:55:01,852][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 12:55:01,854][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:55:01,854][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 12:55:01,855][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:55:01,877][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:55:01,908][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:55:01,922][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 12:55:01,924][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:55:01,924][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 12:55:01,925][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 12:55:01,926][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 12:55:01,926][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 12:55:01,926][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 12:55:01,926][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 12:55:01,926][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8570
[2025-05-02 12:55:01,927][src.data.datasets][INFO] -   Mean: 0.1857, Std: 0.1452
[2025-05-02 12:55:01,927][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 12:55:01,927][src.data.datasets][INFO] - Sample label: 0.32100000977516174
[2025-05-02 12:55:01,927][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 12:55:01,927][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 12:55:01,927][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 12:55:01,927][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 12:55:01,927][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9290
[2025-05-02 12:55:01,927][src.data.datasets][INFO] -   Mean: 0.2504, Std: 0.2216
[2025-05-02 12:55:01,928][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 12:55:01,928][src.data.datasets][INFO] - Sample label: 0.10499999672174454
[2025-05-02 12:55:01,928][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 12:55:01,928][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 12:55:01,928][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 12:55:01,928][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 12:55:01,928][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:55:01,928][src.data.datasets][INFO] -   Mean: 0.3231, Std: 0.2152
[2025-05-02 12:55:01,928][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 12:55:01,928][src.data.datasets][INFO] - Sample label: 0.27799999713897705
[2025-05-02 12:55:01,929][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 12:55:01,929][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 12:55:01,929][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 12:55:01,929][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-02 12:55:01,929][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 12:55:05,937][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 12:55:05,938][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 12:55:05,938][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=1, freeze_model=True
[2025-05-02 12:55:05,939][src.models.model_factory][INFO] - Using provided probe_hidden_size: 96
[2025-05-02 12:55:05,941][src.models.model_factory][INFO] - Model has 84,961 trainable parameters out of 394,206,433 total parameters
[2025-05-02 12:55:05,941][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 84,961 trainable parameters
[2025-05-02 12:55:05,941][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=96, depth=2, activation=silu, normalization=layer
[2025-05-02 12:55:05,942][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 96 hidden size
[2025-05-02 12:55:05,942][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 12:55:05,943][__main__][INFO] - Total parameters: 394,206,433
[2025-05-02 12:55:05,943][__main__][INFO] - Trainable parameters: 84,961 (0.02%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 1.9622Epoch 1/15: [                              ] 2/63 batches, loss: 2.0799Epoch 1/15: [=                             ] 3/63 batches, loss: 2.2968Epoch 1/15: [=                             ] 4/63 batches, loss: 2.4903Epoch 1/15: [==                            ] 5/63 batches, loss: 2.5081Epoch 1/15: [==                            ] 6/63 batches, loss: 2.4329Epoch 1/15: [===                           ] 7/63 batches, loss: 2.3899Epoch 1/15: [===                           ] 8/63 batches, loss: 2.3242Epoch 1/15: [====                          ] 9/63 batches, loss: 2.2075Epoch 1/15: [====                          ] 10/63 batches, loss: 2.2255Epoch 1/15: [=====                         ] 11/63 batches, loss: 2.1891Epoch 1/15: [=====                         ] 12/63 batches, loss: 2.1477Epoch 1/15: [======                        ] 13/63 batches, loss: 2.0641Epoch 1/15: [======                        ] 14/63 batches, loss: 1.9951Epoch 1/15: [=======                       ] 15/63 batches, loss: 1.9321Epoch 1/15: [=======                       ] 16/63 batches, loss: 1.8874Epoch 1/15: [========                      ] 17/63 batches, loss: 1.8395Epoch 1/15: [========                      ] 18/63 batches, loss: 1.7858Epoch 1/15: [=========                     ] 19/63 batches, loss: 1.7392Epoch 1/15: [=========                     ] 20/63 batches, loss: 1.7007Epoch 1/15: [==========                    ] 21/63 batches, loss: 1.6493Epoch 1/15: [==========                    ] 22/63 batches, loss: 1.6027Epoch 1/15: [==========                    ] 23/63 batches, loss: 1.5799Epoch 1/15: [===========                   ] 24/63 batches, loss: 1.5312Epoch 1/15: [===========                   ] 25/63 batches, loss: 1.4956Epoch 1/15: [============                  ] 26/63 batches, loss: 1.4611Epoch 1/15: [============                  ] 27/63 batches, loss: 1.4337Epoch 1/15: [=============                 ] 28/63 batches, loss: 1.3990Epoch 1/15: [=============                 ] 29/63 batches, loss: 1.3677Epoch 1/15: [==============                ] 30/63 batches, loss: 1.3395Epoch 1/15: [==============                ] 31/63 batches, loss: 1.3122Epoch 1/15: [===============               ] 32/63 batches, loss: 1.2788Epoch 1/15: [===============               ] 33/63 batches, loss: 1.2600Epoch 1/15: [================              ] 34/63 batches, loss: 1.2311Epoch 1/15: [================              ] 35/63 batches, loss: 1.2203Epoch 1/15: [=================             ] 36/63 batches, loss: 1.1975Epoch 1/15: [=================             ] 37/63 batches, loss: 1.1761Epoch 1/15: [==================            ] 38/63 batches, loss: 1.1645Epoch 1/15: [==================            ] 39/63 batches, loss: 1.1554Epoch 1/15: [===================           ] 40/63 batches, loss: 1.1482Epoch 1/15: [===================           ] 41/63 batches, loss: 1.1336Epoch 1/15: [====================          ] 42/63 batches, loss: 1.1233Epoch 1/15: [====================          ] 43/63 batches, loss: 1.1015Epoch 1/15: [====================          ] 44/63 batches, loss: 1.0916Epoch 1/15: [=====================         ] 45/63 batches, loss: 1.0745Epoch 1/15: [=====================         ] 46/63 batches, loss: 1.0630Epoch 1/15: [======================        ] 47/63 batches, loss: 1.0518Epoch 1/15: [======================        ] 48/63 batches, loss: 1.0350Epoch 1/15: [=======================       ] 49/63 batches, loss: 1.0244Epoch 1/15: [=======================       ] 50/63 batches, loss: 1.0169Epoch 1/15: [========================      ] 51/63 batches, loss: 1.0052Epoch 1/15: [========================      ] 52/63 batches, loss: 0.9930Epoch 1/15: [=========================     ] 53/63 batches, loss: 0.9841Epoch 1/15: [=========================     ] 54/63 batches, loss: 0.9770Epoch 1/15: [==========================    ] 55/63 batches, loss: 0.9686Epoch 1/15: [==========================    ] 56/63 batches, loss: 0.9611Epoch 1/15: [===========================   ] 57/63 batches, loss: 0.9557Epoch 1/15: [===========================   ] 58/63 batches, loss: 0.9447Epoch 1/15: [============================  ] 59/63 batches, loss: 0.9334Epoch 1/15: [============================  ] 60/63 batches, loss: 0.9251Epoch 1/15: [============================= ] 61/63 batches, loss: 0.9158Epoch 1/15: [============================= ] 62/63 batches, loss: 0.9080Epoch 1/15: [==============================] 63/63 batches, loss: 0.9015
[2025-05-02 12:55:10,567][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 0.9015
[2025-05-02 12:55:10,756][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.3663, Metrics: {'mse': 0.3490062654018402, 'rmse': 0.590767522297765, 'r2': -6.110246658325195}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.3686Epoch 2/15: [                              ] 2/63 batches, loss: 0.3358Epoch 2/15: [=                             ] 3/63 batches, loss: 0.3356Epoch 2/15: [=                             ] 4/63 batches, loss: 0.3678Epoch 2/15: [==                            ] 5/63 batches, loss: 0.3872Epoch 2/15: [==                            ] 6/63 batches, loss: 0.3920Epoch 2/15: [===                           ] 7/63 batches, loss: 0.3710Epoch 2/15: [===                           ] 8/63 batches, loss: 0.3925Epoch 2/15: [====                          ] 9/63 batches, loss: 0.4035Epoch 2/15: [====                          ] 10/63 batches, loss: 0.4402Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.4428Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.4563Epoch 2/15: [======                        ] 13/63 batches, loss: 0.4661Epoch 2/15: [======                        ] 14/63 batches, loss: 0.4688Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.4690Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.4661Epoch 2/15: [========                      ] 17/63 batches, loss: 0.4604Epoch 2/15: [========                      ] 18/63 batches, loss: 0.4606Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.4665Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.4789Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.4684Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.4678Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.4666Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.4653Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.4634Epoch 2/15: [============                  ] 26/63 batches, loss: 0.4637Epoch 2/15: [============                  ] 27/63 batches, loss: 0.4618Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.4621Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.4539Epoch 2/15: [==============                ] 30/63 batches, loss: 0.4612Epoch 2/15: [==============                ] 31/63 batches, loss: 0.4643Epoch 2/15: [===============               ] 32/63 batches, loss: 0.4772Epoch 2/15: [===============               ] 33/63 batches, loss: 0.4773Epoch 2/15: [================              ] 34/63 batches, loss: 0.4752Epoch 2/15: [================              ] 35/63 batches, loss: 0.4711Epoch 2/15: [=================             ] 36/63 batches, loss: 0.4686Epoch 2/15: [=================             ] 37/63 batches, loss: 0.4648Epoch 2/15: [==================            ] 38/63 batches, loss: 0.4648Epoch 2/15: [==================            ] 39/63 batches, loss: 0.4629Epoch 2/15: [===================           ] 40/63 batches, loss: 0.4617Epoch 2/15: [===================           ] 41/63 batches, loss: 0.4611Epoch 2/15: [====================          ] 42/63 batches, loss: 0.4535Epoch 2/15: [====================          ] 43/63 batches, loss: 0.4523Epoch 2/15: [====================          ] 44/63 batches, loss: 0.4522Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.4467Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.4472Epoch 2/15: [======================        ] 47/63 batches, loss: 0.4452Epoch 2/15: [======================        ] 48/63 batches, loss: 0.4469Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.4464Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.4452Epoch 2/15: [========================      ] 51/63 batches, loss: 0.4422Epoch 2/15: [========================      ] 52/63 batches, loss: 0.4407Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.4390Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.4353Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.4348Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.4353Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.4349Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.4344Epoch 2/15: [============================  ] 59/63 batches, loss: 0.4316Epoch 2/15: [============================  ] 60/63 batches, loss: 0.4276Epoch 2/15: [============================= ] 61/63 batches, loss: 0.4250Epoch 2/15: [============================= ] 62/63 batches, loss: 0.4227Epoch 2/15: [==============================] 63/63 batches, loss: 0.4184
[2025-05-02 12:55:13,089][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.4184
[2025-05-02 12:55:13,282][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.2904, Metrics: {'mse': 0.27489280700683594, 'rmse': 0.5243022096146801, 'r2': -4.600345611572266}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.3559Epoch 3/15: [                              ] 2/63 batches, loss: 0.3825Epoch 3/15: [=                             ] 3/63 batches, loss: 0.3634Epoch 3/15: [=                             ] 4/63 batches, loss: 0.3646Epoch 3/15: [==                            ] 5/63 batches, loss: 0.3766Epoch 3/15: [==                            ] 6/63 batches, loss: 0.4089Epoch 3/15: [===                           ] 7/63 batches, loss: 0.4101Epoch 3/15: [===                           ] 8/63 batches, loss: 0.4048Epoch 3/15: [====                          ] 9/63 batches, loss: 0.4027Epoch 3/15: [====                          ] 10/63 batches, loss: 0.4369Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.4438Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.4517Epoch 3/15: [======                        ] 13/63 batches, loss: 0.4310Epoch 3/15: [======                        ] 14/63 batches, loss: 0.4217Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.4323Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.4593Epoch 3/15: [========                      ] 17/63 batches, loss: 0.4658Epoch 3/15: [========                      ] 18/63 batches, loss: 0.4595Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.4573Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.4635Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.4724Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.4669Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.4525Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.4427Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.4353Epoch 3/15: [============                  ] 26/63 batches, loss: 0.4339Epoch 3/15: [============                  ] 27/63 batches, loss: 0.4326Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.4252Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.4178Epoch 3/15: [==============                ] 30/63 batches, loss: 0.4152Epoch 3/15: [==============                ] 31/63 batches, loss: 0.4120Epoch 3/15: [===============               ] 32/63 batches, loss: 0.4126Epoch 3/15: [===============               ] 33/63 batches, loss: 0.4196Epoch 3/15: [================              ] 34/63 batches, loss: 0.4208Epoch 3/15: [================              ] 35/63 batches, loss: 0.4135Epoch 3/15: [=================             ] 36/63 batches, loss: 0.4106Epoch 3/15: [=================             ] 37/63 batches, loss: 0.4071Epoch 3/15: [==================            ] 38/63 batches, loss: 0.4069Epoch 3/15: [==================            ] 39/63 batches, loss: 0.4032Epoch 3/15: [===================           ] 40/63 batches, loss: 0.3977Epoch 3/15: [===================           ] 41/63 batches, loss: 0.3967Epoch 3/15: [====================          ] 42/63 batches, loss: 0.3912Epoch 3/15: [====================          ] 43/63 batches, loss: 0.3933Epoch 3/15: [====================          ] 44/63 batches, loss: 0.3870Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.3820Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.3792Epoch 3/15: [======================        ] 47/63 batches, loss: 0.3776Epoch 3/15: [======================        ] 48/63 batches, loss: 0.3732Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.3759Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.3743Epoch 3/15: [========================      ] 51/63 batches, loss: 0.3721Epoch 3/15: [========================      ] 52/63 batches, loss: 0.3696Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.3698Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.3724Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.3683Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.3656Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.3679Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.3672Epoch 3/15: [============================  ] 59/63 batches, loss: 0.3653Epoch 3/15: [============================  ] 60/63 batches, loss: 0.3633Epoch 3/15: [============================= ] 61/63 batches, loss: 0.3621Epoch 3/15: [============================= ] 62/63 batches, loss: 0.3597Epoch 3/15: [==============================] 63/63 batches, loss: 0.3567
[2025-05-02 12:55:15,627][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.3567
[2025-05-02 12:55:15,832][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.2395, Metrics: {'mse': 0.22513757646083832, 'rmse': 0.4744866451870256, 'r2': -3.5866904258728027}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.2925Epoch 4/15: [                              ] 2/63 batches, loss: 0.3221Epoch 4/15: [=                             ] 3/63 batches, loss: 0.3409Epoch 4/15: [=                             ] 4/63 batches, loss: 0.3238Epoch 4/15: [==                            ] 5/63 batches, loss: 0.3291Epoch 4/15: [==                            ] 6/63 batches, loss: 0.3367Epoch 4/15: [===                           ] 7/63 batches, loss: 0.3455Epoch 4/15: [===                           ] 8/63 batches, loss: 0.3481Epoch 4/15: [====                          ] 9/63 batches, loss: 0.3390Epoch 4/15: [====                          ] 10/63 batches, loss: 0.3306Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.3158Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.3040Epoch 4/15: [======                        ] 13/63 batches, loss: 0.2961Epoch 4/15: [======                        ] 14/63 batches, loss: 0.3017Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.3043Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.2981Epoch 4/15: [========                      ] 17/63 batches, loss: 0.2953Epoch 4/15: [========                      ] 18/63 batches, loss: 0.2956Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.2890Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.2864Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.2828Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.2794Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.2770Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.2790Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.2834Epoch 4/15: [============                  ] 26/63 batches, loss: 0.2916Epoch 4/15: [============                  ] 27/63 batches, loss: 0.2861Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.2919Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.2999Epoch 4/15: [==============                ] 30/63 batches, loss: 0.2995Epoch 4/15: [==============                ] 31/63 batches, loss: 0.2972Epoch 4/15: [===============               ] 32/63 batches, loss: 0.2958Epoch 4/15: [===============               ] 33/63 batches, loss: 0.2945Epoch 4/15: [================              ] 34/63 batches, loss: 0.2988Epoch 4/15: [================              ] 35/63 batches, loss: 0.2984Epoch 4/15: [=================             ] 36/63 batches, loss: 0.2987Epoch 4/15: [=================             ] 37/63 batches, loss: 0.2961Epoch 4/15: [==================            ] 38/63 batches, loss: 0.2953Epoch 4/15: [==================            ] 39/63 batches, loss: 0.2977Epoch 4/15: [===================           ] 40/63 batches, loss: 0.3006Epoch 4/15: [===================           ] 41/63 batches, loss: 0.3020Epoch 4/15: [====================          ] 42/63 batches, loss: 0.2998Epoch 4/15: [====================          ] 43/63 batches, loss: 0.2947Epoch 4/15: [====================          ] 44/63 batches, loss: 0.2979Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.2980Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.2996Epoch 4/15: [======================        ] 47/63 batches, loss: 0.3043Epoch 4/15: [======================        ] 48/63 batches, loss: 0.3068Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.3040Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.3050Epoch 4/15: [========================      ] 51/63 batches, loss: 0.3068Epoch 4/15: [========================      ] 52/63 batches, loss: 0.3075Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.3079Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.3071Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.3059Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.3054Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.3085Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.3075Epoch 4/15: [============================  ] 59/63 batches, loss: 0.3068Epoch 4/15: [============================  ] 60/63 batches, loss: 0.3037Epoch 4/15: [============================= ] 61/63 batches, loss: 0.3020Epoch 4/15: [============================= ] 62/63 batches, loss: 0.3008Epoch 4/15: [==============================] 63/63 batches, loss: 0.3021
[2025-05-02 12:55:18,116][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.3021
[2025-05-02 12:55:18,328][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1757, Metrics: {'mse': 0.16376341879367828, 'rmse': 0.40467693138314453, 'r2': -2.33632493019104}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.3143Epoch 5/15: [                              ] 2/63 batches, loss: 0.2874Epoch 5/15: [=                             ] 3/63 batches, loss: 0.2544Epoch 5/15: [=                             ] 4/63 batches, loss: 0.2430Epoch 5/15: [==                            ] 5/63 batches, loss: 0.2344Epoch 5/15: [==                            ] 6/63 batches, loss: 0.2649Epoch 5/15: [===                           ] 7/63 batches, loss: 0.2779Epoch 5/15: [===                           ] 8/63 batches, loss: 0.2593Epoch 5/15: [====                          ] 9/63 batches, loss: 0.2523Epoch 5/15: [====                          ] 10/63 batches, loss: 0.2558Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.2514Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.2481Epoch 5/15: [======                        ] 13/63 batches, loss: 0.2564Epoch 5/15: [======                        ] 14/63 batches, loss: 0.2614Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.2666Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.2643Epoch 5/15: [========                      ] 17/63 batches, loss: 0.2617Epoch 5/15: [========                      ] 18/63 batches, loss: 0.2910Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.2856Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.2900Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.2865Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.2834Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.2860Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.2852Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.2815Epoch 5/15: [============                  ] 26/63 batches, loss: 0.2794Epoch 5/15: [============                  ] 27/63 batches, loss: 0.2791Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.2794Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.2761Epoch 5/15: [==============                ] 30/63 batches, loss: 0.2772Epoch 5/15: [==============                ] 31/63 batches, loss: 0.2808Epoch 5/15: [===============               ] 32/63 batches, loss: 0.2809Epoch 5/15: [===============               ] 33/63 batches, loss: 0.2807Epoch 5/15: [================              ] 34/63 batches, loss: 0.2850Epoch 5/15: [================              ] 35/63 batches, loss: 0.2863Epoch 5/15: [=================             ] 36/63 batches, loss: 0.2904Epoch 5/15: [=================             ] 37/63 batches, loss: 0.2904Epoch 5/15: [==================            ] 38/63 batches, loss: 0.2868Epoch 5/15: [==================            ] 39/63 batches, loss: 0.2863Epoch 5/15: [===================           ] 40/63 batches, loss: 0.2876Epoch 5/15: [===================           ] 41/63 batches, loss: 0.2908Epoch 5/15: [====================          ] 42/63 batches, loss: 0.2889Epoch 5/15: [====================          ] 43/63 batches, loss: 0.2906Epoch 5/15: [====================          ] 44/63 batches, loss: 0.2868Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.2844Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.2812Epoch 5/15: [======================        ] 47/63 batches, loss: 0.2802Epoch 5/15: [======================        ] 48/63 batches, loss: 0.2805Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.2770Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.2752Epoch 5/15: [========================      ] 51/63 batches, loss: 0.2778Epoch 5/15: [========================      ] 52/63 batches, loss: 0.2758Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.2771Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.2789Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.2793Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.2770Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.2769Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.2750Epoch 5/15: [============================  ] 59/63 batches, loss: 0.2721Epoch 5/15: [============================  ] 60/63 batches, loss: 0.2695Epoch 5/15: [============================= ] 61/63 batches, loss: 0.2740Epoch 5/15: [============================= ] 62/63 batches, loss: 0.2717Epoch 5/15: [==============================] 63/63 batches, loss: 0.2675
[2025-05-02 12:55:20,621][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.2675
[2025-05-02 12:55:20,830][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1521, Metrics: {'mse': 0.14068974554538727, 'rmse': 0.37508631745957793, 'r2': -1.8662488460540771}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.2595Epoch 6/15: [                              ] 2/63 batches, loss: 0.2643Epoch 6/15: [=                             ] 3/63 batches, loss: 0.2674Epoch 6/15: [=                             ] 4/63 batches, loss: 0.2995Epoch 6/15: [==                            ] 5/63 batches, loss: 0.2781Epoch 6/15: [==                            ] 6/63 batches, loss: 0.2467Epoch 6/15: [===                           ] 7/63 batches, loss: 0.2347Epoch 6/15: [===                           ] 8/63 batches, loss: 0.2369Epoch 6/15: [====                          ] 9/63 batches, loss: 0.2413Epoch 6/15: [====                          ] 10/63 batches, loss: 0.2392Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.2379Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.2415Epoch 6/15: [======                        ] 13/63 batches, loss: 0.2611Epoch 6/15: [======                        ] 14/63 batches, loss: 0.2625Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.2526Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.2473Epoch 6/15: [========                      ] 17/63 batches, loss: 0.2462Epoch 6/15: [========                      ] 18/63 batches, loss: 0.2448Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.2446Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.2426Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.2499Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.2461Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.2472Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.2435Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.2489Epoch 6/15: [============                  ] 26/63 batches, loss: 0.2440Epoch 6/15: [============                  ] 27/63 batches, loss: 0.2430Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.2449Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.2420Epoch 6/15: [==============                ] 30/63 batches, loss: 0.2477Epoch 6/15: [==============                ] 31/63 batches, loss: 0.2470Epoch 6/15: [===============               ] 32/63 batches, loss: 0.2461Epoch 6/15: [===============               ] 33/63 batches, loss: 0.2463Epoch 6/15: [================              ] 34/63 batches, loss: 0.2433Epoch 6/15: [================              ] 35/63 batches, loss: 0.2527Epoch 6/15: [=================             ] 36/63 batches, loss: 0.2505Epoch 6/15: [=================             ] 37/63 batches, loss: 0.2493Epoch 6/15: [==================            ] 38/63 batches, loss: 0.2487Epoch 6/15: [==================            ] 39/63 batches, loss: 0.2457Epoch 6/15: [===================           ] 40/63 batches, loss: 0.2472Epoch 6/15: [===================           ] 41/63 batches, loss: 0.2469Epoch 6/15: [====================          ] 42/63 batches, loss: 0.2500Epoch 6/15: [====================          ] 43/63 batches, loss: 0.2497Epoch 6/15: [====================          ] 44/63 batches, loss: 0.2488Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.2487Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.2449Epoch 6/15: [======================        ] 47/63 batches, loss: 0.2446Epoch 6/15: [======================        ] 48/63 batches, loss: 0.2436Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.2435Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.2480Epoch 6/15: [========================      ] 51/63 batches, loss: 0.2472Epoch 6/15: [========================      ] 52/63 batches, loss: 0.2479Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.2479Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.2472Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.2462Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.2473Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.2463Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.2453Epoch 6/15: [============================  ] 59/63 batches, loss: 0.2452Epoch 6/15: [============================  ] 60/63 batches, loss: 0.2467Epoch 6/15: [============================= ] 61/63 batches, loss: 0.2458Epoch 6/15: [============================= ] 62/63 batches, loss: 0.2434Epoch 6/15: [==============================] 63/63 batches, loss: 0.2439
[2025-05-02 12:55:23,151][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.2439
[2025-05-02 12:55:23,361][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1142, Metrics: {'mse': 0.10513272881507874, 'rmse': 0.3242417752466186, 'r2': -1.1418516635894775}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.1636Epoch 7/15: [                              ] 2/63 batches, loss: 0.1928Epoch 7/15: [=                             ] 3/63 batches, loss: 0.2273Epoch 7/15: [=                             ] 4/63 batches, loss: 0.2898Epoch 7/15: [==                            ] 5/63 batches, loss: 0.2622Epoch 7/15: [==                            ] 6/63 batches, loss: 0.2362Epoch 7/15: [===                           ] 7/63 batches, loss: 0.2275Epoch 7/15: [===                           ] 8/63 batches, loss: 0.2167Epoch 7/15: [====                          ] 9/63 batches, loss: 0.2057Epoch 7/15: [====                          ] 10/63 batches, loss: 0.1983Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.2110Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.2026Epoch 7/15: [======                        ] 13/63 batches, loss: 0.1995Epoch 7/15: [======                        ] 14/63 batches, loss: 0.2019Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.2006Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.1967Epoch 7/15: [========                      ] 17/63 batches, loss: 0.1937Epoch 7/15: [========                      ] 18/63 batches, loss: 0.1962Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.1943Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.1985Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.2008Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.2013Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.2005Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.1996Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.1956Epoch 7/15: [============                  ] 26/63 batches, loss: 0.1942Epoch 7/15: [============                  ] 27/63 batches, loss: 0.1936Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.1930Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.1894Epoch 7/15: [==============                ] 30/63 batches, loss: 0.1884Epoch 7/15: [==============                ] 31/63 batches, loss: 0.1861Epoch 7/15: [===============               ] 32/63 batches, loss: 0.1876Epoch 7/15: [===============               ] 33/63 batches, loss: 0.1856Epoch 7/15: [================              ] 34/63 batches, loss: 0.1850Epoch 7/15: [================              ] 35/63 batches, loss: 0.1881Epoch 7/15: [=================             ] 36/63 batches, loss: 0.1904Epoch 7/15: [=================             ] 37/63 batches, loss: 0.1908Epoch 7/15: [==================            ] 38/63 batches, loss: 0.1924Epoch 7/15: [==================            ] 39/63 batches, loss: 0.1959Epoch 7/15: [===================           ] 40/63 batches, loss: 0.1971Epoch 7/15: [===================           ] 41/63 batches, loss: 0.2007Epoch 7/15: [====================          ] 42/63 batches, loss: 0.2012Epoch 7/15: [====================          ] 43/63 batches, loss: 0.2014Epoch 7/15: [====================          ] 44/63 batches, loss: 0.2044Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.2027Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.2034Epoch 7/15: [======================        ] 47/63 batches, loss: 0.2026Epoch 7/15: [======================        ] 48/63 batches, loss: 0.2030Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.2018Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.2013Epoch 7/15: [========================      ] 51/63 batches, loss: 0.1994Epoch 7/15: [========================      ] 52/63 batches, loss: 0.1989Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.1992Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.1977Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.2024Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.2037Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.2040Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.2031Epoch 7/15: [============================  ] 59/63 batches, loss: 0.2018Epoch 7/15: [============================  ] 60/63 batches, loss: 0.2021Epoch 7/15: [============================= ] 61/63 batches, loss: 0.2020Epoch 7/15: [============================= ] 62/63 batches, loss: 0.2024Epoch 7/15: [==============================] 63/63 batches, loss: 0.1996
[2025-05-02 12:55:25,678][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1996
[2025-05-02 12:55:25,879][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1140, Metrics: {'mse': 0.10481467097997665, 'rmse': 0.3237509397360518, 'r2': -1.1353719234466553}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.1760Epoch 8/15: [                              ] 2/63 batches, loss: 0.1489Epoch 8/15: [=                             ] 3/63 batches, loss: 0.1921Epoch 8/15: [=                             ] 4/63 batches, loss: 0.2021Epoch 8/15: [==                            ] 5/63 batches, loss: 0.1850Epoch 8/15: [==                            ] 6/63 batches, loss: 0.1871Epoch 8/15: [===                           ] 7/63 batches, loss: 0.1823Epoch 8/15: [===                           ] 8/63 batches, loss: 0.1743Epoch 8/15: [====                          ] 9/63 batches, loss: 0.1787Epoch 8/15: [====                          ] 10/63 batches, loss: 0.1769Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.1719Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.1746Epoch 8/15: [======                        ] 13/63 batches, loss: 0.1724Epoch 8/15: [======                        ] 14/63 batches, loss: 0.1679Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.1687Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.1719Epoch 8/15: [========                      ] 17/63 batches, loss: 0.1701Epoch 8/15: [========                      ] 18/63 batches, loss: 0.1670Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.1650Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.1693Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.1686Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.1652Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.1664Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.1652Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.1674Epoch 8/15: [============                  ] 26/63 batches, loss: 0.1658Epoch 8/15: [============                  ] 27/63 batches, loss: 0.1693Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.1756Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.1736Epoch 8/15: [==============                ] 30/63 batches, loss: 0.1696Epoch 8/15: [==============                ] 31/63 batches, loss: 0.1684Epoch 8/15: [===============               ] 32/63 batches, loss: 0.1678Epoch 8/15: [===============               ] 33/63 batches, loss: 0.1693Epoch 8/15: [================              ] 34/63 batches, loss: 0.1671Epoch 8/15: [================              ] 35/63 batches, loss: 0.1684Epoch 8/15: [=================             ] 36/63 batches, loss: 0.1700Epoch 8/15: [=================             ] 37/63 batches, loss: 0.1682Epoch 8/15: [==================            ] 38/63 batches, loss: 0.1667Epoch 8/15: [==================            ] 39/63 batches, loss: 0.1670Epoch 8/15: [===================           ] 40/63 batches, loss: 0.1689Epoch 8/15: [===================           ] 41/63 batches, loss: 0.1697Epoch 8/15: [====================          ] 42/63 batches, loss: 0.1690Epoch 8/15: [====================          ] 43/63 batches, loss: 0.1692Epoch 8/15: [====================          ] 44/63 batches, loss: 0.1681Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.1675Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.1677Epoch 8/15: [======================        ] 47/63 batches, loss: 0.1670Epoch 8/15: [======================        ] 48/63 batches, loss: 0.1670Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.1669Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.1691Epoch 8/15: [========================      ] 51/63 batches, loss: 0.1676Epoch 8/15: [========================      ] 52/63 batches, loss: 0.1672Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.1682Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.1688Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.1674Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.1664Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.1663Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.1654Epoch 8/15: [============================  ] 59/63 batches, loss: 0.1642Epoch 8/15: [============================  ] 60/63 batches, loss: 0.1654Epoch 8/15: [============================= ] 61/63 batches, loss: 0.1648Epoch 8/15: [============================= ] 62/63 batches, loss: 0.1651Epoch 8/15: [==============================] 63/63 batches, loss: 0.1683
[2025-05-02 12:55:28,242][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.1683
[2025-05-02 12:55:28,455][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1073, Metrics: {'mse': 0.09859199821949005, 'rmse': 0.3139936276733814, 'r2': -1.0085985660552979}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1620Epoch 9/15: [                              ] 2/63 batches, loss: 0.1488Epoch 9/15: [=                             ] 3/63 batches, loss: 0.1345Epoch 9/15: [=                             ] 4/63 batches, loss: 0.1335Epoch 9/15: [==                            ] 5/63 batches, loss: 0.1439Epoch 9/15: [==                            ] 6/63 batches, loss: 0.1355Epoch 9/15: [===                           ] 7/63 batches, loss: 0.1512Epoch 9/15: [===                           ] 8/63 batches, loss: 0.1467Epoch 9/15: [====                          ] 9/63 batches, loss: 0.1546Epoch 9/15: [====                          ] 10/63 batches, loss: 0.1577Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.1672Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.1664Epoch 9/15: [======                        ] 13/63 batches, loss: 0.1684Epoch 9/15: [======                        ] 14/63 batches, loss: 0.1742Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.1719Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.1679Epoch 9/15: [========                      ] 17/63 batches, loss: 0.1643Epoch 9/15: [========                      ] 18/63 batches, loss: 0.1628Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.1620Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.1675Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.1648Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.1623Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.1609Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.1605Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.1576Epoch 9/15: [============                  ] 26/63 batches, loss: 0.1576Epoch 9/15: [============                  ] 27/63 batches, loss: 0.1572Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.1541Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.1540Epoch 9/15: [==============                ] 30/63 batches, loss: 0.1548Epoch 9/15: [==============                ] 31/63 batches, loss: 0.1558Epoch 9/15: [===============               ] 32/63 batches, loss: 0.1551Epoch 9/15: [===============               ] 33/63 batches, loss: 0.1558Epoch 9/15: [================              ] 34/63 batches, loss: 0.1543Epoch 9/15: [================              ] 35/63 batches, loss: 0.1546Epoch 9/15: [=================             ] 36/63 batches, loss: 0.1543Epoch 9/15: [=================             ] 37/63 batches, loss: 0.1534Epoch 9/15: [==================            ] 38/63 batches, loss: 0.1548Epoch 9/15: [==================            ] 39/63 batches, loss: 0.1543Epoch 9/15: [===================           ] 40/63 batches, loss: 0.1537Epoch 9/15: [===================           ] 41/63 batches, loss: 0.1548Epoch 9/15: [====================          ] 42/63 batches, loss: 0.1525Epoch 9/15: [====================          ] 43/63 batches, loss: 0.1536Epoch 9/15: [====================          ] 44/63 batches, loss: 0.1531Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.1526Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.1524Epoch 9/15: [======================        ] 47/63 batches, loss: 0.1523Epoch 9/15: [======================        ] 48/63 batches, loss: 0.1534Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.1534Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.1533Epoch 9/15: [========================      ] 51/63 batches, loss: 0.1527Epoch 9/15: [========================      ] 52/63 batches, loss: 0.1517Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.1510Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.1514Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.1512Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.1511Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.1524Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.1515Epoch 9/15: [============================  ] 59/63 batches, loss: 0.1516Epoch 9/15: [============================  ] 60/63 batches, loss: 0.1530Epoch 9/15: [============================= ] 61/63 batches, loss: 0.1518Epoch 9/15: [============================= ] 62/63 batches, loss: 0.1516Epoch 9/15: [==============================] 63/63 batches, loss: 0.1501
[2025-05-02 12:55:30,790][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.1501
[2025-05-02 12:55:31,011][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0971, Metrics: {'mse': 0.08903899788856506, 'rmse': 0.29839403125492486, 'r2': -0.8139767646789551}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.1833Epoch 10/15: [                              ] 2/63 batches, loss: 0.1279Epoch 10/15: [=                             ] 3/63 batches, loss: 0.1298Epoch 10/15: [=                             ] 4/63 batches, loss: 0.1546Epoch 10/15: [==                            ] 5/63 batches, loss: 0.1706Epoch 10/15: [==                            ] 6/63 batches, loss: 0.1809Epoch 10/15: [===                           ] 7/63 batches, loss: 0.1840Epoch 10/15: [===                           ] 8/63 batches, loss: 0.1784Epoch 10/15: [====                          ] 9/63 batches, loss: 0.1775Epoch 10/15: [====                          ] 10/63 batches, loss: 0.1739Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.1727Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.1724Epoch 10/15: [======                        ] 13/63 batches, loss: 0.1694Epoch 10/15: [======                        ] 14/63 batches, loss: 0.1644Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.1653Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.1659Epoch 10/15: [========                      ] 17/63 batches, loss: 0.1609Epoch 10/15: [========                      ] 18/63 batches, loss: 0.1579Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.1546Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.1540Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.1557Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.1526Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.1497Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.1558Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.1536Epoch 10/15: [============                  ] 26/63 batches, loss: 0.1544Epoch 10/15: [============                  ] 27/63 batches, loss: 0.1588Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.1580Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.1570Epoch 10/15: [==============                ] 30/63 batches, loss: 0.1545Epoch 10/15: [==============                ] 31/63 batches, loss: 0.1576Epoch 10/15: [===============               ] 32/63 batches, loss: 0.1573Epoch 10/15: [===============               ] 33/63 batches, loss: 0.1564Epoch 10/15: [================              ] 34/63 batches, loss: 0.1572Epoch 10/15: [================              ] 35/63 batches, loss: 0.1570Epoch 10/15: [=================             ] 36/63 batches, loss: 0.1578Epoch 10/15: [=================             ] 37/63 batches, loss: 0.1557Epoch 10/15: [==================            ] 38/63 batches, loss: 0.1535Epoch 10/15: [==================            ] 39/63 batches, loss: 0.1542Epoch 10/15: [===================           ] 40/63 batches, loss: 0.1551Epoch 10/15: [===================           ] 41/63 batches, loss: 0.1548Epoch 10/15: [====================          ] 42/63 batches, loss: 0.1551Epoch 10/15: [====================          ] 43/63 batches, loss: 0.1548Epoch 10/15: [====================          ] 44/63 batches, loss: 0.1535Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.1515Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.1504Epoch 10/15: [======================        ] 47/63 batches, loss: 0.1485Epoch 10/15: [======================        ] 48/63 batches, loss: 0.1467Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.1484Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.1481Epoch 10/15: [========================      ] 51/63 batches, loss: 0.1478Epoch 10/15: [========================      ] 52/63 batches, loss: 0.1474Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.1458Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.1457Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.1467Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.1468Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.1461Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.1449Epoch 10/15: [============================  ] 59/63 batches, loss: 0.1454Epoch 10/15: [============================  ] 60/63 batches, loss: 0.1471Epoch 10/15: [============================= ] 61/63 batches, loss: 0.1465Epoch 10/15: [============================= ] 62/63 batches, loss: 0.1467Epoch 10/15: [==============================] 63/63 batches, loss: 0.1449
[2025-05-02 12:55:33,333][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.1449
[2025-05-02 12:55:33,555][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0906, Metrics: {'mse': 0.08313657343387604, 'rmse': 0.2883341350479961, 'r2': -0.6937276124954224}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.1739Epoch 11/15: [                              ] 2/63 batches, loss: 0.2066Epoch 11/15: [=                             ] 3/63 batches, loss: 0.1728Epoch 11/15: [=                             ] 4/63 batches, loss: 0.1620Epoch 11/15: [==                            ] 5/63 batches, loss: 0.1566Epoch 11/15: [==                            ] 6/63 batches, loss: 0.1585Epoch 11/15: [===                           ] 7/63 batches, loss: 0.1530Epoch 11/15: [===                           ] 8/63 batches, loss: 0.1456Epoch 11/15: [====                          ] 9/63 batches, loss: 0.1392Epoch 11/15: [====                          ] 10/63 batches, loss: 0.1357Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.1308Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.1374Epoch 11/15: [======                        ] 13/63 batches, loss: 0.1394Epoch 11/15: [======                        ] 14/63 batches, loss: 0.1458Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.1391Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.1356Epoch 11/15: [========                      ] 17/63 batches, loss: 0.1357Epoch 11/15: [========                      ] 18/63 batches, loss: 0.1360Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.1338Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.1322Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.1318Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.1344Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.1363Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.1385Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.1346Epoch 11/15: [============                  ] 26/63 batches, loss: 0.1330Epoch 11/15: [============                  ] 27/63 batches, loss: 0.1339Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.1341Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.1342Epoch 11/15: [==============                ] 30/63 batches, loss: 0.1342Epoch 11/15: [==============                ] 31/63 batches, loss: 0.1330Epoch 11/15: [===============               ] 32/63 batches, loss: 0.1322Epoch 11/15: [===============               ] 33/63 batches, loss: 0.1308Epoch 11/15: [================              ] 34/63 batches, loss: 0.1297Epoch 11/15: [================              ] 35/63 batches, loss: 0.1288Epoch 11/15: [=================             ] 36/63 batches, loss: 0.1270Epoch 11/15: [=================             ] 37/63 batches, loss: 0.1281Epoch 11/15: [==================            ] 38/63 batches, loss: 0.1268Epoch 11/15: [==================            ] 39/63 batches, loss: 0.1284Epoch 11/15: [===================           ] 40/63 batches, loss: 0.1293Epoch 11/15: [===================           ] 41/63 batches, loss: 0.1294Epoch 11/15: [====================          ] 42/63 batches, loss: 0.1285Epoch 11/15: [====================          ] 43/63 batches, loss: 0.1283Epoch 11/15: [====================          ] 44/63 batches, loss: 0.1280Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.1297Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.1309Epoch 11/15: [======================        ] 47/63 batches, loss: 0.1342Epoch 11/15: [======================        ] 48/63 batches, loss: 0.1338Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.1339Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.1345Epoch 11/15: [========================      ] 51/63 batches, loss: 0.1334Epoch 11/15: [========================      ] 52/63 batches, loss: 0.1317Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.1340Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.1348Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.1346Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.1332Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.1340Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.1343Epoch 11/15: [============================  ] 59/63 batches, loss: 0.1338Epoch 11/15: [============================  ] 60/63 batches, loss: 0.1348Epoch 11/15: [============================= ] 61/63 batches, loss: 0.1345Epoch 11/15: [============================= ] 62/63 batches, loss: 0.1338Epoch 11/15: [==============================] 63/63 batches, loss: 0.1327
[2025-05-02 12:55:35,897][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.1327
[2025-05-02 12:55:36,127][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0776, Metrics: {'mse': 0.0709252804517746, 'rmse': 0.2663180062477462, 'r2': -0.4449489116668701}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.1015Epoch 12/15: [                              ] 2/63 batches, loss: 0.1262Epoch 12/15: [=                             ] 3/63 batches, loss: 0.1207Epoch 12/15: [=                             ] 4/63 batches, loss: 0.1179Epoch 12/15: [==                            ] 5/63 batches, loss: 0.1071Epoch 12/15: [==                            ] 6/63 batches, loss: 0.1268Epoch 12/15: [===                           ] 7/63 batches, loss: 0.1188Epoch 12/15: [===                           ] 8/63 batches, loss: 0.1208Epoch 12/15: [====                          ] 9/63 batches, loss: 0.1186Epoch 12/15: [====                          ] 10/63 batches, loss: 0.1137Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.1150Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.1181Epoch 12/15: [======                        ] 13/63 batches, loss: 0.1195Epoch 12/15: [======                        ] 14/63 batches, loss: 0.1200Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.1246Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.1223Epoch 12/15: [========                      ] 17/63 batches, loss: 0.1209Epoch 12/15: [========                      ] 18/63 batches, loss: 0.1206Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.1243Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.1249Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.1258Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.1252Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.1259Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.1226Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.1228Epoch 12/15: [============                  ] 26/63 batches, loss: 0.1212Epoch 12/15: [============                  ] 27/63 batches, loss: 0.1194Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.1200Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.1194Epoch 12/15: [==============                ] 30/63 batches, loss: 0.1183Epoch 12/15: [==============                ] 31/63 batches, loss: 0.1188Epoch 12/15: [===============               ] 32/63 batches, loss: 0.1205Epoch 12/15: [===============               ] 33/63 batches, loss: 0.1210Epoch 12/15: [================              ] 34/63 batches, loss: 0.1202Epoch 12/15: [================              ] 35/63 batches, loss: 0.1203Epoch 12/15: [=================             ] 36/63 batches, loss: 0.1200Epoch 12/15: [=================             ] 37/63 batches, loss: 0.1222Epoch 12/15: [==================            ] 38/63 batches, loss: 0.1199Epoch 12/15: [==================            ] 39/63 batches, loss: 0.1216Epoch 12/15: [===================           ] 40/63 batches, loss: 0.1217Epoch 12/15: [===================           ] 41/63 batches, loss: 0.1218Epoch 12/15: [====================          ] 42/63 batches, loss: 0.1198Epoch 12/15: [====================          ] 43/63 batches, loss: 0.1190Epoch 12/15: [====================          ] 44/63 batches, loss: 0.1184Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.1190Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.1175Epoch 12/15: [======================        ] 47/63 batches, loss: 0.1174Epoch 12/15: [======================        ] 48/63 batches, loss: 0.1160Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.1149Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.1155Epoch 12/15: [========================      ] 51/63 batches, loss: 0.1176Epoch 12/15: [========================      ] 52/63 batches, loss: 0.1175Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.1172Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.1183Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.1178Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.1186Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.1180Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.1167Epoch 12/15: [============================  ] 59/63 batches, loss: 0.1166Epoch 12/15: [============================  ] 60/63 batches, loss: 0.1162Epoch 12/15: [============================= ] 61/63 batches, loss: 0.1161Epoch 12/15: [============================= ] 62/63 batches, loss: 0.1152Epoch 12/15: [==============================] 63/63 batches, loss: 0.1156
[2025-05-02 12:55:38,508][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.1156
[2025-05-02 12:55:38,731][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0794, Metrics: {'mse': 0.07284338772296906, 'rmse': 0.2698951420884953, 'r2': -0.48402631282806396}
[2025-05-02 12:55:38,732][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.1261Epoch 13/15: [                              ] 2/63 batches, loss: 0.1377Epoch 13/15: [=                             ] 3/63 batches, loss: 0.1331Epoch 13/15: [=                             ] 4/63 batches, loss: 0.1200Epoch 13/15: [==                            ] 5/63 batches, loss: 0.1183Epoch 13/15: [==                            ] 6/63 batches, loss: 0.1120Epoch 13/15: [===                           ] 7/63 batches, loss: 0.1189Epoch 13/15: [===                           ] 8/63 batches, loss: 0.1159Epoch 13/15: [====                          ] 9/63 batches, loss: 0.1128Epoch 13/15: [====                          ] 10/63 batches, loss: 0.1059Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.1058Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.1051Epoch 13/15: [======                        ] 13/63 batches, loss: 0.1105Epoch 13/15: [======                        ] 14/63 batches, loss: 0.1096Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.1110Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.1092Epoch 13/15: [========                      ] 17/63 batches, loss: 0.1086Epoch 13/15: [========                      ] 18/63 batches, loss: 0.1092Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.1122Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.1166Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.1126Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.1118Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.1109Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.1119Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.1104Epoch 13/15: [============                  ] 26/63 batches, loss: 0.1112Epoch 13/15: [============                  ] 27/63 batches, loss: 0.1115Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.1116Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.1105Epoch 13/15: [==============                ] 30/63 batches, loss: 0.1090Epoch 13/15: [==============                ] 31/63 batches, loss: 0.1108Epoch 13/15: [===============               ] 32/63 batches, loss: 0.1103Epoch 13/15: [===============               ] 33/63 batches, loss: 0.1087Epoch 13/15: [================              ] 34/63 batches, loss: 0.1069Epoch 13/15: [================              ] 35/63 batches, loss: 0.1082Epoch 13/15: [=================             ] 36/63 batches, loss: 0.1065Epoch 13/15: [=================             ] 37/63 batches, loss: 0.1066Epoch 13/15: [==================            ] 38/63 batches, loss: 0.1054Epoch 13/15: [==================            ] 39/63 batches, loss: 0.1041Epoch 13/15: [===================           ] 40/63 batches, loss: 0.1040Epoch 13/15: [===================           ] 41/63 batches, loss: 0.1058Epoch 13/15: [====================          ] 42/63 batches, loss: 0.1058Epoch 13/15: [====================          ] 43/63 batches, loss: 0.1052Epoch 13/15: [====================          ] 44/63 batches, loss: 0.1044Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.1052Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.1049Epoch 13/15: [======================        ] 47/63 batches, loss: 0.1058Epoch 13/15: [======================        ] 48/63 batches, loss: 0.1056Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.1049Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.1041Epoch 13/15: [========================      ] 51/63 batches, loss: 0.1035Epoch 13/15: [========================      ] 52/63 batches, loss: 0.1032Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.1026Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.1028Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.1032Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.1036Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.1031Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.1034Epoch 13/15: [============================  ] 59/63 batches, loss: 0.1035Epoch 13/15: [============================  ] 60/63 batches, loss: 0.1030Epoch 13/15: [============================= ] 61/63 batches, loss: 0.1032Epoch 13/15: [============================= ] 62/63 batches, loss: 0.1035Epoch 13/15: [==============================] 63/63 batches, loss: 0.1020
[2025-05-02 12:55:40,676][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.1020
[2025-05-02 12:55:40,898][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0702, Metrics: {'mse': 0.06417182087898254, 'rmse': 0.25332157602340655, 'r2': -0.3073618412017822}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.1048Epoch 14/15: [                              ] 2/63 batches, loss: 0.1162Epoch 14/15: [=                             ] 3/63 batches, loss: 0.1008Epoch 14/15: [=                             ] 4/63 batches, loss: 0.1062Epoch 14/15: [==                            ] 5/63 batches, loss: 0.0986Epoch 14/15: [==                            ] 6/63 batches, loss: 0.1084Epoch 14/15: [===                           ] 7/63 batches, loss: 0.1013Epoch 14/15: [===                           ] 8/63 batches, loss: 0.1025Epoch 14/15: [====                          ] 9/63 batches, loss: 0.1058Epoch 14/15: [====                          ] 10/63 batches, loss: 0.1074Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.1043Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.0990Epoch 14/15: [======                        ] 13/63 batches, loss: 0.0973Epoch 14/15: [======                        ] 14/63 batches, loss: 0.0965Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.0986Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.0972Epoch 14/15: [========                      ] 17/63 batches, loss: 0.0959Epoch 14/15: [========                      ] 18/63 batches, loss: 0.0983Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.0988Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.0981Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.0982Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.0972Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.0980Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.0955Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.0968Epoch 14/15: [============                  ] 26/63 batches, loss: 0.0954Epoch 14/15: [============                  ] 27/63 batches, loss: 0.0955Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.0950Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.0956Epoch 14/15: [==============                ] 30/63 batches, loss: 0.0952Epoch 14/15: [==============                ] 31/63 batches, loss: 0.0970Epoch 14/15: [===============               ] 32/63 batches, loss: 0.0964Epoch 14/15: [===============               ] 33/63 batches, loss: 0.0943Epoch 14/15: [================              ] 34/63 batches, loss: 0.0941Epoch 14/15: [================              ] 35/63 batches, loss: 0.0935Epoch 14/15: [=================             ] 36/63 batches, loss: 0.0933Epoch 14/15: [=================             ] 37/63 batches, loss: 0.0925Epoch 14/15: [==================            ] 38/63 batches, loss: 0.0918Epoch 14/15: [==================            ] 39/63 batches, loss: 0.0912Epoch 14/15: [===================           ] 40/63 batches, loss: 0.0926Epoch 14/15: [===================           ] 41/63 batches, loss: 0.0918Epoch 14/15: [====================          ] 42/63 batches, loss: 0.0923Epoch 14/15: [====================          ] 43/63 batches, loss: 0.0939Epoch 14/15: [====================          ] 44/63 batches, loss: 0.0931Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.0932Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.0928Epoch 14/15: [======================        ] 47/63 batches, loss: 0.0927Epoch 14/15: [======================        ] 48/63 batches, loss: 0.0937Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.0925Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.0929Epoch 14/15: [========================      ] 51/63 batches, loss: 0.0945Epoch 14/15: [========================      ] 52/63 batches, loss: 0.0936Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.0935Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.0937Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.0939Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.0937Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.0931Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.0941Epoch 14/15: [============================  ] 59/63 batches, loss: 0.0939Epoch 14/15: [============================  ] 60/63 batches, loss: 0.0929Epoch 14/15: [============================= ] 61/63 batches, loss: 0.0929Epoch 14/15: [============================= ] 62/63 batches, loss: 0.0926Epoch 14/15: [==============================] 63/63 batches, loss: 0.0920
[2025-05-02 12:55:43,224][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.0920
[2025-05-02 12:55:43,429][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0704, Metrics: {'mse': 0.06463909149169922, 'rmse': 0.25424219062087083, 'r2': -0.31688153743743896}
[2025-05-02 12:55:43,429][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.1338Epoch 15/15: [                              ] 2/63 batches, loss: 0.1385Epoch 15/15: [=                             ] 3/63 batches, loss: 0.1276Epoch 15/15: [=                             ] 4/63 batches, loss: 0.1102Epoch 15/15: [==                            ] 5/63 batches, loss: 0.1010Epoch 15/15: [==                            ] 6/63 batches, loss: 0.0985Epoch 15/15: [===                           ] 7/63 batches, loss: 0.0937Epoch 15/15: [===                           ] 8/63 batches, loss: 0.0889Epoch 15/15: [====                          ] 9/63 batches, loss: 0.0838Epoch 15/15: [====                          ] 10/63 batches, loss: 0.0885Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.0875Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.0865Epoch 15/15: [======                        ] 13/63 batches, loss: 0.0852Epoch 15/15: [======                        ] 14/63 batches, loss: 0.0874Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.0904Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.0910Epoch 15/15: [========                      ] 17/63 batches, loss: 0.0921Epoch 15/15: [========                      ] 18/63 batches, loss: 0.0932Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.0948Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.0958Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.0941Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.0914Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.0893Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.0902Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.0895Epoch 15/15: [============                  ] 26/63 batches, loss: 0.0878Epoch 15/15: [============                  ] 27/63 batches, loss: 0.0877Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.0880Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.0880Epoch 15/15: [==============                ] 30/63 batches, loss: 0.0885Epoch 15/15: [==============                ] 31/63 batches, loss: 0.0879Epoch 15/15: [===============               ] 32/63 batches, loss: 0.0879Epoch 15/15: [===============               ] 33/63 batches, loss: 0.0883Epoch 15/15: [================              ] 34/63 batches, loss: 0.0901Epoch 15/15: [================              ] 35/63 batches, loss: 0.0897Epoch 15/15: [=================             ] 36/63 batches, loss: 0.0893Epoch 15/15: [=================             ] 37/63 batches, loss: 0.0889Epoch 15/15: [==================            ] 38/63 batches, loss: 0.0884Epoch 15/15: [==================            ] 39/63 batches, loss: 0.0883Epoch 15/15: [===================           ] 40/63 batches, loss: 0.0888Epoch 15/15: [===================           ] 41/63 batches, loss: 0.0887Epoch 15/15: [====================          ] 42/63 batches, loss: 0.0883Epoch 15/15: [====================          ] 43/63 batches, loss: 0.0892Epoch 15/15: [====================          ] 44/63 batches, loss: 0.0886Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.0874Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.0871Epoch 15/15: [======================        ] 47/63 batches, loss: 0.0880Epoch 15/15: [======================        ] 48/63 batches, loss: 0.0872Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.0871Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.0875Epoch 15/15: [========================      ] 51/63 batches, loss: 0.0881Epoch 15/15: [========================      ] 52/63 batches, loss: 0.0884Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.0890Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.0893Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.0894Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.0901Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.0895Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.0909Epoch 15/15: [============================  ] 59/63 batches, loss: 0.0902Epoch 15/15: [============================  ] 60/63 batches, loss: 0.0908Epoch 15/15: [============================= ] 61/63 batches, loss: 0.0907Epoch 15/15: [============================= ] 62/63 batches, loss: 0.0909Epoch 15/15: [==============================] 63/63 batches, loss: 0.0908
[2025-05-02 12:55:45,358][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0908
[2025-05-02 12:55:45,561][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0679, Metrics: {'mse': 0.062471017241477966, 'rmse': 0.24994202776139504, 'r2': -0.2727116346359253}
[2025-05-02 12:55:46,011][src.training.lm_trainer][INFO] - Training completed in 38.27 seconds
[2025-05-02 12:55:46,011][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 12:55:48,516][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.05098531022667885, 'rmse': 0.22579926976560144, 'r2': -1.4181532859802246}
[2025-05-02 12:55:48,517][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.062471017241477966, 'rmse': 0.24994202776139504, 'r2': -0.2727116346359253}
[2025-05-02 12:55:48,517][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.05309218913316727, 'rmse': 0.2304174236753099, 'r2': -0.14632701873779297}
[2025-05-02 12:55:50,206][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer1/ar/ar/model.pt
[2025-05-02 12:55:50,207][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▆▅▄▃▂▂▂▂▂▁▁▁
wandb:     best_val_mse █▆▅▃▃▂▂▂▂▂▁▁▁
wandb:      best_val_r2 ▁▃▄▆▆▇▇▇▇▇███
wandb:    best_val_rmse █▇▆▄▄▃▃▂▂▂▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▄▅▆▇▇▇▇██████
wandb:       train_loss █▄▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▆▅▄▃▂▂▂▂▂▁▁▁▁▁
wandb:          val_mse █▆▅▃▃▂▂▂▂▂▁▁▁▁▁
wandb:           val_r2 ▁▃▄▆▆▇▇▇▇▇█████
wandb:         val_rmse █▇▆▄▄▃▃▂▂▂▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06789
wandb:     best_val_mse 0.06247
wandb:      best_val_r2 -0.27271
wandb:    best_val_rmse 0.24994
wandb:            epoch 15
wandb:   final_test_mse 0.05309
wandb:    final_test_r2 -0.14633
wandb:  final_test_rmse 0.23042
wandb:  final_train_mse 0.05099
wandb:   final_train_r2 -1.41815
wandb: final_train_rmse 0.2258
wandb:    final_val_mse 0.06247
wandb:     final_val_r2 -0.27271
wandb:   final_val_rmse 0.24994
wandb:    learning_rate 2e-05
wandb:       train_loss 0.09077
wandb:       train_time 38.27424
wandb:         val_loss 0.06789
wandb:          val_mse 0.06247
wandb:           val_r2 -0.27271
wandb:         val_rmse 0.24994
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125458-6voj418s
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125458-6voj418s/logs
Experiment probe_layer1_avg_links_len_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer1/ar/ar/results.json for layer 1
=======================
PROBING LAYER 4 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer4_avg_links_len_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=4"         "model.probe_hidden_size=96" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer4_avg_links_len_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer4/ar"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 12:56:01,694][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer4/ar
experiment_name: probe_layer4_avg_links_len_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 4
  num_outputs: 1
  probe_hidden_size: 96
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-02 12:56:01,694][__main__][INFO] - Normalized task: single_submetric
[2025-05-02 12:56:01,694][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 12:56:01,694][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 12:56:01,694][__main__][INFO] - Determined Task Type: regression
[2025-05-02 12:56:01,699][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ar']
[2025-05-02 12:56:01,699][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 12:56:01,699][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 12:56:03,225][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 12:56:05,436][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 12:56:05,436][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:56:05,480][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:56:05,508][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:56:05,595][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 12:56:05,602][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:56:05,602][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 12:56:05,603][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:56:05,621][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:56:05,649][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:56:05,662][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 12:56:05,663][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:56:05,663][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 12:56:05,664][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:56:05,680][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:56:05,710][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:56:05,723][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 12:56:05,724][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:56:05,724][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 12:56:05,725][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 12:56:05,725][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 12:56:05,725][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 12:56:05,725][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 12:56:05,725][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 12:56:05,725][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8570
[2025-05-02 12:56:05,726][src.data.datasets][INFO] -   Mean: 0.1857, Std: 0.1452
[2025-05-02 12:56:05,726][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 12:56:05,726][src.data.datasets][INFO] - Sample label: 0.32100000977516174
[2025-05-02 12:56:05,726][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 12:56:05,726][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 12:56:05,726][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 12:56:05,726][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 12:56:05,726][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9290
[2025-05-02 12:56:05,727][src.data.datasets][INFO] -   Mean: 0.2504, Std: 0.2216
[2025-05-02 12:56:05,727][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 12:56:05,727][src.data.datasets][INFO] - Sample label: 0.10499999672174454
[2025-05-02 12:56:05,727][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 12:56:05,727][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 12:56:05,727][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 12:56:05,727][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 12:56:05,727][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:56:05,727][src.data.datasets][INFO] -   Mean: 0.3231, Std: 0.2152
[2025-05-02 12:56:05,727][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 12:56:05,728][src.data.datasets][INFO] - Sample label: 0.27799999713897705
[2025-05-02 12:56:05,728][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 12:56:05,728][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 12:56:05,728][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 12:56:05,728][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-02 12:56:05,728][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 12:56:09,719][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 12:56:09,720][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 12:56:09,720][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=4, freeze_model=True
[2025-05-02 12:56:09,721][src.models.model_factory][INFO] - Using provided probe_hidden_size: 96
[2025-05-02 12:56:09,723][src.models.model_factory][INFO] - Model has 84,961 trainable parameters out of 394,206,433 total parameters
[2025-05-02 12:56:09,723][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 84,961 trainable parameters
[2025-05-02 12:56:09,723][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=96, depth=2, activation=silu, normalization=layer
[2025-05-02 12:56:09,723][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 96 hidden size
[2025-05-02 12:56:09,724][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 12:56:09,724][__main__][INFO] - Total parameters: 394,206,433
[2025-05-02 12:56:09,724][__main__][INFO] - Trainable parameters: 84,961 (0.02%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 3.4209Epoch 1/15: [                              ] 2/63 batches, loss: 3.4952Epoch 1/15: [=                             ] 3/63 batches, loss: 3.5418Epoch 1/15: [=                             ] 4/63 batches, loss: 3.4676Epoch 1/15: [==                            ] 5/63 batches, loss: 3.2418Epoch 1/15: [==                            ] 6/63 batches, loss: 3.2304Epoch 1/15: [===                           ] 7/63 batches, loss: 3.1056Epoch 1/15: [===                           ] 8/63 batches, loss: 2.9658Epoch 1/15: [====                          ] 9/63 batches, loss: 2.9152Epoch 1/15: [====                          ] 10/63 batches, loss: 2.8721Epoch 1/15: [=====                         ] 11/63 batches, loss: 2.8194Epoch 1/15: [=====                         ] 12/63 batches, loss: 2.7162Epoch 1/15: [======                        ] 13/63 batches, loss: 2.6740Epoch 1/15: [======                        ] 14/63 batches, loss: 2.5810Epoch 1/15: [=======                       ] 15/63 batches, loss: 2.5045Epoch 1/15: [=======                       ] 16/63 batches, loss: 2.4334Epoch 1/15: [========                      ] 17/63 batches, loss: 2.4041Epoch 1/15: [========                      ] 18/63 batches, loss: 2.4143Epoch 1/15: [=========                     ] 19/63 batches, loss: 2.3757Epoch 1/15: [=========                     ] 20/63 batches, loss: 2.3427Epoch 1/15: [==========                    ] 21/63 batches, loss: 2.2805Epoch 1/15: [==========                    ] 22/63 batches, loss: 2.2484Epoch 1/15: [==========                    ] 23/63 batches, loss: 2.2875Epoch 1/15: [===========                   ] 24/63 batches, loss: 2.2355Epoch 1/15: [===========                   ] 25/63 batches, loss: 2.2095Epoch 1/15: [============                  ] 26/63 batches, loss: 2.1692Epoch 1/15: [============                  ] 27/63 batches, loss: 2.1425Epoch 1/15: [=============                 ] 28/63 batches, loss: 2.1066Epoch 1/15: [=============                 ] 29/63 batches, loss: 2.0725Epoch 1/15: [==============                ] 30/63 batches, loss: 2.0400Epoch 1/15: [==============                ] 31/63 batches, loss: 2.0035Epoch 1/15: [===============               ] 32/63 batches, loss: 1.9538Epoch 1/15: [===============               ] 33/63 batches, loss: 1.9202Epoch 1/15: [================              ] 34/63 batches, loss: 1.8912Epoch 1/15: [================              ] 35/63 batches, loss: 1.8697Epoch 1/15: [=================             ] 36/63 batches, loss: 1.8673Epoch 1/15: [=================             ] 37/63 batches, loss: 1.8403Epoch 1/15: [==================            ] 38/63 batches, loss: 1.8157Epoch 1/15: [==================            ] 39/63 batches, loss: 1.7875Epoch 1/15: [===================           ] 40/63 batches, loss: 1.7665Epoch 1/15: [===================           ] 41/63 batches, loss: 1.7331Epoch 1/15: [====================          ] 42/63 batches, loss: 1.7069Epoch 1/15: [====================          ] 43/63 batches, loss: 1.6775Epoch 1/15: [====================          ] 44/63 batches, loss: 1.6545Epoch 1/15: [=====================         ] 45/63 batches, loss: 1.6306Epoch 1/15: [=====================         ] 46/63 batches, loss: 1.6202Epoch 1/15: [======================        ] 47/63 batches, loss: 1.6019Epoch 1/15: [======================        ] 48/63 batches, loss: 1.5785Epoch 1/15: [=======================       ] 49/63 batches, loss: 1.5554Epoch 1/15: [=======================       ] 50/63 batches, loss: 1.5423Epoch 1/15: [========================      ] 51/63 batches, loss: 1.5183Epoch 1/15: [========================      ] 52/63 batches, loss: 1.4919Epoch 1/15: [=========================     ] 53/63 batches, loss: 1.4689Epoch 1/15: [=========================     ] 54/63 batches, loss: 1.4552Epoch 1/15: [==========================    ] 55/63 batches, loss: 1.4397Epoch 1/15: [==========================    ] 56/63 batches, loss: 1.4202Epoch 1/15: [===========================   ] 57/63 batches, loss: 1.4120Epoch 1/15: [===========================   ] 58/63 batches, loss: 1.3984Epoch 1/15: [============================  ] 59/63 batches, loss: 1.3834Epoch 1/15: [============================  ] 60/63 batches, loss: 1.3692Epoch 1/15: [============================= ] 61/63 batches, loss: 1.3526Epoch 1/15: [============================= ] 62/63 batches, loss: 1.3397Epoch 1/15: [==============================] 63/63 batches, loss: 1.3521
[2025-05-02 12:56:14,151][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 1.3521
[2025-05-02 12:56:14,331][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1472, Metrics: {'mse': 0.1509758085012436, 'rmse': 0.3885560558030766, 'r2': -2.075805187225342}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.3091Epoch 2/15: [                              ] 2/63 batches, loss: 0.3307Epoch 2/15: [=                             ] 3/63 batches, loss: 0.2937Epoch 2/15: [=                             ] 4/63 batches, loss: 0.3858Epoch 2/15: [==                            ] 5/63 batches, loss: 0.3773Epoch 2/15: [==                            ] 6/63 batches, loss: 0.3961Epoch 2/15: [===                           ] 7/63 batches, loss: 0.3651Epoch 2/15: [===                           ] 8/63 batches, loss: 0.3494Epoch 2/15: [====                          ] 9/63 batches, loss: 0.3369Epoch 2/15: [====                          ] 10/63 batches, loss: 0.3352Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.3216Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.3214Epoch 2/15: [======                        ] 13/63 batches, loss: 0.3285Epoch 2/15: [======                        ] 14/63 batches, loss: 0.3176Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.3268Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.3340Epoch 2/15: [========                      ] 17/63 batches, loss: 0.3336Epoch 2/15: [========                      ] 18/63 batches, loss: 0.3230Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.3238Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.3329Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.3274Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.3324Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.3257Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.3277Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.3252Epoch 2/15: [============                  ] 26/63 batches, loss: 0.3258Epoch 2/15: [============                  ] 27/63 batches, loss: 0.3250Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.3198Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.3198Epoch 2/15: [==============                ] 30/63 batches, loss: 0.3196Epoch 2/15: [==============                ] 31/63 batches, loss: 0.3347Epoch 2/15: [===============               ] 32/63 batches, loss: 0.3329Epoch 2/15: [===============               ] 33/63 batches, loss: 0.3271Epoch 2/15: [================              ] 34/63 batches, loss: 0.3296Epoch 2/15: [================              ] 35/63 batches, loss: 0.3376Epoch 2/15: [=================             ] 36/63 batches, loss: 0.3367Epoch 2/15: [=================             ] 37/63 batches, loss: 0.3313Epoch 2/15: [==================            ] 38/63 batches, loss: 0.3247Epoch 2/15: [==================            ] 39/63 batches, loss: 0.3208Epoch 2/15: [===================           ] 40/63 batches, loss: 0.3250Epoch 2/15: [===================           ] 41/63 batches, loss: 0.3234Epoch 2/15: [====================          ] 42/63 batches, loss: 0.3192Epoch 2/15: [====================          ] 43/63 batches, loss: 0.3206Epoch 2/15: [====================          ] 44/63 batches, loss: 0.3227Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.3194Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.3146Epoch 2/15: [======================        ] 47/63 batches, loss: 0.3125Epoch 2/15: [======================        ] 48/63 batches, loss: 0.3091Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.3054Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.3121Epoch 2/15: [========================      ] 51/63 batches, loss: 0.3081Epoch 2/15: [========================      ] 52/63 batches, loss: 0.3088Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.3067Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.3036Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.3051Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.3049Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.3051Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.3063Epoch 2/15: [============================  ] 59/63 batches, loss: 0.3050Epoch 2/15: [============================  ] 60/63 batches, loss: 0.3026Epoch 2/15: [============================= ] 61/63 batches, loss: 0.3026Epoch 2/15: [============================= ] 62/63 batches, loss: 0.3005Epoch 2/15: [==============================] 63/63 batches, loss: 0.2966
[2025-05-02 12:56:16,647][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.2966
[2025-05-02 12:56:16,845][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1610, Metrics: {'mse': 0.15795069932937622, 'rmse': 0.39743011879999257, 'r2': -2.2179036140441895}
[2025-05-02 12:56:16,846][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.3986Epoch 3/15: [                              ] 2/63 batches, loss: 0.4049Epoch 3/15: [=                             ] 3/63 batches, loss: 0.3579Epoch 3/15: [=                             ] 4/63 batches, loss: 0.3262Epoch 3/15: [==                            ] 5/63 batches, loss: 0.3211Epoch 3/15: [==                            ] 6/63 batches, loss: 0.3165Epoch 3/15: [===                           ] 7/63 batches, loss: 0.3037Epoch 3/15: [===                           ] 8/63 batches, loss: 0.2864Epoch 3/15: [====                          ] 9/63 batches, loss: 0.2896Epoch 3/15: [====                          ] 10/63 batches, loss: 0.3311Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.3113Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.3163Epoch 3/15: [======                        ] 13/63 batches, loss: 0.3084Epoch 3/15: [======                        ] 14/63 batches, loss: 0.3108Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.3000Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.3088Epoch 3/15: [========                      ] 17/63 batches, loss: 0.3023Epoch 3/15: [========                      ] 18/63 batches, loss: 0.3076Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.3157Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.3172Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.3145Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.3134Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.3087Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.3109Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.3060Epoch 3/15: [============                  ] 26/63 batches, loss: 0.3089Epoch 3/15: [============                  ] 27/63 batches, loss: 0.3080Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.3088Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.3027Epoch 3/15: [==============                ] 30/63 batches, loss: 0.2999Epoch 3/15: [==============                ] 31/63 batches, loss: 0.2995Epoch 3/15: [===============               ] 32/63 batches, loss: 0.3003Epoch 3/15: [===============               ] 33/63 batches, loss: 0.2960Epoch 3/15: [================              ] 34/63 batches, loss: 0.2928Epoch 3/15: [================              ] 35/63 batches, loss: 0.2869Epoch 3/15: [=================             ] 36/63 batches, loss: 0.2873Epoch 3/15: [=================             ] 37/63 batches, loss: 0.2824Epoch 3/15: [==================            ] 38/63 batches, loss: 0.2789Epoch 3/15: [==================            ] 39/63 batches, loss: 0.2774Epoch 3/15: [===================           ] 40/63 batches, loss: 0.2773Epoch 3/15: [===================           ] 41/63 batches, loss: 0.2752Epoch 3/15: [====================          ] 42/63 batches, loss: 0.2716Epoch 3/15: [====================          ] 43/63 batches, loss: 0.2716Epoch 3/15: [====================          ] 44/63 batches, loss: 0.2698Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.2668Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.2641Epoch 3/15: [======================        ] 47/63 batches, loss: 0.2630Epoch 3/15: [======================        ] 48/63 batches, loss: 0.2604Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.2612Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.2594Epoch 3/15: [========================      ] 51/63 batches, loss: 0.2580Epoch 3/15: [========================      ] 52/63 batches, loss: 0.2572Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.2569Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.2538Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.2511Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.2499Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.2494Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.2482Epoch 3/15: [============================  ] 59/63 batches, loss: 0.2507Epoch 3/15: [============================  ] 60/63 batches, loss: 0.2502Epoch 3/15: [============================= ] 61/63 batches, loss: 0.2479Epoch 3/15: [============================= ] 62/63 batches, loss: 0.2461Epoch 3/15: [==============================] 63/63 batches, loss: 0.2438
[2025-05-02 12:56:18,780][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.2438
[2025-05-02 12:56:18,981][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1404, Metrics: {'mse': 0.13811995089054108, 'rmse': 0.3716449258237506, 'r2': -1.8138949871063232}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.2970Epoch 4/15: [                              ] 2/63 batches, loss: 0.2231Epoch 4/15: [=                             ] 3/63 batches, loss: 0.2345Epoch 4/15: [=                             ] 4/63 batches, loss: 0.2451Epoch 4/15: [==                            ] 5/63 batches, loss: 0.2511Epoch 4/15: [==                            ] 6/63 batches, loss: 0.2606Epoch 4/15: [===                           ] 7/63 batches, loss: 0.2549Epoch 4/15: [===                           ] 8/63 batches, loss: 0.2417Epoch 4/15: [====                          ] 9/63 batches, loss: 0.2462Epoch 4/15: [====                          ] 10/63 batches, loss: 0.2490Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.2507Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.2473Epoch 4/15: [======                        ] 13/63 batches, loss: 0.2416Epoch 4/15: [======                        ] 14/63 batches, loss: 0.2354Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.2360Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.2342Epoch 4/15: [========                      ] 17/63 batches, loss: 0.2267Epoch 4/15: [========                      ] 18/63 batches, loss: 0.2403Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.2309Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.2340Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.2301Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.2274Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.2228Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.2206Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.2187Epoch 4/15: [============                  ] 26/63 batches, loss: 0.2195Epoch 4/15: [============                  ] 27/63 batches, loss: 0.2172Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.2190Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.2308Epoch 4/15: [==============                ] 30/63 batches, loss: 0.2301Epoch 4/15: [==============                ] 31/63 batches, loss: 0.2259Epoch 4/15: [===============               ] 32/63 batches, loss: 0.2276Epoch 4/15: [===============               ] 33/63 batches, loss: 0.2265Epoch 4/15: [================              ] 34/63 batches, loss: 0.2251Epoch 4/15: [================              ] 35/63 batches, loss: 0.2283Epoch 4/15: [=================             ] 36/63 batches, loss: 0.2253Epoch 4/15: [=================             ] 37/63 batches, loss: 0.2242Epoch 4/15: [==================            ] 38/63 batches, loss: 0.2247Epoch 4/15: [==================            ] 39/63 batches, loss: 0.2273Epoch 4/15: [===================           ] 40/63 batches, loss: 0.2252Epoch 4/15: [===================           ] 41/63 batches, loss: 0.2277Epoch 4/15: [====================          ] 42/63 batches, loss: 0.2285Epoch 4/15: [====================          ] 43/63 batches, loss: 0.2266Epoch 4/15: [====================          ] 44/63 batches, loss: 0.2241Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.2214Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.2180Epoch 4/15: [======================        ] 47/63 batches, loss: 0.2176Epoch 4/15: [======================        ] 48/63 batches, loss: 0.2179Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.2195Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.2179Epoch 4/15: [========================      ] 51/63 batches, loss: 0.2204Epoch 4/15: [========================      ] 52/63 batches, loss: 0.2209Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.2188Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.2234Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.2228Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.2204Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.2215Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.2200Epoch 4/15: [============================  ] 59/63 batches, loss: 0.2201Epoch 4/15: [============================  ] 60/63 batches, loss: 0.2187Epoch 4/15: [============================= ] 61/63 batches, loss: 0.2181Epoch 4/15: [============================= ] 62/63 batches, loss: 0.2182Epoch 4/15: [==============================] 63/63 batches, loss: 0.2156
[2025-05-02 12:56:21,331][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.2156
[2025-05-02 12:56:21,534][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1289, Metrics: {'mse': 0.12660521268844604, 'rmse': 0.35581626254071924, 'r2': -1.5793068408966064}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1730Epoch 5/15: [                              ] 2/63 batches, loss: 0.1691Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1468Epoch 5/15: [=                             ] 4/63 batches, loss: 0.1577Epoch 5/15: [==                            ] 5/63 batches, loss: 0.1562Epoch 5/15: [==                            ] 6/63 batches, loss: 0.1712Epoch 5/15: [===                           ] 7/63 batches, loss: 0.1702Epoch 5/15: [===                           ] 8/63 batches, loss: 0.1759Epoch 5/15: [====                          ] 9/63 batches, loss: 0.1693Epoch 5/15: [====                          ] 10/63 batches, loss: 0.1725Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.1745Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.1687Epoch 5/15: [======                        ] 13/63 batches, loss: 0.1683Epoch 5/15: [======                        ] 14/63 batches, loss: 0.1760Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.1773Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.1791Epoch 5/15: [========                      ] 17/63 batches, loss: 0.1809Epoch 5/15: [========                      ] 18/63 batches, loss: 0.1903Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.1902Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.1935Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.1928Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.1914Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.1942Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.1925Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.1925Epoch 5/15: [============                  ] 26/63 batches, loss: 0.1934Epoch 5/15: [============                  ] 27/63 batches, loss: 0.1943Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.1915Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.1895Epoch 5/15: [==============                ] 30/63 batches, loss: 0.1920Epoch 5/15: [==============                ] 31/63 batches, loss: 0.1901Epoch 5/15: [===============               ] 32/63 batches, loss: 0.1874Epoch 5/15: [===============               ] 33/63 batches, loss: 0.1876Epoch 5/15: [================              ] 34/63 batches, loss: 0.1885Epoch 5/15: [================              ] 35/63 batches, loss: 0.1874Epoch 5/15: [=================             ] 36/63 batches, loss: 0.1881Epoch 5/15: [=================             ] 37/63 batches, loss: 0.1879Epoch 5/15: [==================            ] 38/63 batches, loss: 0.1845Epoch 5/15: [==================            ] 39/63 batches, loss: 0.1843Epoch 5/15: [===================           ] 40/63 batches, loss: 0.1867Epoch 5/15: [===================           ] 41/63 batches, loss: 0.1897Epoch 5/15: [====================          ] 42/63 batches, loss: 0.1906Epoch 5/15: [====================          ] 43/63 batches, loss: 0.1942Epoch 5/15: [====================          ] 44/63 batches, loss: 0.1953Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.1959Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.1986Epoch 5/15: [======================        ] 47/63 batches, loss: 0.1975Epoch 5/15: [======================        ] 48/63 batches, loss: 0.1983Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.1969Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.1992Epoch 5/15: [========================      ] 51/63 batches, loss: 0.1994Epoch 5/15: [========================      ] 52/63 batches, loss: 0.1998Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.2014Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.2004Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.2009Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.2039Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.2017Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.2016Epoch 5/15: [============================  ] 59/63 batches, loss: 0.2001Epoch 5/15: [============================  ] 60/63 batches, loss: 0.1985Epoch 5/15: [============================= ] 61/63 batches, loss: 0.1988Epoch 5/15: [============================= ] 62/63 batches, loss: 0.1989Epoch 5/15: [==============================] 63/63 batches, loss: 0.1975
[2025-05-02 12:56:23,826][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1975
[2025-05-02 12:56:24,024][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1153, Metrics: {'mse': 0.11323481053113937, 'rmse': 0.3365038046310017, 'r2': -1.3069140911102295}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.1319Epoch 6/15: [                              ] 2/63 batches, loss: 0.1974Epoch 6/15: [=                             ] 3/63 batches, loss: 0.1731Epoch 6/15: [=                             ] 4/63 batches, loss: 0.1437Epoch 6/15: [==                            ] 5/63 batches, loss: 0.1285Epoch 6/15: [==                            ] 6/63 batches, loss: 0.1264Epoch 6/15: [===                           ] 7/63 batches, loss: 0.1250Epoch 6/15: [===                           ] 8/63 batches, loss: 0.1238Epoch 6/15: [====                          ] 9/63 batches, loss: 0.1315Epoch 6/15: [====                          ] 10/63 batches, loss: 0.1247Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.1438Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.1461Epoch 6/15: [======                        ] 13/63 batches, loss: 0.1661Epoch 6/15: [======                        ] 14/63 batches, loss: 0.1652Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.1612Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.1593Epoch 6/15: [========                      ] 17/63 batches, loss: 0.1631Epoch 6/15: [========                      ] 18/63 batches, loss: 0.1597Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.1599Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.1595Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.1626Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.1593Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.1574Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.1562Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.1686Epoch 6/15: [============                  ] 26/63 batches, loss: 0.1691Epoch 6/15: [============                  ] 27/63 batches, loss: 0.1694Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.1684Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.1695Epoch 6/15: [==============                ] 30/63 batches, loss: 0.1717Epoch 6/15: [==============                ] 31/63 batches, loss: 0.1710Epoch 6/15: [===============               ] 32/63 batches, loss: 0.1727Epoch 6/15: [===============               ] 33/63 batches, loss: 0.1729Epoch 6/15: [================              ] 34/63 batches, loss: 0.1723Epoch 6/15: [================              ] 35/63 batches, loss: 0.1753Epoch 6/15: [=================             ] 36/63 batches, loss: 0.1754Epoch 6/15: [=================             ] 37/63 batches, loss: 0.1758Epoch 6/15: [==================            ] 38/63 batches, loss: 0.1767Epoch 6/15: [==================            ] 39/63 batches, loss: 0.1773Epoch 6/15: [===================           ] 40/63 batches, loss: 0.1800Epoch 6/15: [===================           ] 41/63 batches, loss: 0.1789Epoch 6/15: [====================          ] 42/63 batches, loss: 0.1795Epoch 6/15: [====================          ] 43/63 batches, loss: 0.1791Epoch 6/15: [====================          ] 44/63 batches, loss: 0.1782Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.1777Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.1773Epoch 6/15: [======================        ] 47/63 batches, loss: 0.1777Epoch 6/15: [======================        ] 48/63 batches, loss: 0.1772Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.1763Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.1766Epoch 6/15: [========================      ] 51/63 batches, loss: 0.1769Epoch 6/15: [========================      ] 52/63 batches, loss: 0.1774Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.1782Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.1785Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.1784Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.1779Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.1777Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.1763Epoch 6/15: [============================  ] 59/63 batches, loss: 0.1763Epoch 6/15: [============================  ] 60/63 batches, loss: 0.1762Epoch 6/15: [============================= ] 61/63 batches, loss: 0.1779Epoch 6/15: [============================= ] 62/63 batches, loss: 0.1772Epoch 6/15: [==============================] 63/63 batches, loss: 0.1758
[2025-05-02 12:56:26,337][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1758
[2025-05-02 12:56:26,567][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0988, Metrics: {'mse': 0.09732124209403992, 'rmse': 0.3119635268649845, 'r2': -0.982709527015686}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.1201Epoch 7/15: [                              ] 2/63 batches, loss: 0.2072Epoch 7/15: [=                             ] 3/63 batches, loss: 0.2022Epoch 7/15: [=                             ] 4/63 batches, loss: 0.2169Epoch 7/15: [==                            ] 5/63 batches, loss: 0.1842Epoch 7/15: [==                            ] 6/63 batches, loss: 0.1776Epoch 7/15: [===                           ] 7/63 batches, loss: 0.2097Epoch 7/15: [===                           ] 8/63 batches, loss: 0.2026Epoch 7/15: [====                          ] 9/63 batches, loss: 0.1936Epoch 7/15: [====                          ] 10/63 batches, loss: 0.1863Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.1893Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.1838Epoch 7/15: [======                        ] 13/63 batches, loss: 0.1787Epoch 7/15: [======                        ] 14/63 batches, loss: 0.1777Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.1826Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.1821Epoch 7/15: [========                      ] 17/63 batches, loss: 0.1830Epoch 7/15: [========                      ] 18/63 batches, loss: 0.1824Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.1841Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.1829Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.1829Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.1838Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.1804Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.1796Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.1757Epoch 7/15: [============                  ] 26/63 batches, loss: 0.1709Epoch 7/15: [============                  ] 27/63 batches, loss: 0.1690Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.1682Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.1647Epoch 7/15: [==============                ] 30/63 batches, loss: 0.1652Epoch 7/15: [==============                ] 31/63 batches, loss: 0.1656Epoch 7/15: [===============               ] 32/63 batches, loss: 0.1653Epoch 7/15: [===============               ] 33/63 batches, loss: 0.1680Epoch 7/15: [================              ] 34/63 batches, loss: 0.1674Epoch 7/15: [================              ] 35/63 batches, loss: 0.1687Epoch 7/15: [=================             ] 36/63 batches, loss: 0.1706Epoch 7/15: [=================             ] 37/63 batches, loss: 0.1696Epoch 7/15: [==================            ] 38/63 batches, loss: 0.1703Epoch 7/15: [==================            ] 39/63 batches, loss: 0.1705Epoch 7/15: [===================           ] 40/63 batches, loss: 0.1687Epoch 7/15: [===================           ] 41/63 batches, loss: 0.1713Epoch 7/15: [====================          ] 42/63 batches, loss: 0.1749Epoch 7/15: [====================          ] 43/63 batches, loss: 0.1751Epoch 7/15: [====================          ] 44/63 batches, loss: 0.1755Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.1751Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.1741Epoch 7/15: [======================        ] 47/63 batches, loss: 0.1760Epoch 7/15: [======================        ] 48/63 batches, loss: 0.1746Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.1738Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.1747Epoch 7/15: [========================      ] 51/63 batches, loss: 0.1736Epoch 7/15: [========================      ] 52/63 batches, loss: 0.1729Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.1741Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.1734Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.1736Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.1756Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.1759Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.1755Epoch 7/15: [============================  ] 59/63 batches, loss: 0.1777Epoch 7/15: [============================  ] 60/63 batches, loss: 0.1790Epoch 7/15: [============================= ] 61/63 batches, loss: 0.1784Epoch 7/15: [============================= ] 62/63 batches, loss: 0.1784Epoch 7/15: [==============================] 63/63 batches, loss: 0.1757
[2025-05-02 12:56:28,912][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1757
[2025-05-02 12:56:29,125][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0935, Metrics: {'mse': 0.09150493144989014, 'rmse': 0.3024978205704797, 'r2': -0.8642147779464722}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.1143Epoch 8/15: [                              ] 2/63 batches, loss: 0.1459Epoch 8/15: [=                             ] 3/63 batches, loss: 0.1665Epoch 8/15: [=                             ] 4/63 batches, loss: 0.1538Epoch 8/15: [==                            ] 5/63 batches, loss: 0.1338Epoch 8/15: [==                            ] 6/63 batches, loss: 0.1299Epoch 8/15: [===                           ] 7/63 batches, loss: 0.1398Epoch 8/15: [===                           ] 8/63 batches, loss: 0.1381Epoch 8/15: [====                          ] 9/63 batches, loss: 0.1439Epoch 8/15: [====                          ] 10/63 batches, loss: 0.1573Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.1506Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.1635Epoch 8/15: [======                        ] 13/63 batches, loss: 0.1666Epoch 8/15: [======                        ] 14/63 batches, loss: 0.1622Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.1627Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.1624Epoch 8/15: [========                      ] 17/63 batches, loss: 0.1613Epoch 8/15: [========                      ] 18/63 batches, loss: 0.1647Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.1680Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.1679Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.1665Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.1637Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.1620Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.1620Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.1586Epoch 8/15: [============                  ] 26/63 batches, loss: 0.1601Epoch 8/15: [============                  ] 27/63 batches, loss: 0.1629Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.1667Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.1664Epoch 8/15: [==============                ] 30/63 batches, loss: 0.1634Epoch 8/15: [==============                ] 31/63 batches, loss: 0.1627Epoch 8/15: [===============               ] 32/63 batches, loss: 0.1612Epoch 8/15: [===============               ] 33/63 batches, loss: 0.1584Epoch 8/15: [================              ] 34/63 batches, loss: 0.1551Epoch 8/15: [================              ] 35/63 batches, loss: 0.1551Epoch 8/15: [=================             ] 36/63 batches, loss: 0.1556Epoch 8/15: [=================             ] 37/63 batches, loss: 0.1569Epoch 8/15: [==================            ] 38/63 batches, loss: 0.1561Epoch 8/15: [==================            ] 39/63 batches, loss: 0.1564Epoch 8/15: [===================           ] 40/63 batches, loss: 0.1564Epoch 8/15: [===================           ] 41/63 batches, loss: 0.1569Epoch 8/15: [====================          ] 42/63 batches, loss: 0.1545Epoch 8/15: [====================          ] 43/63 batches, loss: 0.1550Epoch 8/15: [====================          ] 44/63 batches, loss: 0.1535Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.1522Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.1511Epoch 8/15: [======================        ] 47/63 batches, loss: 0.1500Epoch 8/15: [======================        ] 48/63 batches, loss: 0.1512Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.1513Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.1530Epoch 8/15: [========================      ] 51/63 batches, loss: 0.1520Epoch 8/15: [========================      ] 52/63 batches, loss: 0.1516Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.1504Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.1511Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.1507Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.1494Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.1499Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.1494Epoch 8/15: [============================  ] 59/63 batches, loss: 0.1491Epoch 8/15: [============================  ] 60/63 batches, loss: 0.1504Epoch 8/15: [============================= ] 61/63 batches, loss: 0.1496Epoch 8/15: [============================= ] 62/63 batches, loss: 0.1502Epoch 8/15: [==============================] 63/63 batches, loss: 0.1512
[2025-05-02 12:56:31,499][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.1512
[2025-05-02 12:56:31,717][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0876, Metrics: {'mse': 0.08544278144836426, 'rmse': 0.29230597231046146, 'r2': -0.7407115697860718}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1139Epoch 9/15: [                              ] 2/63 batches, loss: 0.1193Epoch 9/15: [=                             ] 3/63 batches, loss: 0.1155Epoch 9/15: [=                             ] 4/63 batches, loss: 0.1346Epoch 9/15: [==                            ] 5/63 batches, loss: 0.1399Epoch 9/15: [==                            ] 6/63 batches, loss: 0.1304Epoch 9/15: [===                           ] 7/63 batches, loss: 0.1485Epoch 9/15: [===                           ] 8/63 batches, loss: 0.1390Epoch 9/15: [====                          ] 9/63 batches, loss: 0.1461Epoch 9/15: [====                          ] 10/63 batches, loss: 0.1533Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.1592Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.1573Epoch 9/15: [======                        ] 13/63 batches, loss: 0.1580Epoch 9/15: [======                        ] 14/63 batches, loss: 0.1555Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.1541Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.1566Epoch 9/15: [========                      ] 17/63 batches, loss: 0.1540Epoch 9/15: [========                      ] 18/63 batches, loss: 0.1510Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.1529Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.1557Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.1536Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.1501Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.1481Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.1468Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.1430Epoch 9/15: [============                  ] 26/63 batches, loss: 0.1567Epoch 9/15: [============                  ] 27/63 batches, loss: 0.1540Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.1533Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.1524Epoch 9/15: [==============                ] 30/63 batches, loss: 0.1520Epoch 9/15: [==============                ] 31/63 batches, loss: 0.1560Epoch 9/15: [===============               ] 32/63 batches, loss: 0.1565Epoch 9/15: [===============               ] 33/63 batches, loss: 0.1565Epoch 9/15: [================              ] 34/63 batches, loss: 0.1557Epoch 9/15: [================              ] 35/63 batches, loss: 0.1539Epoch 9/15: [=================             ] 36/63 batches, loss: 0.1562Epoch 9/15: [=================             ] 37/63 batches, loss: 0.1534Epoch 9/15: [==================            ] 38/63 batches, loss: 0.1536Epoch 9/15: [==================            ] 39/63 batches, loss: 0.1544Epoch 9/15: [===================           ] 40/63 batches, loss: 0.1526Epoch 9/15: [===================           ] 41/63 batches, loss: 0.1524Epoch 9/15: [====================          ] 42/63 batches, loss: 0.1541Epoch 9/15: [====================          ] 43/63 batches, loss: 0.1523Epoch 9/15: [====================          ] 44/63 batches, loss: 0.1530Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.1533Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.1525Epoch 9/15: [======================        ] 47/63 batches, loss: 0.1530Epoch 9/15: [======================        ] 48/63 batches, loss: 0.1544Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.1530Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.1541Epoch 9/15: [========================      ] 51/63 batches, loss: 0.1526Epoch 9/15: [========================      ] 52/63 batches, loss: 0.1517Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.1503Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.1503Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.1496Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.1488Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.1503Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.1494Epoch 9/15: [============================  ] 59/63 batches, loss: 0.1509Epoch 9/15: [============================  ] 60/63 batches, loss: 0.1544Epoch 9/15: [============================= ] 61/63 batches, loss: 0.1545Epoch 9/15: [============================= ] 62/63 batches, loss: 0.1545Epoch 9/15: [==============================] 63/63 batches, loss: 0.1555
[2025-05-02 12:56:34,052][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.1555
[2025-05-02 12:56:34,255][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0799, Metrics: {'mse': 0.077750064432621, 'rmse': 0.27883698541015145, 'r2': -0.583989143371582}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.1868Epoch 10/15: [                              ] 2/63 batches, loss: 0.2847Epoch 10/15: [=                             ] 3/63 batches, loss: 0.2234Epoch 10/15: [=                             ] 4/63 batches, loss: 0.1811Epoch 10/15: [==                            ] 5/63 batches, loss: 0.1791Epoch 10/15: [==                            ] 6/63 batches, loss: 0.1751Epoch 10/15: [===                           ] 7/63 batches, loss: 0.1728Epoch 10/15: [===                           ] 8/63 batches, loss: 0.1708Epoch 10/15: [====                          ] 9/63 batches, loss: 0.1625Epoch 10/15: [====                          ] 10/63 batches, loss: 0.1696Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.1676Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.1668Epoch 10/15: [======                        ] 13/63 batches, loss: 0.1610Epoch 10/15: [======                        ] 14/63 batches, loss: 0.1652Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.1707Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.1723Epoch 10/15: [========                      ] 17/63 batches, loss: 0.1686Epoch 10/15: [========                      ] 18/63 batches, loss: 0.1636Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.1590Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.1590Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.1636Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.1616Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.1601Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.1641Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.1618Epoch 10/15: [============                  ] 26/63 batches, loss: 0.1592Epoch 10/15: [============                  ] 27/63 batches, loss: 0.1616Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.1631Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.1620Epoch 10/15: [==============                ] 30/63 batches, loss: 0.1612Epoch 10/15: [==============                ] 31/63 batches, loss: 0.1634Epoch 10/15: [===============               ] 32/63 batches, loss: 0.1627Epoch 10/15: [===============               ] 33/63 batches, loss: 0.1610Epoch 10/15: [================              ] 34/63 batches, loss: 0.1601Epoch 10/15: [================              ] 35/63 batches, loss: 0.1593Epoch 10/15: [=================             ] 36/63 batches, loss: 0.1610Epoch 10/15: [=================             ] 37/63 batches, loss: 0.1594Epoch 10/15: [==================            ] 38/63 batches, loss: 0.1581Epoch 10/15: [==================            ] 39/63 batches, loss: 0.1577Epoch 10/15: [===================           ] 40/63 batches, loss: 0.1574Epoch 10/15: [===================           ] 41/63 batches, loss: 0.1567Epoch 10/15: [====================          ] 42/63 batches, loss: 0.1566Epoch 10/15: [====================          ] 43/63 batches, loss: 0.1571Epoch 10/15: [====================          ] 44/63 batches, loss: 0.1566Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.1557Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.1537Epoch 10/15: [======================        ] 47/63 batches, loss: 0.1532Epoch 10/15: [======================        ] 48/63 batches, loss: 0.1532Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.1526Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.1514Epoch 10/15: [========================      ] 51/63 batches, loss: 0.1527Epoch 10/15: [========================      ] 52/63 batches, loss: 0.1519Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.1508Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.1521Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.1515Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.1514Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.1506Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.1507Epoch 10/15: [============================  ] 59/63 batches, loss: 0.1494Epoch 10/15: [============================  ] 60/63 batches, loss: 0.1510Epoch 10/15: [============================= ] 61/63 batches, loss: 0.1512Epoch 10/15: [============================= ] 62/63 batches, loss: 0.1514Epoch 10/15: [==============================] 63/63 batches, loss: 0.1503
[2025-05-02 12:56:36,595][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.1503
[2025-05-02 12:56:36,823][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0760, Metrics: {'mse': 0.0736709013581276, 'rmse': 0.2714238408064546, 'r2': -0.5008851289749146}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.2017Epoch 11/15: [                              ] 2/63 batches, loss: 0.1700Epoch 11/15: [=                             ] 3/63 batches, loss: 0.1525Epoch 11/15: [=                             ] 4/63 batches, loss: 0.1297Epoch 11/15: [==                            ] 5/63 batches, loss: 0.1232Epoch 11/15: [==                            ] 6/63 batches, loss: 0.1130Epoch 11/15: [===                           ] 7/63 batches, loss: 0.1095Epoch 11/15: [===                           ] 8/63 batches, loss: 0.1112Epoch 11/15: [====                          ] 9/63 batches, loss: 0.1116Epoch 11/15: [====                          ] 10/63 batches, loss: 0.1116Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.1059Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.1136Epoch 11/15: [======                        ] 13/63 batches, loss: 0.1154Epoch 11/15: [======                        ] 14/63 batches, loss: 0.1265Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.1230Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.1250Epoch 11/15: [========                      ] 17/63 batches, loss: 0.1260Epoch 11/15: [========                      ] 18/63 batches, loss: 0.1260Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.1219Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.1212Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.1208Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.1202Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.1187Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.1196Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.1192Epoch 11/15: [============                  ] 26/63 batches, loss: 0.1169Epoch 11/15: [============                  ] 27/63 batches, loss: 0.1213Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.1241Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.1222Epoch 11/15: [==============                ] 30/63 batches, loss: 0.1217Epoch 11/15: [==============                ] 31/63 batches, loss: 0.1222Epoch 11/15: [===============               ] 32/63 batches, loss: 0.1256Epoch 11/15: [===============               ] 33/63 batches, loss: 0.1245Epoch 11/15: [================              ] 34/63 batches, loss: 0.1231Epoch 11/15: [================              ] 35/63 batches, loss: 0.1234Epoch 11/15: [=================             ] 36/63 batches, loss: 0.1238Epoch 11/15: [=================             ] 37/63 batches, loss: 0.1236Epoch 11/15: [==================            ] 38/63 batches, loss: 0.1224Epoch 11/15: [==================            ] 39/63 batches, loss: 0.1246Epoch 11/15: [===================           ] 40/63 batches, loss: 0.1256Epoch 11/15: [===================           ] 41/63 batches, loss: 0.1261Epoch 11/15: [====================          ] 42/63 batches, loss: 0.1263Epoch 11/15: [====================          ] 43/63 batches, loss: 0.1257Epoch 11/15: [====================          ] 44/63 batches, loss: 0.1242Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.1268Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.1258Epoch 11/15: [======================        ] 47/63 batches, loss: 0.1259Epoch 11/15: [======================        ] 48/63 batches, loss: 0.1263Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.1254Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.1258Epoch 11/15: [========================      ] 51/63 batches, loss: 0.1250Epoch 11/15: [========================      ] 52/63 batches, loss: 0.1245Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.1268Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.1259Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.1256Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.1249Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.1238Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.1239Epoch 11/15: [============================  ] 59/63 batches, loss: 0.1236Epoch 11/15: [============================  ] 60/63 batches, loss: 0.1236Epoch 11/15: [============================= ] 61/63 batches, loss: 0.1237Epoch 11/15: [============================= ] 62/63 batches, loss: 0.1231Epoch 11/15: [==============================] 63/63 batches, loss: 0.1226
[2025-05-02 12:56:39,168][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.1226
[2025-05-02 12:56:39,389][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0738, Metrics: {'mse': 0.07136768102645874, 'rmse': 0.2671473021133823, 'r2': -0.45396196842193604}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.1352Epoch 12/15: [                              ] 2/63 batches, loss: 0.0936Epoch 12/15: [=                             ] 3/63 batches, loss: 0.0986Epoch 12/15: [=                             ] 4/63 batches, loss: 0.1172Epoch 12/15: [==                            ] 5/63 batches, loss: 0.1405Epoch 12/15: [==                            ] 6/63 batches, loss: 0.1388Epoch 12/15: [===                           ] 7/63 batches, loss: 0.1301Epoch 12/15: [===                           ] 8/63 batches, loss: 0.1253Epoch 12/15: [====                          ] 9/63 batches, loss: 0.1385Epoch 12/15: [====                          ] 10/63 batches, loss: 0.1371Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.1324Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.1285Epoch 12/15: [======                        ] 13/63 batches, loss: 0.1342Epoch 12/15: [======                        ] 14/63 batches, loss: 0.1308Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.1331Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.1381Epoch 12/15: [========                      ] 17/63 batches, loss: 0.1358Epoch 12/15: [========                      ] 18/63 batches, loss: 0.1328Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.1304Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.1308Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.1320Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.1325Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.1367Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.1330Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.1305Epoch 12/15: [============                  ] 26/63 batches, loss: 0.1297Epoch 12/15: [============                  ] 27/63 batches, loss: 0.1285Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.1249Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.1240Epoch 12/15: [==============                ] 30/63 batches, loss: 0.1231Epoch 12/15: [==============                ] 31/63 batches, loss: 0.1210Epoch 12/15: [===============               ] 32/63 batches, loss: 0.1195Epoch 12/15: [===============               ] 33/63 batches, loss: 0.1207Epoch 12/15: [================              ] 34/63 batches, loss: 0.1206Epoch 12/15: [================              ] 35/63 batches, loss: 0.1229Epoch 12/15: [=================             ] 36/63 batches, loss: 0.1218Epoch 12/15: [=================             ] 37/63 batches, loss: 0.1230Epoch 12/15: [==================            ] 38/63 batches, loss: 0.1209Epoch 12/15: [==================            ] 39/63 batches, loss: 0.1223Epoch 12/15: [===================           ] 40/63 batches, loss: 0.1208Epoch 12/15: [===================           ] 41/63 batches, loss: 0.1207Epoch 12/15: [====================          ] 42/63 batches, loss: 0.1213Epoch 12/15: [====================          ] 43/63 batches, loss: 0.1229Epoch 12/15: [====================          ] 44/63 batches, loss: 0.1226Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.1247Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.1235Epoch 12/15: [======================        ] 47/63 batches, loss: 0.1245Epoch 12/15: [======================        ] 48/63 batches, loss: 0.1244Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.1244Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.1255Epoch 12/15: [========================      ] 51/63 batches, loss: 0.1256Epoch 12/15: [========================      ] 52/63 batches, loss: 0.1250Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.1249Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.1254Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.1253Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.1251Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.1245Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.1244Epoch 12/15: [============================  ] 59/63 batches, loss: 0.1258Epoch 12/15: [============================  ] 60/63 batches, loss: 0.1252Epoch 12/15: [============================= ] 61/63 batches, loss: 0.1252Epoch 12/15: [============================= ] 62/63 batches, loss: 0.1248Epoch 12/15: [==============================] 63/63 batches, loss: 0.1259
[2025-05-02 12:56:41,735][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.1259
[2025-05-02 12:56:41,959][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0703, Metrics: {'mse': 0.06781817972660065, 'rmse': 0.2604192383957081, 'r2': -0.38164854049682617}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.1169Epoch 13/15: [                              ] 2/63 batches, loss: 0.1333Epoch 13/15: [=                             ] 3/63 batches, loss: 0.1221Epoch 13/15: [=                             ] 4/63 batches, loss: 0.1238Epoch 13/15: [==                            ] 5/63 batches, loss: 0.1101Epoch 13/15: [==                            ] 6/63 batches, loss: 0.1064Epoch 13/15: [===                           ] 7/63 batches, loss: 0.0994Epoch 13/15: [===                           ] 8/63 batches, loss: 0.1104Epoch 13/15: [====                          ] 9/63 batches, loss: 0.1048Epoch 13/15: [====                          ] 10/63 batches, loss: 0.1018Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.1034Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.1029Epoch 13/15: [======                        ] 13/63 batches, loss: 0.1028Epoch 13/15: [======                        ] 14/63 batches, loss: 0.1148Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.1219Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.1285Epoch 13/15: [========                      ] 17/63 batches, loss: 0.1261Epoch 13/15: [========                      ] 18/63 batches, loss: 0.1267Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.1270Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.1323Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.1291Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.1271Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.1261Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.1256Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.1250Epoch 13/15: [============                  ] 26/63 batches, loss: 0.1259Epoch 13/15: [============                  ] 27/63 batches, loss: 0.1273Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.1268Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.1262Epoch 13/15: [==============                ] 30/63 batches, loss: 0.1257Epoch 13/15: [==============                ] 31/63 batches, loss: 0.1255Epoch 13/15: [===============               ] 32/63 batches, loss: 0.1282Epoch 13/15: [===============               ] 33/63 batches, loss: 0.1288Epoch 13/15: [================              ] 34/63 batches, loss: 0.1273Epoch 13/15: [================              ] 35/63 batches, loss: 0.1270Epoch 13/15: [=================             ] 36/63 batches, loss: 0.1253Epoch 13/15: [=================             ] 37/63 batches, loss: 0.1245Epoch 13/15: [==================            ] 38/63 batches, loss: 0.1235Epoch 13/15: [==================            ] 39/63 batches, loss: 0.1225Epoch 13/15: [===================           ] 40/63 batches, loss: 0.1241Epoch 13/15: [===================           ] 41/63 batches, loss: 0.1238Epoch 13/15: [====================          ] 42/63 batches, loss: 0.1232Epoch 13/15: [====================          ] 43/63 batches, loss: 0.1223Epoch 13/15: [====================          ] 44/63 batches, loss: 0.1210Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.1232Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.1225Epoch 13/15: [======================        ] 47/63 batches, loss: 0.1229Epoch 13/15: [======================        ] 48/63 batches, loss: 0.1255Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.1277Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.1280Epoch 13/15: [========================      ] 51/63 batches, loss: 0.1277Epoch 13/15: [========================      ] 52/63 batches, loss: 0.1269Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.1272Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.1264Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.1256Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.1251Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.1242Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.1234Epoch 13/15: [============================  ] 59/63 batches, loss: 0.1234Epoch 13/15: [============================  ] 60/63 batches, loss: 0.1231Epoch 13/15: [============================= ] 61/63 batches, loss: 0.1238Epoch 13/15: [============================= ] 62/63 batches, loss: 0.1227Epoch 13/15: [==============================] 63/63 batches, loss: 0.1209
[2025-05-02 12:56:44,282][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.1209
[2025-05-02 12:56:44,504][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0621, Metrics: {'mse': 0.059959277510643005, 'rmse': 0.2448658357359046, 'r2': -0.22154033184051514}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.0925Epoch 14/15: [                              ] 2/63 batches, loss: 0.0888Epoch 14/15: [=                             ] 3/63 batches, loss: 0.1005Epoch 14/15: [=                             ] 4/63 batches, loss: 0.0966Epoch 14/15: [==                            ] 5/63 batches, loss: 0.1067Epoch 14/15: [==                            ] 6/63 batches, loss: 0.1076Epoch 14/15: [===                           ] 7/63 batches, loss: 0.0998Epoch 14/15: [===                           ] 8/63 batches, loss: 0.1054Epoch 14/15: [====                          ] 9/63 batches, loss: 0.1065Epoch 14/15: [====                          ] 10/63 batches, loss: 0.0997Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.1034Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.0990Epoch 14/15: [======                        ] 13/63 batches, loss: 0.0950Epoch 14/15: [======                        ] 14/63 batches, loss: 0.0957Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.0959Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.0942Epoch 14/15: [========                      ] 17/63 batches, loss: 0.0979Epoch 14/15: [========                      ] 18/63 batches, loss: 0.1040Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.1005Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.1004Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.1032Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.1036Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.1049Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.1032Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.1087Epoch 14/15: [============                  ] 26/63 batches, loss: 0.1078Epoch 14/15: [============                  ] 27/63 batches, loss: 0.1101Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.1089Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.1088Epoch 14/15: [==============                ] 30/63 batches, loss: 0.1117Epoch 14/15: [==============                ] 31/63 batches, loss: 0.1128Epoch 14/15: [===============               ] 32/63 batches, loss: 0.1120Epoch 14/15: [===============               ] 33/63 batches, loss: 0.1125Epoch 14/15: [================              ] 34/63 batches, loss: 0.1125Epoch 14/15: [================              ] 35/63 batches, loss: 0.1115Epoch 14/15: [=================             ] 36/63 batches, loss: 0.1118Epoch 14/15: [=================             ] 37/63 batches, loss: 0.1104Epoch 14/15: [==================            ] 38/63 batches, loss: 0.1105Epoch 14/15: [==================            ] 39/63 batches, loss: 0.1129Epoch 14/15: [===================           ] 40/63 batches, loss: 0.1126Epoch 14/15: [===================           ] 41/63 batches, loss: 0.1119Epoch 14/15: [====================          ] 42/63 batches, loss: 0.1130Epoch 14/15: [====================          ] 43/63 batches, loss: 0.1133Epoch 14/15: [====================          ] 44/63 batches, loss: 0.1134Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.1128Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.1130Epoch 14/15: [======================        ] 47/63 batches, loss: 0.1125Epoch 14/15: [======================        ] 48/63 batches, loss: 0.1139Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.1134Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.1130Epoch 14/15: [========================      ] 51/63 batches, loss: 0.1138Epoch 14/15: [========================      ] 52/63 batches, loss: 0.1124Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.1134Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.1145Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.1145Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.1144Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.1160Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.1153Epoch 14/15: [============================  ] 59/63 batches, loss: 0.1155Epoch 14/15: [============================  ] 60/63 batches, loss: 0.1144Epoch 14/15: [============================= ] 61/63 batches, loss: 0.1150Epoch 14/15: [============================= ] 62/63 batches, loss: 0.1143Epoch 14/15: [==============================] 63/63 batches, loss: 0.1130
[2025-05-02 12:56:46,843][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.1130
[2025-05-02 12:56:47,049][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0599, Metrics: {'mse': 0.057728733867406845, 'rmse': 0.24026804587253556, 'r2': -0.17609798908233643}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.1293Epoch 15/15: [                              ] 2/63 batches, loss: 0.0965Epoch 15/15: [=                             ] 3/63 batches, loss: 0.0718Epoch 15/15: [=                             ] 4/63 batches, loss: 0.0829Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0838Epoch 15/15: [==                            ] 6/63 batches, loss: 0.1056Epoch 15/15: [===                           ] 7/63 batches, loss: 0.0968Epoch 15/15: [===                           ] 8/63 batches, loss: 0.0902Epoch 15/15: [====                          ] 9/63 batches, loss: 0.0995Epoch 15/15: [====                          ] 10/63 batches, loss: 0.0970Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.0971Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.0942Epoch 15/15: [======                        ] 13/63 batches, loss: 0.1000Epoch 15/15: [======                        ] 14/63 batches, loss: 0.0976Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.0947Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.0949Epoch 15/15: [========                      ] 17/63 batches, loss: 0.0994Epoch 15/15: [========                      ] 18/63 batches, loss: 0.0978Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.0960Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.0951Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.0924Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.0925Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.0922Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.0907Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.0904Epoch 15/15: [============                  ] 26/63 batches, loss: 0.0905Epoch 15/15: [============                  ] 27/63 batches, loss: 0.0896Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.0892Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.0919Epoch 15/15: [==============                ] 30/63 batches, loss: 0.0928Epoch 15/15: [==============                ] 31/63 batches, loss: 0.0932Epoch 15/15: [===============               ] 32/63 batches, loss: 0.0943Epoch 15/15: [===============               ] 33/63 batches, loss: 0.0930Epoch 15/15: [================              ] 34/63 batches, loss: 0.0955Epoch 15/15: [================              ] 35/63 batches, loss: 0.0955Epoch 15/15: [=================             ] 36/63 batches, loss: 0.0950Epoch 15/15: [=================             ] 37/63 batches, loss: 0.0948Epoch 15/15: [==================            ] 38/63 batches, loss: 0.0941Epoch 15/15: [==================            ] 39/63 batches, loss: 0.0943Epoch 15/15: [===================           ] 40/63 batches, loss: 0.0947Epoch 15/15: [===================           ] 41/63 batches, loss: 0.0937Epoch 15/15: [====================          ] 42/63 batches, loss: 0.0955Epoch 15/15: [====================          ] 43/63 batches, loss: 0.0957Epoch 15/15: [====================          ] 44/63 batches, loss: 0.0958Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.0944Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.0945Epoch 15/15: [======================        ] 47/63 batches, loss: 0.0938Epoch 15/15: [======================        ] 48/63 batches, loss: 0.0937Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.0936Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.0938Epoch 15/15: [========================      ] 51/63 batches, loss: 0.0953Epoch 15/15: [========================      ] 52/63 batches, loss: 0.0951Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.0943Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.0940Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.0936Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.0930Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.0922Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.0933Epoch 15/15: [============================  ] 59/63 batches, loss: 0.0927Epoch 15/15: [============================  ] 60/63 batches, loss: 0.0942Epoch 15/15: [============================= ] 61/63 batches, loss: 0.0946Epoch 15/15: [============================= ] 62/63 batches, loss: 0.0943Epoch 15/15: [==============================] 63/63 batches, loss: 0.0940
[2025-05-02 12:56:49,451][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.0940
[2025-05-02 12:56:49,669][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0619, Metrics: {'mse': 0.05941883102059364, 'rmse': 0.2437597813844475, 'r2': -0.21052992343902588}
[2025-05-02 12:56:49,669][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-02 12:56:49,669][src.training.lm_trainer][INFO] - Training completed in 38.40 seconds
[2025-05-02 12:56:49,669][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 12:56:52,164][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.039761949330568314, 'rmse': 0.1994039852424427, 'r2': -0.8858469724655151}
[2025-05-02 12:56:52,165][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.057728733867406845, 'rmse': 0.24026804587253556, 'r2': -0.17609798908233643}
[2025-05-02 12:56:52,165][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.0794711634516716, 'rmse': 0.28190630261076394, 'r2': -0.7158823013305664}
[2025-05-02 12:56:53,892][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer4/ar/ar/model.pt
[2025-05-02 12:56:53,894][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▇▇▅▄▄▃▃▂▂▂▁▁
wandb:     best_val_mse █▇▆▅▄▄▃▃▂▂▂▁▁
wandb:      best_val_r2 ▁▂▃▄▅▅▆▆▇▇▇██
wandb:    best_val_rmse █▇▆▆▄▄▃▃▂▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▂▁▃▄▄▅▆▆▆▆▇▇▇▇
wandb:       train_loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ▇█▇▆▅▄▃▃▂▂▂▂▁▁▁
wandb:          val_mse ██▇▆▅▄▃▃▂▂▂▂▁▁▁
wandb:           val_r2 ▁▁▂▃▄▅▆▆▇▇▇▇███
wandb:         val_rmse ██▇▆▅▄▄▃▃▂▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.05991
wandb:     best_val_mse 0.05773
wandb:      best_val_r2 -0.1761
wandb:    best_val_rmse 0.24027
wandb:            epoch 15
wandb:   final_test_mse 0.07947
wandb:    final_test_r2 -0.71588
wandb:  final_test_rmse 0.28191
wandb:  final_train_mse 0.03976
wandb:   final_train_r2 -0.88585
wandb: final_train_rmse 0.1994
wandb:    final_val_mse 0.05773
wandb:     final_val_r2 -0.1761
wandb:   final_val_rmse 0.24027
wandb:    learning_rate 2e-05
wandb:       train_loss 0.094
wandb:       train_time 38.39659
wandb:         val_loss 0.06189
wandb:          val_mse 0.05942
wandb:           val_r2 -0.21053
wandb:         val_rmse 0.24376
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125601-cbv1ibf5
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125601-cbv1ibf5/logs
Experiment probe_layer4_avg_links_len_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer4/ar/ar/results.json for layer 4
=======================
PROBING LAYER 6 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer6_avg_links_len_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=6"         "model.probe_hidden_size=96" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer6_avg_links_len_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer6/ar"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 12:57:05,196][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer6/ar
experiment_name: probe_layer6_avg_links_len_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 6
  num_outputs: 1
  probe_hidden_size: 96
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-02 12:57:05,196][__main__][INFO] - Normalized task: single_submetric
[2025-05-02 12:57:05,196][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 12:57:05,196][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 12:57:05,197][__main__][INFO] - Determined Task Type: regression
[2025-05-02 12:57:05,229][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ar']
[2025-05-02 12:57:05,229][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 12:57:05,230][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 12:57:06,681][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 12:57:08,910][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 12:57:08,911][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:57:08,959][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:57:08,987][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:57:09,079][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 12:57:09,087][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:57:09,087][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 12:57:09,088][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:57:09,108][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:57:09,137][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:57:09,150][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 12:57:09,151][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:57:09,151][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 12:57:09,152][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:57:09,171][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:57:09,199][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:57:09,213][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 12:57:09,215][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:57:09,215][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 12:57:09,216][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 12:57:09,217][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 12:57:09,217][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 12:57:09,217][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 12:57:09,217][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 12:57:09,217][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8570
[2025-05-02 12:57:09,217][src.data.datasets][INFO] -   Mean: 0.1857, Std: 0.1452
[2025-05-02 12:57:09,217][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 12:57:09,218][src.data.datasets][INFO] - Sample label: 0.32100000977516174
[2025-05-02 12:57:09,218][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 12:57:09,218][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 12:57:09,218][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 12:57:09,218][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 12:57:09,218][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9290
[2025-05-02 12:57:09,218][src.data.datasets][INFO] -   Mean: 0.2504, Std: 0.2216
[2025-05-02 12:57:09,218][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 12:57:09,218][src.data.datasets][INFO] - Sample label: 0.10499999672174454
[2025-05-02 12:57:09,219][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 12:57:09,219][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 12:57:09,219][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 12:57:09,219][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 12:57:09,219][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:57:09,219][src.data.datasets][INFO] -   Mean: 0.3231, Std: 0.2152
[2025-05-02 12:57:09,219][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 12:57:09,219][src.data.datasets][INFO] - Sample label: 0.27799999713897705
[2025-05-02 12:57:09,219][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 12:57:09,219][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 12:57:09,220][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 12:57:09,220][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-02 12:57:09,220][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 12:57:13,220][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 12:57:13,221][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 12:57:13,221][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=6, freeze_model=True
[2025-05-02 12:57:13,221][src.models.model_factory][INFO] - Using provided probe_hidden_size: 96
[2025-05-02 12:57:13,223][src.models.model_factory][INFO] - Model has 84,961 trainable parameters out of 394,206,433 total parameters
[2025-05-02 12:57:13,224][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 84,961 trainable parameters
[2025-05-02 12:57:13,224][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=96, depth=2, activation=silu, normalization=layer
[2025-05-02 12:57:13,224][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 96 hidden size
[2025-05-02 12:57:13,224][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 12:57:13,225][__main__][INFO] - Total parameters: 394,206,433
[2025-05-02 12:57:13,225][__main__][INFO] - Trainable parameters: 84,961 (0.02%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 3.9973Epoch 1/15: [                              ] 2/63 batches, loss: 4.0622Epoch 1/15: [=                             ] 3/63 batches, loss: 4.1722Epoch 1/15: [=                             ] 4/63 batches, loss: 3.9522Epoch 1/15: [==                            ] 5/63 batches, loss: 3.6586Epoch 1/15: [==                            ] 6/63 batches, loss: 3.7892Epoch 1/15: [===                           ] 7/63 batches, loss: 3.6344Epoch 1/15: [===                           ] 8/63 batches, loss: 3.4293Epoch 1/15: [====                          ] 9/63 batches, loss: 3.3529Epoch 1/15: [====                          ] 10/63 batches, loss: 3.2162Epoch 1/15: [=====                         ] 11/63 batches, loss: 3.1917Epoch 1/15: [=====                         ] 12/63 batches, loss: 3.0682Epoch 1/15: [======                        ] 13/63 batches, loss: 3.0113Epoch 1/15: [======                        ] 14/63 batches, loss: 2.9211Epoch 1/15: [=======                       ] 15/63 batches, loss: 2.8347Epoch 1/15: [=======                       ] 16/63 batches, loss: 2.7723Epoch 1/15: [========                      ] 17/63 batches, loss: 2.7348Epoch 1/15: [========                      ] 18/63 batches, loss: 2.7314Epoch 1/15: [=========                     ] 19/63 batches, loss: 2.6867Epoch 1/15: [=========                     ] 20/63 batches, loss: 2.6484Epoch 1/15: [==========                    ] 21/63 batches, loss: 2.5849Epoch 1/15: [==========                    ] 22/63 batches, loss: 2.5601Epoch 1/15: [==========                    ] 23/63 batches, loss: 2.5996Epoch 1/15: [===========                   ] 24/63 batches, loss: 2.5404Epoch 1/15: [===========                   ] 25/63 batches, loss: 2.5089Epoch 1/15: [============                  ] 26/63 batches, loss: 2.4572Epoch 1/15: [============                  ] 27/63 batches, loss: 2.4313Epoch 1/15: [=============                 ] 28/63 batches, loss: 2.3812Epoch 1/15: [=============                 ] 29/63 batches, loss: 2.3409Epoch 1/15: [==============                ] 30/63 batches, loss: 2.3264Epoch 1/15: [==============                ] 31/63 batches, loss: 2.2905Epoch 1/15: [===============               ] 32/63 batches, loss: 2.2346Epoch 1/15: [===============               ] 33/63 batches, loss: 2.1985Epoch 1/15: [================              ] 34/63 batches, loss: 2.1693Epoch 1/15: [================              ] 35/63 batches, loss: 2.1373Epoch 1/15: [=================             ] 36/63 batches, loss: 2.1302Epoch 1/15: [=================             ] 37/63 batches, loss: 2.1003Epoch 1/15: [==================            ] 38/63 batches, loss: 2.0793Epoch 1/15: [==================            ] 39/63 batches, loss: 2.0513Epoch 1/15: [===================           ] 40/63 batches, loss: 2.0262Epoch 1/15: [===================           ] 41/63 batches, loss: 1.9901Epoch 1/15: [====================          ] 42/63 batches, loss: 1.9600Epoch 1/15: [====================          ] 43/63 batches, loss: 1.9272Epoch 1/15: [====================          ] 44/63 batches, loss: 1.9007Epoch 1/15: [=====================         ] 45/63 batches, loss: 1.8667Epoch 1/15: [=====================         ] 46/63 batches, loss: 1.8529Epoch 1/15: [======================        ] 47/63 batches, loss: 1.8318Epoch 1/15: [======================        ] 48/63 batches, loss: 1.8025Epoch 1/15: [=======================       ] 49/63 batches, loss: 1.7785Epoch 1/15: [=======================       ] 50/63 batches, loss: 1.7628Epoch 1/15: [========================      ] 51/63 batches, loss: 1.7349Epoch 1/15: [========================      ] 52/63 batches, loss: 1.7110Epoch 1/15: [=========================     ] 53/63 batches, loss: 1.6822Epoch 1/15: [=========================     ] 54/63 batches, loss: 1.6608Epoch 1/15: [==========================    ] 55/63 batches, loss: 1.6437Epoch 1/15: [==========================    ] 56/63 batches, loss: 1.6178Epoch 1/15: [===========================   ] 57/63 batches, loss: 1.6016Epoch 1/15: [===========================   ] 58/63 batches, loss: 1.5875Epoch 1/15: [============================  ] 59/63 batches, loss: 1.5746Epoch 1/15: [============================  ] 60/63 batches, loss: 1.5633Epoch 1/15: [============================= ] 61/63 batches, loss: 1.5428Epoch 1/15: [============================= ] 62/63 batches, loss: 1.5273Epoch 1/15: [==============================] 63/63 batches, loss: 1.5452
[2025-05-02 12:57:17,577][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 1.5452
[2025-05-02 12:57:17,773][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1658, Metrics: {'mse': 0.17065049707889557, 'rmse': 0.4130986529618507, 'r2': -2.4766345024108887}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.4093Epoch 2/15: [                              ] 2/63 batches, loss: 0.3520Epoch 2/15: [=                             ] 3/63 batches, loss: 0.3218Epoch 2/15: [=                             ] 4/63 batches, loss: 0.4441Epoch 2/15: [==                            ] 5/63 batches, loss: 0.4516Epoch 2/15: [==                            ] 6/63 batches, loss: 0.4368Epoch 2/15: [===                           ] 7/63 batches, loss: 0.4331Epoch 2/15: [===                           ] 8/63 batches, loss: 0.4103Epoch 2/15: [====                          ] 9/63 batches, loss: 0.4379Epoch 2/15: [====                          ] 10/63 batches, loss: 0.4364Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.4121Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.4080Epoch 2/15: [======                        ] 13/63 batches, loss: 0.4123Epoch 2/15: [======                        ] 14/63 batches, loss: 0.3979Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.3974Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.3992Epoch 2/15: [========                      ] 17/63 batches, loss: 0.4197Epoch 2/15: [========                      ] 18/63 batches, loss: 0.4052Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.4085Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.4144Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.4068Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.4099Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.3966Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.3995Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.4125Epoch 2/15: [============                  ] 26/63 batches, loss: 0.4130Epoch 2/15: [============                  ] 27/63 batches, loss: 0.4067Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.3974Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.3912Epoch 2/15: [==============                ] 30/63 batches, loss: 0.3861Epoch 2/15: [==============                ] 31/63 batches, loss: 0.3875Epoch 2/15: [===============               ] 32/63 batches, loss: 0.3870Epoch 2/15: [===============               ] 33/63 batches, loss: 0.3794Epoch 2/15: [================              ] 34/63 batches, loss: 0.3770Epoch 2/15: [================              ] 35/63 batches, loss: 0.3857Epoch 2/15: [=================             ] 36/63 batches, loss: 0.3857Epoch 2/15: [=================             ] 37/63 batches, loss: 0.3799Epoch 2/15: [==================            ] 38/63 batches, loss: 0.3741Epoch 2/15: [==================            ] 39/63 batches, loss: 0.3729Epoch 2/15: [===================           ] 40/63 batches, loss: 0.3713Epoch 2/15: [===================           ] 41/63 batches, loss: 0.3709Epoch 2/15: [====================          ] 42/63 batches, loss: 0.3672Epoch 2/15: [====================          ] 43/63 batches, loss: 0.3661Epoch 2/15: [====================          ] 44/63 batches, loss: 0.3683Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.3719Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.3659Epoch 2/15: [======================        ] 47/63 batches, loss: 0.3631Epoch 2/15: [======================        ] 48/63 batches, loss: 0.3580Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.3527Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.3597Epoch 2/15: [========================      ] 51/63 batches, loss: 0.3560Epoch 2/15: [========================      ] 52/63 batches, loss: 0.3577Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.3541Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.3513Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.3510Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.3501Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.3502Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.3516Epoch 2/15: [============================  ] 59/63 batches, loss: 0.3508Epoch 2/15: [============================  ] 60/63 batches, loss: 0.3486Epoch 2/15: [============================= ] 61/63 batches, loss: 0.3470Epoch 2/15: [============================= ] 62/63 batches, loss: 0.3435Epoch 2/15: [==============================] 63/63 batches, loss: 0.3402
[2025-05-02 12:57:20,117][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.3402
[2025-05-02 12:57:20,314][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1635, Metrics: {'mse': 0.16039539873600006, 'rmse': 0.4004939434448417, 'r2': -2.2677087783813477}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.4169Epoch 3/15: [                              ] 2/63 batches, loss: 0.4056Epoch 3/15: [=                             ] 3/63 batches, loss: 0.3251Epoch 3/15: [=                             ] 4/63 batches, loss: 0.3115Epoch 3/15: [==                            ] 5/63 batches, loss: 0.2862Epoch 3/15: [==                            ] 6/63 batches, loss: 0.2963Epoch 3/15: [===                           ] 7/63 batches, loss: 0.2912Epoch 3/15: [===                           ] 8/63 batches, loss: 0.2834Epoch 3/15: [====                          ] 9/63 batches, loss: 0.2739Epoch 3/15: [====                          ] 10/63 batches, loss: 0.3246Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.3094Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.3073Epoch 3/15: [======                        ] 13/63 batches, loss: 0.3020Epoch 3/15: [======                        ] 14/63 batches, loss: 0.3199Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.3108Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.3142Epoch 3/15: [========                      ] 17/63 batches, loss: 0.3105Epoch 3/15: [========                      ] 18/63 batches, loss: 0.3184Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.3336Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.3302Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.3285Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.3254Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.3286Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.3278Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.3237Epoch 3/15: [============                  ] 26/63 batches, loss: 0.3218Epoch 3/15: [============                  ] 27/63 batches, loss: 0.3198Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.3202Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.3139Epoch 3/15: [==============                ] 30/63 batches, loss: 0.3101Epoch 3/15: [==============                ] 31/63 batches, loss: 0.3096Epoch 3/15: [===============               ] 32/63 batches, loss: 0.3137Epoch 3/15: [===============               ] 33/63 batches, loss: 0.3105Epoch 3/15: [================              ] 34/63 batches, loss: 0.3056Epoch 3/15: [================              ] 35/63 batches, loss: 0.2990Epoch 3/15: [=================             ] 36/63 batches, loss: 0.3023Epoch 3/15: [=================             ] 37/63 batches, loss: 0.2983Epoch 3/15: [==================            ] 38/63 batches, loss: 0.2949Epoch 3/15: [==================            ] 39/63 batches, loss: 0.2932Epoch 3/15: [===================           ] 40/63 batches, loss: 0.2930Epoch 3/15: [===================           ] 41/63 batches, loss: 0.2912Epoch 3/15: [====================          ] 42/63 batches, loss: 0.2873Epoch 3/15: [====================          ] 43/63 batches, loss: 0.2867Epoch 3/15: [====================          ] 44/63 batches, loss: 0.2837Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.2827Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.2796Epoch 3/15: [======================        ] 47/63 batches, loss: 0.2789Epoch 3/15: [======================        ] 48/63 batches, loss: 0.2771Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.2794Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.2776Epoch 3/15: [========================      ] 51/63 batches, loss: 0.2779Epoch 3/15: [========================      ] 52/63 batches, loss: 0.2758Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.2762Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.2734Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.2704Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.2696Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.2697Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.2677Epoch 3/15: [============================  ] 59/63 batches, loss: 0.2695Epoch 3/15: [============================  ] 60/63 batches, loss: 0.2693Epoch 3/15: [============================= ] 61/63 batches, loss: 0.2675Epoch 3/15: [============================= ] 62/63 batches, loss: 0.2653Epoch 3/15: [==============================] 63/63 batches, loss: 0.2633
[2025-05-02 12:57:22,644][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.2633
[2025-05-02 12:57:22,851][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1497, Metrics: {'mse': 0.14706364274024963, 'rmse': 0.38348877785438473, 'r2': -1.996103286743164}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.2396Epoch 4/15: [                              ] 2/63 batches, loss: 0.2125Epoch 4/15: [=                             ] 3/63 batches, loss: 0.2691Epoch 4/15: [=                             ] 4/63 batches, loss: 0.2655Epoch 4/15: [==                            ] 5/63 batches, loss: 0.2609Epoch 4/15: [==                            ] 6/63 batches, loss: 0.2815Epoch 4/15: [===                           ] 7/63 batches, loss: 0.2834Epoch 4/15: [===                           ] 8/63 batches, loss: 0.2686Epoch 4/15: [====                          ] 9/63 batches, loss: 0.2736Epoch 4/15: [====                          ] 10/63 batches, loss: 0.2687Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.2644Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.2583Epoch 4/15: [======                        ] 13/63 batches, loss: 0.2508Epoch 4/15: [======                        ] 14/63 batches, loss: 0.2481Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.2450Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.2467Epoch 4/15: [========                      ] 17/63 batches, loss: 0.2427Epoch 4/15: [========                      ] 18/63 batches, loss: 0.2668Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.2559Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.2605Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.2560Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.2503Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.2455Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.2437Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.2442Epoch 4/15: [============                  ] 26/63 batches, loss: 0.2433Epoch 4/15: [============                  ] 27/63 batches, loss: 0.2406Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.2452Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.2562Epoch 4/15: [==============                ] 30/63 batches, loss: 0.2527Epoch 4/15: [==============                ] 31/63 batches, loss: 0.2508Epoch 4/15: [===============               ] 32/63 batches, loss: 0.2508Epoch 4/15: [===============               ] 33/63 batches, loss: 0.2465Epoch 4/15: [================              ] 34/63 batches, loss: 0.2416Epoch 4/15: [================              ] 35/63 batches, loss: 0.2470Epoch 4/15: [=================             ] 36/63 batches, loss: 0.2443Epoch 4/15: [=================             ] 37/63 batches, loss: 0.2436Epoch 4/15: [==================            ] 38/63 batches, loss: 0.2447Epoch 4/15: [==================            ] 39/63 batches, loss: 0.2488Epoch 4/15: [===================           ] 40/63 batches, loss: 0.2468Epoch 4/15: [===================           ] 41/63 batches, loss: 0.2503Epoch 4/15: [====================          ] 42/63 batches, loss: 0.2495Epoch 4/15: [====================          ] 43/63 batches, loss: 0.2476Epoch 4/15: [====================          ] 44/63 batches, loss: 0.2458Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.2445Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.2421Epoch 4/15: [======================        ] 47/63 batches, loss: 0.2411Epoch 4/15: [======================        ] 48/63 batches, loss: 0.2407Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.2419Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.2405Epoch 4/15: [========================      ] 51/63 batches, loss: 0.2442Epoch 4/15: [========================      ] 52/63 batches, loss: 0.2446Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.2423Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.2448Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.2461Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.2439Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.2435Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.2418Epoch 4/15: [============================  ] 59/63 batches, loss: 0.2429Epoch 4/15: [============================  ] 60/63 batches, loss: 0.2418Epoch 4/15: [============================= ] 61/63 batches, loss: 0.2402Epoch 4/15: [============================= ] 62/63 batches, loss: 0.2387Epoch 4/15: [==============================] 63/63 batches, loss: 0.2377
[2025-05-02 12:57:25,173][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.2377
[2025-05-02 12:57:25,383][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1462, Metrics: {'mse': 0.14319993555545807, 'rmse': 0.37841767341848354, 'r2': -1.9173884391784668}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1863Epoch 5/15: [                              ] 2/63 batches, loss: 0.1858Epoch 5/15: [=                             ] 3/63 batches, loss: 0.2492Epoch 5/15: [=                             ] 4/63 batches, loss: 0.2362Epoch 5/15: [==                            ] 5/63 batches, loss: 0.2237Epoch 5/15: [==                            ] 6/63 batches, loss: 0.2152Epoch 5/15: [===                           ] 7/63 batches, loss: 0.2201Epoch 5/15: [===                           ] 8/63 batches, loss: 0.2143Epoch 5/15: [====                          ] 9/63 batches, loss: 0.2008Epoch 5/15: [====                          ] 10/63 batches, loss: 0.2009Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.2003Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.1901Epoch 5/15: [======                        ] 13/63 batches, loss: 0.1884Epoch 5/15: [======                        ] 14/63 batches, loss: 0.2100Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.2100Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.2063Epoch 5/15: [========                      ] 17/63 batches, loss: 0.2028Epoch 5/15: [========                      ] 18/63 batches, loss: 0.2148Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.2145Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.2144Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.2154Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.2151Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.2164Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.2149Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.2140Epoch 5/15: [============                  ] 26/63 batches, loss: 0.2121Epoch 5/15: [============                  ] 27/63 batches, loss: 0.2107Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.2078Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.2060Epoch 5/15: [==============                ] 30/63 batches, loss: 0.2048Epoch 5/15: [==============                ] 31/63 batches, loss: 0.2035Epoch 5/15: [===============               ] 32/63 batches, loss: 0.2018Epoch 5/15: [===============               ] 33/63 batches, loss: 0.2005Epoch 5/15: [================              ] 34/63 batches, loss: 0.2010Epoch 5/15: [================              ] 35/63 batches, loss: 0.1993Epoch 5/15: [=================             ] 36/63 batches, loss: 0.1987Epoch 5/15: [=================             ] 37/63 batches, loss: 0.1979Epoch 5/15: [==================            ] 38/63 batches, loss: 0.1954Epoch 5/15: [==================            ] 39/63 batches, loss: 0.1947Epoch 5/15: [===================           ] 40/63 batches, loss: 0.1985Epoch 5/15: [===================           ] 41/63 batches, loss: 0.1981Epoch 5/15: [====================          ] 42/63 batches, loss: 0.1981Epoch 5/15: [====================          ] 43/63 batches, loss: 0.2021Epoch 5/15: [====================          ] 44/63 batches, loss: 0.2050Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.2048Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.2038Epoch 5/15: [======================        ] 47/63 batches, loss: 0.2026Epoch 5/15: [======================        ] 48/63 batches, loss: 0.2010Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.1991Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.2039Epoch 5/15: [========================      ] 51/63 batches, loss: 0.2033Epoch 5/15: [========================      ] 52/63 batches, loss: 0.2040Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.2054Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.2047Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.2051Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.2081Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.2053Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.2094Epoch 5/15: [============================  ] 59/63 batches, loss: 0.2086Epoch 5/15: [============================  ] 60/63 batches, loss: 0.2067Epoch 5/15: [============================= ] 61/63 batches, loss: 0.2083Epoch 5/15: [============================= ] 62/63 batches, loss: 0.2089Epoch 5/15: [==============================] 63/63 batches, loss: 0.2070
[2025-05-02 12:57:27,672][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.2070
[2025-05-02 12:57:27,888][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1296, Metrics: {'mse': 0.1271473914384842, 'rmse': 0.3565773288341313, 'r2': -1.5903527736663818}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.1656Epoch 6/15: [                              ] 2/63 batches, loss: 0.2110Epoch 6/15: [=                             ] 3/63 batches, loss: 0.1732Epoch 6/15: [=                             ] 4/63 batches, loss: 0.1561Epoch 6/15: [==                            ] 5/63 batches, loss: 0.1431Epoch 6/15: [==                            ] 6/63 batches, loss: 0.1385Epoch 6/15: [===                           ] 7/63 batches, loss: 0.1430Epoch 6/15: [===                           ] 8/63 batches, loss: 0.1445Epoch 6/15: [====                          ] 9/63 batches, loss: 0.1497Epoch 6/15: [====                          ] 10/63 batches, loss: 0.1420Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.1802Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.1806Epoch 6/15: [======                        ] 13/63 batches, loss: 0.1999Epoch 6/15: [======                        ] 14/63 batches, loss: 0.1935Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.1907Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.1856Epoch 6/15: [========                      ] 17/63 batches, loss: 0.1843Epoch 6/15: [========                      ] 18/63 batches, loss: 0.1894Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.1890Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.1849Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.1820Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.1812Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.1795Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.1797Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.1876Epoch 6/15: [============                  ] 26/63 batches, loss: 0.1868Epoch 6/15: [============                  ] 27/63 batches, loss: 0.1884Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.1911Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.1913Epoch 6/15: [==============                ] 30/63 batches, loss: 0.1917Epoch 6/15: [==============                ] 31/63 batches, loss: 0.1921Epoch 6/15: [===============               ] 32/63 batches, loss: 0.1929Epoch 6/15: [===============               ] 33/63 batches, loss: 0.1911Epoch 6/15: [================              ] 34/63 batches, loss: 0.1911Epoch 6/15: [================              ] 35/63 batches, loss: 0.1919Epoch 6/15: [=================             ] 36/63 batches, loss: 0.1936Epoch 6/15: [=================             ] 37/63 batches, loss: 0.1925Epoch 6/15: [==================            ] 38/63 batches, loss: 0.1918Epoch 6/15: [==================            ] 39/63 batches, loss: 0.1937Epoch 6/15: [===================           ] 40/63 batches, loss: 0.1973Epoch 6/15: [===================           ] 41/63 batches, loss: 0.1959Epoch 6/15: [====================          ] 42/63 batches, loss: 0.1991Epoch 6/15: [====================          ] 43/63 batches, loss: 0.1991Epoch 6/15: [====================          ] 44/63 batches, loss: 0.1981Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.1961Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.1963Epoch 6/15: [======================        ] 47/63 batches, loss: 0.1977Epoch 6/15: [======================        ] 48/63 batches, loss: 0.1959Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.1945Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.1939Epoch 6/15: [========================      ] 51/63 batches, loss: 0.1939Epoch 6/15: [========================      ] 52/63 batches, loss: 0.1936Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.1919Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.1909Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.1917Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.1940Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.1946Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.1931Epoch 6/15: [============================  ] 59/63 batches, loss: 0.1936Epoch 6/15: [============================  ] 60/63 batches, loss: 0.1937Epoch 6/15: [============================= ] 61/63 batches, loss: 0.1943Epoch 6/15: [============================= ] 62/63 batches, loss: 0.1923Epoch 6/15: [==============================] 63/63 batches, loss: 0.1907
[2025-05-02 12:57:30,224][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1907
[2025-05-02 12:57:30,439][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1134, Metrics: {'mse': 0.11160631477832794, 'rmse': 0.33407531303334576, 'r2': -1.2737369537353516}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.1698Epoch 7/15: [                              ] 2/63 batches, loss: 0.2085Epoch 7/15: [=                             ] 3/63 batches, loss: 0.2271Epoch 7/15: [=                             ] 4/63 batches, loss: 0.2255Epoch 7/15: [==                            ] 5/63 batches, loss: 0.2019Epoch 7/15: [==                            ] 6/63 batches, loss: 0.1883Epoch 7/15: [===                           ] 7/63 batches, loss: 0.2125Epoch 7/15: [===                           ] 8/63 batches, loss: 0.2075Epoch 7/15: [====                          ] 9/63 batches, loss: 0.1951Epoch 7/15: [====                          ] 10/63 batches, loss: 0.1851Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.1876Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.1853Epoch 7/15: [======                        ] 13/63 batches, loss: 0.1799Epoch 7/15: [======                        ] 14/63 batches, loss: 0.1834Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.1886Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.1852Epoch 7/15: [========                      ] 17/63 batches, loss: 0.1947Epoch 7/15: [========                      ] 18/63 batches, loss: 0.2057Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.2094Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.2049Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.2002Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.1977Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.1929Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.1910Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.1860Epoch 7/15: [============                  ] 26/63 batches, loss: 0.1804Epoch 7/15: [============                  ] 27/63 batches, loss: 0.1800Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.1818Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.1776Epoch 7/15: [==============                ] 30/63 batches, loss: 0.1778Epoch 7/15: [==============                ] 31/63 batches, loss: 0.1801Epoch 7/15: [===============               ] 32/63 batches, loss: 0.1817Epoch 7/15: [===============               ] 33/63 batches, loss: 0.1877Epoch 7/15: [================              ] 34/63 batches, loss: 0.1869Epoch 7/15: [================              ] 35/63 batches, loss: 0.1871Epoch 7/15: [=================             ] 36/63 batches, loss: 0.1882Epoch 7/15: [=================             ] 37/63 batches, loss: 0.1858Epoch 7/15: [==================            ] 38/63 batches, loss: 0.1858Epoch 7/15: [==================            ] 39/63 batches, loss: 0.1857Epoch 7/15: [===================           ] 40/63 batches, loss: 0.1834Epoch 7/15: [===================           ] 41/63 batches, loss: 0.1850Epoch 7/15: [====================          ] 42/63 batches, loss: 0.1872Epoch 7/15: [====================          ] 43/63 batches, loss: 0.1885Epoch 7/15: [====================          ] 44/63 batches, loss: 0.1885Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.1881Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.1854Epoch 7/15: [======================        ] 47/63 batches, loss: 0.1856Epoch 7/15: [======================        ] 48/63 batches, loss: 0.1842Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.1831Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.1843Epoch 7/15: [========================      ] 51/63 batches, loss: 0.1846Epoch 7/15: [========================      ] 52/63 batches, loss: 0.1838Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.1840Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.1838Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.1841Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.1851Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.1845Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.1842Epoch 7/15: [============================  ] 59/63 batches, loss: 0.1860Epoch 7/15: [============================  ] 60/63 batches, loss: 0.1869Epoch 7/15: [============================= ] 61/63 batches, loss: 0.1869Epoch 7/15: [============================= ] 62/63 batches, loss: 0.1861Epoch 7/15: [==============================] 63/63 batches, loss: 0.1844
[2025-05-02 12:57:32,797][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1844
[2025-05-02 12:57:32,998][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1062, Metrics: {'mse': 0.10438092052936554, 'rmse': 0.3230803623394117, 'r2': -1.126535177230835}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0753Epoch 8/15: [                              ] 2/63 batches, loss: 0.1416Epoch 8/15: [=                             ] 3/63 batches, loss: 0.1479Epoch 8/15: [=                             ] 4/63 batches, loss: 0.1303Epoch 8/15: [==                            ] 5/63 batches, loss: 0.1231Epoch 8/15: [==                            ] 6/63 batches, loss: 0.1244Epoch 8/15: [===                           ] 7/63 batches, loss: 0.1452Epoch 8/15: [===                           ] 8/63 batches, loss: 0.1463Epoch 8/15: [====                          ] 9/63 batches, loss: 0.1504Epoch 8/15: [====                          ] 10/63 batches, loss: 0.1617Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.1553Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.1745Epoch 8/15: [======                        ] 13/63 batches, loss: 0.1796Epoch 8/15: [======                        ] 14/63 batches, loss: 0.1752Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.1797Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.1748Epoch 8/15: [========                      ] 17/63 batches, loss: 0.1729Epoch 8/15: [========                      ] 18/63 batches, loss: 0.1743Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.1763Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.1741Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.1841Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.1811Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.1804Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.1803Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.1758Epoch 8/15: [============                  ] 26/63 batches, loss: 0.1770Epoch 8/15: [============                  ] 27/63 batches, loss: 0.1788Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.1840Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.1819Epoch 8/15: [==============                ] 30/63 batches, loss: 0.1792Epoch 8/15: [==============                ] 31/63 batches, loss: 0.1806Epoch 8/15: [===============               ] 32/63 batches, loss: 0.1786Epoch 8/15: [===============               ] 33/63 batches, loss: 0.1776Epoch 8/15: [================              ] 34/63 batches, loss: 0.1757Epoch 8/15: [================              ] 35/63 batches, loss: 0.1737Epoch 8/15: [=================             ] 36/63 batches, loss: 0.1727Epoch 8/15: [=================             ] 37/63 batches, loss: 0.1744Epoch 8/15: [==================            ] 38/63 batches, loss: 0.1716Epoch 8/15: [==================            ] 39/63 batches, loss: 0.1720Epoch 8/15: [===================           ] 40/63 batches, loss: 0.1712Epoch 8/15: [===================           ] 41/63 batches, loss: 0.1719Epoch 8/15: [====================          ] 42/63 batches, loss: 0.1707Epoch 8/15: [====================          ] 43/63 batches, loss: 0.1693Epoch 8/15: [====================          ] 44/63 batches, loss: 0.1670Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.1654Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.1666Epoch 8/15: [======================        ] 47/63 batches, loss: 0.1649Epoch 8/15: [======================        ] 48/63 batches, loss: 0.1647Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.1652Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.1677Epoch 8/15: [========================      ] 51/63 batches, loss: 0.1668Epoch 8/15: [========================      ] 52/63 batches, loss: 0.1651Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.1637Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.1635Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.1629Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.1618Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.1623Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.1625Epoch 8/15: [============================  ] 59/63 batches, loss: 0.1621Epoch 8/15: [============================  ] 60/63 batches, loss: 0.1621Epoch 8/15: [============================= ] 61/63 batches, loss: 0.1609Epoch 8/15: [============================= ] 62/63 batches, loss: 0.1621Epoch 8/15: [==============================] 63/63 batches, loss: 0.1611
[2025-05-02 12:57:35,387][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.1611
[2025-05-02 12:57:35,596][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.1025, Metrics: {'mse': 0.10057950019836426, 'rmse': 0.3171427126679159, 'r2': -1.0490894317626953}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.0929Epoch 9/15: [                              ] 2/63 batches, loss: 0.1274Epoch 9/15: [=                             ] 3/63 batches, loss: 0.1259Epoch 9/15: [=                             ] 4/63 batches, loss: 0.1405Epoch 9/15: [==                            ] 5/63 batches, loss: 0.1696Epoch 9/15: [==                            ] 6/63 batches, loss: 0.1590Epoch 9/15: [===                           ] 7/63 batches, loss: 0.1796Epoch 9/15: [===                           ] 8/63 batches, loss: 0.1693Epoch 9/15: [====                          ] 9/63 batches, loss: 0.1794Epoch 9/15: [====                          ] 10/63 batches, loss: 0.1877Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.1776Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.1761Epoch 9/15: [======                        ] 13/63 batches, loss: 0.1710Epoch 9/15: [======                        ] 14/63 batches, loss: 0.1674Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.1660Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.1662Epoch 9/15: [========                      ] 17/63 batches, loss: 0.1632Epoch 9/15: [========                      ] 18/63 batches, loss: 0.1648Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.1681Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.1743Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.1723Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.1766Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.1746Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.1741Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.1697Epoch 9/15: [============                  ] 26/63 batches, loss: 0.1775Epoch 9/15: [============                  ] 27/63 batches, loss: 0.1746Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.1767Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.1756Epoch 9/15: [==============                ] 30/63 batches, loss: 0.1745Epoch 9/15: [==============                ] 31/63 batches, loss: 0.1745Epoch 9/15: [===============               ] 32/63 batches, loss: 0.1721Epoch 9/15: [===============               ] 33/63 batches, loss: 0.1709Epoch 9/15: [================              ] 34/63 batches, loss: 0.1713Epoch 9/15: [================              ] 35/63 batches, loss: 0.1684Epoch 9/15: [=================             ] 36/63 batches, loss: 0.1769Epoch 9/15: [=================             ] 37/63 batches, loss: 0.1739Epoch 9/15: [==================            ] 38/63 batches, loss: 0.1751Epoch 9/15: [==================            ] 39/63 batches, loss: 0.1764Epoch 9/15: [===================           ] 40/63 batches, loss: 0.1743Epoch 9/15: [===================           ] 41/63 batches, loss: 0.1730Epoch 9/15: [====================          ] 42/63 batches, loss: 0.1733Epoch 9/15: [====================          ] 43/63 batches, loss: 0.1721Epoch 9/15: [====================          ] 44/63 batches, loss: 0.1726Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.1725Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.1711Epoch 9/15: [======================        ] 47/63 batches, loss: 0.1703Epoch 9/15: [======================        ] 48/63 batches, loss: 0.1710Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.1694Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.1700Epoch 9/15: [========================      ] 51/63 batches, loss: 0.1679Epoch 9/15: [========================      ] 52/63 batches, loss: 0.1661Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.1654Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.1667Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.1654Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.1640Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.1660Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.1651Epoch 9/15: [============================  ] 59/63 batches, loss: 0.1671Epoch 9/15: [============================  ] 60/63 batches, loss: 0.1687Epoch 9/15: [============================= ] 61/63 batches, loss: 0.1687Epoch 9/15: [============================= ] 62/63 batches, loss: 0.1691Epoch 9/15: [==============================] 63/63 batches, loss: 0.1707
[2025-05-02 12:57:37,929][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.1707
[2025-05-02 12:57:38,153][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0959, Metrics: {'mse': 0.09394688159227371, 'rmse': 0.30650755552232917, 'r2': -0.9139641523361206}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.1931Epoch 10/15: [                              ] 2/63 batches, loss: 0.3223Epoch 10/15: [=                             ] 3/63 batches, loss: 0.2355Epoch 10/15: [=                             ] 4/63 batches, loss: 0.2081Epoch 10/15: [==                            ] 5/63 batches, loss: 0.1987Epoch 10/15: [==                            ] 6/63 batches, loss: 0.1800Epoch 10/15: [===                           ] 7/63 batches, loss: 0.1801Epoch 10/15: [===                           ] 8/63 batches, loss: 0.1744Epoch 10/15: [====                          ] 9/63 batches, loss: 0.1656Epoch 10/15: [====                          ] 10/63 batches, loss: 0.1653Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.1673Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.1708Epoch 10/15: [======                        ] 13/63 batches, loss: 0.1692Epoch 10/15: [======                        ] 14/63 batches, loss: 0.1818Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.1828Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.1799Epoch 10/15: [========                      ] 17/63 batches, loss: 0.1768Epoch 10/15: [========                      ] 18/63 batches, loss: 0.1755Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.1722Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.1709Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.1795Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.1751Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.1733Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.1747Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.1715Epoch 10/15: [============                  ] 26/63 batches, loss: 0.1711Epoch 10/15: [============                  ] 27/63 batches, loss: 0.1710Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.1731Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.1702Epoch 10/15: [==============                ] 30/63 batches, loss: 0.1688Epoch 10/15: [==============                ] 31/63 batches, loss: 0.1707Epoch 10/15: [===============               ] 32/63 batches, loss: 0.1700Epoch 10/15: [===============               ] 33/63 batches, loss: 0.1675Epoch 10/15: [================              ] 34/63 batches, loss: 0.1671Epoch 10/15: [================              ] 35/63 batches, loss: 0.1707Epoch 10/15: [=================             ] 36/63 batches, loss: 0.1721Epoch 10/15: [=================             ] 37/63 batches, loss: 0.1694Epoch 10/15: [==================            ] 38/63 batches, loss: 0.1671Epoch 10/15: [==================            ] 39/63 batches, loss: 0.1683Epoch 10/15: [===================           ] 40/63 batches, loss: 0.1687Epoch 10/15: [===================           ] 41/63 batches, loss: 0.1673Epoch 10/15: [====================          ] 42/63 batches, loss: 0.1661Epoch 10/15: [====================          ] 43/63 batches, loss: 0.1662Epoch 10/15: [====================          ] 44/63 batches, loss: 0.1663Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.1664Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.1650Epoch 10/15: [======================        ] 47/63 batches, loss: 0.1640Epoch 10/15: [======================        ] 48/63 batches, loss: 0.1650Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.1649Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.1634Epoch 10/15: [========================      ] 51/63 batches, loss: 0.1640Epoch 10/15: [========================      ] 52/63 batches, loss: 0.1624Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.1619Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.1625Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.1624Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.1614Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.1602Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.1605Epoch 10/15: [============================  ] 59/63 batches, loss: 0.1600Epoch 10/15: [============================  ] 60/63 batches, loss: 0.1606Epoch 10/15: [============================= ] 61/63 batches, loss: 0.1598Epoch 10/15: [============================= ] 62/63 batches, loss: 0.1602Epoch 10/15: [==============================] 63/63 batches, loss: 0.1656
[2025-05-02 12:57:40,489][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.1656
[2025-05-02 12:57:40,719][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0910, Metrics: {'mse': 0.08907689154148102, 'rmse': 0.298457520497442, 'r2': -0.8147487640380859}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.2169Epoch 11/15: [                              ] 2/63 batches, loss: 0.1850Epoch 11/15: [=                             ] 3/63 batches, loss: 0.1520Epoch 11/15: [=                             ] 4/63 batches, loss: 0.1290Epoch 11/15: [==                            ] 5/63 batches, loss: 0.1142Epoch 11/15: [==                            ] 6/63 batches, loss: 0.1076Epoch 11/15: [===                           ] 7/63 batches, loss: 0.1040Epoch 11/15: [===                           ] 8/63 batches, loss: 0.1116Epoch 11/15: [====                          ] 9/63 batches, loss: 0.1124Epoch 11/15: [====                          ] 10/63 batches, loss: 0.1124Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.1078Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.1186Epoch 11/15: [======                        ] 13/63 batches, loss: 0.1224Epoch 11/15: [======                        ] 14/63 batches, loss: 0.1377Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.1321Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.1392Epoch 11/15: [========                      ] 17/63 batches, loss: 0.1390Epoch 11/15: [========                      ] 18/63 batches, loss: 0.1369Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.1325Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.1306Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.1299Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.1289Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.1279Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.1286Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.1272Epoch 11/15: [============                  ] 26/63 batches, loss: 0.1243Epoch 11/15: [============                  ] 27/63 batches, loss: 0.1286Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.1305Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.1308Epoch 11/15: [==============                ] 30/63 batches, loss: 0.1307Epoch 11/15: [==============                ] 31/63 batches, loss: 0.1316Epoch 11/15: [===============               ] 32/63 batches, loss: 0.1342Epoch 11/15: [===============               ] 33/63 batches, loss: 0.1338Epoch 11/15: [================              ] 34/63 batches, loss: 0.1335Epoch 11/15: [================              ] 35/63 batches, loss: 0.1349Epoch 11/15: [=================             ] 36/63 batches, loss: 0.1363Epoch 11/15: [=================             ] 37/63 batches, loss: 0.1384Epoch 11/15: [==================            ] 38/63 batches, loss: 0.1369Epoch 11/15: [==================            ] 39/63 batches, loss: 0.1393Epoch 11/15: [===================           ] 40/63 batches, loss: 0.1401Epoch 11/15: [===================           ] 41/63 batches, loss: 0.1405Epoch 11/15: [====================          ] 42/63 batches, loss: 0.1396Epoch 11/15: [====================          ] 43/63 batches, loss: 0.1393Epoch 11/15: [====================          ] 44/63 batches, loss: 0.1382Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.1398Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.1393Epoch 11/15: [======================        ] 47/63 batches, loss: 0.1390Epoch 11/15: [======================        ] 48/63 batches, loss: 0.1410Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.1397Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.1409Epoch 11/15: [========================      ] 51/63 batches, loss: 0.1405Epoch 11/15: [========================      ] 52/63 batches, loss: 0.1400Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.1414Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.1406Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.1401Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.1390Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.1378Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.1388Epoch 11/15: [============================  ] 59/63 batches, loss: 0.1384Epoch 11/15: [============================  ] 60/63 batches, loss: 0.1383Epoch 11/15: [============================= ] 61/63 batches, loss: 0.1378Epoch 11/15: [============================= ] 62/63 batches, loss: 0.1368Epoch 11/15: [==============================] 63/63 batches, loss: 0.1355
[2025-05-02 12:57:43,098][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.1355
[2025-05-02 12:57:43,318][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0870, Metrics: {'mse': 0.08505642414093018, 'rmse': 0.2916443452922243, 'r2': -0.7328404188156128}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.1114Epoch 12/15: [                              ] 2/63 batches, loss: 0.0851Epoch 12/15: [=                             ] 3/63 batches, loss: 0.0924Epoch 12/15: [=                             ] 4/63 batches, loss: 0.1145Epoch 12/15: [==                            ] 5/63 batches, loss: 0.1403Epoch 12/15: [==                            ] 6/63 batches, loss: 0.1395Epoch 12/15: [===                           ] 7/63 batches, loss: 0.1549Epoch 12/15: [===                           ] 8/63 batches, loss: 0.1513Epoch 12/15: [====                          ] 9/63 batches, loss: 0.1662Epoch 12/15: [====                          ] 10/63 batches, loss: 0.1627Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.1642Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.1577Epoch 12/15: [======                        ] 13/63 batches, loss: 0.1602Epoch 12/15: [======                        ] 14/63 batches, loss: 0.1563Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.1585Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.1571Epoch 12/15: [========                      ] 17/63 batches, loss: 0.1530Epoch 12/15: [========                      ] 18/63 batches, loss: 0.1482Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.1461Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.1436Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.1450Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.1462Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.1493Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.1457Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.1424Epoch 12/15: [============                  ] 26/63 batches, loss: 0.1429Epoch 12/15: [============                  ] 27/63 batches, loss: 0.1434Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.1396Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.1381Epoch 12/15: [==============                ] 30/63 batches, loss: 0.1448Epoch 12/15: [==============                ] 31/63 batches, loss: 0.1430Epoch 12/15: [===============               ] 32/63 batches, loss: 0.1418Epoch 12/15: [===============               ] 33/63 batches, loss: 0.1421Epoch 12/15: [================              ] 34/63 batches, loss: 0.1422Epoch 12/15: [================              ] 35/63 batches, loss: 0.1436Epoch 12/15: [=================             ] 36/63 batches, loss: 0.1427Epoch 12/15: [=================             ] 37/63 batches, loss: 0.1439Epoch 12/15: [==================            ] 38/63 batches, loss: 0.1410Epoch 12/15: [==================            ] 39/63 batches, loss: 0.1416Epoch 12/15: [===================           ] 40/63 batches, loss: 0.1395Epoch 12/15: [===================           ] 41/63 batches, loss: 0.1404Epoch 12/15: [====================          ] 42/63 batches, loss: 0.1399Epoch 12/15: [====================          ] 43/63 batches, loss: 0.1403Epoch 12/15: [====================          ] 44/63 batches, loss: 0.1405Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.1412Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.1397Epoch 12/15: [======================        ] 47/63 batches, loss: 0.1416Epoch 12/15: [======================        ] 48/63 batches, loss: 0.1411Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.1402Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.1421Epoch 12/15: [========================      ] 51/63 batches, loss: 0.1430Epoch 12/15: [========================      ] 52/63 batches, loss: 0.1421Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.1416Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.1404Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.1400Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.1394Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.1390Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.1431Epoch 12/15: [============================  ] 59/63 batches, loss: 0.1452Epoch 12/15: [============================  ] 60/63 batches, loss: 0.1447Epoch 12/15: [============================= ] 61/63 batches, loss: 0.1444Epoch 12/15: [============================= ] 62/63 batches, loss: 0.1439Epoch 12/15: [==============================] 63/63 batches, loss: 0.1439
[2025-05-02 12:57:45,682][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.1439
[2025-05-02 12:57:45,918][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0845, Metrics: {'mse': 0.08255431801080704, 'rmse': 0.2873226722881559, 'r2': -0.6818654537200928}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.0695Epoch 13/15: [                              ] 2/63 batches, loss: 0.0857Epoch 13/15: [=                             ] 3/63 batches, loss: 0.0895Epoch 13/15: [=                             ] 4/63 batches, loss: 0.0863Epoch 13/15: [==                            ] 5/63 batches, loss: 0.0835Epoch 13/15: [==                            ] 6/63 batches, loss: 0.0844Epoch 13/15: [===                           ] 7/63 batches, loss: 0.0842Epoch 13/15: [===                           ] 8/63 batches, loss: 0.0983Epoch 13/15: [====                          ] 9/63 batches, loss: 0.0981Epoch 13/15: [====                          ] 10/63 batches, loss: 0.0965Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.0955Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.0948Epoch 13/15: [======                        ] 13/63 batches, loss: 0.0991Epoch 13/15: [======                        ] 14/63 batches, loss: 0.1023Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.1054Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.1092Epoch 13/15: [========                      ] 17/63 batches, loss: 0.1084Epoch 13/15: [========                      ] 18/63 batches, loss: 0.1110Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.1118Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.1111Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.1099Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.1112Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.1116Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.1122Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.1154Epoch 13/15: [============                  ] 26/63 batches, loss: 0.1167Epoch 13/15: [============                  ] 27/63 batches, loss: 0.1226Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.1230Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.1233Epoch 13/15: [==============                ] 30/63 batches, loss: 0.1236Epoch 13/15: [==============                ] 31/63 batches, loss: 0.1234Epoch 13/15: [===============               ] 32/63 batches, loss: 0.1270Epoch 13/15: [===============               ] 33/63 batches, loss: 0.1257Epoch 13/15: [================              ] 34/63 batches, loss: 0.1240Epoch 13/15: [================              ] 35/63 batches, loss: 0.1258Epoch 13/15: [=================             ] 36/63 batches, loss: 0.1239Epoch 13/15: [=================             ] 37/63 batches, loss: 0.1237Epoch 13/15: [==================            ] 38/63 batches, loss: 0.1222Epoch 13/15: [==================            ] 39/63 batches, loss: 0.1216Epoch 13/15: [===================           ] 40/63 batches, loss: 0.1252Epoch 13/15: [===================           ] 41/63 batches, loss: 0.1242Epoch 13/15: [====================          ] 42/63 batches, loss: 0.1256Epoch 13/15: [====================          ] 43/63 batches, loss: 0.1243Epoch 13/15: [====================          ] 44/63 batches, loss: 0.1225Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.1259Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.1243Epoch 13/15: [======================        ] 47/63 batches, loss: 0.1241Epoch 13/15: [======================        ] 48/63 batches, loss: 0.1246Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.1242Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.1236Epoch 13/15: [========================      ] 51/63 batches, loss: 0.1239Epoch 13/15: [========================      ] 52/63 batches, loss: 0.1241Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.1245Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.1236Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.1227Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.1222Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.1214Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.1208Epoch 13/15: [============================  ] 59/63 batches, loss: 0.1216Epoch 13/15: [============================  ] 60/63 batches, loss: 0.1216Epoch 13/15: [============================= ] 61/63 batches, loss: 0.1224Epoch 13/15: [============================= ] 62/63 batches, loss: 0.1217Epoch 13/15: [==============================] 63/63 batches, loss: 0.1201
[2025-05-02 12:57:48,267][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.1201
[2025-05-02 12:57:48,497][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0767, Metrics: {'mse': 0.07527173310518265, 'rmse': 0.27435694470011623, 'r2': -0.5334986448287964}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.0460Epoch 14/15: [                              ] 2/63 batches, loss: 0.0912Epoch 14/15: [=                             ] 3/63 batches, loss: 0.1021Epoch 14/15: [=                             ] 4/63 batches, loss: 0.1020Epoch 14/15: [==                            ] 5/63 batches, loss: 0.1359Epoch 14/15: [==                            ] 6/63 batches, loss: 0.1319Epoch 14/15: [===                           ] 7/63 batches, loss: 0.1199Epoch 14/15: [===                           ] 8/63 batches, loss: 0.1211Epoch 14/15: [====                          ] 9/63 batches, loss: 0.1218Epoch 14/15: [====                          ] 10/63 batches, loss: 0.1204Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.1204Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.1235Epoch 14/15: [======                        ] 13/63 batches, loss: 0.1230Epoch 14/15: [======                        ] 14/63 batches, loss: 0.1193Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.1202Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.1182Epoch 14/15: [========                      ] 17/63 batches, loss: 0.1213Epoch 14/15: [========                      ] 18/63 batches, loss: 0.1245Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.1199Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.1207Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.1211Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.1200Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.1198Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.1179Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.1248Epoch 14/15: [============                  ] 26/63 batches, loss: 0.1240Epoch 14/15: [============                  ] 27/63 batches, loss: 0.1231Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.1215Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.1218Epoch 14/15: [==============                ] 30/63 batches, loss: 0.1234Epoch 14/15: [==============                ] 31/63 batches, loss: 0.1251Epoch 14/15: [===============               ] 32/63 batches, loss: 0.1239Epoch 14/15: [===============               ] 33/63 batches, loss: 0.1259Epoch 14/15: [================              ] 34/63 batches, loss: 0.1243Epoch 14/15: [================              ] 35/63 batches, loss: 0.1228Epoch 14/15: [=================             ] 36/63 batches, loss: 0.1223Epoch 14/15: [=================             ] 37/63 batches, loss: 0.1210Epoch 14/15: [==================            ] 38/63 batches, loss: 0.1223Epoch 14/15: [==================            ] 39/63 batches, loss: 0.1262Epoch 14/15: [===================           ] 40/63 batches, loss: 0.1267Epoch 14/15: [===================           ] 41/63 batches, loss: 0.1251Epoch 14/15: [====================          ] 42/63 batches, loss: 0.1267Epoch 14/15: [====================          ] 43/63 batches, loss: 0.1256Epoch 14/15: [====================          ] 44/63 batches, loss: 0.1261Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.1253Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.1253Epoch 14/15: [======================        ] 47/63 batches, loss: 0.1241Epoch 14/15: [======================        ] 48/63 batches, loss: 0.1271Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.1269Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.1275Epoch 14/15: [========================      ] 51/63 batches, loss: 0.1276Epoch 14/15: [========================      ] 52/63 batches, loss: 0.1263Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.1276Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.1280Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.1276Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.1276Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.1288Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.1280Epoch 14/15: [============================  ] 59/63 batches, loss: 0.1279Epoch 14/15: [============================  ] 60/63 batches, loss: 0.1263Epoch 14/15: [============================= ] 61/63 batches, loss: 0.1264Epoch 14/15: [============================= ] 62/63 batches, loss: 0.1256Epoch 14/15: [==============================] 63/63 batches, loss: 0.1277
[2025-05-02 12:57:50,852][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.1277
[2025-05-02 12:57:51,068][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0748, Metrics: {'mse': 0.07320576161146164, 'rmse': 0.2705656327242276, 'r2': -0.4914088249206543}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.1364Epoch 15/15: [                              ] 2/63 batches, loss: 0.1038Epoch 15/15: [=                             ] 3/63 batches, loss: 0.0853Epoch 15/15: [=                             ] 4/63 batches, loss: 0.0892Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0923Epoch 15/15: [==                            ] 6/63 batches, loss: 0.1008Epoch 15/15: [===                           ] 7/63 batches, loss: 0.0995Epoch 15/15: [===                           ] 8/63 batches, loss: 0.0946Epoch 15/15: [====                          ] 9/63 batches, loss: 0.0964Epoch 15/15: [====                          ] 10/63 batches, loss: 0.0977Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.0982Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.0978Epoch 15/15: [======                        ] 13/63 batches, loss: 0.0992Epoch 15/15: [======                        ] 14/63 batches, loss: 0.0959Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.0977Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.0973Epoch 15/15: [========                      ] 17/63 batches, loss: 0.1002Epoch 15/15: [========                      ] 18/63 batches, loss: 0.1011Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.0980Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.0980Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.0950Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.0935Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.0947Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.0924Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.0905Epoch 15/15: [============                  ] 26/63 batches, loss: 0.0920Epoch 15/15: [============                  ] 27/63 batches, loss: 0.0913Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.0921Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.0948Epoch 15/15: [==============                ] 30/63 batches, loss: 0.0940Epoch 15/15: [==============                ] 31/63 batches, loss: 0.0960Epoch 15/15: [===============               ] 32/63 batches, loss: 0.0959Epoch 15/15: [===============               ] 33/63 batches, loss: 0.0947Epoch 15/15: [================              ] 34/63 batches, loss: 0.0961Epoch 15/15: [================              ] 35/63 batches, loss: 0.0982Epoch 15/15: [=================             ] 36/63 batches, loss: 0.0989Epoch 15/15: [=================             ] 37/63 batches, loss: 0.0984Epoch 15/15: [==================            ] 38/63 batches, loss: 0.0991Epoch 15/15: [==================            ] 39/63 batches, loss: 0.0993Epoch 15/15: [===================           ] 40/63 batches, loss: 0.0984Epoch 15/15: [===================           ] 41/63 batches, loss: 0.0973Epoch 15/15: [====================          ] 42/63 batches, loss: 0.1003Epoch 15/15: [====================          ] 43/63 batches, loss: 0.1005Epoch 15/15: [====================          ] 44/63 batches, loss: 0.1010Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.1005Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.1006Epoch 15/15: [======================        ] 47/63 batches, loss: 0.0995Epoch 15/15: [======================        ] 48/63 batches, loss: 0.0985Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.0985Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.0989Epoch 15/15: [========================      ] 51/63 batches, loss: 0.1002Epoch 15/15: [========================      ] 52/63 batches, loss: 0.1007Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.1009Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.1002Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.1001Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.1003Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.1001Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.1022Epoch 15/15: [============================  ] 59/63 batches, loss: 0.1019Epoch 15/15: [============================  ] 60/63 batches, loss: 0.1028Epoch 15/15: [============================= ] 61/63 batches, loss: 0.1044Epoch 15/15: [============================= ] 62/63 batches, loss: 0.1038Epoch 15/15: [==============================] 63/63 batches, loss: 0.1038
[2025-05-02 12:57:53,477][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.1038
[2025-05-02 12:57:53,696][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0751, Metrics: {'mse': 0.07317238301038742, 'rmse': 0.27050394268917305, 'r2': -0.4907287359237671}
[2025-05-02 12:57:53,696][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-02 12:57:53,696][src.training.lm_trainer][INFO] - Training completed in 38.85 seconds
[2025-05-02 12:57:53,696][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 12:57:56,186][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.051010794937610626, 'rmse': 0.2258556949417274, 'r2': -1.4193620681762695}
[2025-05-02 12:57:56,187][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07320576161146164, 'rmse': 0.2705656327242276, 'r2': -0.4914088249206543}
[2025-05-02 12:57:56,187][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.09049781411886215, 'rmse': 0.30082854605050724, 'r2': -0.9539614915847778}
[2025-05-02 12:57:57,892][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer6/ar/ar/model.pt
[2025-05-02 12:57:57,894][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss ██▇▆▅▄▃▃▃▂▂▂▁▁
wandb:     best_val_mse █▇▆▆▅▄▃▃▂▂▂▂▁▁
wandb:      best_val_r2 ▁▂▃▃▄▅▆▆▇▇▇▇██
wandb:    best_val_rmse █▇▇▆▅▄▄▃▃▂▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▁▂▃▄▅▅▅▆▆▆▆▇▇
wandb:       train_loss █▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss ██▇▆▅▄▃▃▃▂▂▂▁▁▁
wandb:          val_mse █▇▆▆▅▄▃▃▂▂▂▂▁▁▁
wandb:           val_r2 ▁▂▃▃▄▅▆▆▇▇▇▇███
wandb:         val_rmse █▇▇▆▅▄▄▃▃▂▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07481
wandb:     best_val_mse 0.07321
wandb:      best_val_r2 -0.49141
wandb:    best_val_rmse 0.27057
wandb:            epoch 15
wandb:   final_test_mse 0.0905
wandb:    final_test_r2 -0.95396
wandb:  final_test_rmse 0.30083
wandb:  final_train_mse 0.05101
wandb:   final_train_r2 -1.41936
wandb: final_train_rmse 0.22586
wandb:    final_val_mse 0.07321
wandb:     final_val_r2 -0.49141
wandb:   final_val_rmse 0.27057
wandb:    learning_rate 2e-05
wandb:       train_loss 0.10376
wandb:       train_time 38.85474
wandb:         val_loss 0.07509
wandb:          val_mse 0.07317
wandb:           val_r2 -0.49073
wandb:         val_rmse 0.2705
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125705-jxqi9jnk
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125705-jxqi9jnk/logs
Experiment probe_layer6_avg_links_len_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer6/ar/ar/results.json for layer 6
=======================
PROBING LAYER 9 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer9_avg_links_len_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=9"         "model.probe_hidden_size=96" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer9_avg_links_len_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer9/ar"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 12:58:10,786][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer9/ar
experiment_name: probe_layer9_avg_links_len_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 9
  num_outputs: 1
  probe_hidden_size: 96
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-02 12:58:10,786][__main__][INFO] - Normalized task: single_submetric
[2025-05-02 12:58:10,787][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 12:58:10,787][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 12:58:10,787][__main__][INFO] - Determined Task Type: regression
[2025-05-02 12:58:10,791][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ar']
[2025-05-02 12:58:10,791][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 12:58:10,791][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 12:58:12,468][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 12:58:14,692][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 12:58:14,692][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:58:14,741][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:58:14,771][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:58:14,864][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 12:58:14,871][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:58:14,872][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 12:58:14,873][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:58:14,891][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:58:14,920][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:58:14,933][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 12:58:14,934][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:58:14,934][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 12:58:14,935][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:58:14,954][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:58:14,982][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:58:14,997][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 12:58:15,001][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:58:15,001][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 12:58:15,003][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 12:58:15,004][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 12:58:15,004][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 12:58:15,004][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 12:58:15,004][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 12:58:15,005][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8570
[2025-05-02 12:58:15,005][src.data.datasets][INFO] -   Mean: 0.1857, Std: 0.1452
[2025-05-02 12:58:15,005][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 12:58:15,005][src.data.datasets][INFO] - Sample label: 0.32100000977516174
[2025-05-02 12:58:15,005][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 12:58:15,005][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 12:58:15,005][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 12:58:15,006][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 12:58:15,006][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9290
[2025-05-02 12:58:15,006][src.data.datasets][INFO] -   Mean: 0.2504, Std: 0.2216
[2025-05-02 12:58:15,006][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 12:58:15,006][src.data.datasets][INFO] - Sample label: 0.10499999672174454
[2025-05-02 12:58:15,006][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 12:58:15,006][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 12:58:15,006][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 12:58:15,006][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 12:58:15,007][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:58:15,007][src.data.datasets][INFO] -   Mean: 0.3231, Std: 0.2152
[2025-05-02 12:58:15,007][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 12:58:15,007][src.data.datasets][INFO] - Sample label: 0.27799999713897705
[2025-05-02 12:58:15,007][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 12:58:15,007][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 12:58:15,007][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 12:58:15,008][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-02 12:58:15,008][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 12:58:19,062][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 12:58:19,063][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 12:58:19,063][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=9, freeze_model=True
[2025-05-02 12:58:19,064][src.models.model_factory][INFO] - Using provided probe_hidden_size: 96
[2025-05-02 12:58:19,066][src.models.model_factory][INFO] - Model has 84,961 trainable parameters out of 394,206,433 total parameters
[2025-05-02 12:58:19,066][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 84,961 trainable parameters
[2025-05-02 12:58:19,066][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=96, depth=2, activation=silu, normalization=layer
[2025-05-02 12:58:19,067][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 96 hidden size
[2025-05-02 12:58:19,067][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 12:58:19,067][__main__][INFO] - Total parameters: 394,206,433
[2025-05-02 12:58:19,068][__main__][INFO] - Trainable parameters: 84,961 (0.02%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 3.8530Epoch 1/15: [                              ] 2/63 batches, loss: 4.0973Epoch 1/15: [=                             ] 3/63 batches, loss: 4.0134Epoch 1/15: [=                             ] 4/63 batches, loss: 3.8778Epoch 1/15: [==                            ] 5/63 batches, loss: 3.5325Epoch 1/15: [==                            ] 6/63 batches, loss: 3.6008Epoch 1/15: [===                           ] 7/63 batches, loss: 3.4796Epoch 1/15: [===                           ] 8/63 batches, loss: 3.3163Epoch 1/15: [====                          ] 9/63 batches, loss: 3.2227Epoch 1/15: [====                          ] 10/63 batches, loss: 3.1515Epoch 1/15: [=====                         ] 11/63 batches, loss: 3.1067Epoch 1/15: [=====                         ] 12/63 batches, loss: 2.9937Epoch 1/15: [======                        ] 13/63 batches, loss: 2.9569Epoch 1/15: [======                        ] 14/63 batches, loss: 2.9040Epoch 1/15: [=======                       ] 15/63 batches, loss: 2.8460Epoch 1/15: [=======                       ] 16/63 batches, loss: 2.7918Epoch 1/15: [========                      ] 17/63 batches, loss: 2.7549Epoch 1/15: [========                      ] 18/63 batches, loss: 2.7604Epoch 1/15: [=========                     ] 19/63 batches, loss: 2.7129Epoch 1/15: [=========                     ] 20/63 batches, loss: 2.6712Epoch 1/15: [==========                    ] 21/63 batches, loss: 2.6196Epoch 1/15: [==========                    ] 22/63 batches, loss: 2.5968Epoch 1/15: [==========                    ] 23/63 batches, loss: 2.5706Epoch 1/15: [===========                   ] 24/63 batches, loss: 2.5272Epoch 1/15: [===========                   ] 25/63 batches, loss: 2.4998Epoch 1/15: [============                  ] 26/63 batches, loss: 2.4720Epoch 1/15: [============                  ] 27/63 batches, loss: 2.4399Epoch 1/15: [=============                 ] 28/63 batches, loss: 2.3935Epoch 1/15: [=============                 ] 29/63 batches, loss: 2.3534Epoch 1/15: [==============                ] 30/63 batches, loss: 2.3396Epoch 1/15: [==============                ] 31/63 batches, loss: 2.3162Epoch 1/15: [===============               ] 32/63 batches, loss: 2.2636Epoch 1/15: [===============               ] 33/63 batches, loss: 2.2194Epoch 1/15: [================              ] 34/63 batches, loss: 2.1934Epoch 1/15: [================              ] 35/63 batches, loss: 2.1600Epoch 1/15: [=================             ] 36/63 batches, loss: 2.1588Epoch 1/15: [=================             ] 37/63 batches, loss: 2.1315Epoch 1/15: [==================            ] 38/63 batches, loss: 2.1190Epoch 1/15: [==================            ] 39/63 batches, loss: 2.0892Epoch 1/15: [===================           ] 40/63 batches, loss: 2.0672Epoch 1/15: [===================           ] 41/63 batches, loss: 2.0335Epoch 1/15: [====================          ] 42/63 batches, loss: 2.0068Epoch 1/15: [====================          ] 43/63 batches, loss: 1.9766Epoch 1/15: [====================          ] 44/63 batches, loss: 1.9433Epoch 1/15: [=====================         ] 45/63 batches, loss: 1.9108Epoch 1/15: [=====================         ] 46/63 batches, loss: 1.8931Epoch 1/15: [======================        ] 47/63 batches, loss: 1.8719Epoch 1/15: [======================        ] 48/63 batches, loss: 1.8467Epoch 1/15: [=======================       ] 49/63 batches, loss: 1.8238Epoch 1/15: [=======================       ] 50/63 batches, loss: 1.8106Epoch 1/15: [========================      ] 51/63 batches, loss: 1.7858Epoch 1/15: [========================      ] 52/63 batches, loss: 1.7584Epoch 1/15: [=========================     ] 53/63 batches, loss: 1.7287Epoch 1/15: [=========================     ] 54/63 batches, loss: 1.7098Epoch 1/15: [==========================    ] 55/63 batches, loss: 1.6940Epoch 1/15: [==========================    ] 56/63 batches, loss: 1.6674Epoch 1/15: [===========================   ] 57/63 batches, loss: 1.6509Epoch 1/15: [===========================   ] 58/63 batches, loss: 1.6375Epoch 1/15: [============================  ] 59/63 batches, loss: 1.6216Epoch 1/15: [============================  ] 60/63 batches, loss: 1.6034Epoch 1/15: [============================= ] 61/63 batches, loss: 1.5815Epoch 1/15: [============================= ] 62/63 batches, loss: 1.5661Epoch 1/15: [==============================] 63/63 batches, loss: 1.5679
[2025-05-02 12:58:23,359][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 1.5679
[2025-05-02 12:58:23,544][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.2915, Metrics: {'mse': 0.2979237139225006, 'rmse': 0.5458238854452053, 'r2': -5.069550514221191}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.4397Epoch 2/15: [                              ] 2/63 batches, loss: 0.3945Epoch 2/15: [=                             ] 3/63 batches, loss: 0.3456Epoch 2/15: [=                             ] 4/63 batches, loss: 0.4164Epoch 2/15: [==                            ] 5/63 batches, loss: 0.4376Epoch 2/15: [==                            ] 6/63 batches, loss: 0.4617Epoch 2/15: [===                           ] 7/63 batches, loss: 0.4277Epoch 2/15: [===                           ] 8/63 batches, loss: 0.4074Epoch 2/15: [====                          ] 9/63 batches, loss: 0.3971Epoch 2/15: [====                          ] 10/63 batches, loss: 0.3933Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.4065Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.4033Epoch 2/15: [======                        ] 13/63 batches, loss: 0.4153Epoch 2/15: [======                        ] 14/63 batches, loss: 0.3968Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.3961Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.3961Epoch 2/15: [========                      ] 17/63 batches, loss: 0.4048Epoch 2/15: [========                      ] 18/63 batches, loss: 0.3948Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.3965Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.4116Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.4091Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.4068Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.3935Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.3927Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.3903Epoch 2/15: [============                  ] 26/63 batches, loss: 0.3852Epoch 2/15: [============                  ] 27/63 batches, loss: 0.3754Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.3685Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.3670Epoch 2/15: [==============                ] 30/63 batches, loss: 0.3649Epoch 2/15: [==============                ] 31/63 batches, loss: 0.3637Epoch 2/15: [===============               ] 32/63 batches, loss: 0.3642Epoch 2/15: [===============               ] 33/63 batches, loss: 0.3594Epoch 2/15: [================              ] 34/63 batches, loss: 0.3603Epoch 2/15: [================              ] 35/63 batches, loss: 0.3733Epoch 2/15: [=================             ] 36/63 batches, loss: 0.3747Epoch 2/15: [=================             ] 37/63 batches, loss: 0.3706Epoch 2/15: [==================            ] 38/63 batches, loss: 0.3649Epoch 2/15: [==================            ] 39/63 batches, loss: 0.3656Epoch 2/15: [===================           ] 40/63 batches, loss: 0.3649Epoch 2/15: [===================           ] 41/63 batches, loss: 0.3698Epoch 2/15: [====================          ] 42/63 batches, loss: 0.3669Epoch 2/15: [====================          ] 43/63 batches, loss: 0.3629Epoch 2/15: [====================          ] 44/63 batches, loss: 0.3645Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.3630Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.3581Epoch 2/15: [======================        ] 47/63 batches, loss: 0.3533Epoch 2/15: [======================        ] 48/63 batches, loss: 0.3472Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.3440Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.3454Epoch 2/15: [========================      ] 51/63 batches, loss: 0.3425Epoch 2/15: [========================      ] 52/63 batches, loss: 0.3409Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.3384Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.3353Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.3394Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.3372Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.3339Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.3345Epoch 2/15: [============================  ] 59/63 batches, loss: 0.3354Epoch 2/15: [============================  ] 60/63 batches, loss: 0.3329Epoch 2/15: [============================= ] 61/63 batches, loss: 0.3321Epoch 2/15: [============================= ] 62/63 batches, loss: 0.3311Epoch 2/15: [==============================] 63/63 batches, loss: 0.3308
[2025-05-02 12:58:25,867][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.3308
[2025-05-02 12:58:26,073][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1266, Metrics: {'mse': 0.1282714605331421, 'rmse': 0.35815005309666237, 'r2': -1.613253116607666}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.3443Epoch 3/15: [                              ] 2/63 batches, loss: 0.3968Epoch 3/15: [=                             ] 3/63 batches, loss: 0.3059Epoch 3/15: [=                             ] 4/63 batches, loss: 0.2790Epoch 3/15: [==                            ] 5/63 batches, loss: 0.3003Epoch 3/15: [==                            ] 6/63 batches, loss: 0.2942Epoch 3/15: [===                           ] 7/63 batches, loss: 0.2787Epoch 3/15: [===                           ] 8/63 batches, loss: 0.2869Epoch 3/15: [====                          ] 9/63 batches, loss: 0.2779Epoch 3/15: [====                          ] 10/63 batches, loss: 0.3073Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.2952Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.2914Epoch 3/15: [======                        ] 13/63 batches, loss: 0.2884Epoch 3/15: [======                        ] 14/63 batches, loss: 0.2901Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.2820Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.2793Epoch 3/15: [========                      ] 17/63 batches, loss: 0.2751Epoch 3/15: [========                      ] 18/63 batches, loss: 0.2742Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.2770Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.2745Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.2765Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.2738Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.2730Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.2759Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.2741Epoch 3/15: [============                  ] 26/63 batches, loss: 0.2738Epoch 3/15: [============                  ] 27/63 batches, loss: 0.2761Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.2773Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.2728Epoch 3/15: [==============                ] 30/63 batches, loss: 0.2694Epoch 3/15: [==============                ] 31/63 batches, loss: 0.2694Epoch 3/15: [===============               ] 32/63 batches, loss: 0.2689Epoch 3/15: [===============               ] 33/63 batches, loss: 0.2678Epoch 3/15: [================              ] 34/63 batches, loss: 0.2640Epoch 3/15: [================              ] 35/63 batches, loss: 0.2589Epoch 3/15: [=================             ] 36/63 batches, loss: 0.2600Epoch 3/15: [=================             ] 37/63 batches, loss: 0.2579Epoch 3/15: [==================            ] 38/63 batches, loss: 0.2582Epoch 3/15: [==================            ] 39/63 batches, loss: 0.2579Epoch 3/15: [===================           ] 40/63 batches, loss: 0.2619Epoch 3/15: [===================           ] 41/63 batches, loss: 0.2613Epoch 3/15: [====================          ] 42/63 batches, loss: 0.2586Epoch 3/15: [====================          ] 43/63 batches, loss: 0.2562Epoch 3/15: [====================          ] 44/63 batches, loss: 0.2527Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.2514Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.2481Epoch 3/15: [======================        ] 47/63 batches, loss: 0.2464Epoch 3/15: [======================        ] 48/63 batches, loss: 0.2463Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.2462Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.2447Epoch 3/15: [========================      ] 51/63 batches, loss: 0.2434Epoch 3/15: [========================      ] 52/63 batches, loss: 0.2425Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.2413Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.2403Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.2379Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.2394Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.2398Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.2390Epoch 3/15: [============================  ] 59/63 batches, loss: 0.2396Epoch 3/15: [============================  ] 60/63 batches, loss: 0.2388Epoch 3/15: [============================= ] 61/63 batches, loss: 0.2380Epoch 3/15: [============================= ] 62/63 batches, loss: 0.2369Epoch 3/15: [==============================] 63/63 batches, loss: 0.2360
[2025-05-02 12:58:28,418][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.2360
[2025-05-02 12:58:28,635][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1213, Metrics: {'mse': 0.12237998843193054, 'rmse': 0.3498285128915746, 'r2': -1.493227243423462}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.1487Epoch 4/15: [                              ] 2/63 batches, loss: 0.1719Epoch 4/15: [=                             ] 3/63 batches, loss: 0.1593Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1708Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1824Epoch 4/15: [==                            ] 6/63 batches, loss: 0.2109Epoch 4/15: [===                           ] 7/63 batches, loss: 0.2152Epoch 4/15: [===                           ] 8/63 batches, loss: 0.2145Epoch 4/15: [====                          ] 9/63 batches, loss: 0.2435Epoch 4/15: [====                          ] 10/63 batches, loss: 0.2422Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.2474Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.2348Epoch 4/15: [======                        ] 13/63 batches, loss: 0.2322Epoch 4/15: [======                        ] 14/63 batches, loss: 0.2276Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.2239Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.2216Epoch 4/15: [========                      ] 17/63 batches, loss: 0.2190Epoch 4/15: [========                      ] 18/63 batches, loss: 0.2331Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.2237Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.2295Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.2243Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.2233Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.2239Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.2249Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.2211Epoch 4/15: [============                  ] 26/63 batches, loss: 0.2211Epoch 4/15: [============                  ] 27/63 batches, loss: 0.2178Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.2242Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.2245Epoch 4/15: [==============                ] 30/63 batches, loss: 0.2218Epoch 4/15: [==============                ] 31/63 batches, loss: 0.2182Epoch 4/15: [===============               ] 32/63 batches, loss: 0.2204Epoch 4/15: [===============               ] 33/63 batches, loss: 0.2177Epoch 4/15: [================              ] 34/63 batches, loss: 0.2144Epoch 4/15: [================              ] 35/63 batches, loss: 0.2159Epoch 4/15: [=================             ] 36/63 batches, loss: 0.2142Epoch 4/15: [=================             ] 37/63 batches, loss: 0.2145Epoch 4/15: [==================            ] 38/63 batches, loss: 0.2157Epoch 4/15: [==================            ] 39/63 batches, loss: 0.2192Epoch 4/15: [===================           ] 40/63 batches, loss: 0.2169Epoch 4/15: [===================           ] 41/63 batches, loss: 0.2208Epoch 4/15: [====================          ] 42/63 batches, loss: 0.2237Epoch 4/15: [====================          ] 43/63 batches, loss: 0.2216Epoch 4/15: [====================          ] 44/63 batches, loss: 0.2200Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.2196Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.2185Epoch 4/15: [======================        ] 47/63 batches, loss: 0.2169Epoch 4/15: [======================        ] 48/63 batches, loss: 0.2160Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.2153Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.2135Epoch 4/15: [========================      ] 51/63 batches, loss: 0.2166Epoch 4/15: [========================      ] 52/63 batches, loss: 0.2180Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.2156Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.2152Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.2152Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.2142Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.2124Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.2120Epoch 4/15: [============================  ] 59/63 batches, loss: 0.2118Epoch 4/15: [============================  ] 60/63 batches, loss: 0.2114Epoch 4/15: [============================= ] 61/63 batches, loss: 0.2099Epoch 4/15: [============================= ] 62/63 batches, loss: 0.2085Epoch 4/15: [==============================] 63/63 batches, loss: 0.2076
[2025-05-02 12:58:30,912][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.2076
[2025-05-02 12:58:31,112][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1155, Metrics: {'mse': 0.11617007106542587, 'rmse': 0.3408373087932509, 'r2': -1.366713523864746}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1197Epoch 5/15: [                              ] 2/63 batches, loss: 0.1253Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1648Epoch 5/15: [=                             ] 4/63 batches, loss: 0.1650Epoch 5/15: [==                            ] 5/63 batches, loss: 0.1844Epoch 5/15: [==                            ] 6/63 batches, loss: 0.1915Epoch 5/15: [===                           ] 7/63 batches, loss: 0.1893Epoch 5/15: [===                           ] 8/63 batches, loss: 0.1931Epoch 5/15: [====                          ] 9/63 batches, loss: 0.1809Epoch 5/15: [====                          ] 10/63 batches, loss: 0.1813Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.1816Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.1725Epoch 5/15: [======                        ] 13/63 batches, loss: 0.1719Epoch 5/15: [======                        ] 14/63 batches, loss: 0.1900Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.1874Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.1828Epoch 5/15: [========                      ] 17/63 batches, loss: 0.1836Epoch 5/15: [========                      ] 18/63 batches, loss: 0.1871Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.1890Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.1838Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.1840Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.1809Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.1816Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.1851Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.1855Epoch 5/15: [============                  ] 26/63 batches, loss: 0.1834Epoch 5/15: [============                  ] 27/63 batches, loss: 0.1839Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.1833Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.1847Epoch 5/15: [==============                ] 30/63 batches, loss: 0.1849Epoch 5/15: [==============                ] 31/63 batches, loss: 0.1871Epoch 5/15: [===============               ] 32/63 batches, loss: 0.1866Epoch 5/15: [===============               ] 33/63 batches, loss: 0.1860Epoch 5/15: [================              ] 34/63 batches, loss: 0.1853Epoch 5/15: [================              ] 35/63 batches, loss: 0.1833Epoch 5/15: [=================             ] 36/63 batches, loss: 0.1841Epoch 5/15: [=================             ] 37/63 batches, loss: 0.1816Epoch 5/15: [==================            ] 38/63 batches, loss: 0.1810Epoch 5/15: [==================            ] 39/63 batches, loss: 0.1812Epoch 5/15: [===================           ] 40/63 batches, loss: 0.1826Epoch 5/15: [===================           ] 41/63 batches, loss: 0.1816Epoch 5/15: [====================          ] 42/63 batches, loss: 0.1827Epoch 5/15: [====================          ] 43/63 batches, loss: 0.1871Epoch 5/15: [====================          ] 44/63 batches, loss: 0.1891Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.1890Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.1886Epoch 5/15: [======================        ] 47/63 batches, loss: 0.1897Epoch 5/15: [======================        ] 48/63 batches, loss: 0.1881Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.1869Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.1869Epoch 5/15: [========================      ] 51/63 batches, loss: 0.1861Epoch 5/15: [========================      ] 52/63 batches, loss: 0.1857Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.1893Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.1902Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.1913Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.1939Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.1931Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.1970Epoch 5/15: [============================  ] 59/63 batches, loss: 0.1970Epoch 5/15: [============================  ] 60/63 batches, loss: 0.1959Epoch 5/15: [============================= ] 61/63 batches, loss: 0.1957Epoch 5/15: [============================= ] 62/63 batches, loss: 0.1965Epoch 5/15: [==============================] 63/63 batches, loss: 0.1936
[2025-05-02 12:58:33,389][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1936
[2025-05-02 12:58:33,594][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1082, Metrics: {'mse': 0.10863213986158371, 'rmse': 0.3295939014326323, 'r2': -1.2131445407867432}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.1430Epoch 6/15: [                              ] 2/63 batches, loss: 0.1613Epoch 6/15: [=                             ] 3/63 batches, loss: 0.1674Epoch 6/15: [=                             ] 4/63 batches, loss: 0.1624Epoch 6/15: [==                            ] 5/63 batches, loss: 0.1495Epoch 6/15: [==                            ] 6/63 batches, loss: 0.1409Epoch 6/15: [===                           ] 7/63 batches, loss: 0.1417Epoch 6/15: [===                           ] 8/63 batches, loss: 0.1425Epoch 6/15: [====                          ] 9/63 batches, loss: 0.1673Epoch 6/15: [====                          ] 10/63 batches, loss: 0.1578Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.1787Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.1724Epoch 6/15: [======                        ] 13/63 batches, loss: 0.1931Epoch 6/15: [======                        ] 14/63 batches, loss: 0.1847Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.1816Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.1827Epoch 6/15: [========                      ] 17/63 batches, loss: 0.1794Epoch 6/15: [========                      ] 18/63 batches, loss: 0.1743Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.1700Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.1674Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.1671Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.1697Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.1683Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.1669Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.1696Epoch 6/15: [============                  ] 26/63 batches, loss: 0.1715Epoch 6/15: [============                  ] 27/63 batches, loss: 0.1742Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.1766Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.1747Epoch 6/15: [==============                ] 30/63 batches, loss: 0.1753Epoch 6/15: [==============                ] 31/63 batches, loss: 0.1756Epoch 6/15: [===============               ] 32/63 batches, loss: 0.1760Epoch 6/15: [===============               ] 33/63 batches, loss: 0.1762Epoch 6/15: [================              ] 34/63 batches, loss: 0.1773Epoch 6/15: [================              ] 35/63 batches, loss: 0.1787Epoch 6/15: [=================             ] 36/63 batches, loss: 0.1799Epoch 6/15: [=================             ] 37/63 batches, loss: 0.1800Epoch 6/15: [==================            ] 38/63 batches, loss: 0.1803Epoch 6/15: [==================            ] 39/63 batches, loss: 0.1803Epoch 6/15: [===================           ] 40/63 batches, loss: 0.1842Epoch 6/15: [===================           ] 41/63 batches, loss: 0.1839Epoch 6/15: [====================          ] 42/63 batches, loss: 0.1843Epoch 6/15: [====================          ] 43/63 batches, loss: 0.1842Epoch 6/15: [====================          ] 44/63 batches, loss: 0.1824Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.1804Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.1798Epoch 6/15: [======================        ] 47/63 batches, loss: 0.1834Epoch 6/15: [======================        ] 48/63 batches, loss: 0.1843Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.1826Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.1826Epoch 6/15: [========================      ] 51/63 batches, loss: 0.1820Epoch 6/15: [========================      ] 52/63 batches, loss: 0.1821Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.1808Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.1791Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.1795Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.1799Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.1800Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.1781Epoch 6/15: [============================  ] 59/63 batches, loss: 0.1787Epoch 6/15: [============================  ] 60/63 batches, loss: 0.1781Epoch 6/15: [============================= ] 61/63 batches, loss: 0.1777Epoch 6/15: [============================= ] 62/63 batches, loss: 0.1759Epoch 6/15: [==============================] 63/63 batches, loss: 0.1745
[2025-05-02 12:58:35,867][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1745
[2025-05-02 12:58:36,086][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.0993, Metrics: {'mse': 0.09964117407798767, 'rmse': 0.3156599025501777, 'r2': -1.029973030090332}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.1431Epoch 7/15: [                              ] 2/63 batches, loss: 0.1425Epoch 7/15: [=                             ] 3/63 batches, loss: 0.1528Epoch 7/15: [=                             ] 4/63 batches, loss: 0.1847Epoch 7/15: [==                            ] 5/63 batches, loss: 0.1735Epoch 7/15: [==                            ] 6/63 batches, loss: 0.1754Epoch 7/15: [===                           ] 7/63 batches, loss: 0.2127Epoch 7/15: [===                           ] 8/63 batches, loss: 0.1988Epoch 7/15: [====                          ] 9/63 batches, loss: 0.1871Epoch 7/15: [====                          ] 10/63 batches, loss: 0.1772Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.1809Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.1797Epoch 7/15: [======                        ] 13/63 batches, loss: 0.1757Epoch 7/15: [======                        ] 14/63 batches, loss: 0.1764Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.1745Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.1701Epoch 7/15: [========                      ] 17/63 batches, loss: 0.1737Epoch 7/15: [========                      ] 18/63 batches, loss: 0.1722Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.1744Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.1745Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.1711Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.1654Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.1618Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.1618Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.1574Epoch 7/15: [============                  ] 26/63 batches, loss: 0.1540Epoch 7/15: [============                  ] 27/63 batches, loss: 0.1545Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.1549Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.1531Epoch 7/15: [==============                ] 30/63 batches, loss: 0.1533Epoch 7/15: [==============                ] 31/63 batches, loss: 0.1545Epoch 7/15: [===============               ] 32/63 batches, loss: 0.1549Epoch 7/15: [===============               ] 33/63 batches, loss: 0.1606Epoch 7/15: [================              ] 34/63 batches, loss: 0.1596Epoch 7/15: [================              ] 35/63 batches, loss: 0.1608Epoch 7/15: [=================             ] 36/63 batches, loss: 0.1602Epoch 7/15: [=================             ] 37/63 batches, loss: 0.1583Epoch 7/15: [==================            ] 38/63 batches, loss: 0.1582Epoch 7/15: [==================            ] 39/63 batches, loss: 0.1585Epoch 7/15: [===================           ] 40/63 batches, loss: 0.1569Epoch 7/15: [===================           ] 41/63 batches, loss: 0.1576Epoch 7/15: [====================          ] 42/63 batches, loss: 0.1603Epoch 7/15: [====================          ] 43/63 batches, loss: 0.1609Epoch 7/15: [====================          ] 44/63 batches, loss: 0.1609Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.1651Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.1640Epoch 7/15: [======================        ] 47/63 batches, loss: 0.1643Epoch 7/15: [======================        ] 48/63 batches, loss: 0.1622Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.1617Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.1627Epoch 7/15: [========================      ] 51/63 batches, loss: 0.1643Epoch 7/15: [========================      ] 52/63 batches, loss: 0.1636Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.1638Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.1625Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.1625Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.1649Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.1656Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.1658Epoch 7/15: [============================  ] 59/63 batches, loss: 0.1671Epoch 7/15: [============================  ] 60/63 batches, loss: 0.1673Epoch 7/15: [============================= ] 61/63 batches, loss: 0.1679Epoch 7/15: [============================= ] 62/63 batches, loss: 0.1669Epoch 7/15: [==============================] 63/63 batches, loss: 0.1653
[2025-05-02 12:58:38,411][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1653
[2025-05-02 12:58:38,611][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.0928, Metrics: {'mse': 0.09296837449073792, 'rmse': 0.3049071571654852, 'r2': -0.8940293788909912}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0887Epoch 8/15: [                              ] 2/63 batches, loss: 0.1153Epoch 8/15: [=                             ] 3/63 batches, loss: 0.1114Epoch 8/15: [=                             ] 4/63 batches, loss: 0.0995Epoch 8/15: [==                            ] 5/63 batches, loss: 0.0925Epoch 8/15: [==                            ] 6/63 batches, loss: 0.1084Epoch 8/15: [===                           ] 7/63 batches, loss: 0.1202Epoch 8/15: [===                           ] 8/63 batches, loss: 0.1222Epoch 8/15: [====                          ] 9/63 batches, loss: 0.1318Epoch 8/15: [====                          ] 10/63 batches, loss: 0.1469Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.1414Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.1428Epoch 8/15: [======                        ] 13/63 batches, loss: 0.1476Epoch 8/15: [======                        ] 14/63 batches, loss: 0.1425Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.1465Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.1463Epoch 8/15: [========                      ] 17/63 batches, loss: 0.1467Epoch 8/15: [========                      ] 18/63 batches, loss: 0.1473Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.1466Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.1479Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.1550Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.1553Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.1533Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.1533Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.1502Epoch 8/15: [============                  ] 26/63 batches, loss: 0.1516Epoch 8/15: [============                  ] 27/63 batches, loss: 0.1522Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.1531Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.1517Epoch 8/15: [==============                ] 30/63 batches, loss: 0.1508Epoch 8/15: [==============                ] 31/63 batches, loss: 0.1521Epoch 8/15: [===============               ] 32/63 batches, loss: 0.1508Epoch 8/15: [===============               ] 33/63 batches, loss: 0.1505Epoch 8/15: [================              ] 34/63 batches, loss: 0.1509Epoch 8/15: [================              ] 35/63 batches, loss: 0.1493Epoch 8/15: [=================             ] 36/63 batches, loss: 0.1494Epoch 8/15: [=================             ] 37/63 batches, loss: 0.1506Epoch 8/15: [==================            ] 38/63 batches, loss: 0.1483Epoch 8/15: [==================            ] 39/63 batches, loss: 0.1490Epoch 8/15: [===================           ] 40/63 batches, loss: 0.1496Epoch 8/15: [===================           ] 41/63 batches, loss: 0.1507Epoch 8/15: [====================          ] 42/63 batches, loss: 0.1504Epoch 8/15: [====================          ] 43/63 batches, loss: 0.1502Epoch 8/15: [====================          ] 44/63 batches, loss: 0.1487Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.1473Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.1477Epoch 8/15: [======================        ] 47/63 batches, loss: 0.1465Epoch 8/15: [======================        ] 48/63 batches, loss: 0.1455Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.1456Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.1466Epoch 8/15: [========================      ] 51/63 batches, loss: 0.1465Epoch 8/15: [========================      ] 52/63 batches, loss: 0.1466Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.1452Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.1455Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.1451Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.1449Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.1461Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.1498Epoch 8/15: [============================  ] 59/63 batches, loss: 0.1489Epoch 8/15: [============================  ] 60/63 batches, loss: 0.1491Epoch 8/15: [============================= ] 61/63 batches, loss: 0.1485Epoch 8/15: [============================= ] 62/63 batches, loss: 0.1487Epoch 8/15: [==============================] 63/63 batches, loss: 0.1476
[2025-05-02 12:58:40,981][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.1476
[2025-05-02 12:58:41,198][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0895, Metrics: {'mse': 0.08943075686693192, 'rmse': 0.29904975650706006, 'r2': -0.8219579458236694}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1084Epoch 9/15: [                              ] 2/63 batches, loss: 0.1180Epoch 9/15: [=                             ] 3/63 batches, loss: 0.1213Epoch 9/15: [=                             ] 4/63 batches, loss: 0.1354Epoch 9/15: [==                            ] 5/63 batches, loss: 0.1374Epoch 9/15: [==                            ] 6/63 batches, loss: 0.1314Epoch 9/15: [===                           ] 7/63 batches, loss: 0.1494Epoch 9/15: [===                           ] 8/63 batches, loss: 0.1474Epoch 9/15: [====                          ] 9/63 batches, loss: 0.1517Epoch 9/15: [====                          ] 10/63 batches, loss: 0.1537Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.1504Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.1522Epoch 9/15: [======                        ] 13/63 batches, loss: 0.1481Epoch 9/15: [======                        ] 14/63 batches, loss: 0.1464Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.1448Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.1449Epoch 9/15: [========                      ] 17/63 batches, loss: 0.1452Epoch 9/15: [========                      ] 18/63 batches, loss: 0.1449Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.1458Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.1462Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.1444Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.1473Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.1502Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.1513Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.1513Epoch 9/15: [============                  ] 26/63 batches, loss: 0.1521Epoch 9/15: [============                  ] 27/63 batches, loss: 0.1499Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.1520Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.1520Epoch 9/15: [==============                ] 30/63 batches, loss: 0.1528Epoch 9/15: [==============                ] 31/63 batches, loss: 0.1609Epoch 9/15: [===============               ] 32/63 batches, loss: 0.1590Epoch 9/15: [===============               ] 33/63 batches, loss: 0.1562Epoch 9/15: [================              ] 34/63 batches, loss: 0.1569Epoch 9/15: [================              ] 35/63 batches, loss: 0.1559Epoch 9/15: [=================             ] 36/63 batches, loss: 0.1579Epoch 9/15: [=================             ] 37/63 batches, loss: 0.1554Epoch 9/15: [==================            ] 38/63 batches, loss: 0.1561Epoch 9/15: [==================            ] 39/63 batches, loss: 0.1566Epoch 9/15: [===================           ] 40/63 batches, loss: 0.1542Epoch 9/15: [===================           ] 41/63 batches, loss: 0.1534Epoch 9/15: [====================          ] 42/63 batches, loss: 0.1552Epoch 9/15: [====================          ] 43/63 batches, loss: 0.1545Epoch 9/15: [====================          ] 44/63 batches, loss: 0.1550Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.1554Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.1558Epoch 9/15: [======================        ] 47/63 batches, loss: 0.1563Epoch 9/15: [======================        ] 48/63 batches, loss: 0.1553Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.1541Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.1554Epoch 9/15: [========================      ] 51/63 batches, loss: 0.1541Epoch 9/15: [========================      ] 52/63 batches, loss: 0.1525Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.1510Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.1509Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.1497Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.1486Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.1499Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.1495Epoch 9/15: [============================  ] 59/63 batches, loss: 0.1497Epoch 9/15: [============================  ] 60/63 batches, loss: 0.1497Epoch 9/15: [============================= ] 61/63 batches, loss: 0.1502Epoch 9/15: [============================= ] 62/63 batches, loss: 0.1502Epoch 9/15: [==============================] 63/63 batches, loss: 0.1495
[2025-05-02 12:58:43,540][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.1495
[2025-05-02 12:58:43,752][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0839, Metrics: {'mse': 0.08362743258476257, 'rmse': 0.2891840807941588, 'r2': -0.7037278413772583}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.1523Epoch 10/15: [                              ] 2/63 batches, loss: 0.1763Epoch 10/15: [=                             ] 3/63 batches, loss: 0.1407Epoch 10/15: [=                             ] 4/63 batches, loss: 0.1276Epoch 10/15: [==                            ] 5/63 batches, loss: 0.1303Epoch 10/15: [==                            ] 6/63 batches, loss: 0.1262Epoch 10/15: [===                           ] 7/63 batches, loss: 0.1271Epoch 10/15: [===                           ] 8/63 batches, loss: 0.1343Epoch 10/15: [====                          ] 9/63 batches, loss: 0.1269Epoch 10/15: [====                          ] 10/63 batches, loss: 0.1312Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.1325Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.1363Epoch 10/15: [======                        ] 13/63 batches, loss: 0.1401Epoch 10/15: [======                        ] 14/63 batches, loss: 0.1428Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.1401Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.1391Epoch 10/15: [========                      ] 17/63 batches, loss: 0.1344Epoch 10/15: [========                      ] 18/63 batches, loss: 0.1347Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.1350Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.1351Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.1385Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.1372Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.1369Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.1398Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.1381Epoch 10/15: [============                  ] 26/63 batches, loss: 0.1384Epoch 10/15: [============                  ] 27/63 batches, loss: 0.1403Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.1415Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.1403Epoch 10/15: [==============                ] 30/63 batches, loss: 0.1398Epoch 10/15: [==============                ] 31/63 batches, loss: 0.1401Epoch 10/15: [===============               ] 32/63 batches, loss: 0.1400Epoch 10/15: [===============               ] 33/63 batches, loss: 0.1414Epoch 10/15: [================              ] 34/63 batches, loss: 0.1421Epoch 10/15: [================              ] 35/63 batches, loss: 0.1427Epoch 10/15: [=================             ] 36/63 batches, loss: 0.1457Epoch 10/15: [=================             ] 37/63 batches, loss: 0.1458Epoch 10/15: [==================            ] 38/63 batches, loss: 0.1439Epoch 10/15: [==================            ] 39/63 batches, loss: 0.1450Epoch 10/15: [===================           ] 40/63 batches, loss: 0.1449Epoch 10/15: [===================           ] 41/63 batches, loss: 0.1442Epoch 10/15: [====================          ] 42/63 batches, loss: 0.1434Epoch 10/15: [====================          ] 43/63 batches, loss: 0.1439Epoch 10/15: [====================          ] 44/63 batches, loss: 0.1434Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.1447Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.1442Epoch 10/15: [======================        ] 47/63 batches, loss: 0.1442Epoch 10/15: [======================        ] 48/63 batches, loss: 0.1449Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.1452Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.1441Epoch 10/15: [========================      ] 51/63 batches, loss: 0.1455Epoch 10/15: [========================      ] 52/63 batches, loss: 0.1457Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.1460Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.1490Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.1488Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.1483Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.1472Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.1472Epoch 10/15: [============================  ] 59/63 batches, loss: 0.1469Epoch 10/15: [============================  ] 60/63 batches, loss: 0.1471Epoch 10/15: [============================= ] 61/63 batches, loss: 0.1478Epoch 10/15: [============================= ] 62/63 batches, loss: 0.1468Epoch 10/15: [==============================] 63/63 batches, loss: 0.1465
[2025-05-02 12:58:46,092][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.1465
[2025-05-02 12:58:46,307][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0776, Metrics: {'mse': 0.07721585780382156, 'rmse': 0.2778774150661071, 'r2': -0.5731058120727539}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.2371Epoch 11/15: [                              ] 2/63 batches, loss: 0.2015Epoch 11/15: [=                             ] 3/63 batches, loss: 0.1681Epoch 11/15: [=                             ] 4/63 batches, loss: 0.1444Epoch 11/15: [==                            ] 5/63 batches, loss: 0.1297Epoch 11/15: [==                            ] 6/63 batches, loss: 0.1185Epoch 11/15: [===                           ] 7/63 batches, loss: 0.1160Epoch 11/15: [===                           ] 8/63 batches, loss: 0.1169Epoch 11/15: [====                          ] 9/63 batches, loss: 0.1265Epoch 11/15: [====                          ] 10/63 batches, loss: 0.1275Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.1225Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.1269Epoch 11/15: [======                        ] 13/63 batches, loss: 0.1400Epoch 11/15: [======                        ] 14/63 batches, loss: 0.1477Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.1426Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.1414Epoch 11/15: [========                      ] 17/63 batches, loss: 0.1435Epoch 11/15: [========                      ] 18/63 batches, loss: 0.1431Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.1414Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.1411Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.1386Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.1396Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.1387Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.1381Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.1356Epoch 11/15: [============                  ] 26/63 batches, loss: 0.1327Epoch 11/15: [============                  ] 27/63 batches, loss: 0.1367Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.1365Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.1352Epoch 11/15: [==============                ] 30/63 batches, loss: 0.1347Epoch 11/15: [==============                ] 31/63 batches, loss: 0.1356Epoch 11/15: [===============               ] 32/63 batches, loss: 0.1369Epoch 11/15: [===============               ] 33/63 batches, loss: 0.1365Epoch 11/15: [================              ] 34/63 batches, loss: 0.1351Epoch 11/15: [================              ] 35/63 batches, loss: 0.1356Epoch 11/15: [=================             ] 36/63 batches, loss: 0.1354Epoch 11/15: [=================             ] 37/63 batches, loss: 0.1369Epoch 11/15: [==================            ] 38/63 batches, loss: 0.1372Epoch 11/15: [==================            ] 39/63 batches, loss: 0.1395Epoch 11/15: [===================           ] 40/63 batches, loss: 0.1409Epoch 11/15: [===================           ] 41/63 batches, loss: 0.1428Epoch 11/15: [====================          ] 42/63 batches, loss: 0.1423Epoch 11/15: [====================          ] 43/63 batches, loss: 0.1429Epoch 11/15: [====================          ] 44/63 batches, loss: 0.1412Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.1413Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.1407Epoch 11/15: [======================        ] 47/63 batches, loss: 0.1405Epoch 11/15: [======================        ] 48/63 batches, loss: 0.1401Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.1394Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.1400Epoch 11/15: [========================      ] 51/63 batches, loss: 0.1388Epoch 11/15: [========================      ] 52/63 batches, loss: 0.1377Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.1394Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.1383Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.1375Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.1370Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.1366Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.1395Epoch 11/15: [============================  ] 59/63 batches, loss: 0.1390Epoch 11/15: [============================  ] 60/63 batches, loss: 0.1383Epoch 11/15: [============================= ] 61/63 batches, loss: 0.1385Epoch 11/15: [============================= ] 62/63 batches, loss: 0.1377Epoch 11/15: [==============================] 63/63 batches, loss: 0.1366
[2025-05-02 12:58:48,629][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.1366
[2025-05-02 12:58:48,854][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0756, Metrics: {'mse': 0.07496055960655212, 'rmse': 0.2737892613061223, 'r2': -0.5271589756011963}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.1100Epoch 12/15: [                              ] 2/63 batches, loss: 0.1228Epoch 12/15: [=                             ] 3/63 batches, loss: 0.1240Epoch 12/15: [=                             ] 4/63 batches, loss: 0.1295Epoch 12/15: [==                            ] 5/63 batches, loss: 0.1346Epoch 12/15: [==                            ] 6/63 batches, loss: 0.1436Epoch 12/15: [===                           ] 7/63 batches, loss: 0.1453Epoch 12/15: [===                           ] 8/63 batches, loss: 0.1435Epoch 12/15: [====                          ] 9/63 batches, loss: 0.1614Epoch 12/15: [====                          ] 10/63 batches, loss: 0.1590Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.1534Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.1515Epoch 12/15: [======                        ] 13/63 batches, loss: 0.1464Epoch 12/15: [======                        ] 14/63 batches, loss: 0.1429Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.1429Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.1428Epoch 12/15: [========                      ] 17/63 batches, loss: 0.1385Epoch 12/15: [========                      ] 18/63 batches, loss: 0.1355Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.1336Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.1301Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.1319Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.1334Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.1360Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.1339Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.1326Epoch 12/15: [============                  ] 26/63 batches, loss: 0.1313Epoch 12/15: [============                  ] 27/63 batches, loss: 0.1289Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.1255Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.1252Epoch 12/15: [==============                ] 30/63 batches, loss: 0.1262Epoch 12/15: [==============                ] 31/63 batches, loss: 0.1232Epoch 12/15: [===============               ] 32/63 batches, loss: 0.1232Epoch 12/15: [===============               ] 33/63 batches, loss: 0.1239Epoch 12/15: [================              ] 34/63 batches, loss: 0.1236Epoch 12/15: [================              ] 35/63 batches, loss: 0.1240Epoch 12/15: [=================             ] 36/63 batches, loss: 0.1233Epoch 12/15: [=================             ] 37/63 batches, loss: 0.1246Epoch 12/15: [==================            ] 38/63 batches, loss: 0.1226Epoch 12/15: [==================            ] 39/63 batches, loss: 0.1231Epoch 12/15: [===================           ] 40/63 batches, loss: 0.1221Epoch 12/15: [===================           ] 41/63 batches, loss: 0.1218Epoch 12/15: [====================          ] 42/63 batches, loss: 0.1216Epoch 12/15: [====================          ] 43/63 batches, loss: 0.1223Epoch 12/15: [====================          ] 44/63 batches, loss: 0.1228Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.1233Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.1220Epoch 12/15: [======================        ] 47/63 batches, loss: 0.1222Epoch 12/15: [======================        ] 48/63 batches, loss: 0.1217Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.1212Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.1222Epoch 12/15: [========================      ] 51/63 batches, loss: 0.1229Epoch 12/15: [========================      ] 52/63 batches, loss: 0.1224Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.1224Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.1222Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.1215Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.1217Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.1222Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.1230Epoch 12/15: [============================  ] 59/63 batches, loss: 0.1250Epoch 12/15: [============================  ] 60/63 batches, loss: 0.1242Epoch 12/15: [============================= ] 61/63 batches, loss: 0.1239Epoch 12/15: [============================= ] 62/63 batches, loss: 0.1231Epoch 12/15: [==============================] 63/63 batches, loss: 0.1243
[2025-05-02 12:58:51,212][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.1243
[2025-05-02 12:58:51,426][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0733, Metrics: {'mse': 0.07251018285751343, 'rmse': 0.2692771487845068, 'r2': -0.4772379398345947}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.0726Epoch 13/15: [                              ] 2/63 batches, loss: 0.0868Epoch 13/15: [=                             ] 3/63 batches, loss: 0.0752Epoch 13/15: [=                             ] 4/63 batches, loss: 0.0781Epoch 13/15: [==                            ] 5/63 batches, loss: 0.0753Epoch 13/15: [==                            ] 6/63 batches, loss: 0.0787Epoch 13/15: [===                           ] 7/63 batches, loss: 0.0836Epoch 13/15: [===                           ] 8/63 batches, loss: 0.0954Epoch 13/15: [====                          ] 9/63 batches, loss: 0.0936Epoch 13/15: [====                          ] 10/63 batches, loss: 0.0909Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.0928Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.0954Epoch 13/15: [======                        ] 13/63 batches, loss: 0.0955Epoch 13/15: [======                        ] 14/63 batches, loss: 0.1026Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.1046Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.1109Epoch 13/15: [========                      ] 17/63 batches, loss: 0.1132Epoch 13/15: [========                      ] 18/63 batches, loss: 0.1179Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.1200Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.1207Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.1219Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.1204Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.1200Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.1186Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.1201Epoch 13/15: [============                  ] 26/63 batches, loss: 0.1198Epoch 13/15: [============                  ] 27/63 batches, loss: 0.1225Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.1213Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.1230Epoch 13/15: [==============                ] 30/63 batches, loss: 0.1219Epoch 13/15: [==============                ] 31/63 batches, loss: 0.1206Epoch 13/15: [===============               ] 32/63 batches, loss: 0.1229Epoch 13/15: [===============               ] 33/63 batches, loss: 0.1230Epoch 13/15: [================              ] 34/63 batches, loss: 0.1224Epoch 13/15: [================              ] 35/63 batches, loss: 0.1221Epoch 13/15: [=================             ] 36/63 batches, loss: 0.1200Epoch 13/15: [=================             ] 37/63 batches, loss: 0.1203Epoch 13/15: [==================            ] 38/63 batches, loss: 0.1193Epoch 13/15: [==================            ] 39/63 batches, loss: 0.1193Epoch 13/15: [===================           ] 40/63 batches, loss: 0.1211Epoch 13/15: [===================           ] 41/63 batches, loss: 0.1200Epoch 13/15: [====================          ] 42/63 batches, loss: 0.1207Epoch 13/15: [====================          ] 43/63 batches, loss: 0.1203Epoch 13/15: [====================          ] 44/63 batches, loss: 0.1193Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.1207Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.1202Epoch 13/15: [======================        ] 47/63 batches, loss: 0.1203Epoch 13/15: [======================        ] 48/63 batches, loss: 0.1201Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.1193Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.1186Epoch 13/15: [========================      ] 51/63 batches, loss: 0.1187Epoch 13/15: [========================      ] 52/63 batches, loss: 0.1199Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.1203Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.1194Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.1193Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.1196Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.1192Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.1190Epoch 13/15: [============================  ] 59/63 batches, loss: 0.1199Epoch 13/15: [============================  ] 60/63 batches, loss: 0.1196Epoch 13/15: [============================= ] 61/63 batches, loss: 0.1202Epoch 13/15: [============================= ] 62/63 batches, loss: 0.1198Epoch 13/15: [==============================] 63/63 batches, loss: 0.1179
[2025-05-02 12:58:53,794][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.1179
[2025-05-02 12:58:53,999][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0673, Metrics: {'mse': 0.06649908423423767, 'rmse': 0.2578741635647854, 'r2': -0.3547748327255249}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.1360Epoch 14/15: [                              ] 2/63 batches, loss: 0.1681Epoch 14/15: [=                             ] 3/63 batches, loss: 0.1422Epoch 14/15: [=                             ] 4/63 batches, loss: 0.1418Epoch 14/15: [==                            ] 5/63 batches, loss: 0.1583Epoch 14/15: [==                            ] 6/63 batches, loss: 0.1527Epoch 14/15: [===                           ] 7/63 batches, loss: 0.1434Epoch 14/15: [===                           ] 8/63 batches, loss: 0.1372Epoch 14/15: [====                          ] 9/63 batches, loss: 0.1364Epoch 14/15: [====                          ] 10/63 batches, loss: 0.1297Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.1252Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.1249Epoch 14/15: [======                        ] 13/63 batches, loss: 0.1250Epoch 14/15: [======                        ] 14/63 batches, loss: 0.1232Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.1259Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.1271Epoch 14/15: [========                      ] 17/63 batches, loss: 0.1256Epoch 14/15: [========                      ] 18/63 batches, loss: 0.1265Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.1228Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.1285Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.1272Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.1259Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.1270Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.1240Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.1275Epoch 14/15: [============                  ] 26/63 batches, loss: 0.1259Epoch 14/15: [============                  ] 27/63 batches, loss: 0.1234Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.1229Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.1229Epoch 14/15: [==============                ] 30/63 batches, loss: 0.1238Epoch 14/15: [==============                ] 31/63 batches, loss: 0.1232Epoch 14/15: [===============               ] 32/63 batches, loss: 0.1243Epoch 14/15: [===============               ] 33/63 batches, loss: 0.1269Epoch 14/15: [================              ] 34/63 batches, loss: 0.1256Epoch 14/15: [================              ] 35/63 batches, loss: 0.1246Epoch 14/15: [=================             ] 36/63 batches, loss: 0.1234Epoch 14/15: [=================             ] 37/63 batches, loss: 0.1218Epoch 14/15: [==================            ] 38/63 batches, loss: 0.1215Epoch 14/15: [==================            ] 39/63 batches, loss: 0.1228Epoch 14/15: [===================           ] 40/63 batches, loss: 0.1235Epoch 14/15: [===================           ] 41/63 batches, loss: 0.1226Epoch 14/15: [====================          ] 42/63 batches, loss: 0.1238Epoch 14/15: [====================          ] 43/63 batches, loss: 0.1237Epoch 14/15: [====================          ] 44/63 batches, loss: 0.1233Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.1230Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.1236Epoch 14/15: [======================        ] 47/63 batches, loss: 0.1229Epoch 14/15: [======================        ] 48/63 batches, loss: 0.1244Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.1250Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.1244Epoch 14/15: [========================      ] 51/63 batches, loss: 0.1241Epoch 14/15: [========================      ] 52/63 batches, loss: 0.1233Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.1243Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.1242Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.1242Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.1232Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.1248Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.1242Epoch 14/15: [============================  ] 59/63 batches, loss: 0.1236Epoch 14/15: [============================  ] 60/63 batches, loss: 0.1221Epoch 14/15: [============================= ] 61/63 batches, loss: 0.1230Epoch 14/15: [============================= ] 62/63 batches, loss: 0.1221Epoch 14/15: [==============================] 63/63 batches, loss: 0.1231
[2025-05-02 12:58:56,332][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.1231
[2025-05-02 12:58:56,531][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0645, Metrics: {'mse': 0.06354677677154541, 'rmse': 0.2520848602584959, 'r2': -0.2946279048919678}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.0984Epoch 15/15: [                              ] 2/63 batches, loss: 0.0831Epoch 15/15: [=                             ] 3/63 batches, loss: 0.0788Epoch 15/15: [=                             ] 4/63 batches, loss: 0.0790Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0924Epoch 15/15: [==                            ] 6/63 batches, loss: 0.0989Epoch 15/15: [===                           ] 7/63 batches, loss: 0.0981Epoch 15/15: [===                           ] 8/63 batches, loss: 0.1110Epoch 15/15: [====                          ] 9/63 batches, loss: 0.1192Epoch 15/15: [====                          ] 10/63 batches, loss: 0.1158Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.1128Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.1099Epoch 15/15: [======                        ] 13/63 batches, loss: 0.1149Epoch 15/15: [======                        ] 14/63 batches, loss: 0.1152Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.1106Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.1088Epoch 15/15: [========                      ] 17/63 batches, loss: 0.1106Epoch 15/15: [========                      ] 18/63 batches, loss: 0.1118Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.1096Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.1107Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.1084Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.1065Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.1053Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.1028Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.1016Epoch 15/15: [============                  ] 26/63 batches, loss: 0.1021Epoch 15/15: [============                  ] 27/63 batches, loss: 0.1010Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.1001Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.1014Epoch 15/15: [==============                ] 30/63 batches, loss: 0.1020Epoch 15/15: [==============                ] 31/63 batches, loss: 0.1028Epoch 15/15: [===============               ] 32/63 batches, loss: 0.1028Epoch 15/15: [===============               ] 33/63 batches, loss: 0.1014Epoch 15/15: [================              ] 34/63 batches, loss: 0.1013Epoch 15/15: [================              ] 35/63 batches, loss: 0.1021Epoch 15/15: [=================             ] 36/63 batches, loss: 0.1015Epoch 15/15: [=================             ] 37/63 batches, loss: 0.1009Epoch 15/15: [==================            ] 38/63 batches, loss: 0.0996Epoch 15/15: [==================            ] 39/63 batches, loss: 0.0998Epoch 15/15: [===================           ] 40/63 batches, loss: 0.1004Epoch 15/15: [===================           ] 41/63 batches, loss: 0.0999Epoch 15/15: [====================          ] 42/63 batches, loss: 0.1020Epoch 15/15: [====================          ] 43/63 batches, loss: 0.1017Epoch 15/15: [====================          ] 44/63 batches, loss: 0.1023Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.1020Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.1021Epoch 15/15: [======================        ] 47/63 batches, loss: 0.1025Epoch 15/15: [======================        ] 48/63 batches, loss: 0.1016Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.1016Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.1017Epoch 15/15: [========================      ] 51/63 batches, loss: 0.1027Epoch 15/15: [========================      ] 52/63 batches, loss: 0.1025Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.1020Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.1008Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.1016Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.1017Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.1013Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.1032Epoch 15/15: [============================  ] 59/63 batches, loss: 0.1021Epoch 15/15: [============================  ] 60/63 batches, loss: 0.1025Epoch 15/15: [============================= ] 61/63 batches, loss: 0.1033Epoch 15/15: [============================= ] 62/63 batches, loss: 0.1029Epoch 15/15: [==============================] 63/63 batches, loss: 0.1025
[2025-05-02 12:58:58,936][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.1025
[2025-05-02 12:58:59,167][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0653, Metrics: {'mse': 0.06413974612951279, 'rmse': 0.2532582597458823, 'r2': -0.3067084550857544}
[2025-05-02 12:58:59,168][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-02 12:58:59,168][src.training.lm_trainer][INFO] - Training completed in 38.61 seconds
[2025-05-02 12:58:59,168][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 12:59:01,724][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.037971410900354385, 'rmse': 0.19486254360536914, 'r2': -0.80092453956604}
[2025-05-02 12:59:01,725][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.06354677677154541, 'rmse': 0.2520848602584959, 'r2': -0.2946279048919678}
[2025-05-02 12:59:01,725][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.0940978154540062, 'rmse': 0.3067536722746872, 'r2': -1.0316898822784424}
[2025-05-02 12:59:03,401][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer9/ar/ar/model.pt
[2025-05-02 12:59:03,402][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▃▃▃▂▂▂▂▂▁▁▁▁▁
wandb:     best_val_mse █▃▃▃▂▂▂▂▂▁▁▁▁▁
wandb:      best_val_r2 ▁▆▆▆▇▇▇▇▇█████
wandb:    best_val_rmse █▄▃▃▃▃▂▂▂▂▂▁▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▇▇▇▇▇▇▇▇█████
wandb:       train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▃▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:          val_mse █▃▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:           val_r2 ▁▆▆▆▇▇▇▇▇██████
wandb:         val_rmse █▄▃▃▃▃▂▂▂▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.06451
wandb:     best_val_mse 0.06355
wandb:      best_val_r2 -0.29463
wandb:    best_val_rmse 0.25208
wandb:            epoch 15
wandb:   final_test_mse 0.0941
wandb:    final_test_r2 -1.03169
wandb:  final_test_rmse 0.30675
wandb:  final_train_mse 0.03797
wandb:   final_train_r2 -0.80092
wandb: final_train_rmse 0.19486
wandb:    final_val_mse 0.06355
wandb:     final_val_r2 -0.29463
wandb:   final_val_rmse 0.25208
wandb:    learning_rate 2e-05
wandb:       train_loss 0.10251
wandb:       train_time 38.61471
wandb:         val_loss 0.06526
wandb:          val_mse 0.06414
wandb:           val_r2 -0.30671
wandb:         val_rmse 0.25326
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125810-le2y5wf9
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125810-le2y5wf9/logs
Experiment probe_layer9_avg_links_len_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer9/ar/ar/results.json for layer 9
=======================
PROBING LAYER 11 (SUBMETRIC EXPERIMENTS)
=======================
Running experiment: probe_layer11_avg_links_len_ar
Command: python -m src.experiments.run_experiment         "hydra.job.chdir=False"         "hydra.run.dir=."         "experiment=single_submetric"         "experiment.tasks=single_submetric"         "experiment.type=lm_probe"         "model=lm_probe"         "model.model_type=lm_probe"         "model.lm_name=cis-lmu/glot500-base"         "model.freeze_model=true"         "model.layer_wise=true"         "model.layer_index=11"         "model.probe_hidden_size=96" "model.probe_depth=2" "model.dropout=0.1" "model.activation=silu" "model.normalization=layer" "model.output_standardization=true" "model.use_mean_pooling=true"         "data.languages=[ar]"         "data.cache_dir=/data/leuven/371/vsc37132/qtype-eval/data/cache"         "training.task_type=regression"         "training.num_epochs=15"         "training.batch_size=16"         "training.lr=2e-5" "training.patience=4" "training.scheduler_factor=0.5" "training.scheduler_patience=2" "+training.gradient_accumulation_steps=2"                  "experiment_name=probe_layer11_avg_links_len_ar"         "output_dir=/scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer11/ar"         "wandb.mode=offline" "experiment.submetric=avg_links_len"
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:415: UserWarning: In config: Invalid overriding of hydra/launcher:
Default list overrides requires 'override' keyword.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override for more information.

  deprecation_warning(msg)
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/runpy.py:197: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  return _run_code(code, main_globals, None,
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/hydra/main.py:94: UserWarning: 
'hydra/launcher/submitit_slurm' is validated against ConfigStore schema with the same name.
This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
  _run_hydra(
Using Dataset: rokokot/question-type-and-complexity
Cache Directory: /data/leuven/371/vsc37132/qtype-eval/data/cache
[2025-05-02 12:59:15,349][__main__][INFO] - Configuration:
seed: 42
output_dir: /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer11/ar
experiment_name: probe_layer11_avg_links_len_ar
wandb:
  project: multilingual-question-probing
  entity: rokii-ku-leuven
  mode: offline
slurm:
  partition: gpu
  time: '24:00:00'
  gpus_per_node: 1
  cpus_per_task: 4
  mem_per_cpu: 8
  job_name: ${experiment_name}
  account: intro_vsc37132
data:
  dataset_name: rokokot/question-type-and-complexity
  cache_dir: /data/leuven/371/vsc37132/qtype-eval/data/cache
  vectors_dir: ./data/features
  languages:
  - ar
  train_language: null
  eval_language: null
model:
  model_type: lm_probe
  lm_name: cis-lmu/glot500-base
  dropout: 0.1
  freeze_model: true
  layer_wise: true
  layer_index: 11
  num_outputs: 1
  probe_hidden_size: 96
  probe_depth: 2
  activation: silu
  normalization: layer
  weight_init: xavier
  output_standardization: true
  use_linear_probe: false
  use_mean_pooling: true
  use_class_weights: false
training:
  task_type: regression
  batch_size: 16
  num_epochs: 15
  lr: 2.0e-05
  weight_decay: 0.01
  patience: 4
  scheduler_factor: 0.5
  scheduler_patience: 2
  random_state: 42
  num_workers: 4
  gradient_accumulation_steps: 2
experiment:
  type: lm_probe
  tasks: single_submetric
  submetric: avg_links_len
  available_submetrics:
  - avg_links_len
  - avg_max_depth
  - avg_subordinate_chain_len
  - avg_verb_edges
  - lexical_density
  - n_tokens
  use_controls: false
  control_index: null
  num_controls: 3
  eval_on_orig_test: true
  cross_lingual: false
  task_type: regression

[2025-05-02 12:59:15,349][__main__][INFO] - Normalized task: single_submetric
[2025-05-02 12:59:15,349][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 12:59:15,349][__main__][INFO] - Using explicit task_type from config: regression
[2025-05-02 12:59:15,349][__main__][INFO] - Determined Task Type: regression
[2025-05-02 12:59:15,354][__main__][INFO] - Running LM experiment for task 'single_submetric' (type: regression) on languages: ['ar']
[2025-05-02 12:59:15,354][__main__][INFO] - Using submetric: avg_links_len
[2025-05-02 12:59:15,354][__main__][INFO] - Processing language: ar
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[2025-05-02 12:59:16,707][src.data.datasets][INFO] - Creating dataloaders for language: 'ar', task: 'single_submetric', submetric: 'avg_links_len'
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2025-05-02 12:59:18,967][src.data.datasets][INFO] - Successfully loaded tokenizer for cis-lmu/glot500-base
[2025-05-02 12:59:18,968][src.data.datasets][INFO] - Loading 'base' dataset for ar language (train)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:59:19,023][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:59:19,054][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:59:19,145][src.data.datasets][INFO] - Filtered from 7460 to 995 examples for language 'ar'
[2025-05-02 12:59:19,154][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:59:19,155][src.data.datasets][INFO] - Loaded 995 examples for ar (train)
[2025-05-02 12:59:19,156][src.data.datasets][INFO] - Loading 'base' dataset for ar language (validation)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:59:19,177][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:59:19,207][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:59:19,220][src.data.datasets][INFO] - Filtered from 441 to 44 examples for language 'ar'
[2025-05-02 12:59:19,222][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:59:19,222][src.data.datasets][INFO] - Loaded 44 examples for ar (validation)
[2025-05-02 12:59:19,223][src.data.datasets][INFO] - Loading 'base' dataset for ar language (test)
Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
[2025-05-02 12:59:19,245][datasets.load][WARNING] - Using the latest cached version of the dataset since rokokot/question-type-and-complexity couldn't be found on the Hugging Face Hub (offline mode is enabled).
Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:59:19,277][datasets.packaged_modules.cache.cache][WARNING] - Found the latest cached dataset configuration 'base' at /data/leuven/371/vsc37132/qtype-eval/data/cache/rokokot___question-type-and-complexity/base/0.0.0/73f7e9fabe3af1c8a61564a268551f52ed221358 (last modified on Mon Apr  7 15:43:17 2025).
[2025-05-02 12:59:19,290][src.data.datasets][INFO] - Filtered from 719 to 77 examples for language 'ar'
[2025-05-02 12:59:19,291][src.data.datasets][INFO] - Columns in dataset: ['unique_id', 'text', 'language', 'avg_links_len', 'avg_max_depth', 'avg_subordinate_chain_len', 'avg_verb_edges', 'lexical_density', 'n_tokens', 'question_type', 'complexity_score', 'lang_norm_complexity_score']
[2025-05-02 12:59:19,291][src.data.datasets][INFO] - Loaded 77 examples for ar (test)
[2025-05-02 12:59:19,292][src.data.datasets][INFO] - Loaded datasets: train=995, val=44, test=77 examples
[2025-05-02 12:59:19,293][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 12:59:19,293][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 12:59:19,293][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 12:59:19,293][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 12:59:19,293][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.8570
[2025-05-02 12:59:19,293][src.data.datasets][INFO] -   Mean: 0.1857, Std: 0.1452
[2025-05-02 12:59:19,293][src.data.datasets][INFO] - Sample text: هل النمر العربي معرض للانقراض؟...
[2025-05-02 12:59:19,294][src.data.datasets][INFO] - Sample label: 0.32100000977516174
[2025-05-02 12:59:19,294][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 12:59:19,294][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 12:59:19,294][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 12:59:19,294][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 12:59:19,294][src.data.datasets][INFO] -   Min: 0.0000, Max: 0.9290
[2025-05-02 12:59:19,294][src.data.datasets][INFO] -   Mean: 0.2504, Std: 0.2216
[2025-05-02 12:59:19,294][src.data.datasets][INFO] - Sample text: من هو مخترع الليزر ؟...
[2025-05-02 12:59:19,294][src.data.datasets][INFO] - Sample label: 0.10499999672174454
[2025-05-02 12:59:19,295][src.data.datasets][INFO] - Task 'single_submetric' is classification: False
[2025-05-02 12:59:19,295][src.data.datasets][INFO] - Getting feature name for task: 'single_submetric', submetric: 'avg_links_len'
[2025-05-02 12:59:19,295][src.data.datasets][INFO] - Selected feature name: 'avg_links_len' for task: 'single_submetric'
[2025-05-02 12:59:19,295][src.data.datasets][INFO] - Label statistics for single_submetric (feature: avg_links_len):
[2025-05-02 12:59:19,295][src.data.datasets][INFO] -   Min: 0.0000, Max: 1.0000
[2025-05-02 12:59:19,295][src.data.datasets][INFO] -   Mean: 0.3231, Std: 0.2152
[2025-05-02 12:59:19,295][src.data.datasets][INFO] - Sample text: قبل الإقدام على خطوات يعلن ـ هو نفسه ـ أنها تتناقض...
[2025-05-02 12:59:19,295][src.data.datasets][INFO] - Sample label: 0.27799999713897705
[2025-05-02 12:59:19,295][src.data.datasets][INFO] - Created datasets: train=995, val=44, test=77
[2025-05-02 12:59:19,295][src.data.datasets][INFO] - Creating dataloaders with 4 workers
[2025-05-02 12:59:19,296][src.data.datasets][INFO] - Successfully created all dataloaders
[2025-05-02 12:59:19,296][__main__][INFO] - Using model type: lm_probe for submetric avg_links_len
[2025-05-02 12:59:19,296][src.models.model_factory][INFO] - Creating lm_probe model for regression task
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
/data/leuven/371/vsc37132/miniconda3/envs/qtype-eval/lib/python3.9/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  _torch_pytree._register_pytree_node(
Some weights of XLMRobertaModel were not initialized from the model checkpoint at cis-lmu/glot500-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-05-02 12:59:23,029][src.models.model_factory][INFO] - Loaded model from local cache: cis-lmu/glot500-base
[2025-05-02 12:59:23,030][src.models.model_factory][INFO] - Language model parameters frozen
[2025-05-02 12:59:23,030][src.models.model_factory][INFO] - Base model configuration: layer-wise=True, layer_index=11, freeze_model=True
[2025-05-02 12:59:23,030][src.models.model_factory][INFO] - Using provided probe_hidden_size: 96
[2025-05-02 12:59:23,033][src.models.model_factory][INFO] - Model has 84,961 trainable parameters out of 394,206,433 total parameters
[2025-05-02 12:59:23,033][src.models.model_factory][INFO] - Encoder: 0 trainable parameters, Head: 84,961 trainable parameters
[2025-05-02 12:59:23,033][src.models.model_factory][INFO] - MLP probe configuration: hidden_size=96, depth=2, activation=silu, normalization=layer
[2025-05-02 12:59:23,033][src.models.model_factory][INFO] - Created specialized regression probe with 2 layers, 96 hidden size
[2025-05-02 12:59:23,033][__main__][INFO] - Successfully created lm_probe model for ar
[2025-05-02 12:59:23,034][__main__][INFO] - Total parameters: 394,206,433
[2025-05-02 12:59:23,034][__main__][INFO] - Trainable parameters: 84,961 (0.02%)
Epoch 1/15: [Epoch 1/15: [                              ] 1/63 batches, loss: 2.7223Epoch 1/15: [                              ] 2/63 batches, loss: 3.2593Epoch 1/15: [=                             ] 3/63 batches, loss: 3.2543Epoch 1/15: [=                             ] 4/63 batches, loss: 3.2547Epoch 1/15: [==                            ] 5/63 batches, loss: 3.0165Epoch 1/15: [==                            ] 6/63 batches, loss: 3.0926Epoch 1/15: [===                           ] 7/63 batches, loss: 3.0278Epoch 1/15: [===                           ] 8/63 batches, loss: 2.8951Epoch 1/15: [====                          ] 9/63 batches, loss: 2.8440Epoch 1/15: [====                          ] 10/63 batches, loss: 2.7372Epoch 1/15: [=====                         ] 11/63 batches, loss: 2.7082Epoch 1/15: [=====                         ] 12/63 batches, loss: 2.6118Epoch 1/15: [======                        ] 13/63 batches, loss: 2.5924Epoch 1/15: [======                        ] 14/63 batches, loss: 2.5391Epoch 1/15: [=======                       ] 15/63 batches, loss: 2.4993Epoch 1/15: [=======                       ] 16/63 batches, loss: 2.4648Epoch 1/15: [========                      ] 17/63 batches, loss: 2.4394Epoch 1/15: [========                      ] 18/63 batches, loss: 2.4522Epoch 1/15: [=========                     ] 19/63 batches, loss: 2.4099Epoch 1/15: [=========                     ] 20/63 batches, loss: 2.3810Epoch 1/15: [==========                    ] 21/63 batches, loss: 2.3191Epoch 1/15: [==========                    ] 22/63 batches, loss: 2.2907Epoch 1/15: [==========                    ] 23/63 batches, loss: 2.2828Epoch 1/15: [===========                   ] 24/63 batches, loss: 2.2483Epoch 1/15: [===========                   ] 25/63 batches, loss: 2.2300Epoch 1/15: [============                  ] 26/63 batches, loss: 2.2047Epoch 1/15: [============                  ] 27/63 batches, loss: 2.1862Epoch 1/15: [=============                 ] 28/63 batches, loss: 2.1440Epoch 1/15: [=============                 ] 29/63 batches, loss: 2.1167Epoch 1/15: [==============                ] 30/63 batches, loss: 2.0859Epoch 1/15: [==============                ] 31/63 batches, loss: 2.0622Epoch 1/15: [===============               ] 32/63 batches, loss: 2.0121Epoch 1/15: [===============               ] 33/63 batches, loss: 1.9745Epoch 1/15: [================              ] 34/63 batches, loss: 1.9571Epoch 1/15: [================              ] 35/63 batches, loss: 1.9280Epoch 1/15: [=================             ] 36/63 batches, loss: 1.9103Epoch 1/15: [=================             ] 37/63 batches, loss: 1.8809Epoch 1/15: [==================            ] 38/63 batches, loss: 1.8698Epoch 1/15: [==================            ] 39/63 batches, loss: 1.8444Epoch 1/15: [===================           ] 40/63 batches, loss: 1.8264Epoch 1/15: [===================           ] 41/63 batches, loss: 1.7964Epoch 1/15: [====================          ] 42/63 batches, loss: 1.7747Epoch 1/15: [====================          ] 43/63 batches, loss: 1.7475Epoch 1/15: [====================          ] 44/63 batches, loss: 1.7190Epoch 1/15: [=====================         ] 45/63 batches, loss: 1.6910Epoch 1/15: [=====================         ] 46/63 batches, loss: 1.6770Epoch 1/15: [======================        ] 47/63 batches, loss: 1.6603Epoch 1/15: [======================        ] 48/63 batches, loss: 1.6376Epoch 1/15: [=======================       ] 49/63 batches, loss: 1.6204Epoch 1/15: [=======================       ] 50/63 batches, loss: 1.6025Epoch 1/15: [========================      ] 51/63 batches, loss: 1.5806Epoch 1/15: [========================      ] 52/63 batches, loss: 1.5557Epoch 1/15: [=========================     ] 53/63 batches, loss: 1.5296Epoch 1/15: [=========================     ] 54/63 batches, loss: 1.5130Epoch 1/15: [==========================    ] 55/63 batches, loss: 1.4942Epoch 1/15: [==========================    ] 56/63 batches, loss: 1.4701Epoch 1/15: [===========================   ] 57/63 batches, loss: 1.4570Epoch 1/15: [===========================   ] 58/63 batches, loss: 1.4443Epoch 1/15: [============================  ] 59/63 batches, loss: 1.4349Epoch 1/15: [============================  ] 60/63 batches, loss: 1.4215Epoch 1/15: [============================= ] 61/63 batches, loss: 1.4029Epoch 1/15: [============================= ] 62/63 batches, loss: 1.3896Epoch 1/15: [==============================] 63/63 batches, loss: 1.3881
[2025-05-02 12:59:27,779][src.training.lm_trainer][INFO] - Epoch 1/15, Train Loss: 1.3881
[2025-05-02 12:59:27,971][src.training.lm_trainer][INFO] - Epoch 1/15, Val Loss: 0.1988, Metrics: {'mse': 0.20228910446166992, 'rmse': 0.4497656105814115, 'r2': -3.1212024688720703}
Epoch 2/15: [Epoch 2/15: [                              ] 1/63 batches, loss: 0.3434Epoch 2/15: [                              ] 2/63 batches, loss: 0.3071Epoch 2/15: [=                             ] 3/63 batches, loss: 0.2763Epoch 2/15: [=                             ] 4/63 batches, loss: 0.3335Epoch 2/15: [==                            ] 5/63 batches, loss: 0.3829Epoch 2/15: [==                            ] 6/63 batches, loss: 0.4133Epoch 2/15: [===                           ] 7/63 batches, loss: 0.3778Epoch 2/15: [===                           ] 8/63 batches, loss: 0.3550Epoch 2/15: [====                          ] 9/63 batches, loss: 0.3289Epoch 2/15: [====                          ] 10/63 batches, loss: 0.3313Epoch 2/15: [=====                         ] 11/63 batches, loss: 0.3330Epoch 2/15: [=====                         ] 12/63 batches, loss: 0.3313Epoch 2/15: [======                        ] 13/63 batches, loss: 0.3316Epoch 2/15: [======                        ] 14/63 batches, loss: 0.3212Epoch 2/15: [=======                       ] 15/63 batches, loss: 0.3231Epoch 2/15: [=======                       ] 16/63 batches, loss: 0.3299Epoch 2/15: [========                      ] 17/63 batches, loss: 0.3369Epoch 2/15: [========                      ] 18/63 batches, loss: 0.3379Epoch 2/15: [=========                     ] 19/63 batches, loss: 0.3339Epoch 2/15: [=========                     ] 20/63 batches, loss: 0.3483Epoch 2/15: [==========                    ] 21/63 batches, loss: 0.3470Epoch 2/15: [==========                    ] 22/63 batches, loss: 0.3405Epoch 2/15: [==========                    ] 23/63 batches, loss: 0.3301Epoch 2/15: [===========                   ] 24/63 batches, loss: 0.3304Epoch 2/15: [===========                   ] 25/63 batches, loss: 0.3258Epoch 2/15: [============                  ] 26/63 batches, loss: 0.3260Epoch 2/15: [============                  ] 27/63 batches, loss: 0.3177Epoch 2/15: [=============                 ] 28/63 batches, loss: 0.3106Epoch 2/15: [=============                 ] 29/63 batches, loss: 0.3072Epoch 2/15: [==============                ] 30/63 batches, loss: 0.3022Epoch 2/15: [==============                ] 31/63 batches, loss: 0.3011Epoch 2/15: [===============               ] 32/63 batches, loss: 0.3006Epoch 2/15: [===============               ] 33/63 batches, loss: 0.2988Epoch 2/15: [================              ] 34/63 batches, loss: 0.2981Epoch 2/15: [================              ] 35/63 batches, loss: 0.3065Epoch 2/15: [=================             ] 36/63 batches, loss: 0.3077Epoch 2/15: [=================             ] 37/63 batches, loss: 0.3029Epoch 2/15: [==================            ] 38/63 batches, loss: 0.2983Epoch 2/15: [==================            ] 39/63 batches, loss: 0.2983Epoch 2/15: [===================           ] 40/63 batches, loss: 0.2972Epoch 2/15: [===================           ] 41/63 batches, loss: 0.2971Epoch 2/15: [====================          ] 42/63 batches, loss: 0.2960Epoch 2/15: [====================          ] 43/63 batches, loss: 0.2953Epoch 2/15: [====================          ] 44/63 batches, loss: 0.2964Epoch 2/15: [=====================         ] 45/63 batches, loss: 0.2936Epoch 2/15: [=====================         ] 46/63 batches, loss: 0.2913Epoch 2/15: [======================        ] 47/63 batches, loss: 0.2886Epoch 2/15: [======================        ] 48/63 batches, loss: 0.2844Epoch 2/15: [=======================       ] 49/63 batches, loss: 0.2812Epoch 2/15: [=======================       ] 50/63 batches, loss: 0.2863Epoch 2/15: [========================      ] 51/63 batches, loss: 0.2848Epoch 2/15: [========================      ] 52/63 batches, loss: 0.2832Epoch 2/15: [=========================     ] 53/63 batches, loss: 0.2811Epoch 2/15: [=========================     ] 54/63 batches, loss: 0.2783Epoch 2/15: [==========================    ] 55/63 batches, loss: 0.2810Epoch 2/15: [==========================    ] 56/63 batches, loss: 0.2800Epoch 2/15: [===========================   ] 57/63 batches, loss: 0.2773Epoch 2/15: [===========================   ] 58/63 batches, loss: 0.2779Epoch 2/15: [============================  ] 59/63 batches, loss: 0.2846Epoch 2/15: [============================  ] 60/63 batches, loss: 0.2830Epoch 2/15: [============================= ] 61/63 batches, loss: 0.2831Epoch 2/15: [============================= ] 62/63 batches, loss: 0.2830Epoch 2/15: [==============================] 63/63 batches, loss: 0.2840
[2025-05-02 12:59:30,308][src.training.lm_trainer][INFO] - Epoch 2/15, Train Loss: 0.2840
[2025-05-02 12:59:30,497][src.training.lm_trainer][INFO] - Epoch 2/15, Val Loss: 0.1333, Metrics: {'mse': 0.13439685106277466, 'rmse': 0.3666017608560748, 'r2': -1.7380449771881104}
Epoch 3/15: [Epoch 3/15: [                              ] 1/63 batches, loss: 0.2753Epoch 3/15: [                              ] 2/63 batches, loss: 0.2693Epoch 3/15: [=                             ] 3/63 batches, loss: 0.2181Epoch 3/15: [=                             ] 4/63 batches, loss: 0.1998Epoch 3/15: [==                            ] 5/63 batches, loss: 0.2413Epoch 3/15: [==                            ] 6/63 batches, loss: 0.2398Epoch 3/15: [===                           ] 7/63 batches, loss: 0.2293Epoch 3/15: [===                           ] 8/63 batches, loss: 0.2342Epoch 3/15: [====                          ] 9/63 batches, loss: 0.2334Epoch 3/15: [====                          ] 10/63 batches, loss: 0.2386Epoch 3/15: [=====                         ] 11/63 batches, loss: 0.2367Epoch 3/15: [=====                         ] 12/63 batches, loss: 0.2383Epoch 3/15: [======                        ] 13/63 batches, loss: 0.2383Epoch 3/15: [======                        ] 14/63 batches, loss: 0.2480Epoch 3/15: [=======                       ] 15/63 batches, loss: 0.2505Epoch 3/15: [=======                       ] 16/63 batches, loss: 0.2535Epoch 3/15: [========                      ] 17/63 batches, loss: 0.2518Epoch 3/15: [========                      ] 18/63 batches, loss: 0.2504Epoch 3/15: [=========                     ] 19/63 batches, loss: 0.2526Epoch 3/15: [=========                     ] 20/63 batches, loss: 0.2528Epoch 3/15: [==========                    ] 21/63 batches, loss: 0.2514Epoch 3/15: [==========                    ] 22/63 batches, loss: 0.2477Epoch 3/15: [==========                    ] 23/63 batches, loss: 0.2461Epoch 3/15: [===========                   ] 24/63 batches, loss: 0.2467Epoch 3/15: [===========                   ] 25/63 batches, loss: 0.2487Epoch 3/15: [============                  ] 26/63 batches, loss: 0.2476Epoch 3/15: [============                  ] 27/63 batches, loss: 0.2488Epoch 3/15: [=============                 ] 28/63 batches, loss: 0.2478Epoch 3/15: [=============                 ] 29/63 batches, loss: 0.2438Epoch 3/15: [==============                ] 30/63 batches, loss: 0.2425Epoch 3/15: [==============                ] 31/63 batches, loss: 0.2481Epoch 3/15: [===============               ] 32/63 batches, loss: 0.2470Epoch 3/15: [===============               ] 33/63 batches, loss: 0.2476Epoch 3/15: [================              ] 34/63 batches, loss: 0.2436Epoch 3/15: [================              ] 35/63 batches, loss: 0.2389Epoch 3/15: [=================             ] 36/63 batches, loss: 0.2368Epoch 3/15: [=================             ] 37/63 batches, loss: 0.2364Epoch 3/15: [==================            ] 38/63 batches, loss: 0.2375Epoch 3/15: [==================            ] 39/63 batches, loss: 0.2366Epoch 3/15: [===================           ] 40/63 batches, loss: 0.2383Epoch 3/15: [===================           ] 41/63 batches, loss: 0.2380Epoch 3/15: [====================          ] 42/63 batches, loss: 0.2369Epoch 3/15: [====================          ] 43/63 batches, loss: 0.2362Epoch 3/15: [====================          ] 44/63 batches, loss: 0.2343Epoch 3/15: [=====================         ] 45/63 batches, loss: 0.2336Epoch 3/15: [=====================         ] 46/63 batches, loss: 0.2318Epoch 3/15: [======================        ] 47/63 batches, loss: 0.2313Epoch 3/15: [======================        ] 48/63 batches, loss: 0.2308Epoch 3/15: [=======================       ] 49/63 batches, loss: 0.2309Epoch 3/15: [=======================       ] 50/63 batches, loss: 0.2297Epoch 3/15: [========================      ] 51/63 batches, loss: 0.2301Epoch 3/15: [========================      ] 52/63 batches, loss: 0.2290Epoch 3/15: [=========================     ] 53/63 batches, loss: 0.2283Epoch 3/15: [=========================     ] 54/63 batches, loss: 0.2268Epoch 3/15: [==========================    ] 55/63 batches, loss: 0.2246Epoch 3/15: [==========================    ] 56/63 batches, loss: 0.2259Epoch 3/15: [===========================   ] 57/63 batches, loss: 0.2260Epoch 3/15: [===========================   ] 58/63 batches, loss: 0.2268Epoch 3/15: [============================  ] 59/63 batches, loss: 0.2259Epoch 3/15: [============================  ] 60/63 batches, loss: 0.2248Epoch 3/15: [============================= ] 61/63 batches, loss: 0.2244Epoch 3/15: [============================= ] 62/63 batches, loss: 0.2242Epoch 3/15: [==============================] 63/63 batches, loss: 0.2230
[2025-05-02 12:59:32,826][src.training.lm_trainer][INFO] - Epoch 3/15, Train Loss: 0.2230
[2025-05-02 12:59:33,025][src.training.lm_trainer][INFO] - Epoch 3/15, Val Loss: 0.1265, Metrics: {'mse': 0.12728005647659302, 'rmse': 0.3567633059559139, 'r2': -1.5930554866790771}
Epoch 4/15: [Epoch 4/15: [                              ] 1/63 batches, loss: 0.1275Epoch 4/15: [                              ] 2/63 batches, loss: 0.1182Epoch 4/15: [=                             ] 3/63 batches, loss: 0.1371Epoch 4/15: [=                             ] 4/63 batches, loss: 0.1529Epoch 4/15: [==                            ] 5/63 batches, loss: 0.1544Epoch 4/15: [==                            ] 6/63 batches, loss: 0.1889Epoch 4/15: [===                           ] 7/63 batches, loss: 0.1990Epoch 4/15: [===                           ] 8/63 batches, loss: 0.2132Epoch 4/15: [====                          ] 9/63 batches, loss: 0.2339Epoch 4/15: [====                          ] 10/63 batches, loss: 0.2462Epoch 4/15: [=====                         ] 11/63 batches, loss: 0.2411Epoch 4/15: [=====                         ] 12/63 batches, loss: 0.2329Epoch 4/15: [======                        ] 13/63 batches, loss: 0.2294Epoch 4/15: [======                        ] 14/63 batches, loss: 0.2285Epoch 4/15: [=======                       ] 15/63 batches, loss: 0.2255Epoch 4/15: [=======                       ] 16/63 batches, loss: 0.2240Epoch 4/15: [========                      ] 17/63 batches, loss: 0.2176Epoch 4/15: [========                      ] 18/63 batches, loss: 0.2280Epoch 4/15: [=========                     ] 19/63 batches, loss: 0.2218Epoch 4/15: [=========                     ] 20/63 batches, loss: 0.2270Epoch 4/15: [==========                    ] 21/63 batches, loss: 0.2222Epoch 4/15: [==========                    ] 22/63 batches, loss: 0.2181Epoch 4/15: [==========                    ] 23/63 batches, loss: 0.2213Epoch 4/15: [===========                   ] 24/63 batches, loss: 0.2194Epoch 4/15: [===========                   ] 25/63 batches, loss: 0.2177Epoch 4/15: [============                  ] 26/63 batches, loss: 0.2166Epoch 4/15: [============                  ] 27/63 batches, loss: 0.2148Epoch 4/15: [=============                 ] 28/63 batches, loss: 0.2185Epoch 4/15: [=============                 ] 29/63 batches, loss: 0.2156Epoch 4/15: [==============                ] 30/63 batches, loss: 0.2147Epoch 4/15: [==============                ] 31/63 batches, loss: 0.2111Epoch 4/15: [===============               ] 32/63 batches, loss: 0.2122Epoch 4/15: [===============               ] 33/63 batches, loss: 0.2107Epoch 4/15: [================              ] 34/63 batches, loss: 0.2061Epoch 4/15: [================              ] 35/63 batches, loss: 0.2074Epoch 4/15: [=================             ] 36/63 batches, loss: 0.2061Epoch 4/15: [=================             ] 37/63 batches, loss: 0.2087Epoch 4/15: [==================            ] 38/63 batches, loss: 0.2067Epoch 4/15: [==================            ] 39/63 batches, loss: 0.2127Epoch 4/15: [===================           ] 40/63 batches, loss: 0.2102Epoch 4/15: [===================           ] 41/63 batches, loss: 0.2117Epoch 4/15: [====================          ] 42/63 batches, loss: 0.2133Epoch 4/15: [====================          ] 43/63 batches, loss: 0.2100Epoch 4/15: [====================          ] 44/63 batches, loss: 0.2090Epoch 4/15: [=====================         ] 45/63 batches, loss: 0.2112Epoch 4/15: [=====================         ] 46/63 batches, loss: 0.2095Epoch 4/15: [======================        ] 47/63 batches, loss: 0.2079Epoch 4/15: [======================        ] 48/63 batches, loss: 0.2070Epoch 4/15: [=======================       ] 49/63 batches, loss: 0.2074Epoch 4/15: [=======================       ] 50/63 batches, loss: 0.2063Epoch 4/15: [========================      ] 51/63 batches, loss: 0.2079Epoch 4/15: [========================      ] 52/63 batches, loss: 0.2093Epoch 4/15: [=========================     ] 53/63 batches, loss: 0.2073Epoch 4/15: [=========================     ] 54/63 batches, loss: 0.2069Epoch 4/15: [==========================    ] 55/63 batches, loss: 0.2064Epoch 4/15: [==========================    ] 56/63 batches, loss: 0.2059Epoch 4/15: [===========================   ] 57/63 batches, loss: 0.2039Epoch 4/15: [===========================   ] 58/63 batches, loss: 0.2024Epoch 4/15: [============================  ] 59/63 batches, loss: 0.2015Epoch 4/15: [============================  ] 60/63 batches, loss: 0.2020Epoch 4/15: [============================= ] 61/63 batches, loss: 0.2011Epoch 4/15: [============================= ] 62/63 batches, loss: 0.1995Epoch 4/15: [==============================] 63/63 batches, loss: 0.1985
[2025-05-02 12:59:35,313][src.training.lm_trainer][INFO] - Epoch 4/15, Train Loss: 0.1985
[2025-05-02 12:59:35,524][src.training.lm_trainer][INFO] - Epoch 4/15, Val Loss: 0.1213, Metrics: {'mse': 0.12178485095500946, 'rmse': 0.34897686306546094, 'r2': -1.481102466583252}
Epoch 5/15: [Epoch 5/15: [                              ] 1/63 batches, loss: 0.1099Epoch 5/15: [                              ] 2/63 batches, loss: 0.1137Epoch 5/15: [=                             ] 3/63 batches, loss: 0.1133Epoch 5/15: [=                             ] 4/63 batches, loss: 0.1195Epoch 5/15: [==                            ] 5/63 batches, loss: 0.1436Epoch 5/15: [==                            ] 6/63 batches, loss: 0.1471Epoch 5/15: [===                           ] 7/63 batches, loss: 0.1436Epoch 5/15: [===                           ] 8/63 batches, loss: 0.1373Epoch 5/15: [====                          ] 9/63 batches, loss: 0.1349Epoch 5/15: [====                          ] 10/63 batches, loss: 0.1366Epoch 5/15: [=====                         ] 11/63 batches, loss: 0.1502Epoch 5/15: [=====                         ] 12/63 batches, loss: 0.1447Epoch 5/15: [======                        ] 13/63 batches, loss: 0.1510Epoch 5/15: [======                        ] 14/63 batches, loss: 0.1635Epoch 5/15: [=======                       ] 15/63 batches, loss: 0.1628Epoch 5/15: [=======                       ] 16/63 batches, loss: 0.1611Epoch 5/15: [========                      ] 17/63 batches, loss: 0.1621Epoch 5/15: [========                      ] 18/63 batches, loss: 0.1612Epoch 5/15: [=========                     ] 19/63 batches, loss: 0.1627Epoch 5/15: [=========                     ] 20/63 batches, loss: 0.1589Epoch 5/15: [==========                    ] 21/63 batches, loss: 0.1652Epoch 5/15: [==========                    ] 22/63 batches, loss: 0.1626Epoch 5/15: [==========                    ] 23/63 batches, loss: 0.1639Epoch 5/15: [===========                   ] 24/63 batches, loss: 0.1685Epoch 5/15: [===========                   ] 25/63 batches, loss: 0.1672Epoch 5/15: [============                  ] 26/63 batches, loss: 0.1700Epoch 5/15: [============                  ] 27/63 batches, loss: 0.1706Epoch 5/15: [=============                 ] 28/63 batches, loss: 0.1699Epoch 5/15: [=============                 ] 29/63 batches, loss: 0.1680Epoch 5/15: [==============                ] 30/63 batches, loss: 0.1674Epoch 5/15: [==============                ] 31/63 batches, loss: 0.1692Epoch 5/15: [===============               ] 32/63 batches, loss: 0.1690Epoch 5/15: [===============               ] 33/63 batches, loss: 0.1685Epoch 5/15: [================              ] 34/63 batches, loss: 0.1670Epoch 5/15: [================              ] 35/63 batches, loss: 0.1677Epoch 5/15: [=================             ] 36/63 batches, loss: 0.1673Epoch 5/15: [=================             ] 37/63 batches, loss: 0.1663Epoch 5/15: [==================            ] 38/63 batches, loss: 0.1665Epoch 5/15: [==================            ] 39/63 batches, loss: 0.1675Epoch 5/15: [===================           ] 40/63 batches, loss: 0.1702Epoch 5/15: [===================           ] 41/63 batches, loss: 0.1711Epoch 5/15: [====================          ] 42/63 batches, loss: 0.1731Epoch 5/15: [====================          ] 43/63 batches, loss: 0.1760Epoch 5/15: [====================          ] 44/63 batches, loss: 0.1772Epoch 5/15: [=====================         ] 45/63 batches, loss: 0.1768Epoch 5/15: [=====================         ] 46/63 batches, loss: 0.1787Epoch 5/15: [======================        ] 47/63 batches, loss: 0.1772Epoch 5/15: [======================        ] 48/63 batches, loss: 0.1760Epoch 5/15: [=======================       ] 49/63 batches, loss: 0.1752Epoch 5/15: [=======================       ] 50/63 batches, loss: 0.1758Epoch 5/15: [========================      ] 51/63 batches, loss: 0.1766Epoch 5/15: [========================      ] 52/63 batches, loss: 0.1764Epoch 5/15: [=========================     ] 53/63 batches, loss: 0.1793Epoch 5/15: [=========================     ] 54/63 batches, loss: 0.1801Epoch 5/15: [==========================    ] 55/63 batches, loss: 0.1799Epoch 5/15: [==========================    ] 56/63 batches, loss: 0.1816Epoch 5/15: [===========================   ] 57/63 batches, loss: 0.1816Epoch 5/15: [===========================   ] 58/63 batches, loss: 0.1852Epoch 5/15: [============================  ] 59/63 batches, loss: 0.1858Epoch 5/15: [============================  ] 60/63 batches, loss: 0.1847Epoch 5/15: [============================= ] 61/63 batches, loss: 0.1843Epoch 5/15: [============================= ] 62/63 batches, loss: 0.1852Epoch 5/15: [==============================] 63/63 batches, loss: 0.1836
[2025-05-02 12:59:37,809][src.training.lm_trainer][INFO] - Epoch 5/15, Train Loss: 0.1836
[2025-05-02 12:59:38,019][src.training.lm_trainer][INFO] - Epoch 5/15, Val Loss: 0.1164, Metrics: {'mse': 0.11659599095582962, 'rmse': 0.3414615512115963, 'r2': -1.3753907680511475}
Epoch 6/15: [Epoch 6/15: [                              ] 1/63 batches, loss: 0.2736Epoch 6/15: [                              ] 2/63 batches, loss: 0.2147Epoch 6/15: [=                             ] 3/63 batches, loss: 0.2059Epoch 6/15: [=                             ] 4/63 batches, loss: 0.1968Epoch 6/15: [==                            ] 5/63 batches, loss: 0.1706Epoch 6/15: [==                            ] 6/63 batches, loss: 0.1678Epoch 6/15: [===                           ] 7/63 batches, loss: 0.1671Epoch 6/15: [===                           ] 8/63 batches, loss: 0.1654Epoch 6/15: [====                          ] 9/63 batches, loss: 0.1761Epoch 6/15: [====                          ] 10/63 batches, loss: 0.1677Epoch 6/15: [=====                         ] 11/63 batches, loss: 0.1879Epoch 6/15: [=====                         ] 12/63 batches, loss: 0.1786Epoch 6/15: [======                        ] 13/63 batches, loss: 0.1880Epoch 6/15: [======                        ] 14/63 batches, loss: 0.1809Epoch 6/15: [=======                       ] 15/63 batches, loss: 0.1775Epoch 6/15: [=======                       ] 16/63 batches, loss: 0.1788Epoch 6/15: [========                      ] 17/63 batches, loss: 0.1791Epoch 6/15: [========                      ] 18/63 batches, loss: 0.1762Epoch 6/15: [=========                     ] 19/63 batches, loss: 0.1749Epoch 6/15: [=========                     ] 20/63 batches, loss: 0.1730Epoch 6/15: [==========                    ] 21/63 batches, loss: 0.1744Epoch 6/15: [==========                    ] 22/63 batches, loss: 0.1725Epoch 6/15: [==========                    ] 23/63 batches, loss: 0.1697Epoch 6/15: [===========                   ] 24/63 batches, loss: 0.1695Epoch 6/15: [===========                   ] 25/63 batches, loss: 0.1700Epoch 6/15: [============                  ] 26/63 batches, loss: 0.1718Epoch 6/15: [============                  ] 27/63 batches, loss: 0.1794Epoch 6/15: [=============                 ] 28/63 batches, loss: 0.1830Epoch 6/15: [=============                 ] 29/63 batches, loss: 0.1826Epoch 6/15: [==============                ] 30/63 batches, loss: 0.1880Epoch 6/15: [==============                ] 31/63 batches, loss: 0.1885Epoch 6/15: [===============               ] 32/63 batches, loss: 0.1894Epoch 6/15: [===============               ] 33/63 batches, loss: 0.1879Epoch 6/15: [================              ] 34/63 batches, loss: 0.1877Epoch 6/15: [================              ] 35/63 batches, loss: 0.1884Epoch 6/15: [=================             ] 36/63 batches, loss: 0.1920Epoch 6/15: [=================             ] 37/63 batches, loss: 0.1894Epoch 6/15: [==================            ] 38/63 batches, loss: 0.1911Epoch 6/15: [==================            ] 39/63 batches, loss: 0.1895Epoch 6/15: [===================           ] 40/63 batches, loss: 0.1922Epoch 6/15: [===================           ] 41/63 batches, loss: 0.1905Epoch 6/15: [====================          ] 42/63 batches, loss: 0.1905Epoch 6/15: [====================          ] 43/63 batches, loss: 0.1912Epoch 6/15: [====================          ] 44/63 batches, loss: 0.1888Epoch 6/15: [=====================         ] 45/63 batches, loss: 0.1880Epoch 6/15: [=====================         ] 46/63 batches, loss: 0.1865Epoch 6/15: [======================        ] 47/63 batches, loss: 0.1888Epoch 6/15: [======================        ] 48/63 batches, loss: 0.1905Epoch 6/15: [=======================       ] 49/63 batches, loss: 0.1887Epoch 6/15: [=======================       ] 50/63 batches, loss: 0.1881Epoch 6/15: [========================      ] 51/63 batches, loss: 0.1917Epoch 6/15: [========================      ] 52/63 batches, loss: 0.1916Epoch 6/15: [=========================     ] 53/63 batches, loss: 0.1911Epoch 6/15: [=========================     ] 54/63 batches, loss: 0.1896Epoch 6/15: [==========================    ] 55/63 batches, loss: 0.1907Epoch 6/15: [==========================    ] 56/63 batches, loss: 0.1938Epoch 6/15: [===========================   ] 57/63 batches, loss: 0.1945Epoch 6/15: [===========================   ] 58/63 batches, loss: 0.1927Epoch 6/15: [============================  ] 59/63 batches, loss: 0.1941Epoch 6/15: [============================  ] 60/63 batches, loss: 0.1943Epoch 6/15: [============================= ] 61/63 batches, loss: 0.1928Epoch 6/15: [============================= ] 62/63 batches, loss: 0.1915Epoch 6/15: [==============================] 63/63 batches, loss: 0.1891
[2025-05-02 12:59:40,310][src.training.lm_trainer][INFO] - Epoch 6/15, Train Loss: 0.1891
[2025-05-02 12:59:40,522][src.training.lm_trainer][INFO] - Epoch 6/15, Val Loss: 0.1061, Metrics: {'mse': 0.10597933083772659, 'rmse': 0.3255446679608293, 'r2': -1.1590993404388428}
Epoch 7/15: [Epoch 7/15: [                              ] 1/63 batches, loss: 0.1082Epoch 7/15: [                              ] 2/63 batches, loss: 0.1264Epoch 7/15: [=                             ] 3/63 batches, loss: 0.1433Epoch 7/15: [=                             ] 4/63 batches, loss: 0.1824Epoch 7/15: [==                            ] 5/63 batches, loss: 0.1667Epoch 7/15: [==                            ] 6/63 batches, loss: 0.1634Epoch 7/15: [===                           ] 7/63 batches, loss: 0.1891Epoch 7/15: [===                           ] 8/63 batches, loss: 0.1839Epoch 7/15: [====                          ] 9/63 batches, loss: 0.1785Epoch 7/15: [====                          ] 10/63 batches, loss: 0.1720Epoch 7/15: [=====                         ] 11/63 batches, loss: 0.1686Epoch 7/15: [=====                         ] 12/63 batches, loss: 0.1662Epoch 7/15: [======                        ] 13/63 batches, loss: 0.1642Epoch 7/15: [======                        ] 14/63 batches, loss: 0.1683Epoch 7/15: [=======                       ] 15/63 batches, loss: 0.1645Epoch 7/15: [=======                       ] 16/63 batches, loss: 0.1623Epoch 7/15: [========                      ] 17/63 batches, loss: 0.1594Epoch 7/15: [========                      ] 18/63 batches, loss: 0.1617Epoch 7/15: [=========                     ] 19/63 batches, loss: 0.1652Epoch 7/15: [=========                     ] 20/63 batches, loss: 0.1726Epoch 7/15: [==========                    ] 21/63 batches, loss: 0.1701Epoch 7/15: [==========                    ] 22/63 batches, loss: 0.1657Epoch 7/15: [==========                    ] 23/63 batches, loss: 0.1608Epoch 7/15: [===========                   ] 24/63 batches, loss: 0.1627Epoch 7/15: [===========                   ] 25/63 batches, loss: 0.1605Epoch 7/15: [============                  ] 26/63 batches, loss: 0.1574Epoch 7/15: [============                  ] 27/63 batches, loss: 0.1559Epoch 7/15: [=============                 ] 28/63 batches, loss: 0.1571Epoch 7/15: [=============                 ] 29/63 batches, loss: 0.1555Epoch 7/15: [==============                ] 30/63 batches, loss: 0.1548Epoch 7/15: [==============                ] 31/63 batches, loss: 0.1554Epoch 7/15: [===============               ] 32/63 batches, loss: 0.1578Epoch 7/15: [===============               ] 33/63 batches, loss: 0.1595Epoch 7/15: [================              ] 34/63 batches, loss: 0.1580Epoch 7/15: [================              ] 35/63 batches, loss: 0.1602Epoch 7/15: [=================             ] 36/63 batches, loss: 0.1606Epoch 7/15: [=================             ] 37/63 batches, loss: 0.1584Epoch 7/15: [==================            ] 38/63 batches, loss: 0.1584Epoch 7/15: [==================            ] 39/63 batches, loss: 0.1604Epoch 7/15: [===================           ] 40/63 batches, loss: 0.1595Epoch 7/15: [===================           ] 41/63 batches, loss: 0.1617Epoch 7/15: [====================          ] 42/63 batches, loss: 0.1647Epoch 7/15: [====================          ] 43/63 batches, loss: 0.1658Epoch 7/15: [====================          ] 44/63 batches, loss: 0.1672Epoch 7/15: [=====================         ] 45/63 batches, loss: 0.1697Epoch 7/15: [=====================         ] 46/63 batches, loss: 0.1681Epoch 7/15: [======================        ] 47/63 batches, loss: 0.1690Epoch 7/15: [======================        ] 48/63 batches, loss: 0.1679Epoch 7/15: [=======================       ] 49/63 batches, loss: 0.1686Epoch 7/15: [=======================       ] 50/63 batches, loss: 0.1687Epoch 7/15: [========================      ] 51/63 batches, loss: 0.1689Epoch 7/15: [========================      ] 52/63 batches, loss: 0.1685Epoch 7/15: [=========================     ] 53/63 batches, loss: 0.1687Epoch 7/15: [=========================     ] 54/63 batches, loss: 0.1673Epoch 7/15: [==========================    ] 55/63 batches, loss: 0.1729Epoch 7/15: [==========================    ] 56/63 batches, loss: 0.1743Epoch 7/15: [===========================   ] 57/63 batches, loss: 0.1762Epoch 7/15: [===========================   ] 58/63 batches, loss: 0.1770Epoch 7/15: [============================  ] 59/63 batches, loss: 0.1783Epoch 7/15: [============================  ] 60/63 batches, loss: 0.1787Epoch 7/15: [============================= ] 61/63 batches, loss: 0.1790Epoch 7/15: [============================= ] 62/63 batches, loss: 0.1776Epoch 7/15: [==============================] 63/63 batches, loss: 0.1756
[2025-05-02 12:59:42,871][src.training.lm_trainer][INFO] - Epoch 7/15, Train Loss: 0.1756
[2025-05-02 12:59:43,068][src.training.lm_trainer][INFO] - Epoch 7/15, Val Loss: 0.1020, Metrics: {'mse': 0.10160086303949356, 'rmse': 0.31874890280516033, 'r2': -1.0698976516723633}
Epoch 8/15: [Epoch 8/15: [                              ] 1/63 batches, loss: 0.0740Epoch 8/15: [                              ] 2/63 batches, loss: 0.1678Epoch 8/15: [=                             ] 3/63 batches, loss: 0.1600Epoch 8/15: [=                             ] 4/63 batches, loss: 0.1408Epoch 8/15: [==                            ] 5/63 batches, loss: 0.1296Epoch 8/15: [==                            ] 6/63 batches, loss: 0.1380Epoch 8/15: [===                           ] 7/63 batches, loss: 0.1505Epoch 8/15: [===                           ] 8/63 batches, loss: 0.1551Epoch 8/15: [====                          ] 9/63 batches, loss: 0.1655Epoch 8/15: [====                          ] 10/63 batches, loss: 0.1770Epoch 8/15: [=====                         ] 11/63 batches, loss: 0.1717Epoch 8/15: [=====                         ] 12/63 batches, loss: 0.1735Epoch 8/15: [======                        ] 13/63 batches, loss: 0.1762Epoch 8/15: [======                        ] 14/63 batches, loss: 0.1684Epoch 8/15: [=======                       ] 15/63 batches, loss: 0.1714Epoch 8/15: [=======                       ] 16/63 batches, loss: 0.1713Epoch 8/15: [========                      ] 17/63 batches, loss: 0.1724Epoch 8/15: [========                      ] 18/63 batches, loss: 0.1722Epoch 8/15: [=========                     ] 19/63 batches, loss: 0.1710Epoch 8/15: [=========                     ] 20/63 batches, loss: 0.1768Epoch 8/15: [==========                    ] 21/63 batches, loss: 0.1799Epoch 8/15: [==========                    ] 22/63 batches, loss: 0.1781Epoch 8/15: [==========                    ] 23/63 batches, loss: 0.1776Epoch 8/15: [===========                   ] 24/63 batches, loss: 0.1774Epoch 8/15: [===========                   ] 25/63 batches, loss: 0.1741Epoch 8/15: [============                  ] 26/63 batches, loss: 0.1754Epoch 8/15: [============                  ] 27/63 batches, loss: 0.1772Epoch 8/15: [=============                 ] 28/63 batches, loss: 0.1783Epoch 8/15: [=============                 ] 29/63 batches, loss: 0.1755Epoch 8/15: [==============                ] 30/63 batches, loss: 0.1774Epoch 8/15: [==============                ] 31/63 batches, loss: 0.1808Epoch 8/15: [===============               ] 32/63 batches, loss: 0.1791Epoch 8/15: [===============               ] 33/63 batches, loss: 0.1774Epoch 8/15: [================              ] 34/63 batches, loss: 0.1802Epoch 8/15: [================              ] 35/63 batches, loss: 0.1778Epoch 8/15: [=================             ] 36/63 batches, loss: 0.1774Epoch 8/15: [=================             ] 37/63 batches, loss: 0.1795Epoch 8/15: [==================            ] 38/63 batches, loss: 0.1766Epoch 8/15: [==================            ] 39/63 batches, loss: 0.1759Epoch 8/15: [===================           ] 40/63 batches, loss: 0.1754Epoch 8/15: [===================           ] 41/63 batches, loss: 0.1765Epoch 8/15: [====================          ] 42/63 batches, loss: 0.1752Epoch 8/15: [====================          ] 43/63 batches, loss: 0.1753Epoch 8/15: [====================          ] 44/63 batches, loss: 0.1724Epoch 8/15: [=====================         ] 45/63 batches, loss: 0.1706Epoch 8/15: [=====================         ] 46/63 batches, loss: 0.1699Epoch 8/15: [======================        ] 47/63 batches, loss: 0.1676Epoch 8/15: [======================        ] 48/63 batches, loss: 0.1670Epoch 8/15: [=======================       ] 49/63 batches, loss: 0.1669Epoch 8/15: [=======================       ] 50/63 batches, loss: 0.1679Epoch 8/15: [========================      ] 51/63 batches, loss: 0.1680Epoch 8/15: [========================      ] 52/63 batches, loss: 0.1682Epoch 8/15: [=========================     ] 53/63 batches, loss: 0.1666Epoch 8/15: [=========================     ] 54/63 batches, loss: 0.1666Epoch 8/15: [==========================    ] 55/63 batches, loss: 0.1658Epoch 8/15: [==========================    ] 56/63 batches, loss: 0.1651Epoch 8/15: [===========================   ] 57/63 batches, loss: 0.1656Epoch 8/15: [===========================   ] 58/63 batches, loss: 0.1658Epoch 8/15: [============================  ] 59/63 batches, loss: 0.1652Epoch 8/15: [============================  ] 60/63 batches, loss: 0.1656Epoch 8/15: [============================= ] 61/63 batches, loss: 0.1647Epoch 8/15: [============================= ] 62/63 batches, loss: 0.1647Epoch 8/15: [==============================] 63/63 batches, loss: 0.1645
[2025-05-02 12:59:45,457][src.training.lm_trainer][INFO] - Epoch 8/15, Train Loss: 0.1645
[2025-05-02 12:59:45,667][src.training.lm_trainer][INFO] - Epoch 8/15, Val Loss: 0.0979, Metrics: {'mse': 0.09724626690149307, 'rmse': 0.3118433371125525, 'r2': -0.9811820983886719}
Epoch 9/15: [Epoch 9/15: [                              ] 1/63 batches, loss: 0.1009Epoch 9/15: [                              ] 2/63 batches, loss: 0.1646Epoch 9/15: [=                             ] 3/63 batches, loss: 0.1288Epoch 9/15: [=                             ] 4/63 batches, loss: 0.1329Epoch 9/15: [==                            ] 5/63 batches, loss: 0.1452Epoch 9/15: [==                            ] 6/63 batches, loss: 0.1340Epoch 9/15: [===                           ] 7/63 batches, loss: 0.1575Epoch 9/15: [===                           ] 8/63 batches, loss: 0.1521Epoch 9/15: [====                          ] 9/63 batches, loss: 0.1549Epoch 9/15: [====                          ] 10/63 batches, loss: 0.1631Epoch 9/15: [=====                         ] 11/63 batches, loss: 0.1550Epoch 9/15: [=====                         ] 12/63 batches, loss: 0.1596Epoch 9/15: [======                        ] 13/63 batches, loss: 0.1577Epoch 9/15: [======                        ] 14/63 batches, loss: 0.1560Epoch 9/15: [=======                       ] 15/63 batches, loss: 0.1546Epoch 9/15: [=======                       ] 16/63 batches, loss: 0.1569Epoch 9/15: [========                      ] 17/63 batches, loss: 0.1559Epoch 9/15: [========                      ] 18/63 batches, loss: 0.1564Epoch 9/15: [=========                     ] 19/63 batches, loss: 0.1568Epoch 9/15: [=========                     ] 20/63 batches, loss: 0.1583Epoch 9/15: [==========                    ] 21/63 batches, loss: 0.1559Epoch 9/15: [==========                    ] 22/63 batches, loss: 0.1553Epoch 9/15: [==========                    ] 23/63 batches, loss: 0.1586Epoch 9/15: [===========                   ] 24/63 batches, loss: 0.1583Epoch 9/15: [===========                   ] 25/63 batches, loss: 0.1557Epoch 9/15: [============                  ] 26/63 batches, loss: 0.1555Epoch 9/15: [============                  ] 27/63 batches, loss: 0.1518Epoch 9/15: [=============                 ] 28/63 batches, loss: 0.1539Epoch 9/15: [=============                 ] 29/63 batches, loss: 0.1537Epoch 9/15: [==============                ] 30/63 batches, loss: 0.1525Epoch 9/15: [==============                ] 31/63 batches, loss: 0.1614Epoch 9/15: [===============               ] 32/63 batches, loss: 0.1602Epoch 9/15: [===============               ] 33/63 batches, loss: 0.1580Epoch 9/15: [================              ] 34/63 batches, loss: 0.1578Epoch 9/15: [================              ] 35/63 batches, loss: 0.1558Epoch 9/15: [=================             ] 36/63 batches, loss: 0.1574Epoch 9/15: [=================             ] 37/63 batches, loss: 0.1552Epoch 9/15: [==================            ] 38/63 batches, loss: 0.1557Epoch 9/15: [==================            ] 39/63 batches, loss: 0.1547Epoch 9/15: [===================           ] 40/63 batches, loss: 0.1532Epoch 9/15: [===================           ] 41/63 batches, loss: 0.1525Epoch 9/15: [====================          ] 42/63 batches, loss: 0.1552Epoch 9/15: [====================          ] 43/63 batches, loss: 0.1552Epoch 9/15: [====================          ] 44/63 batches, loss: 0.1542Epoch 9/15: [=====================         ] 45/63 batches, loss: 0.1547Epoch 9/15: [=====================         ] 46/63 batches, loss: 0.1539Epoch 9/15: [======================        ] 47/63 batches, loss: 0.1539Epoch 9/15: [======================        ] 48/63 batches, loss: 0.1540Epoch 9/15: [=======================       ] 49/63 batches, loss: 0.1530Epoch 9/15: [=======================       ] 50/63 batches, loss: 0.1547Epoch 9/15: [========================      ] 51/63 batches, loss: 0.1535Epoch 9/15: [========================      ] 52/63 batches, loss: 0.1531Epoch 9/15: [=========================     ] 53/63 batches, loss: 0.1511Epoch 9/15: [=========================     ] 54/63 batches, loss: 0.1514Epoch 9/15: [==========================    ] 55/63 batches, loss: 0.1511Epoch 9/15: [==========================    ] 56/63 batches, loss: 0.1503Epoch 9/15: [===========================   ] 57/63 batches, loss: 0.1515Epoch 9/15: [===========================   ] 58/63 batches, loss: 0.1523Epoch 9/15: [============================  ] 59/63 batches, loss: 0.1539Epoch 9/15: [============================  ] 60/63 batches, loss: 0.1557Epoch 9/15: [============================= ] 61/63 batches, loss: 0.1557Epoch 9/15: [============================= ] 62/63 batches, loss: 0.1557Epoch 9/15: [==============================] 63/63 batches, loss: 0.1560
[2025-05-02 12:59:47,987][src.training.lm_trainer][INFO] - Epoch 9/15, Train Loss: 0.1560
[2025-05-02 12:59:48,208][src.training.lm_trainer][INFO] - Epoch 9/15, Val Loss: 0.0924, Metrics: {'mse': 0.09156078845262527, 'rmse': 0.3025901327747243, 'r2': -0.8653527498245239}
Epoch 10/15: [Epoch 10/15: [                              ] 1/63 batches, loss: 0.1635Epoch 10/15: [                              ] 2/63 batches, loss: 0.1493Epoch 10/15: [=                             ] 3/63 batches, loss: 0.1265Epoch 10/15: [=                             ] 4/63 batches, loss: 0.1208Epoch 10/15: [==                            ] 5/63 batches, loss: 0.1148Epoch 10/15: [==                            ] 6/63 batches, loss: 0.1167Epoch 10/15: [===                           ] 7/63 batches, loss: 0.1266Epoch 10/15: [===                           ] 8/63 batches, loss: 0.1324Epoch 10/15: [====                          ] 9/63 batches, loss: 0.1303Epoch 10/15: [====                          ] 10/63 batches, loss: 0.1334Epoch 10/15: [=====                         ] 11/63 batches, loss: 0.1329Epoch 10/15: [=====                         ] 12/63 batches, loss: 0.1330Epoch 10/15: [======                        ] 13/63 batches, loss: 0.1329Epoch 10/15: [======                        ] 14/63 batches, loss: 0.1357Epoch 10/15: [=======                       ] 15/63 batches, loss: 0.1348Epoch 10/15: [=======                       ] 16/63 batches, loss: 0.1345Epoch 10/15: [========                      ] 17/63 batches, loss: 0.1302Epoch 10/15: [========                      ] 18/63 batches, loss: 0.1296Epoch 10/15: [=========                     ] 19/63 batches, loss: 0.1285Epoch 10/15: [=========                     ] 20/63 batches, loss: 0.1288Epoch 10/15: [==========                    ] 21/63 batches, loss: 0.1343Epoch 10/15: [==========                    ] 22/63 batches, loss: 0.1340Epoch 10/15: [==========                    ] 23/63 batches, loss: 0.1346Epoch 10/15: [===========                   ] 24/63 batches, loss: 0.1404Epoch 10/15: [===========                   ] 25/63 batches, loss: 0.1387Epoch 10/15: [============                  ] 26/63 batches, loss: 0.1383Epoch 10/15: [============                  ] 27/63 batches, loss: 0.1379Epoch 10/15: [=============                 ] 28/63 batches, loss: 0.1374Epoch 10/15: [=============                 ] 29/63 batches, loss: 0.1401Epoch 10/15: [==============                ] 30/63 batches, loss: 0.1420Epoch 10/15: [==============                ] 31/63 batches, loss: 0.1426Epoch 10/15: [===============               ] 32/63 batches, loss: 0.1459Epoch 10/15: [===============               ] 33/63 batches, loss: 0.1475Epoch 10/15: [================              ] 34/63 batches, loss: 0.1480Epoch 10/15: [================              ] 35/63 batches, loss: 0.1479Epoch 10/15: [=================             ] 36/63 batches, loss: 0.1521Epoch 10/15: [=================             ] 37/63 batches, loss: 0.1535Epoch 10/15: [==================            ] 38/63 batches, loss: 0.1511Epoch 10/15: [==================            ] 39/63 batches, loss: 0.1509Epoch 10/15: [===================           ] 40/63 batches, loss: 0.1497Epoch 10/15: [===================           ] 41/63 batches, loss: 0.1484Epoch 10/15: [====================          ] 42/63 batches, loss: 0.1488Epoch 10/15: [====================          ] 43/63 batches, loss: 0.1485Epoch 10/15: [====================          ] 44/63 batches, loss: 0.1487Epoch 10/15: [=====================         ] 45/63 batches, loss: 0.1502Epoch 10/15: [=====================         ] 46/63 batches, loss: 0.1494Epoch 10/15: [======================        ] 47/63 batches, loss: 0.1500Epoch 10/15: [======================        ] 48/63 batches, loss: 0.1502Epoch 10/15: [=======================       ] 49/63 batches, loss: 0.1512Epoch 10/15: [=======================       ] 50/63 batches, loss: 0.1498Epoch 10/15: [========================      ] 51/63 batches, loss: 0.1506Epoch 10/15: [========================      ] 52/63 batches, loss: 0.1513Epoch 10/15: [=========================     ] 53/63 batches, loss: 0.1514Epoch 10/15: [=========================     ] 54/63 batches, loss: 0.1527Epoch 10/15: [==========================    ] 55/63 batches, loss: 0.1534Epoch 10/15: [==========================    ] 56/63 batches, loss: 0.1529Epoch 10/15: [===========================   ] 57/63 batches, loss: 0.1521Epoch 10/15: [===========================   ] 58/63 batches, loss: 0.1520Epoch 10/15: [============================  ] 59/63 batches, loss: 0.1521Epoch 10/15: [============================  ] 60/63 batches, loss: 0.1528Epoch 10/15: [============================= ] 61/63 batches, loss: 0.1531Epoch 10/15: [============================= ] 62/63 batches, loss: 0.1522Epoch 10/15: [==============================] 63/63 batches, loss: 0.1517
[2025-05-02 12:59:50,587][src.training.lm_trainer][INFO] - Epoch 10/15, Train Loss: 0.1517
[2025-05-02 12:59:50,794][src.training.lm_trainer][INFO] - Epoch 10/15, Val Loss: 0.0871, Metrics: {'mse': 0.08602878451347351, 'rmse': 0.2933066390545456, 'r2': -0.7526501417160034}
Epoch 11/15: [Epoch 11/15: [                              ] 1/63 batches, loss: 0.2085Epoch 11/15: [                              ] 2/63 batches, loss: 0.2111Epoch 11/15: [=                             ] 3/63 batches, loss: 0.1804Epoch 11/15: [=                             ] 4/63 batches, loss: 0.1769Epoch 11/15: [==                            ] 5/63 batches, loss: 0.1504Epoch 11/15: [==                            ] 6/63 batches, loss: 0.1314Epoch 11/15: [===                           ] 7/63 batches, loss: 0.1314Epoch 11/15: [===                           ] 8/63 batches, loss: 0.1292Epoch 11/15: [====                          ] 9/63 batches, loss: 0.1339Epoch 11/15: [====                          ] 10/63 batches, loss: 0.1348Epoch 11/15: [=====                         ] 11/63 batches, loss: 0.1319Epoch 11/15: [=====                         ] 12/63 batches, loss: 0.1372Epoch 11/15: [======                        ] 13/63 batches, loss: 0.1454Epoch 11/15: [======                        ] 14/63 batches, loss: 0.1486Epoch 11/15: [=======                       ] 15/63 batches, loss: 0.1435Epoch 11/15: [=======                       ] 16/63 batches, loss: 0.1407Epoch 11/15: [========                      ] 17/63 batches, loss: 0.1411Epoch 11/15: [========                      ] 18/63 batches, loss: 0.1408Epoch 11/15: [=========                     ] 19/63 batches, loss: 0.1389Epoch 11/15: [=========                     ] 20/63 batches, loss: 0.1378Epoch 11/15: [==========                    ] 21/63 batches, loss: 0.1378Epoch 11/15: [==========                    ] 22/63 batches, loss: 0.1402Epoch 11/15: [==========                    ] 23/63 batches, loss: 0.1408Epoch 11/15: [===========                   ] 24/63 batches, loss: 0.1396Epoch 11/15: [===========                   ] 25/63 batches, loss: 0.1385Epoch 11/15: [============                  ] 26/63 batches, loss: 0.1393Epoch 11/15: [============                  ] 27/63 batches, loss: 0.1435Epoch 11/15: [=============                 ] 28/63 batches, loss: 0.1438Epoch 11/15: [=============                 ] 29/63 batches, loss: 0.1416Epoch 11/15: [==============                ] 30/63 batches, loss: 0.1399Epoch 11/15: [==============                ] 31/63 batches, loss: 0.1417Epoch 11/15: [===============               ] 32/63 batches, loss: 0.1456Epoch 11/15: [===============               ] 33/63 batches, loss: 0.1452Epoch 11/15: [================              ] 34/63 batches, loss: 0.1443Epoch 11/15: [================              ] 35/63 batches, loss: 0.1446Epoch 11/15: [=================             ] 36/63 batches, loss: 0.1455Epoch 11/15: [=================             ] 37/63 batches, loss: 0.1481Epoch 11/15: [==================            ] 38/63 batches, loss: 0.1485Epoch 11/15: [==================            ] 39/63 batches, loss: 0.1489Epoch 11/15: [===================           ] 40/63 batches, loss: 0.1515Epoch 11/15: [===================           ] 41/63 batches, loss: 0.1509Epoch 11/15: [====================          ] 42/63 batches, loss: 0.1517Epoch 11/15: [====================          ] 43/63 batches, loss: 0.1523Epoch 11/15: [====================          ] 44/63 batches, loss: 0.1509Epoch 11/15: [=====================         ] 45/63 batches, loss: 0.1515Epoch 11/15: [=====================         ] 46/63 batches, loss: 0.1531Epoch 11/15: [======================        ] 47/63 batches, loss: 0.1533Epoch 11/15: [======================        ] 48/63 batches, loss: 0.1524Epoch 11/15: [=======================       ] 49/63 batches, loss: 0.1509Epoch 11/15: [=======================       ] 50/63 batches, loss: 0.1515Epoch 11/15: [========================      ] 51/63 batches, loss: 0.1501Epoch 11/15: [========================      ] 52/63 batches, loss: 0.1484Epoch 11/15: [=========================     ] 53/63 batches, loss: 0.1492Epoch 11/15: [=========================     ] 54/63 batches, loss: 0.1480Epoch 11/15: [==========================    ] 55/63 batches, loss: 0.1469Epoch 11/15: [==========================    ] 56/63 batches, loss: 0.1461Epoch 11/15: [===========================   ] 57/63 batches, loss: 0.1453Epoch 11/15: [===========================   ] 58/63 batches, loss: 0.1469Epoch 11/15: [============================  ] 59/63 batches, loss: 0.1464Epoch 11/15: [============================  ] 60/63 batches, loss: 0.1454Epoch 11/15: [============================= ] 61/63 batches, loss: 0.1445Epoch 11/15: [============================= ] 62/63 batches, loss: 0.1435Epoch 11/15: [==============================] 63/63 batches, loss: 0.1435
[2025-05-02 12:59:53,121][src.training.lm_trainer][INFO] - Epoch 11/15, Train Loss: 0.1435
[2025-05-02 12:59:53,340][src.training.lm_trainer][INFO] - Epoch 11/15, Val Loss: 0.0856, Metrics: {'mse': 0.0843113362789154, 'rmse': 0.2903641442721801, 'r2': -0.7176607847213745}
Epoch 12/15: [Epoch 12/15: [                              ] 1/63 batches, loss: 0.1062Epoch 12/15: [                              ] 2/63 batches, loss: 0.1150Epoch 12/15: [=                             ] 3/63 batches, loss: 0.1177Epoch 12/15: [=                             ] 4/63 batches, loss: 0.1289Epoch 12/15: [==                            ] 5/63 batches, loss: 0.1494Epoch 12/15: [==                            ] 6/63 batches, loss: 0.1602Epoch 12/15: [===                           ] 7/63 batches, loss: 0.1541Epoch 12/15: [===                           ] 8/63 batches, loss: 0.1549Epoch 12/15: [====                          ] 9/63 batches, loss: 0.1517Epoch 12/15: [====                          ] 10/63 batches, loss: 0.1496Epoch 12/15: [=====                         ] 11/63 batches, loss: 0.1426Epoch 12/15: [=====                         ] 12/63 batches, loss: 0.1393Epoch 12/15: [======                        ] 13/63 batches, loss: 0.1389Epoch 12/15: [======                        ] 14/63 batches, loss: 0.1407Epoch 12/15: [=======                       ] 15/63 batches, loss: 0.1437Epoch 12/15: [=======                       ] 16/63 batches, loss: 0.1481Epoch 12/15: [========                      ] 17/63 batches, loss: 0.1433Epoch 12/15: [========                      ] 18/63 batches, loss: 0.1434Epoch 12/15: [=========                     ] 19/63 batches, loss: 0.1435Epoch 12/15: [=========                     ] 20/63 batches, loss: 0.1432Epoch 12/15: [==========                    ] 21/63 batches, loss: 0.1462Epoch 12/15: [==========                    ] 22/63 batches, loss: 0.1452Epoch 12/15: [==========                    ] 23/63 batches, loss: 0.1484Epoch 12/15: [===========                   ] 24/63 batches, loss: 0.1471Epoch 12/15: [===========                   ] 25/63 batches, loss: 0.1464Epoch 12/15: [============                  ] 26/63 batches, loss: 0.1436Epoch 12/15: [============                  ] 27/63 batches, loss: 0.1425Epoch 12/15: [=============                 ] 28/63 batches, loss: 0.1396Epoch 12/15: [=============                 ] 29/63 batches, loss: 0.1381Epoch 12/15: [==============                ] 30/63 batches, loss: 0.1412Epoch 12/15: [==============                ] 31/63 batches, loss: 0.1382Epoch 12/15: [===============               ] 32/63 batches, loss: 0.1378Epoch 12/15: [===============               ] 33/63 batches, loss: 0.1426Epoch 12/15: [================              ] 34/63 batches, loss: 0.1416Epoch 12/15: [================              ] 35/63 batches, loss: 0.1406Epoch 12/15: [=================             ] 36/63 batches, loss: 0.1390Epoch 12/15: [=================             ] 37/63 batches, loss: 0.1391Epoch 12/15: [==================            ] 38/63 batches, loss: 0.1373Epoch 12/15: [==================            ] 39/63 batches, loss: 0.1378Epoch 12/15: [===================           ] 40/63 batches, loss: 0.1355Epoch 12/15: [===================           ] 41/63 batches, loss: 0.1359Epoch 12/15: [====================          ] 42/63 batches, loss: 0.1361Epoch 12/15: [====================          ] 43/63 batches, loss: 0.1362Epoch 12/15: [====================          ] 44/63 batches, loss: 0.1370Epoch 12/15: [=====================         ] 45/63 batches, loss: 0.1380Epoch 12/15: [=====================         ] 46/63 batches, loss: 0.1370Epoch 12/15: [======================        ] 47/63 batches, loss: 0.1369Epoch 12/15: [======================        ] 48/63 batches, loss: 0.1360Epoch 12/15: [=======================       ] 49/63 batches, loss: 0.1347Epoch 12/15: [=======================       ] 50/63 batches, loss: 0.1352Epoch 12/15: [========================      ] 51/63 batches, loss: 0.1354Epoch 12/15: [========================      ] 52/63 batches, loss: 0.1351Epoch 12/15: [=========================     ] 53/63 batches, loss: 0.1351Epoch 12/15: [=========================     ] 54/63 batches, loss: 0.1373Epoch 12/15: [==========================    ] 55/63 batches, loss: 0.1376Epoch 12/15: [==========================    ] 56/63 batches, loss: 0.1374Epoch 12/15: [===========================   ] 57/63 batches, loss: 0.1373Epoch 12/15: [===========================   ] 58/63 batches, loss: 0.1370Epoch 12/15: [============================  ] 59/63 batches, loss: 0.1391Epoch 12/15: [============================  ] 60/63 batches, loss: 0.1378Epoch 12/15: [============================= ] 61/63 batches, loss: 0.1365Epoch 12/15: [============================= ] 62/63 batches, loss: 0.1361Epoch 12/15: [==============================] 63/63 batches, loss: 0.1366
[2025-05-02 12:59:55,662][src.training.lm_trainer][INFO] - Epoch 12/15, Train Loss: 0.1366
[2025-05-02 12:59:55,882][src.training.lm_trainer][INFO] - Epoch 12/15, Val Loss: 0.0815, Metrics: {'mse': 0.08017953485250473, 'rmse': 0.28315991039076266, 'r2': -0.6334843635559082}
Epoch 13/15: [Epoch 13/15: [                              ] 1/63 batches, loss: 0.1034Epoch 13/15: [                              ] 2/63 batches, loss: 0.1089Epoch 13/15: [=                             ] 3/63 batches, loss: 0.0926Epoch 13/15: [=                             ] 4/63 batches, loss: 0.0967Epoch 13/15: [==                            ] 5/63 batches, loss: 0.0959Epoch 13/15: [==                            ] 6/63 batches, loss: 0.0967Epoch 13/15: [===                           ] 7/63 batches, loss: 0.1038Epoch 13/15: [===                           ] 8/63 batches, loss: 0.1118Epoch 13/15: [====                          ] 9/63 batches, loss: 0.1084Epoch 13/15: [====                          ] 10/63 batches, loss: 0.1063Epoch 13/15: [=====                         ] 11/63 batches, loss: 0.1029Epoch 13/15: [=====                         ] 12/63 batches, loss: 0.1087Epoch 13/15: [======                        ] 13/63 batches, loss: 0.1097Epoch 13/15: [======                        ] 14/63 batches, loss: 0.1170Epoch 13/15: [=======                       ] 15/63 batches, loss: 0.1155Epoch 13/15: [=======                       ] 16/63 batches, loss: 0.1281Epoch 13/15: [========                      ] 17/63 batches, loss: 0.1281Epoch 13/15: [========                      ] 18/63 batches, loss: 0.1278Epoch 13/15: [=========                     ] 19/63 batches, loss: 0.1253Epoch 13/15: [=========                     ] 20/63 batches, loss: 0.1253Epoch 13/15: [==========                    ] 21/63 batches, loss: 0.1233Epoch 13/15: [==========                    ] 22/63 batches, loss: 0.1234Epoch 13/15: [==========                    ] 23/63 batches, loss: 0.1248Epoch 13/15: [===========                   ] 24/63 batches, loss: 0.1225Epoch 13/15: [===========                   ] 25/63 batches, loss: 0.1235Epoch 13/15: [============                  ] 26/63 batches, loss: 0.1250Epoch 13/15: [============                  ] 27/63 batches, loss: 0.1290Epoch 13/15: [=============                 ] 28/63 batches, loss: 0.1288Epoch 13/15: [=============                 ] 29/63 batches, loss: 0.1316Epoch 13/15: [==============                ] 30/63 batches, loss: 0.1307Epoch 13/15: [==============                ] 31/63 batches, loss: 0.1295Epoch 13/15: [===============               ] 32/63 batches, loss: 0.1319Epoch 13/15: [===============               ] 33/63 batches, loss: 0.1308Epoch 13/15: [================              ] 34/63 batches, loss: 0.1288Epoch 13/15: [================              ] 35/63 batches, loss: 0.1298Epoch 13/15: [=================             ] 36/63 batches, loss: 0.1279Epoch 13/15: [=================             ] 37/63 batches, loss: 0.1283Epoch 13/15: [==================            ] 38/63 batches, loss: 0.1269Epoch 13/15: [==================            ] 39/63 batches, loss: 0.1271Epoch 13/15: [===================           ] 40/63 batches, loss: 0.1303Epoch 13/15: [===================           ] 41/63 batches, loss: 0.1291Epoch 13/15: [====================          ] 42/63 batches, loss: 0.1299Epoch 13/15: [====================          ] 43/63 batches, loss: 0.1287Epoch 13/15: [====================          ] 44/63 batches, loss: 0.1280Epoch 13/15: [=====================         ] 45/63 batches, loss: 0.1306Epoch 13/15: [=====================         ] 46/63 batches, loss: 0.1299Epoch 13/15: [======================        ] 47/63 batches, loss: 0.1296Epoch 13/15: [======================        ] 48/63 batches, loss: 0.1297Epoch 13/15: [=======================       ] 49/63 batches, loss: 0.1282Epoch 13/15: [=======================       ] 50/63 batches, loss: 0.1278Epoch 13/15: [========================      ] 51/63 batches, loss: 0.1275Epoch 13/15: [========================      ] 52/63 batches, loss: 0.1281Epoch 13/15: [=========================     ] 53/63 batches, loss: 0.1293Epoch 13/15: [=========================     ] 54/63 batches, loss: 0.1288Epoch 13/15: [==========================    ] 55/63 batches, loss: 0.1290Epoch 13/15: [==========================    ] 56/63 batches, loss: 0.1288Epoch 13/15: [===========================   ] 57/63 batches, loss: 0.1286Epoch 13/15: [===========================   ] 58/63 batches, loss: 0.1279Epoch 13/15: [============================  ] 59/63 batches, loss: 0.1279Epoch 13/15: [============================  ] 60/63 batches, loss: 0.1278Epoch 13/15: [============================= ] 61/63 batches, loss: 0.1273Epoch 13/15: [============================= ] 62/63 batches, loss: 0.1274Epoch 13/15: [==============================] 63/63 batches, loss: 0.1254
[2025-05-02 12:59:58,245][src.training.lm_trainer][INFO] - Epoch 13/15, Train Loss: 0.1254
[2025-05-02 12:59:58,462][src.training.lm_trainer][INFO] - Epoch 13/15, Val Loss: 0.0766, Metrics: {'mse': 0.07506866008043289, 'rmse': 0.27398660565880384, 'r2': -0.5293614864349365}
Epoch 14/15: [Epoch 14/15: [                              ] 1/63 batches, loss: 0.0569Epoch 14/15: [                              ] 2/63 batches, loss: 0.1124Epoch 14/15: [=                             ] 3/63 batches, loss: 0.0987Epoch 14/15: [=                             ] 4/63 batches, loss: 0.1098Epoch 14/15: [==                            ] 5/63 batches, loss: 0.1324Epoch 14/15: [==                            ] 6/63 batches, loss: 0.1403Epoch 14/15: [===                           ] 7/63 batches, loss: 0.1257Epoch 14/15: [===                           ] 8/63 batches, loss: 0.1193Epoch 14/15: [====                          ] 9/63 batches, loss: 0.1264Epoch 14/15: [====                          ] 10/63 batches, loss: 0.1259Epoch 14/15: [=====                         ] 11/63 batches, loss: 0.1231Epoch 14/15: [=====                         ] 12/63 batches, loss: 0.1251Epoch 14/15: [======                        ] 13/63 batches, loss: 0.1227Epoch 14/15: [======                        ] 14/63 batches, loss: 0.1214Epoch 14/15: [=======                       ] 15/63 batches, loss: 0.1220Epoch 14/15: [=======                       ] 16/63 batches, loss: 0.1253Epoch 14/15: [========                      ] 17/63 batches, loss: 0.1252Epoch 14/15: [========                      ] 18/63 batches, loss: 0.1267Epoch 14/15: [=========                     ] 19/63 batches, loss: 0.1223Epoch 14/15: [=========                     ] 20/63 batches, loss: 0.1255Epoch 14/15: [==========                    ] 21/63 batches, loss: 0.1333Epoch 14/15: [==========                    ] 22/63 batches, loss: 0.1301Epoch 14/15: [==========                    ] 23/63 batches, loss: 0.1306Epoch 14/15: [===========                   ] 24/63 batches, loss: 0.1280Epoch 14/15: [===========                   ] 25/63 batches, loss: 0.1337Epoch 14/15: [============                  ] 26/63 batches, loss: 0.1318Epoch 14/15: [============                  ] 27/63 batches, loss: 0.1314Epoch 14/15: [=============                 ] 28/63 batches, loss: 0.1308Epoch 14/15: [=============                 ] 29/63 batches, loss: 0.1306Epoch 14/15: [==============                ] 30/63 batches, loss: 0.1312Epoch 14/15: [==============                ] 31/63 batches, loss: 0.1293Epoch 14/15: [===============               ] 32/63 batches, loss: 0.1289Epoch 14/15: [===============               ] 33/63 batches, loss: 0.1308Epoch 14/15: [================              ] 34/63 batches, loss: 0.1297Epoch 14/15: [================              ] 35/63 batches, loss: 0.1278Epoch 14/15: [=================             ] 36/63 batches, loss: 0.1263Epoch 14/15: [=================             ] 37/63 batches, loss: 0.1255Epoch 14/15: [==================            ] 38/63 batches, loss: 0.1253Epoch 14/15: [==================            ] 39/63 batches, loss: 0.1267Epoch 14/15: [===================           ] 40/63 batches, loss: 0.1266Epoch 14/15: [===================           ] 41/63 batches, loss: 0.1257Epoch 14/15: [====================          ] 42/63 batches, loss: 0.1253Epoch 14/15: [====================          ] 43/63 batches, loss: 0.1247Epoch 14/15: [====================          ] 44/63 batches, loss: 0.1243Epoch 14/15: [=====================         ] 45/63 batches, loss: 0.1233Epoch 14/15: [=====================         ] 46/63 batches, loss: 0.1235Epoch 14/15: [======================        ] 47/63 batches, loss: 0.1234Epoch 14/15: [======================        ] 48/63 batches, loss: 0.1233Epoch 14/15: [=======================       ] 49/63 batches, loss: 0.1224Epoch 14/15: [=======================       ] 50/63 batches, loss: 0.1228Epoch 14/15: [========================      ] 51/63 batches, loss: 0.1226Epoch 14/15: [========================      ] 52/63 batches, loss: 0.1221Epoch 14/15: [=========================     ] 53/63 batches, loss: 0.1215Epoch 14/15: [=========================     ] 54/63 batches, loss: 0.1222Epoch 14/15: [==========================    ] 55/63 batches, loss: 0.1225Epoch 14/15: [==========================    ] 56/63 batches, loss: 0.1220Epoch 14/15: [===========================   ] 57/63 batches, loss: 0.1225Epoch 14/15: [===========================   ] 58/63 batches, loss: 0.1225Epoch 14/15: [============================  ] 59/63 batches, loss: 0.1222Epoch 14/15: [============================  ] 60/63 batches, loss: 0.1209Epoch 14/15: [============================= ] 61/63 batches, loss: 0.1212Epoch 14/15: [============================= ] 62/63 batches, loss: 0.1207Epoch 14/15: [==============================] 63/63 batches, loss: 0.1207
[2025-05-02 13:00:00,803][src.training.lm_trainer][INFO] - Epoch 14/15, Train Loss: 0.1207
[2025-05-02 13:00:01,008][src.training.lm_trainer][INFO] - Epoch 14/15, Val Loss: 0.0727, Metrics: {'mse': 0.07116376608610153, 'rmse': 0.26676537647547427, 'r2': -0.44980764389038086}
Epoch 15/15: [Epoch 15/15: [                              ] 1/63 batches, loss: 0.1179Epoch 15/15: [                              ] 2/63 batches, loss: 0.0849Epoch 15/15: [=                             ] 3/63 batches, loss: 0.0770Epoch 15/15: [=                             ] 4/63 batches, loss: 0.0834Epoch 15/15: [==                            ] 5/63 batches, loss: 0.0946Epoch 15/15: [==                            ] 6/63 batches, loss: 0.1019Epoch 15/15: [===                           ] 7/63 batches, loss: 0.1042Epoch 15/15: [===                           ] 8/63 batches, loss: 0.1010Epoch 15/15: [====                          ] 9/63 batches, loss: 0.1101Epoch 15/15: [====                          ] 10/63 batches, loss: 0.1049Epoch 15/15: [=====                         ] 11/63 batches, loss: 0.1066Epoch 15/15: [=====                         ] 12/63 batches, loss: 0.1042Epoch 15/15: [======                        ] 13/63 batches, loss: 0.1068Epoch 15/15: [======                        ] 14/63 batches, loss: 0.1091Epoch 15/15: [=======                       ] 15/63 batches, loss: 0.1042Epoch 15/15: [=======                       ] 16/63 batches, loss: 0.1025Epoch 15/15: [========                      ] 17/63 batches, loss: 0.1045Epoch 15/15: [========                      ] 18/63 batches, loss: 0.1055Epoch 15/15: [=========                     ] 19/63 batches, loss: 0.1016Epoch 15/15: [=========                     ] 20/63 batches, loss: 0.1011Epoch 15/15: [==========                    ] 21/63 batches, loss: 0.1003Epoch 15/15: [==========                    ] 22/63 batches, loss: 0.1007Epoch 15/15: [==========                    ] 23/63 batches, loss: 0.1000Epoch 15/15: [===========                   ] 24/63 batches, loss: 0.0990Epoch 15/15: [===========                   ] 25/63 batches, loss: 0.0990Epoch 15/15: [============                  ] 26/63 batches, loss: 0.1000Epoch 15/15: [============                  ] 27/63 batches, loss: 0.0996Epoch 15/15: [=============                 ] 28/63 batches, loss: 0.0985Epoch 15/15: [=============                 ] 29/63 batches, loss: 0.1003Epoch 15/15: [==============                ] 30/63 batches, loss: 0.0995Epoch 15/15: [==============                ] 31/63 batches, loss: 0.0996Epoch 15/15: [===============               ] 32/63 batches, loss: 0.1002Epoch 15/15: [===============               ] 33/63 batches, loss: 0.0990Epoch 15/15: [================              ] 34/63 batches, loss: 0.1013Epoch 15/15: [================              ] 35/63 batches, loss: 0.1009Epoch 15/15: [=================             ] 36/63 batches, loss: 0.1005Epoch 15/15: [=================             ] 37/63 batches, loss: 0.1004Epoch 15/15: [==================            ] 38/63 batches, loss: 0.0998Epoch 15/15: [==================            ] 39/63 batches, loss: 0.0998Epoch 15/15: [===================           ] 40/63 batches, loss: 0.1003Epoch 15/15: [===================           ] 41/63 batches, loss: 0.0995Epoch 15/15: [====================          ] 42/63 batches, loss: 0.1018Epoch 15/15: [====================          ] 43/63 batches, loss: 0.1016Epoch 15/15: [====================          ] 44/63 batches, loss: 0.1029Epoch 15/15: [=====================         ] 45/63 batches, loss: 0.1031Epoch 15/15: [=====================         ] 46/63 batches, loss: 0.1033Epoch 15/15: [======================        ] 47/63 batches, loss: 0.1038Epoch 15/15: [======================        ] 48/63 batches, loss: 0.1031Epoch 15/15: [=======================       ] 49/63 batches, loss: 0.1036Epoch 15/15: [=======================       ] 50/63 batches, loss: 0.1048Epoch 15/15: [========================      ] 51/63 batches, loss: 0.1060Epoch 15/15: [========================      ] 52/63 batches, loss: 0.1062Epoch 15/15: [=========================     ] 53/63 batches, loss: 0.1058Epoch 15/15: [=========================     ] 54/63 batches, loss: 0.1045Epoch 15/15: [==========================    ] 55/63 batches, loss: 0.1063Epoch 15/15: [==========================    ] 56/63 batches, loss: 0.1068Epoch 15/15: [===========================   ] 57/63 batches, loss: 0.1064Epoch 15/15: [===========================   ] 58/63 batches, loss: 0.1101Epoch 15/15: [============================  ] 59/63 batches, loss: 0.1090Epoch 15/15: [============================  ] 60/63 batches, loss: 0.1095Epoch 15/15: [============================= ] 61/63 batches, loss: 0.1095Epoch 15/15: [============================= ] 62/63 batches, loss: 0.1088Epoch 15/15: [==============================] 63/63 batches, loss: 0.1095
[2025-05-02 13:00:03,416][src.training.lm_trainer][INFO] - Epoch 15/15, Train Loss: 0.1095
[2025-05-02 13:00:03,641][src.training.lm_trainer][INFO] - Epoch 15/15, Val Loss: 0.0734, Metrics: {'mse': 0.07184221595525742, 'rmse': 0.2680339828366124, 'r2': -0.46362948417663574}
[2025-05-02 13:00:03,642][src.training.lm_trainer][INFO] - Validation did not improve. Patience: 1/4
[2025-05-02 13:00:03,642][src.training.lm_trainer][INFO] - Training completed in 38.67 seconds
[2025-05-02 13:00:03,642][src.training.lm_trainer][INFO] - Loading best model for final evaluation
[2025-05-02 13:00:06,165][src.training.lm_trainer][INFO] - Final evaluation - Train metrics: {'mse': 0.04034104570746422, 'rmse': 0.2008508045975027, 'r2': -0.913312554359436}
[2025-05-02 13:00:06,165][src.training.lm_trainer][INFO] - Final evaluation - Validation metrics: {'mse': 0.07116376608610153, 'rmse': 0.26676537647547427, 'r2': -0.44980764389038086}
[2025-05-02 13:00:06,165][src.training.lm_trainer][INFO] - Final evaluation - Test metrics: {'mse': 0.09721885621547699, 'rmse': 0.3117993845655841, 'r2': -1.0990772247314453}
[2025-05-02 13:00:07,898][src.training.lm_trainer][INFO] - Model saved to /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer11/ar/ar/model.pt
[2025-05-02 13:00:07,899][src.training.lm_trainer][INFO] - GPU memory cleared
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:    best_val_loss █▄▄▄▃▃▃▂▂▂▂▁▁▁
wandb:     best_val_mse █▄▄▄▃▃▃▂▂▂▂▁▁▁
wandb:      best_val_r2 ▁▅▅▅▆▆▆▇▇▇▇███
wandb:    best_val_rmse █▅▄▄▄▃▃▃▂▂▂▂▁▁
wandb:            epoch ▁▁▁▁▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██
wandb:   final_test_mse ▁
wandb:    final_test_r2 ▁
wandb:  final_test_rmse ▁
wandb:  final_train_mse ▁
wandb:   final_train_r2 ▁
wandb: final_train_rmse ▁
wandb:    final_val_mse ▁
wandb:     final_val_r2 ▁
wandb:   final_val_rmse ▁
wandb:    learning_rate █▁▅▅▅▆▆▆▆▆▇▇▇▇▇
wandb:       train_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       train_time ▁
wandb:         val_loss █▄▄▄▃▃▃▂▂▂▂▁▁▁▁
wandb:          val_mse █▄▄▄▃▃▃▂▂▂▂▁▁▁▁
wandb:           val_r2 ▁▅▅▅▆▆▆▇▇▇▇████
wandb:         val_rmse █▅▄▄▄▃▃▃▂▂▂▂▁▁▁
wandb: 
wandb: Run summary:
wandb:    best_val_loss 0.07274
wandb:     best_val_mse 0.07116
wandb:      best_val_r2 -0.44981
wandb:    best_val_rmse 0.26677
wandb:            epoch 15
wandb:   final_test_mse 0.09722
wandb:    final_test_r2 -1.09908
wandb:  final_test_rmse 0.3118
wandb:  final_train_mse 0.04034
wandb:   final_train_r2 -0.91331
wandb: final_train_rmse 0.20085
wandb:    final_val_mse 0.07116
wandb:     final_val_r2 -0.44981
wandb:   final_val_rmse 0.26677
wandb:    learning_rate 2e-05
wandb:       train_loss 0.10951
wandb:       train_time 38.67128
wandb:         val_loss 0.0734
wandb:          val_mse 0.07184
wandb:           val_r2 -0.46363
wandb:         val_rmse 0.26803
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125915-k2mbv6gf
wandb: Find logs at: /scratch/leuven/371/vsc37132/wandb/wandb/offline-run-20250502_125915-k2mbv6gf/logs
Experiment probe_layer11_avg_links_len_ar completed successfully
Successfully extracted metrics from /scratch/leuven/371/vsc37132/probe_output/submetrics/avg_links_len/layer11/ar/ar/results.json for layer 11
Running control submetric probing experiments...
=======================
PROBING LAYER 1 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 4 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 6 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 9 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
=======================
PROBING LAYER 11 (SUBMETRIC CONTROL EXPERIMENTS)
=======================
All experiments completed successfully!
==============================================
Layer-wise probing experiments completed!
==============================================
Total planned experiments: 15
Successfully completed: 65
Failed experiments: 0
Success rate: 433%
Results available in: /scratch/leuven/371/vsc37132/probe_output
Layer summary: /scratch/leuven/371/vsc37132/probe_output/layer_performance_summary.json
==============================================
