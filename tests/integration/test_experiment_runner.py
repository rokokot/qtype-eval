# tests/integration/test_experiment_runner.py
"""
Comprehensive integration tests for experiment runners.
Tests the complete experiment workflow from configuration to results.
"""

import pytest
import tempfile
import json
import numpy as np
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from omegaconf import OmegaConf, DictConfig
from typing import Dict, Any, List
import logging

from scripts.run_tfidf_experiments import TfidfExperimentRunner
from src.data.tfidf_features import create_test_features
from tests.fixtures.mock_datasets import MockExperimentEnvironment, MockDatasetLoader
from utils.test_helpers import create_test_config, validate_test_environment
from utils.assertions import (
    assert_valid_metrics, assert_experiment_result_structure,
    assert_config_validity, assert_file_exists
)

logger = logging.getLogger(__name__)


@pytest.mark.integration
class TestTfidfExperimentRunner:
    """Integration tests for TF-IDF experiment runner."""
    
    @pytest.fixture(autouse=True)
    def setup_experiment_environment(self, tmp_path):
        """Set up complete experiment test environment."""
        self.test_dir = tmp_path
        self.features_dir = self.test_dir / "tfidf_features"
        self.output_dir = self.test_dir / "outputs"
        self.cache_dir = self.test_dir / "cache"
        
        # Create directories
        for dir_path in [self.features_dir, self.output_dir, self.cache_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # Create test TF-IDF features
        create_test_features(str(self.features_dir), n_samples=100, vocab_size=200)
        
        # Create basic test configuration
        self.base_config = OmegaConf.create({
            'experiment_type': 'tfidf_baselines',
            'tfidf': {'features_dir': str(self.features_dir)},
            'models': ['dummy'],
            'tasks': ['question_type'],
            'languages': [['en']],
            'controls': {'enabled': False},
            'model_params': {
                'dummy': {'classification': {'strategy': 'most_frequent'}}
            },
            'output_dir': str(self.output_dir),
            'data': {'cache_dir': str(self.cache_dir)}
        })
        
        logger.info(f"Experiment test environment created in {self.test_dir}")
    
    def test_experiment_runner_initialization(self):
        """Test TF-IDF experiment runner initialization."""
        runner = TfidfExperimentRunner(self.base_config)
        
        assert runner.config == self.base_config
        assert runner.features_dir == Path(str(self.features_dir))
        assert isinstance(runner.results, dict)
        assert len(runner.results) == 0
    
    def test_experiment_runner_initialization_missing_features(self):
        """Test runner initialization with missing features directory."""
        bad_config = self.base_config.copy()
        bad_config.tfidf.features_dir = "/nonexistent/directory"
        
        with pytest.raises(FileNotFoundError):
            TfidfExperimentRunner(bad_config)
    
    @patch('src.data.datasets.load_sklearn_data')
    def test_single_experiment_execution(self, mock_load_sklearn_data):
        """Test execution of a single TF-IDF experiment."""
        # Mock data loading
        from scipy.sparse import csr_matrix
        mock_features = csr_matrix((50, 200))  # 50 samples, 200 features
        mock_labels = np.random.randint(0, 2, 50)
        
        mock_load_sklearn_data.return_value = (
            (mock_features, mock_labels),  # train
            (mock_features[:20], mock_labels[:20]),  # val  
            (mock_features[:10], mock_labels[:10])   # test
        )
        
        runner = TfidfExperimentRunner(self.base_config)
        
        # Run single experiment
        result = runner.run_single_experiment(
            model_type='dummy',
            task='question_type',
            languages=['en']
        )
        
        # Validate result structure
        assert isinstance(result, dict)
        assert 'experiment_name' in result
        assert 'model_type' in result
        assert 'task' in result
        assert 'test_metrics' in result
        
        # Validate metrics
        assert_valid_metrics(result['test_metrics'], 'classification')
        
        # Check experiment name format
        expected_name = 'tfidf_dummy_question_type_en'
        assert result['experiment_name'] == expected_name
    
    @patch('src.data.datasets.load_sklearn_data')
    def test_single_experiment_with_controls(self, mock_load_sklearn_data):
        """Test single experiment with control conditions."""
        # Mock data loading
        from scipy.sparse import csr_matrix
        mock_features = csr_matrix((50, 200))
        mock_labels = np.random.randint(0, 2, 50)
        
        mock_load_sklearn_data.return_value = (
            (mock_features, mock_labels),
            (mock_features[:20], mock_labels[:20]),
            (mock_features[:10], mock_labels[:10])
        )
        
        runner = TfidfExperimentRunner(self.base_config)
        
        # Run experiment with control
        result = runner.run_single_experiment(
            model_type='dummy',
            task='question_type',
            languages=['en'],
            control_index=1
        )
        
        # Should include control information
        assert 'control_index' in result
        assert result['control_index'] == 1
        assert 'control1' in result['experiment_name']
    
    @patch('src.data.datasets.load_sklearn_data')
    def test_single_experiment_with_submetrics(self, mock_load_sklearn_data):
        """Test single experiment with submetrics."""
        # Mock data loading for regression task
        from scipy.sparse import csr_matrix
        mock_features = csr_matrix((50, 200))
        mock_labels = np.random.random(50)  # Regression labels
        
        mock_load_sklearn_data.return_value = (
            (mock_features, mock_labels),
            (mock_features[:20], mock_labels[:20]),
            (mock_features[:10], mock_labels[:10])
        )
        
        runner = TfidfExperimentRunner(self.base_config)
        
        # Run experiment with submetric
        result = runner.run_single_experiment(
            model_type='dummy',
            task='single_submetric',
            languages=['en'],
            submetric='avg_links_len'
        )
        
        # Should be regression task
        assert result['task_type'] == 'regression'
        assert 'submetric' in result
        assert result['submetric'] == 'avg_links_len'
        assert 'avg_links_len' in result['experiment_name']
    
    @patch('src.data.datasets.load_sklearn_data')
    def test_multiple_model_types(self, mock_load_sklearn_data):
        """Test experiment runner with multiple model types."""
        # Mock data loading
        from scipy.sparse import csr_matrix
        mock_features = csr_matrix((50, 200))
        mock_labels = np.random.randint(0, 2, 50)
        
        mock_load_sklearn_data.return_value = (
            (mock_features, mock_labels),
            (mock_features[:20], mock_labels[:20]),
            (mock_features[:10], mock_labels[:10])
        )
        
        # Update config for multiple models
        config = self.base_config.copy()
        config.models = ['dummy', 'logistic']
        config.model_params.logistic = {'C': 1.0, 'solver': 'liblinear'}
        
        runner = TfidfExperimentRunner(config)
        
        results = {}
        for model_type in ['dummy', 'logistic']:
            result = runner.run_single_experiment(
                model_type=model_type,
                task='question_type', 
                languages=['en']
            )
            results[model_type] = result
        
        # Both should succeed
        assert len(results) == 2
        for model_type, result in results.items():
            assert result['model_type'] == model_type
            assert 'test_metrics' in result
            assert_valid_metrics(result['test_metrics'], 'classification')
    
    @patch('src.data.datasets.load_sklearn_data')
    def test_multiple_languages(self, mock_load_sklearn_data):
        """Test experiment runner with multiple languages."""
        # Mock data loading
        from scipy.sparse import csr_matrix
        mock_features = csr_matrix((50, 200))
        mock_labels = np.random.randint(0, 2, 50)
        
        mock_load_sklearn_data.return_value = (
            (mock_features, mock_labels),
            (mock_features[:20], mock_labels[:20]),
            (mock_features[:10], mock_labels[:10])
        )
        
        runner = TfidfExperimentRunner(self.base_config)
        
        # Test different language configurations
        language_configs = [['en'], ['ru'], ['en', 'ru']]
        
        results = {}
        for languages in language_configs:
            result = runner.run_single_experiment(
                model_type='dummy',
                task='question_type',
                languages=languages
            )
            results[str(languages)] = result
        
        # All should succeed
        assert len(results) == 3
        for lang_config, result in results.items():
            assert 'languages' in result
            assert result['languages'] is not None
    
    def test_experiment_runner_error_handling(self):
        """Test experiment runner error handling."""
        runner = TfidfExperimentRunner(self.base_config)
        
        # Test with invalid model type
        result = runner.run_single_experiment(
            model_type='invalid_model',
            task='question_type',
            languages=['en']
        )
        
        # Should return error information
        assert 'error' in result
        assert result['experiment_name'] == 'tfidf_invalid_model_question_type_en'
    
    @patch('src.data.datasets.load_sklearn_data')
    def test_full_experiment_suite(self, mock_load_sklearn_data):
        """Test running complete experiment suite."""
        # Mock data loading
        from scipy.sparse import csr_matrix
        mock_features_cls = csr_matrix((50, 200))
        mock_labels_cls = np.random.randint(0, 2, 50)
        mock_features_reg = csr_matrix((50, 200))
        mock_labels_reg = np.random.random(50)
        
        def mock_load_side_effect(*args, **kwargs):
            task = kwargs.get('task', 'question_type')
            if task == 'question_type':
                return (
                    (mock_features_cls, mock_labels_cls),
                    (mock_features_cls[:20], mock_labels_cls[:20]),
                    (mock_features_cls[:10], mock_labels_cls[:10])
                )
            else:  # regression task
                return (
                    (mock_features_reg, mock_labels_reg),
                    (mock_features_reg[:20], mock_labels_reg[:20]),
                    (mock_features_reg[:10], mock_labels_reg[:10])
                )
        
        mock_load_sklearn_data.side_effect = mock_load_side_effect
        
        # Configure for multiple experiments
        config = self.base_config.copy()
        config.models = ['dummy']
        config.tasks = ['question_type', 'complexity']  
        config.languages = [['en'], ['ru']]
        
        runner = TfidfExperimentRunner(config)
        
        # Run all experiments
        all_results = runner.run_all_experiments()
        
        # Should have results for all combinations
        expected_experiments = 2 * 2  # 2 tasks Ã— 2 language configs
        assert len(all_results) >= expected_experiments
        
        # Validate each result
        for exp_name, result in all_results.items():
            if 'error' not in result:
                assert_experiment_result_structure(result, 'tfidf')
    
    def test_results_saving(self):
        """Test experiment results saving."""
        runner = TfidfExperimentRunner(self.base_config)
        
        # Add mock results
        runner.results = {
            'test_experiment': {
                'experiment_name': 'test_experiment',
                'model_type': 'dummy',
                'task': 'question_type',
                'test_metrics': {'accuracy': 0.8, 'f1': 0.75}
            }
        }
        
        # Save results
        runner.save_results()
        
        # Check files were created
        summary_file = self.output_dir / "tfidf_experiments_summary.json"
        table_file = self.output_dir / "results_table.json"
        
        assert_file_exists(summary_file, "summary file")
        assert_file_exists(table_file, "results table")
        
        # Validate summary content
        with open(summary_file) as f:
            summary = json.load(f)
        
        assert 'test_experiment' in summary
        assert summary['test_experiment']['model_type'] == 'dummy'
        
        # Validate table content
        with open(table_file) as f:
            table = json.load(f)
        
        assert len(table) == 1
        assert table[0]['experiment'] == 'test_experiment'
        assert table[0]['test_accuracy'] == 0.8


@pytest.mark.integration
class TestExperimentConfiguration:
    """Test experiment configuration and parameter handling."""
    
    def test_config_validation(self):
        """Test experiment configuration validation."""
        # Valid config
        valid_config = create_test_config()
        assert_config_validity(valid_config, 'experiment')
        
        # Invalid configs
        invalid_configs = [
            {},  # Empty config
            {'models': []},  # Empty models list
            {'models': ['dummy'], 'tasks': []},  # Empty tasks list
            {'models': ['dummy'], 'tasks': ['question_type'], 'languages': []},  # Empty languages
        ]
        
        for invalid_config in invalid_configs:
            with pytest.raises(AssertionError):
                assert_config_validity(invalid_config, 'experiment')
    
    def test_model_parameter_inheritance(self, tmp_path):
        """Test model parameter inheritance and override."""
        features_dir = tmp_path / "features"
        create_test_features(str(features_dir), n_samples=50)
        
        config = OmegaConf.create({
            'experiment_type': 'tfidf_baselines',
            'tfidf': {'features_dir': str(features_dir)},
            'models': ['logistic'],
            'tasks': ['question_type'],
            'languages': [['en']],
            'model_params': {
                'logistic': {
                    'C': 2.0,  # Override default
                    'solver': 'saga',  # Override default
                    'max_iter': 500
                }
            },
            'output_dir': str(tmp_path / "outputs"),
            'data': {'cache_dir': str(tmp_path / "cache")}
        })
        
        runner = TfidfExperimentRunner(config)
        
        # Check that parameters are correctly inherited
        model_params = config.model_params.logistic
        assert model_params.C == 2.0
        assert model_params.solver == 'saga'
        assert model_params.max_iter == 500
    
    def test_language_configuration_variants(self, tmp_path):
        """Test different language configuration formats."""
        features_dir = tmp_path / "features"
        create_test_features(str(features_dir), n_samples=50)
        
        # Test different language config formats
        language_configs = [
            [['en']],  # Single language in list
            [['en', 'ru']],  # Multiple languages in single config
            [['en'], ['ru']],  # Multiple single-language configs
            [['all']],  # All languages
        ]
        
        for lang_config in language_configs:
            config = OmegaConf.create({
                'experiment_type': 'tfidf_baselines',
                'tfidf': {'features_dir': str(features_dir)},
                'models': ['dummy'],
                'tasks': ['question_type'],
                'languages': lang_config,
                'controls': {'enabled': False},
                'model_params': {'dummy': {'classification': {'strategy': 'most_frequent'}}},
                'output_dir': str(tmp_path / "outputs"),
                'data': {'cache_dir': str(tmp_path / "cache")}
            })
            
            # Should initialize without error
            runner = TfidfExperimentRunner(config)
            assert runner.config.languages == lang_config


@pytest.mark.integration  
class TestExperimentWorkflows:
    """Test complete experiment workflows."""
    
    @pytest.fixture(autouse=True)
    def setup_workflow_environment(self, tmp_path):
        """Set up workflow test environment."""
        self.test_dir = tmp_path
        self.mock_env = MockExperimentEnvironment(str(self.test_dir))
        
    def test_minimal_tfidf_workflow(self):
        """Test minimal TF-IDF workflow end-to-end."""
        with self.mock_env as env:
            # Create minimal config
            config = OmegaConf.create({
                'experiment_type': 'tfidf_baselines',
                'tfidf': {'features_dir': env.get_features_dir()},
                'models': ['dummy'],
                'tasks': ['question_type'],
                'languages': [['en']],
                'controls': {'enabled': False},
                'model_params': {
                    'dummy': {'classification': {'strategy': 'most_frequent'}}
                },
                'output_dir': str(self.test_dir / "outputs"),
                'data': {'cache_dir': str(self.test_dir / "cache")}
            })
            
            # Add custom dataset to mock environment
            mock_data = {
                'text': ['Question?', 'Statement.'] * 25,
                'language': ['en'] * 50,
                'question_type': [1, 0] * 25,
                'lang_norm_complexity_score': [0.6, 0.4] * 25
            }
            env.add_custom_dataset("rokokot/question-type-and-complexity", "base", mock_data)
            
            # Run experiment
            runner = TfidfExperimentRunner(config)
            
            # This should complete without errors in the mock environment
            try:
                result = runner.run_single_experiment(
                    model_type='dummy',
                    task='question_type',
                    languages=['en']
                )
                
                # Validate result
                assert 'experiment_name' in result
                assert result['model_type'] == 'dummy'
                assert result['task'] == 'question_type'
                
            except Exception as e:
                # Expected in mock environment due to shape mismatches
                logger.info(f"Minimal workflow test completed with expected mock exception: {e}")
    
    def test_comprehensive_tfidf_workflow(self):
        """Test comprehensive TF-IDF workflow with multiple configurations."""
        with self.mock_env as env:
            # Create comprehensive config
            config = OmegaConf.create({
                'experiment_type': 'tfidf_baselines',
                'tfidf': {'features_dir': env.get_features_dir()},
                'models': ['dummy', 'logistic'],
                'tasks': ['question_type', 'complexity'],
                'languages': [['en'], ['ru']],
                'controls': {'enabled': True, 'indices': [1, 2]},
                'model_params': {
                    'dummy': {
                        'classification': {'strategy': 'most_frequent'},
                        'regression': {'strategy': 'mean'}
                    },
                    'logistic': {'C': 1.0, 'solver': 'liblinear'}
                },
                'output_dir': str(self.test_dir / "outputs"),
                'data': {'cache_dir': str(self.test_dir / "cache")}
            })
            
            # Add comprehensive mock dataset
            mock_data = {
                'text': [f'Sample text {i}' for i in range(100)],
                'language': ['en'] * 50 + ['ru'] * 50,
                'question_type': [i % 2 for i in range(100)],
                'lang_norm_complexity_score': [(i % 10) * 0.1 for i in range(100)]
            }
            env.add_custom_dataset("rokokot/question-type-and-complexity", "base", mock_data)
            
            # Test runner initialization
            runner = TfidfExperimentRunner(config)
            
            # Should handle complex configuration without errors
            assert runner.config.models == ['dummy', 'logistic']
            assert runner.config.tasks == ['question_type', 'complexity']
            assert runner.config.controls.enabled == True
    
    def test_experiment_error_recovery(self):
        """Test experiment error recovery and partial results."""
        with self.mock_env as env:
            config = OmegaConf.create({
                'experiment_type': 'tfidf_baselines',
                'tfidf': {'features_dir': env.get_features_dir()},
                'models': ['dummy', 'invalid_model', 'logistic'],  # Include invalid model
                'tasks': ['question_type'],
                'languages': [['en']],
                'controls': {'enabled': False},
                'model_params': {
                    'dummy': {'classification': {'strategy': 'most_frequent'}},
                    'logistic': {'C': 1.0}
                },
                'output_dir': str(self.test_dir / "outputs"),
                'data': {'cache_dir': str(self.test_dir / "cache")}
            })
            
            runner = TfidfExperimentRunner(config)
            
            # Test that runner handles mixed success/failure
            results = {}
            for model_type in config.models:
                try:
                    result = runner.run_single_experiment(
                        model_type=model_type,
                        task='question_type',
                        languages=['en']
                    )
                    results[model_type] = result
                except Exception as e:
                    results[model_type] = {'error': str(e)}
            
            # Should have attempted all models
            assert len(results) == 3
            
            # Valid models should succeed, invalid should fail
            assert 'error' not in results.get('dummy', {})
            assert 'error' in results.get('invalid_model', {})


@pytest.mark.integration
class TestExperimentIntegrationWithDataPipeline:
    """Test experiment runner integration with data pipeline."""
    
    def test_experiment_with_real_tfidf_features(self, tmp_path):
        """Test experiment using actual TF-IDF feature files."""
        # Create realistic TF-IDF features
        features_dir = tmp_path / "real_features"
        create_test_features(str(features_dir), n_samples=200, vocab_size=500)
        
        config = OmegaConf.create({
            'experiment_type': 'tfidf_baselines',
            'tfidf': {'features_dir': str(features_dir)},
            'models': ['dummy'],
            'tasks': ['question_type'],
            'languages': [['en']],
            'controls': {'enabled': False},
            'model_params': {
                'dummy': {'classification': {'strategy': 'most_frequent'}}
            },
            'output_dir': str(tmp_path / "outputs"),
            'data': {'cache_dir': str(tmp_path / "cache")}
        })
        
        # Test that runner can work with real feature files
        runner = TfidfExperimentRunner(config)
        assert runner.features_dir.exists()
        
        # Verify features are loadable
        from src.data.tfidf_features import TfidfFeatureLoader
        loader = TfidfFeatureLoader(str(features_dir))
        features = loader.load_all_features()
        
        assert len(features) == 3  # train, val, test
        assert all(matrix.shape[1] == 500 for matrix in features.values())
    
    def test_experiment_output_structure(self, tmp_path):
        """Test that experiments produce correct output structure."""
        features_dir = tmp_path / "features"
        output_dir = tmp_path / "outputs"
        create_test_features(str(features_dir), n_samples=50)
        
        config = OmegaConf.create({
            'experiment_type': 'tfidf_baselines',
            'tfidf': {'features_dir': str(features_dir)},
            'models': ['dummy'],
            'tasks': ['question_type'],
            'languages': [['en']],
            'controls': {'enabled': False},
            'model_params': {
                'dummy': {'classification': {'strategy': 'most_frequent'}}
            },
            'output_dir': str(output_dir),
            'data': {'cache_dir': str(tmp_path / "cache")}
        })
        
        runner = TfidfExperimentRunner(config)
        
        # Mock successful experiment
        runner.results = {
            'tfidf_dummy_question_type_en': {
                'experiment_name': 'tfidf_dummy_question_type_en',
                'model_type': 'dummy',
                'task': 'question_type',
                'task_type': 'classification',
                'languages': ['en'],
                'test_metrics': {'accuracy': 0.8, 'f1': 0.75}
            }
        }
        
        # Save results
        runner.save_results()
        
        # Check output structure
        expected_files = [
            "tfidf_experiments_summary.json",
            "results_table.json"
        ]
        
        for filename in expected_files:
            file_path = output_dir / filename
            assert_file_exists(file_path, filename)
    
    def test_experiment_reproducibility(self, tmp_path):
        """Test experiment reproducibility with fixed random seeds."""
        features_dir = tmp_path / "features"
        create_test_features(str(features_dir), n_samples=100)
        
        config = OmegaConf.create({
            'experiment_type': 'tfidf_baselines',
            'tfidf': {'features_dir': str(features_dir)},
            'models': ['dummy'],
            'tasks': ['question_type'],
            'languages': [['en']],
            'controls': {'enabled': False},
            'model_params': {
                'dummy': {'classification': {'strategy': 'most_frequent'}}
            },
            'output_dir': str(tmp_path / "outputs"),
            'data': {'cache_dir': str(tmp_path / "cache")},
            'seed': 42  # Fixed seed for reproducibility
        })
        
        # Run same experiment twice
        runner1 = TfidfExperimentRunner(config)
        runner2 = TfidfExperimentRunner(config)
        
        # Both should be initialized identically
        assert runner1.config == runner2.config
        assert runner1.features_dir == runner2.features_dir


@pytest.mark.slow
class TestExperimentPerformance:
    """Performance tests for experiment runners."""
    
    def test_experiment_runner_performance(self, tmp_path):
        """Test experiment runner performance with larger datasets."""
        # Create larger test features
        features_dir = tmp_path / "large_features"
        create_test_features(str(features_dir), n_samples=1000, vocab_size=2000)
        
        config = OmegaConf.create({
            'experiment_type': 'tfidf_baselines',
            'tfidf': {'features_dir': str(features_dir)},
            'models': ['dummy'],
            'tasks': ['question_type'],
            'languages': [['en']],
            'controls': {'enabled': False},
            'model_params': {
                'dummy': {'classification': {'strategy': 'most_frequent'}}
            },
            'output_dir': str(tmp_path / "outputs"),
            'data': {'cache_dir': str(tmp_path / "cache")}
        })
        
        # Measure performance
        import time
        start_time = time.time()
        
        runner = TfidfExperimentRunner(config)
        
        # Initialization should be fast
        init_time = time.time() - start_time
        assert init_time < 5.0, f"Initialization too slow: {init_time:.2f}s"
        
        logger.info(f"Performance test - Initialization: {init_time:.2f}s")
    
    def test_memory_usage_monitoring(self, tmp_path):
        """Test memory usage during experiments."""
        try:
            import psutil
            import os
            
            process = psutil.Process(os.getpid())
            start_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # Create and run experiment
            features_dir = tmp_path / "features"
            create_test_features(str(features_dir), n_samples=500, vocab_size=1000)
            
            config = OmegaConf.create({
                'experiment_type': 'tfidf_baselines',
                'tfidf': {'features_dir': str(features_dir)},
                'models': ['dummy'],
                'tasks': ['question_type'],
                'languages': [['en']],
                'controls': {'enabled': False},
                'model_params': {
                    'dummy': {'classification': {'strategy': 'most_frequent'}}
                },
                'output_dir': str(tmp_path / "outputs"),
                'data': {'cache_dir': str(tmp_path / "cache")}
            })
            
            runner = TfidfExperimentRunner(config)
            
            end_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = end_memory - start_memory
            
            # Memory increase should be reasonable
            assert memory_increase < 500, f"Memory usage too high: {memory_increase:.1f}MB"
            
            logger.info(f"Memory usage test - Increase: {memory_increase:.1f}MB")
            
        except ImportError:
            pytest.skip("psutil not available for memory monitoring")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])